,repo,language,readme_contents
0,nasa/cumulus,JavaScript,"# Cumulus Framework

[![npm version](https://badge.fury.io/js/%40cumulus%2Fapi.svg)](https://badge.fury.io/js/%40cumulus%2Fapi)
[![Coverage Status](https://coveralls.io/repos/github/nasa/cumulus/badge.svg?branch=master)](https://coveralls.io/github/nasa/cumulus?branch=master)

## üìñ Documentation

- Documentation for the latest [released version](https://nasa.github.io/cumulus).
- Documentation for the [unreleased work](https://nasa.github.io/cumulus/docs/next/cumulus-docs-readme).

## More Information

For more information about this project of more about NASA's Earth Observing System Data and Information System (EOSDIS) and its cloud work, please contact [Katie Baynes](mailto:katie.baynes@nasa.gov) or visit us at https://earthdata.nasa.gov.

# üî® Development

The Cumulus core repo is a [monorepo](https://en.wikipedia.org/wiki/Monorepo)
managed by [Lerna](https://lerna.js.org/). Lerna is responsible for installing
the dependencies of the packages and tasks that belong in this repo. In general,
Cumulus's npm packages can be found in the [packages](./packages) directory, and
workflow tasks can be found in the [tasks](./tasks) directory.

To help cut down on the time and disk space required to install the dependencies
of the packages in this monorepo, all `devDependencies` are defined in the
top-level [package.json](./package.json). The
[Node module resolution algorithm](https://nodejs.org/api/modules.html#modules_loading_from_node_modules_folders)
allows all of the packages and tasks to find their dev dependencies in that
top-level `node_modules` directory.

TL;DR - If you need to add a `devDependency` to a package, add it to the
top-level [package.json](./package.json) file, not the `package.json` associated
with an individual package.

## Installation

This is for installation for Cumulus development.  See the [Cumulus deployment instructions](https://nasa.github.io/cumulus/docs/deployment/deployment-readme) for instructions on deploying the released Cumulus packages.

### Prerequisites

- [NVM](https://github.com/creationix/nvm) and node version 12.18
- [AWS CLI](http://docs.aws.amazon.com/cli/latest/userguide/installing.html)
- BASH
- Docker (only required for testing)
- docker-compose (only required for testing `pip install docker-compose`)
- Python 3.6+
- [pipenv](https://pypi.org/project/pipenv/)

Install the correct node version:

```bash
nvm install
nvm use
```

### Install Lerna

We use Lerna to manage multiple Cumulus packages in the same repo. You need to install lerna as a global module first:

    $ npm install -g lerna

### Install Local Dependencies

We use npm for local package management

    $ npm install
    $ npm run bootstrap

Building All packages:

    $ npm run build

Build and watch packages:

    $ npm run watch

## Running the Cumulus APIs locally

Start the API:

    $ npm run serve

Or start the distribution API:

    $ npm run serve-dist

See the [API package documentation](packages/api/README.md#running-the-api-locally) for more options.

## üìù Tests

### Unit Tests

#### LocalStack

[LocalStack](https://github.com/localstack/localstack) provides local versions of most AWS services for testing.

The LocalStack repository has [installation instructions](https://github.com/localstack/localstack#installing).

Localstack is included in the docker-compose file. You only need to run the docker-compose command in the next section in order to use it with your tests.

#### Docker containers

Turn on the docker containers first:

    $ npm run start-unit-test-stack

Stop localstack/unit test services:

    $ npm run stop-unit-test-stack

#### Run database migrations

```
$ npm run db:local:migrate
```

#### Run tests

Run the test commands next
```
    $ export LOCAL_ES_HOST=127.0.0.1
    $ export LOCALSTACK_HOST=127.0.0.1
    $ npm test
```

### Integration Tests

For more information please [read this](docs/development/integration-tests.md).

### Running tests via VS Code debugger

Copy the `.vscode.example` directory to `.vscode` to create your debugger launch configuration. Refer to the [VS Code documentation on how to use the debugger](https://code.visualstudio.com/docs/editor/debugging).

## üî¶ Code Coverage and Quality

For more information please [read this](docs/development/quality-and-coverage.md).

## üì¶ Adding New Packages

Create a new folder under `packages` if it is a common library or create folder under `cumulus/tasks` if it is a lambda task. `cd` to the folder and run `npm init`.

Make sure to name the package as `@cumulus/package-name`.

## Running command in all package folders

    $ lerna exec -- rm -rf ./package-lock.json

## Cleaning Up all the repos

    $ npm run clean

## Contribution

Please refer to: https://github.com/nasa/cumulus/blob/master/CONTRIBUTING.md for more information.

## üõí Release

To release a new version of cumulus [read this](docs/development/release.md).
"
1,nasa/openmct,JavaScript,"# Open MCT [![license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)

Open MCT (Open Mission Control Technologies) is a next-generation mission control framework for visualization of data on desktop and mobile devices. It is developed at NASA's Ames Research Center, and is being used by NASA for data analysis of spacecraft missions, as well as planning and operation of experimental rover systems. As a generalizable and open source framework, Open MCT could be used as the basis for building applications for planning, operation, and analysis of any systems producing telemetry data.

Please visit our [Official Site](https://nasa.github.io/openmct/) and [Getting Started Guide](https://nasa.github.io/openmct/getting-started/)

## See Open MCT in Action

Try Open MCT now with our [live demo](https://openmct-demo.herokuapp.com/).
![Demo](https://nasa.github.io/openmct/static/res/images/Open-MCT.Browse.Layout.Mars-Weather-1.jpg)

## Building and Running Open MCT Locally

Building and running Open MCT in your local dev environment is very easy. Be sure you have [Git](https://git-scm.com/downloads) and [Node.js](https://nodejs.org/) installed, then follow the directions below. Need additional information? Check out the [Getting Started](https://nasa.github.io/openmct/getting-started/) page on our website.
(These instructions assume you are installing as a non-root user; developers have [reported issues](https://github.com/nasa/openmct/issues/1151) running these steps with root privileges.)

1. Clone the source code

 `git clone https://github.com/nasa/openmct.git`

2. Install development dependencies

 `npm install`

3. Run a local development server

 `npm start`

Open MCT is now running, and can be accessed by pointing a web browser at [http://localhost:8080/](http://localhost:8080/)

## Open MCT v1.0.0
This represents a major overhaul of Open MCT with significant changes under the hood. We aim to maintain backward compatibility but if you do find compatibility issues, please let us know by filing an issue in this repository. If you are having major issues with v1.0.0 please check-out the v0.14.0 tag until we can resolve them for you.

If you are migrating an application built with Open MCT as a dependency to v1.0.0 from an earlier version, please refer to [our migration guide](https://nasa.github.io/openmct/documentation/migration-guide).

## Documentation

Documentation is available on the [Open MCT website](https://nasa.github.io/openmct/documentation/).

### Examples

The clearest examples for developing Open MCT plugins are in the
[tutorials](https://github.com/nasa/openmct-tutorial) provided in
our documentation.

We want Open MCT to be as easy to use, install, run, and develop for as
possible, and your feedback will help us get there! Feedback can be provided via [GitHub issues](https://github.com/nasa/openmct/issues), or by emailing us at [arc-dl-openmct@mail.nasa.gov](mailto:arc-dl-openmct@mail.nasa.gov).

## Building Applications With Open MCT

Open MCT is built using [`npm`](http://npmjs.com/) and [`webpack`](https://webpack.js.org/).

See our documentation for a guide on [building Applications with Open MCT](https://github.com/nasa/openmct/blob/master/API.md#starting-an-open-mct-application).

## Plugins

Open MCT can be extended via plugins that make calls to the Open MCT API. A plugin is a group 
of software components (including source code and resources such as images and HTML templates)
that is intended to be added or removed as a single unit.

As well as providing an extension mechanism, most of the core Open MCT codebase is also 
written as plugins.

For information on writing plugins, please see [our API documentation](https://github.com/nasa/openmct/blob/master/API.md#plugins).

## Tests

Tests are written for [Jasmine 3](https://jasmine.github.io/api/3.1/global)
and run by [Karma](http://karma-runner.github.io). To run:

`npm test`

The test suite is configured to load any scripts ending with `Spec.js` found
in the `src` hierarchy. Full configuration details are found in
`karma.conf.js`. By convention, unit test scripts should be located
alongside the units that they test; for example, `src/foo/Bar.js` would be
tested by `src/foo/BarSpec.js`. (For legacy reasons, some existing tests may
be located in separate `test` folders near the units they test, but the
naming convention is otherwise the same.)

### Test Reporting

When `npm test` is run, test results will be written as HTML to
`dist/reports/tests/`. Code coverage information is written to `dist/reports/coverage`.

# Glossary

Certain terms are used throughout Open MCT with consistent meanings
or conventions. Any deviations from the below are issues and should be
addressed (either by updating this glossary or changing code to reflect
correct usage.) Other developer documentation, particularly in-line
documentation, may presume an understanding of these terms.

* _plugin_: A plugin is a removable, reusable grouping of software elements.
  The application is composed of plugins.
* _composition_: In the context of a domain object, this refers to the set of
  other domain objects that compose or are contained by that object. A domain
  object's composition is the set of domain objects that should appear
  immediately beneath it in a tree hierarchy. A domain object's composition is
  described in its model as an array of id's; its composition capability
  provides a means to retrieve the actual domain object instances associated
  with these identifiers asynchronously.
* _description_: When used as an object property, this refers to the human-readable
  description of a thing; usually a single sentence or short paragraph.
  (Most often used in the context of extensions, domain
  object models, or other similar application-specific objects.)
* _domain object_: A meaningful object to the user; a distinct thing in
  the work support by Open MCT. Anything that appears in the left-hand
  tree is a domain object.
* _identifier_: A tuple consisting of a namespace and a key, which together uniquely
  identifies a domain object.
* _model_: The persistent state associated with a domain object. A domain
  object's model is a JavaScript object which can be converted to JSON
  without losing information (that is, it contains no methods.)
* _name_: When used as an object property, this refers to the human-readable
  name for a thing. (Most often used in the context of extensions, domain
  object models, or other similar application-specific objects.)
* _navigation_: Refers to the current state of the application with respect
  to the user's expressed interest in a specific domain object; e.g. when
  a user clicks on a domain object in the tree, they are _navigating_ to
  it, and it is thereafter considered the _navigated_ object (until the
  user makes another such choice.)
* _namespace_: A name used to identify a persistence store. A running open MCT 
application could potentially use multiple persistence stores, with the 
"
2,nasa/astrobee_android,Java,"# Astrobee Robot Software - Android submodule

## About

Astrobee is a free-flying robot that is designed to operate as a payload inside
the International Space Station (ISS). The Astrobee Robot Software consists of
embedded (on-board) software, supporting tools and a simulator. The Astrobee
Robot Software operates on Astrobee's three internal single board computers and
uses the open-source Robot Operating System (ROS) framework as message-passing
middleware. The Astrobee Robot Software performs vision-based localization,
provides autonomous navigation, docking and perching, manages various sensors
and actuators, and supports user interaction via screen-based displays, light
signaling, and sound. The Astrobee Robot Software enables Astrobee to be
operated in multiple modes: plan-based task execution (command sequencing),
teleoperation, or autonomously through execution of hosted code uploaded by
project partners (guest science). The software simulator enables Astrobee Robot
Software to be evaluated without the need for robot hardware.

This repository contains the libraries and API to support Guest Science
application running on the Astrobee High Level Processor (HLP). The HLP runs the
[Android Nougat](https://www.android.com/versions/nougat-7-0/)
Operating System (7.1.1). The Astrobee Robot Software exposes a Java
API that can be used either in pure Java land or Android land to interact with
robot internal messaging system based on ROS.

A distinct repository, [`astrobee`](https://github.com/nasa/astrobee), contains
the core flight software for Astrobee. Note that `astrobee` repository is
required to used `astrobee_android` (it contains all the message definitions).

Also note that the Astrobee Robot Software is in beta stage. This means that
some features are missing and some functionalities are incomplete. Please
consult [RELEASE.md](https://github.com/nasa/astrobee/blob/HEAD/RELEASE.md) for
the current list of features and limitations.

Please see the [Guest Science Readme](guest_science_readme.md) for a description
of how guest science should interact with the Astrobee flight software. This
documentation also contains examples to help guest science interface with
Astrobee.

Please see the
[Guest Science Resources](https://www.nasa.gov/content/guest-science-resources)
page for information on guest science capabilities on Astrobee.

## License

Copyright (c) 2017, United States Government, as represented by the
Administrator of the National Aeronautics and Space Administration.
All rights reserved.

The Astrobee platform is licensed under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance with the License. You
may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.

Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"
3,nasa/astrobee,C++,"# Astrobee Robot Software

### About

<p>
<img src=""doc/images/astrobee.png"" srcset=""../images/astrobee.png 1x"" 
  title=""Astrobee"" align=""right"" style=""display: inline""/>
Astrobee is a free-flying robot designed to operate as a payload inside
the International Space Station (ISS). The Astrobee Robot Software consists of
embedded (on-board) software, supporting tools and a simulator. The Astrobee
Robot Software operates on Astrobee's three internal single board computers and
uses the open-source Robot Operating System (ROS) framework as message-passing
middleware. The Astrobee Robot Software performs vision-based localization,
provides autonomous navigation, docking and perching, manages various sensors
and actuators, and supports user interaction via screen-based displays, light
signaling, and sound. The Astrobee Robot Software enables Astrobee to be
operated in multiple modes: plan-based task execution (command sequencing),
teleoperation, or autonomously through execution of hosted code uploaded by
project partners (guest science). The software simulator enables Astrobee Robot
Software to be evaluated without the need for robot hardware.
</p>

This repository provides flight software and a simulator, both primarily written
in C++. The repository also provides several other utilities, including a tool
for creating maps for localization. A separate repository,
[`astrobee_android`](https://github.com/nasa/astrobee_android), contains the
Java API, which uses the ROS messaging system to communicate with flight
software.

The Astrobee Robot Software is in a beta stage. This means that some
features are incomplete, and extensive changes can be expected. Please consult the
[release](https://nasa.github.io/astrobee/html/md_RELEASE.html) for the current list of features and limitations.

### Usage instructions

To install and use astrobee, please see the
[usage instructions](https://nasa.github.io/astrobee/html/md_INSTALL.html).

### Contributors

The Astrobee Robot Software is open source, and we welcome contributions
from the public. Please submit pull requests to the develop branch.
For us to merge any pull requests, we must request that contributors sign and submit a
[Contributor License Agreement](https://www.nasa.gov/sites/default/files/atoms/files/astrobee_individual_contributor_license_agreement.pdf)
due to NASA legal requirements. Thank you for your understanding.

### Documentation

To view all the Astrobee documentation, please visit [documentation](https://nasa.github.io/astrobee/documentation.html).

If you want to perform research using the astrobee platform, a good tutorial guide is [""A Brief Guide to Astrobee‚Äôs Flight Software""](https://github.com/albee/a-brief-guide-to-astrobee/raw/master/a_brief_guide_to_astrobee_v1.0.pdf). This will teach you what Astrobee is, how the robot works, how to make your own package, and much more!

For more information, read the Astrobee [publications](https://www.nasa.gov/content/research-publications-0).
Learning about the Astrobee [platform](https://www.nasa.gov/sites/default/files/atoms/files/bualat_spaceops_2018_paper.pdf),
[software](https://www.nasa.gov/sites/default/files/atoms/files/fluckiger2018astrobee.pdf),
and [localization](https://www.nasa.gov/sites/default/files/atoms/files/coltin2016localization.pdf)
are good starting points.

### Guest Science

If you are interested in guest science, please checkout the astrobee_android nasa github
project (if you followed the usage instructions, you should have checked this
out already). Once that is checked out, please see
[`astrobee_android/README.md`](https://github.com/nasa/astrobee_android/blob/master/README.md)
located in the `astrobee_android/` folder.

### License

Copyright (c) 2017, United States Government, as represented by the
Administrator of the National Aeronautics and Space Administration.
All rights reserved.

The Astrobee platform is licensed under the Apache License, Version 2.0 (the
""License""); you may not use this file except in compliance with the License. You
may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.

Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
"
4,nasa/earthdata-search,JavaScript,"# [Earthdata Search](https://search.earthdata.nasa.gov)

[![serverless](http://public.serverless.com/badges/v3.svg)](http://www.serverless.com)
![Build Status](https://github.com/nasa/earthdata-search/workflows/CI/badge.svg?branch=master)
[![codecov](https://codecov.io/gh/nasa/earthdata-search/branch/master/graph/badge.svg?token=kIkZQ0NrqK)](https://codecov.io/gh/nasa/earthdata-search)
[![Known Vulnerabilities](https://snyk.io/test/github/nasa/earthdata-search/badge.svg)](https://snyk.io/test/github/nasa/earthdata-search)

## About
Earthdata Search is a web application developed by [NASA](http://nasa.gov) [EOSDIS](https://earthdata.nasa.gov)
to enable data discovery, search, comparison, visualization, and access across EOSDIS' Earth Science data holdings.
It builds upon several public-facing services provided by EOSDIS, including
the [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/) for data discovery and access,
EOSDIS [User Registration System (URS)](https://urs.earthdata.nasa.gov) authentication,
the [Global Imagery Browse Services (GIBS)](https://earthdata.nasa.gov/gibs) for visualization,
and a number of OPeNDAP services hosted by data providers.

## License

> Copyright ¬© 2007-2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
>
> Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>    http://www.apache.org/licenses/LICENSE-2.0
>
>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

## Application Installation and Usage

The Earthdata Search application uses Node v12.16.3 and Webpack 4.39.3 to generate static assets. The serverless application utilizes the following AWS services (important to note if deploying to an AWS environment):
- S3
  - We highly recommend using CloudFront in front of S3.
- SQS
- API Gateway
- Lambda
- Cloudwatch (Events)

### Prerequisites

##### Node
Earthdata Search runs on Node.js, in order to run the application you'll need to [install it](https://nodejs.org/en/download/).

**Recommended:** Use Homebrew

	brew install node

##### NPM
npm is a separate project from Node.js, and tends to update more frequently. As a result, even if you‚Äôve just downloaded Node.js (and therefore npm), you‚Äôll probably need to update your npm. Luckily, npm knows how to update itself! To update your npm, type this into your terminal:

    npm install -g npm@latest

##### NVM
To ensure that you're using the correct version of Node it is recommended that you use Node Version Manager. Installation instructions can be found on [the repository](https://github.com/nvm-sh/nvm#install--update-script). The version used is defined in .nvmrc and will be used automatically if NVM is configured correctly.


##### Serverless Framework
Earthdata Search utilizes the [Serverless Framework](https://serverless.com/) for managing AWS resources. In order to fully run and manage the application you'll need to install it:

    npm install -g serverless@latest

##### PostgreSQL
Earthdata Search uses PostgreSQL in production on AWS RDS. If you don't already have it installed, [download](https://www.postgresql.org/download/) and install it to your development environment.

**Recommended:** Use Homebrew

    brew install postgresql

If you decide to install via Homebrew you'll need to create the default user.

    /usr/local/opt/postgres/bin/createuser -s postgres

### Initial Setup

##### Package Installation

Once npm is installed locally, you need to download the dependencies by executing the command below in the project root directory:

    npm install

##### Configuration

###### Secrets

For local development Earthdata Search uses a json configuration file to store secure files, an example is provided and should be copied and completed before attempting to go any further.

	cp secret.config.json.example secret.config.json

In order to operate against a local database this file will need `dbUsername` and `dbPassword` values set (you may need to update `dbHost`, `dbName` or `dbPort` in `static.config.json` if you have custom configuration locally)

###### Public (Non-Secure)
Non-secure values are stored in `static.config.json`. In order to prevent conflicts amongst developers you copy the static config into `overrideStatic.config.json` and change the config values there. Do not commit changes to `static.config.json`.

    cp static.config.json overrideStatic.config.json

##### Database Migration

Ensure that you have a database created:

	createdb edsc_dev

Our database migrations run within Lambda due to the fact that in non-development environments our resources are not publicly accessible. To run the migrations you'll need to invoke the Lambda:

	serverless invoke local --function migrateDatabase


### Building the Application

The production build of the application will be output in the `/static/dist/` directory:

    npm run build


### Run the Application Locally

The local development environment for the static assets can be started by executing the command below in the project root directory:

    npm run start

This will run the React application at [http://localhost:8080](http://localhost:8080) -- please see `Serverless Framework` below for enabling the 'server' side functionality.


### Serverless Framework

The [serverless framework](https://serverless.com/framework/docs/providers/aws/) offers many plugins which allow for local development utilizing many of the services AWS offers. For the most part we only need API Gateway and Lambda for this application but there are plugins for many more services (a list of known exceptions will be maintained below).

##### Exceptions
- SQS

	While there is an sqs-offline plugin for serverless it still requires an actual queue be running, we may investigate this in the future but for now sqs functionality isn't available while developing locally which means the following pieces of functionality will not operate locally:
	- Generating Colormaps

#### Running API Gateway and Lambda Locally

Running the following command will spin up API Gateway and Lambda locally which will open up a vast majority of the functionality the backend offers.

	serverless offline

This will provide access to API Gateway at [http://localhost:3001](http://localhost:3001)

Additionally, this ties in with the `serverless webpack` plugin which will ensure that your lambdas are re-built when changes are detected.


### Run the Automated [Jest](https://jestjs.io/) tests

Once the project is built, you must ensure that the automated unit tests pass:

    npm run test

### Run the Automated [Cypress](https://www.cypress.io/) tests

You must also ensure that the automated integration tests pass:

    npm run cypress:run

You can also use the Cypress GUI with:

    npm run cypress:open


##### Configuration

###### Cypress Secrets

When adding new Cypress tests, you will need to modify the secrets.config.json file. You will need to edit the ""cypress"" object to include data from your local database:

    ""cypress"": {
        ""user"": {
        ""id"": 1, // This should match the ID of your user in the 'users' database table
        ""username"": ""your username here"" // Replace with the urs_id field of your user in the 'users' database table
        }
    }

### Deployment

When the time comes to deploy the application, first ensure that you have the required ENV vars set:

- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY

This application runs in a VPC for NASA security purposes, therefore the following values are expected when a deployment occurs:

- VPC_ID
- SUBNET_ID_A
- SUBNET_ID_B

For production use, this application uses Scatter Swap to obfuscate some IDs -- the library does not require a value be provided but if you'd like to control it you can set the following ENV vars:

- OBFUSCATION_SPIN
- OBFUSCATION_SPIN_SHAPEFILES

To deploy the full application use the following:

	NODE_ENV=production serverless deploy --stage UNIQUE_STAGE
"
5,nasa/daa-displays,TypeScript,"# DAA-Displays: A Toolkit for the Analysis of Detect-And-Avoid Functions in Cockpit Displays
DAA-Displays is a toolkit for model-based design and analysis of software functions in cockpit
displays. It includes a library of interactive graphical display elements (widgets)
for cockpit systems, and simulations tools supporting comparative analysis of cockpit displays.

![](src/images/danti.gif """")

## Latest version
daa-displays-1.0.10

## Functionalities
- Library of interactive graphical display elements (called widgets) for cockpit systems: Compass, Interactive Map, Airspeed Tape, Altitude Tape, Vertical Speed, Virtual Horizon 
- Simulation tools supporting comparative analysis of cockpit displays

## Requirements
The following software is necessary to compile and execute DAA-Displays
- NodeJS (v14.16.1 or greater) https://nodejs.org/en/download
- Java Open JDK (1.9 or greater) https://openjdk.java.net/install
- C++ compiler (gcc version 7.4.0 for Linux, Apple clang 11.0.0 for MacOS)
- Google Chrome (80.0.x or greater) https://www.google.com/chrome

## Installation instructions
1. Download the latest release of DAA-Displays from the github repository.
2. Open a terminal window and change directory to the `daa-displays` folder.
3. Run `make` in the terminal window.
   This command will download the dependencies and create a folder `dist/` with the distribution.

## Use instructions
1. Open a terminal in the `daa-displays` folder, and run the bash script `./restart.sh` in the terminal window. The script will launch the daa-server on port 8082. *(Please keep the terminal window open otherwise the execution of the server will be terminated.)*
2. Open Google Chrome at http://localhost:8082
3. Launch one of the DAA Apps (`DANTi`, `single-view`, etc.) by clicking on the corresponding icon.
4. Select a flight scenario, a configuration, and a DAA specification using the drop-down menus provided by the App. Click `Load Selected Scenario and Configuration` to initialize the simulation.
5. Click `Play` to simulate the DAA specification in the selected flight scenario. Use the other simulation controls to jump to specific time instants, change the simulation speed, and generate plot diagrams.

## Tips for developers
The `./restart.sh` script supports the following options:
- `-help`                (Shows the available options)
- `-pvsio`               (Enables the pvsio process; pvsio must be in the execution path; requires nasalib)
- `-pvsio <path>`        (Enables the pvsio process; the given pvsio path is used for executing the pvsio environment; requires PVS and NASALib)
- `-fast`                (Enables optimizations, including caching of simulation results)
- `-port <port number>`  (The server will use the given port)

## Publications
Paolo Masci and C√©sar Mu√±oz, [A Graphical Toolkit for the Validation of Requirements for Detect and Avoid Systems](https://doi.org/10.1007/978-3-030-50995-8_9), Proceedings of the 14th International Conference on Tests and Proofs (TAP 2020), Lecture Notes in Computer Science, Vol. 12165, pp. 155-166, 2020 [[PDF](https://shemesh.larc.nasa.gov/fm/papers/TAP2020-MM.pdf)][[BibTex](https://shemesh.larc.nasa.gov/fm/papers/TAP2020-MM.bib)]

## Use Cases
The following examples illustrate concrete applications of the daa-displays toolkit for the analysis of DAA functions in cockpit displays.

### **Example 1**: Demonstration of a DAA algorithm
The rapid prototyping functionalities of the toolkit allow developers to create realistic simulations of cockpit displays suitable to demonstrate DAA specifications and implementations on concrete encounters.
The following example is for the analysis of maneuver guidance provided by a DAA algorithm
to help pilots resolve route conflicts in a flight scenario. 
Maneuver guidance has the form of *bands*, i.e., ranges
of heading, horizontal speed, vertical speed, and altitude maneuvers that
are predicted to be conflict free (the prediction is based on a mathematical 
formula that uses distance and time separation thresholds). Bands are color-coded: 
yellow denotes a corrective maneuver, red denotes a warning maneuver, and green
denotes a recovery maneuver.

The simulation shown in the figure below, illustrate a scenario where a traffic 
aircraft is overtaking the ownship.
Bands on the flight display indicate maneuvers that can be performed to avoid
the conflict. For example, yellow and red bands on the right side of the compass 
indicate that the ownship should avoid right turns.

![](screenshots/scenario_6_danti.gif """")

To reproduce the demonstration shown in the figure:
1. Launch the `DANTi` app (see *Use instructions* above).
2. Select flight scenario `scenario_6` and click `Load Selected Scenario and Configuration`.
3. Click `Play` to watch the behavior of the DAA specification for the selected flight scenario.

### **Example 2**: Comparison of different DAA configurations
Split-view simulations facilitate the comparative analysis of 
two DAA implementations and formal specifications on the same encounter.
In the following example, a newer version of a DAA reference implementation
(on the left-hand side of the split-view) is compared with an older version. 
The newer version introduces additional maneuver guidance in the form of 
speed/heading/altitude bugs rendered on the flight display. 

![](screenshots/s_1_turn_L_wind_0_50.gif """")

To reproduce the demonstration shown in the figure:
1. Launch the `split-view` app (see *Use instructions* above).
2. Select flight scenario `s_1_turn_L_wind_0_50`, then select `WellClear-2.0.f.jar` on the left player and `WellClear-1.0.1.jar` in the right player. Click `Load Selected Scenario and Configuration`.
3. Click `Play` to watch the behavior of the two versions in the same flight scenario.

### **Example 3**: 3D Simulation
3D simulations move the focus of the analysis from a cockpit-centric 
view to a scenario-centric view that includes the wider airspace 
around the ownship. The viewport can be adjusted by tilting, 
panning, and zooming the view.  This capability
can be used to gain a better understanding of spatial information 
on the trajectories followed by the ownship and
traffic aircraft in a given scenario. This is useful, e.g., when
assessing DAA algorithms with computer-generated flight scenarios, as
this view provides a tangible idea of what the scenario is about.

The following example 3D simulation is used to examine 
the same flight scenario of Example 1, where a traffic aircraft overtakes the ownship.

![](screenshots/scenario_6_3d.gif """")

To reproduce the demonstration shown in the figure:
1. Launch the `3D view` app (see *Use instructions* above).
2. Select flight scenario `scenario_6` and click `Load Selected Scenario and Configuration`.
3. Click `Play` to watch the behavior of the DAA specification for the selected flight scenario. Position the mouse in the view and use mouse wheel to zoom in/out, use the central mouse button to tilt/pan the view.


## Structure
```
.
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ daa-displays                     // DAA-Displays widgets library
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-airspeed-tape.ts       // Airspeed Tape widget
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-altitude-tape.ts       // Altitude Tape widget
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-compass.ts             // Compass Display widget
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-interactive-map.ts     // Interactive Map widget
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-vertical-speed-tape.ts // Vertical Speed Tape widget
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-virtual-horizon.ts     // Virtual Horizon widget
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-player.ts              // Single-view player
‚îÇ   ‚îÇ     ‚îú‚îÄ‚îÄ daa-split-view.ts          // Split-view player
‚îÇ   ‚îÇ     ‚îî‚îÄ‚îÄ daa-spectrogram.ts         // Spectrogram renderer
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ daa-config                       // Well-Clear configuration files
‚îÇ   ‚îú‚îÄ‚îÄ daa-logic                        // Well-Clear logic
‚îÇ   ‚îú‚îÄ‚îÄ daa-output                       // Output files generated by simulation runs
‚îÇ   ‚îú‚îÄ‚îÄ daa-scenarios                    // Scenario files for running simulation runs
‚îÇ   ‚îú‚îÄ‚îÄ daa-server                       // Server-side component of DAA-Displays
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ LICENSES                         // NASA Open Source License Agreement
‚îÇ   ‚îî‚îÄ‚îÄ index.html                       // Client entry-point
‚îÇ
‚îú‚îÄ‚îÄ restart.sh                           // Script for launching the daa-server
‚îú‚îÄ‚îÄ Makefile                             // Compilation targets
‚îî‚îÄ‚îÄ package.json                         // Manifest file

```


## Notices
### Copyright 
Copyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
 
### Disclaimers
**No Warranty**: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY
  WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,
  INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE
  WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF
  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
  INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
  FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO
  THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER,
  CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT
  OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY
  OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.
  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
  REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
  AND DISTRIBUTES IT ""AS IS.""
 
**Waiver and Indemnity**: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS
  AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND
  SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF
  THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,
  EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM
  PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT
  SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
  STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
  PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE
  REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL
  TERMINATION OF THIS AGREEMENT.


## Contacts
* Paolo Masci (NIA) (paolo.masci@nianet.org)
* Cesar Munoz (NASA LaRC) (cesar.a.munoz@nasa.gov)
"
6,nasa/code-nasa-gov,CSS,"# CODE.NASA.GOV

[![Build Status](https://travis-ci.org/nasa/code-nasa-gov.svg?branch=master)](https://travis-ci.org/nasa/code-nasa-gov)

Catalog of Open Source Software from NASA. Built using [Polymer](https://www.polymer-project.org).

## Do You have a Open-Source Code Project For This Site?

#### Instructions
Instructions for releasing a NASA open-source project can be found on <a href=""https://code.nasa.gov/#/guide"">https://code.nasa.gov/#/guide</a>.

#### Code.json vs Category.json
Newly approved code projects for release are added to code.json. You can add your approved open-source NASA project to <a href=""https://raw.githubusercontent.com/nasa/Open-Source-Catalog/master/code.json""><b>code.json</b></a>, <a href=""https://github.com/nasa/Open-Source-Catalog"">here</a>.

All federal agencies are mandated to have a code.json that is then harvested by the General Services Adminstration (GSA) and aggregated into code.gov. 

Code.json is reformatted by a script run by NASA's open-innovation team into <a href=""https://raw.githubusercontent.com/nasa/code-nasa-gov/master/data/catalog.json"">category.json</a>. Category.json has some attributes not in code.json and is used to build the project page on code.nasa.gov.

Additionally, at this time, only category.json has the A.I.-generated keyword tags in addition to the human-generated tags. This may change in the future. 

#### Why code.json is bigger than category.json
Some of the code projects in code.json have open-source licenses. Other projects in code.json have government-source only licenses, meaning sharing is constrainted to government agencies. All of the code projects listed in category.json have open-source licenses. 

### Making your own data visualization with the JSONs that drive code.nasa.gov:
- https://observablehq.com/@justingosses/finding-recent-additions-to-code-nasa-gov
- https://observablehq.com/@briantoliveira/untitled

If you make your own visualization, please add it as an issue. We would love to see it!

## Running The Code In This Repository

### Setup

test

### Prerequisites

Install bower and [polymer-cli](https://github.com/Polymer/polymer-cli):

    npm install -g bower polymer-cli

Check that you are using Node v8+

    node -v
    
### Install dependencies

    bower i

### Start the development server

This command serves the app at `http://localhost:8080` and provides basic URL
routing for the app:

    polymer serve --open


### Build

This command performs HTML, CSS, and JS minification on the application
dependencies and generates a service-worker.js file with code to pre-cache the
dependencies based on the entrypoint and fragments specified in `polymer.json`.
The minified files are output to the `build/unbundled` folder, and are suitable
for serving from a HTTP/2+Push compatible server.

In addition the command also creates a fallback `build/bundled` folder,
generated using fragment bundling, suitable for serving from non
H2/push-compatible servers or to clients that do not support H2/Push.

    polymer build

### Preview the build

This command serves the minified version of the app at `http://localhost:8080`
in an unbundled state, as it would be served by a push-compatible server:

    polymer serve build/unbundled

This command serves the minified version of the app at `http://localhost:8080`
generated using fragment bundling:

    polymer serve build/bundled
    
### Deploying

When deploying to a static web server (with no HTTP/2+Push), be sure to copy only
the files from `build/bundled` directory (**NOT** the project directory) which
contains a functional service worker and minified files. Put them in a top level part of the directory, not within another build/bundled directory within the production directory.

### Adding a new view

You can extend the app by adding more views that will be demand-loaded
e.g. based on the route, or to progressively render non-critical sections
of the application.  Each new demand-loaded fragment should be added to the
list of `fragments` in the included `polymer.json` file.  This will ensure
those components and their dependencies are added to the list of pre-cached
components (and will have bundles created in the fallback `bundled` build).
"
7,nasa/cumulus-orca,Python,"## Clone and build Operational Recovery Cloud Archive (ORCA)

Clone the `dr-podaac-swot` repo from https://github.com/ghrcdaac/operational-recovery-cloud-archive

```
git clone https://github.com/ghrcdaac/operational-recovery-cloud-archive
```
## Build lambdas
Before you can deploy this infrastructure, you must download the release zip or the build the lambda function source-code locally.

`./bin/build_tasks.sh` will crawl the `tasks` directory and build a `.zip` file (currently by just `zipping` all python files and dependencies) in each of it's sub-directories. That `.zip` is then referenced in the `modules/lambdas/main.tf` lamdba definitions.

```
./bin/build_tasks.sh
```

# ORCA Deployment

The ORCA deployment is done with [Terraform root module](https://www.terraform.io/docs/configuration/modules.html),
`main.tf`.

The following instructions will walk you through installing Terraform,
configuring Terraform, and deploying the root module.

## Install Terraform

If you are using a Mac and [Homebrew](https://brew.sh), installing Terraform is
as simple as:

```shell
brew update
brew install terraform
```

For other cases,
Visit the [Terraform documentation](https://learn.hashicorp.com/terraform/getting-started/install.html) for installation instructions.

Verify that the version of Terraform installed is at least v0.12.0.

```shell
$ terraform --version
Terraform v0.12.2
```

## Configure the Terraform backend

The state of the Terraform deployment is stored in S3. In the following
examples, it will be assumed that state is being stored in a bucket called
`dr-tf-state`. You can also use an existing bucket, if desired.

Create the state bucket:

```shell
aws s3api create-bucket --bucket dr-tf-state --create-bucket-configuration LocationConstraint=us-west-2
```
**Note:** The `--create-bucket-configuration` line is only necessary if you are creating your bucket outside of `us-east-1`.

In order to help prevent loss of state information, it is recommended that
versioning be enabled on the state bucket:

```shell
$ aws s3api put-bucket-versioning \
    --bucket dr-tf-state \
    --versioning-configuration Status=Enabled
```

Terraform uses a lock stored in DynamoDB in order to prevent multiple
simultaneous updates. In the following examples, that table will be called
`dr-tf-locks`.

Create the locks table:

‚ö†Ô∏è **Note:** The `--billing-mode` option was recently added to the AWS CLI. You
may need to upgrade your version of the AWS CLI if you get an error about
provisioned throughput when creating the table.

```shell
$ aws dynamodb create-table \
    --table-name dr-tf-locks \
    --attribute-definitions AttributeName=LockID,AttributeType=S \
    --key-schema AttributeName=LockID,KeyType=HASH \
    --billing-mode PAY_PER_REQUEST
```

## Configure and deploy the `main` root module

These steps should be executed in the root directory of the repo.

Create a `terraform.tf` file, substituting the appropriate values for `bucket`
and `dynamodb_table`. This tells Terraform where to store its
remote state.

**terraform.tf**

```
terraform {
  backend ""s3"" {
    region         = ""us-west-2""
    bucket         = ""dr-tf-state""
    key            = ""terraform.tfstate""
    dynamodb_table = ""dr-tf-locks""
  }
}
```

## Variables
First, run a `mv terraform.tfvars.example terraform.tfvars` to get a template `terraform.tfvars` in your working directory. This is where you will place input variables to Terraform.

**Necessary:**
* `ngap_subnets` - NGAP Subnets (array)
* `vpc_id` - ID of VPC to place resources in - recommended that this be a private VPC (or at least one with restricted access).
* `glacier_bucket` - Bucket with Glacier policy
* `public_bucket` - Bucket with public permissions (Cumulus public bucket)
* `private_bucket` - Bucket with private permissions (Cumulus private bucket)
* `internal_bucket` - Analogous to the Cumulus internal bucket 
* `protected_bucket` - Analogous to the Cumulus protected bucket
* `permissions_boundary_arn` - Permission Boundary Arn (Policy) for NGAP compliance
* `postgres_user_pw` - password for the postgres user
* `database_name` - disaster_recovery
* `database_app_user` - druser 
* `database_app_user_pw` - the password for the application user

**Optional:**
* `prefix` - Prefix that will be pre-pended to resource names created by terraform. 
  Defaults to `dr`.
* `profile` - AWS CLI Profile (configured via `aws configure`) to use. 
  Defaults to `default`.
* `region` - Your AWS region. 
  Defaults to `us-west-2`.
* `restore_expire_days` - How many days to restore a file for. 
  Defaults to 5.
* `restore_request_retries` - How many times to retry a restore request to Glacier. 
  Defaults to 3.
* `restore_retry_sleep_secs` - How many seconds to wait between retry calls to `restore_object`. 
  Defaults to 3.
* `restore_retrieval_type` -  the Tier for the restore request. Valid values are 'Standard'|'Bulk'|'Expedited'. 
  Defaults to `Standard`. Understand the costs associated with the tiers before modifying.
* `copy_retries` - How many times to retry a copy request from the restore location to the archive location. 
  Defaults to 3.
* `copy_retry_sleep_secs` - How many seconds to wait between retry calls to `copy_object`. 
  Defaults to 0.
* `ddl_dir` - the location of the ddl dir that contains the sql to create the application database. 
  Defaults to 'ddl/'.
* `drop_database` - Whether or not to drop the database if it exists (True), or keep it (False). 
  Defaults to False.
* `database_port` - the port for the postgres database. 
  Defaults to '5432'.
* `platform` - indicates if running locally (onprem) or in AWS (AWS). 
  Defaults to 'AWS'.

## Deploying with Terraform
Run `terraform init`.
Run `terraform plan` #optional, but allows you to preview the deploy.
Run `terraform apply`.

This will deploy ORCA.

## Delete Terraform stack
If you want to remove it:
```
terraform destroy
```

## Integrating with Cumulus
Integrate ORCA with Cumulus to be able to recover a granule from the Cumulus Dashboard.

### Define the ORCA workflow (Cumulus < v1.15)

Copy the workflow from `workflows/workflows.yml.dr` into your Cumulus workflow.

Set the values of these environment variables to the ARN for the 
{prefix}-extract-filepaths-for-granule and {prefix}-request-files lambdas,
respectively:
```
DR_EXTRACT_LAMBDA_ARN=arn:aws:lambda:us-west-2:012345678912:function:dr_extract_filepaths_for_granule

DR_REQUEST_LAMBDA_ARN=arn:aws:lambda:us-west-2:012345678912:function:dr_request_files
```

Add an `aws` provider to `main.tf`:

```
provider ""aws"" {
  version = ""~> 2.13""
  region  = var.region
  profile = var.profile
}
```

### Integrating ORCA With Cumulus >= v1.15

#### Adding a ORCA module to the Cumulus deployment

Navigate to `cumulus-tf/main.tf` within your Cumulus deployment directory and add the following module:
```
module ""orca"" {
  source = ""https://github.com/ghrcdaac/operational-recovery-cloud-archive/releases/download/1.0.2/orca-1.0.2.zip""

  prefix = var.prefix
  subnet_ids = module.ngap.ngap_subnets_ids
  database_port = ""5432""
  database_user_pw = var.database_user_pw
  database_name = var.database_name
  database_app_user = var.database_app_user
  database_app_user_pw = var.database_app_user_pw
  ddl_dir = ""ddl/""
  drop_database = ""False""
  platform = ""AWS""
  lambda_timeout = 300
  restore_complete_filter_prefix = """"
  vpc_id = module.ngap.ngap_vpc.id
  copy_retry_sleep_secs = 2
  permissions_boundary_arn = var.permissions_boundary_arn
  buckets = var.buckets
  workflow_config = module.cumulus.workflow_config
  region = var.region
}
```

*Note*: This above snippet assumes that you've configured your Cumulus deployment. More information on that process can be found in their [documentation](https://nasa.github.io/cumulus/docs/deployment/deployment-readme#configure-and-deploy-the-cumulus-tf-root-module)

#### Add necessary variables (unique to ORCA) to the Cumulus TF configuration

To support this module, you'll have to add the following values to your `cumulus-tf/variables.tf` file:
```
# Variables specific to ORCA
variable ""database_user_pw"" {
  type = string
}

variable ""database_name"" {
  type = string
}

variable ""database_app_user"" {
  type = string
}

variable ""database_app_user_pw"" {
  type = string
}
```

The values corresponding to these variables must be set in your `cumulus-tf/terraform.tfvars` file, but note that many of these variables are actually hardcoded at the time of updating this README

#### Adding the Copy To Glacier Step to the Ingest Workflow
Navigate to `cumulus-tf/ingest_granule_workflow.tf` then add the following step after the PostToCMR step being sure to change the PostToCMR's ""Next"" paramter equal to ""CopyToGlacier""
```
""CopyToGlacier"":{
         ""Parameters"":{
            ""cma"":{
               ""event.$"":""$"",
               ""task_config"":{
                  ""bucket"":""{$.meta.buckets.internal.name}"",
                  ""buckets"":""{$.meta.buckets}"",
                  ""distribution_endpoint"":""{$.meta.distribution_endpoint}"",
                  ""files_config"":""{$.meta.collection.files}"",
                  ""fileStagingDir"":""{$.meta.collection.url_path}"",
                  ""granuleIdExtraction"":""{$.meta.collection.granuleIdExtraction}"",
                  ""collection"":""{$.meta.collection}"",
                  ""cumulus_message"":{
                     ""input"":""{[$.payload.granules[*].files[*].filename]}"",
                     ""outputs"":[
                        {
                           ""source"":""{$}"",
                           ""destination"":""{$.payload}""
                        }
                     ]
                  }
               }
            }
         },
         ""Type"":""Task"",
         ""Resource"":""${module.orca.copy_to_glacier_lambda_arn}"",
         ""Catch"":[
            {
               ""ErrorEquals"":[
                  ""States.ALL""
               ],
               ""ResultPath"":""$.exception"",
               ""Next"":""WorkflowFailed""
            }
         ],
         ""Retry"":[
            {
               ""ErrorEquals"":[
                  ""States.ALL""
               ],
               ""IntervalSeconds"":2,
               ""MaxAttempts"":3
            }
         ],
         ""Next"":""WorkflowSucceeded""
      },
```

### Collection configuration
To configure a collection to enable ORCA, add the line
`""granuleRecoveryWorkflow"": ""DrRecoveryWorkflow""` to the collection configuration:
```
{
  ""queriedAt"": ""2019-11-07T22:49:46.842Z"",
  ""name"": ""L0A_HR_RAW"",
  ""version"": ""1"",
  ""sampleFileName"": ""L0A_HR_RAW_product_0001-of-0420.h5"",
  ""dataType"": ""L0A_HR_RAW"",
  ""granuleIdExtraction"": ""^(.*)((\\.cmr\\.json)|(\\.iso\\.xml)|(\\.tar\\.gz)|(\\.h5)|(\\.h5\\.mp))$"",
  ""reportToEms"": true,
  ""createdAt"": 1561749178492,
  ""granuleId"": ""^.*$"",
  ""provider_path"": ""L0A_HR_RAW/"",
  ""meta"": {
    ""response-endpoint"": ""arn:aws:sns:us-west-2:012345678912:providerResponseSNS"",
    ""granuleRecoveryWorkflow"": ""DrRecoveryWorkflow""
  },
  ""files"": [
    {
```
### Enable `Recover Granule` button

To enable the `Recover Granule` button on the Cumulus Dashboard (available at github.com/nasa/cumulus-dashboard), 
set the environment variable `ENABLE_RECOVERY=true`.

Here is an example command to run the Cumulus Dashboard locally:
```
  APIROOT=https://uttm5y1jcj.execute-api.us-west-2.amazonaws.com:8000/dev ENABLE_RECOVERY=true npm run serve
```

## Release Documentation:

Information about how to create an ORCA release can be found [here](docs/release.md).


## ORCA Static Documentation

Nake sure you are using the following node.js versions to view the documentation.
- npm 6.14.10
- node 12.15.0

ORCA documentation can be read locally by performing the following:
```
cd website
npm install
npm run start
```

Once the server is running, documentation should be available on `http://localhost:3000`.
"
8,nasa/dorado-scheduling,Python,"# Dorado observation planning and scheduling simulations

[![Build Status](https://github.com/nasa/dorado-scheduling/actions/workflows/build-and-test.yml/badge.svg)](https://github.com/nasa/dorado-scheduling/actions)
[![Documentation Status](https://readthedocs.org/projects/dorado-scheduling/badge/?version=latest)](https://dorado-scheduling.readthedocs.io/en/latest/?badge=latest)
[![Codecov](https://img.shields.io/codecov/c/github/nasa/dorado-scheduling)](https://app.codecov.io/gh/nasa/dorado-scheduling)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dorado-scheduling)](https://pypi.org/project/dorado-scheduling/)

Dorado is a proposed space mission for ultraviolet follow-up of gravitational
wave events. This repository contains a simple target of opportunity
observation planner for Dorado.

This project is free and open source, but it calls commercial software: it uses
[IBM ILOG CPLEX Optimization Studio (""CPLEX"")][CPLEX] for mathematical
optimization. CPLEX is free for students, faculty, and staff at accredited
educational institutions through the [IBM Academic Initiative].

**To get started with dorado-scheduling, see the [quick start instructions] in
the [manual].**

![Example Dorado observing plan](examples/6.gif)

## Features

*   **Global**: jointly and globally solves the problems of tiling (the set of
    telescope boresight orientations and roll angles) and the scheduling (which
    tile is observed at what time), rather than solving each sub-problem one at
    a time
*   **Optimal**: generally solves all the way to optimality, rather than
    finding merely a ""good enough"" solution
*   **Fast**: solve an entire orbit in about 5 minutes
*   **General**: does not depend on heuristics of any kind
*   **Flexible**: problem is formulated in the versatile framework of
    [mixed integer programming]

## Dependencies

*   [Astropy]
*   [Astroplan] for calculating the field of regard
*   [HEALPix], [cdshealpix], and [astropy-healpix] for observation footprints
*   [sgp4] for orbit propagation
*   [CPLEX] (via [docplex] Python interface) for constrained optimization

[quick start instructions]: https://dorado-scheduling.readthedocs.io/en/latest/quickstart.html
[manual]: https://dorado-scheduling.readthedocs.io/
[mixed integer programming]: https://en.wikipedia.org/wiki/Integer_programming
[Astropy]: https://www.astropy.org
[Astroplan]: https://github.com/astropy/astroplan
[HEALPix]: https://healpix.jpl.nasa.gov
[astropy-healpix]: https://github.com/astropy/astropy-healpix
[cdshealpix]: https://github.com/cds-astro/cds-healpix-python
[sgp4]: https://pypi.org/project/sgp4/
[CPLEX]: https://www.ibm.com/products/ilog-cplex-optimization-studio
[IBM Academic Initiative]: https://www.ibm.com/academic/technology/data-science
"
9,nasa/cFS,CMake,"[![Build Status](https://travis-ci.com/nasa/cFS.svg)](https://travis-ci.com/nasa/cFS)
[![LGTM Alerts](https://img.shields.io/lgtm/alerts/github/nasa/cFS)](https://lgtm.com/projects/g/nasa/cFS/alerts/?mode=list)
[![LGTM Grade](https://img.shields.io/lgtm/grade/python/github/nasa/cFS)](https://lgtm.com/projects/g/nasa/cFS/alerts/?mode=list)
[![LGTM Grade](https://img.shields.io/lgtm/grade/cpp/github/nasa/cFS)](https://lgtm.com/projects/g/nasa/cFS/alerts/?mode=list)

# Core Flight System - BUNDLE

The Core Flight System (cFS) is a generic flight software architecture framework used on flagship spacecraft, human spacecraft, cubesats, and Raspberry Pi.  This repository is a bundle of submodules that make up the cFS framework.  Note the ""lab"" apps are intended as examples only, and enable this bundle to build, execute, receive commands, and send telemetry.  This is not a flight distribution, which is typically made up of the cFE, OSAL, PSP, and a selection of flight apps that correspond to specific mission requirements.

This bundle has not been fully verified as an operational system, and is provided as a starting point vs an end product.  Testing of this bundle consists of building, executing, sending setup commands and verifying receipt of telemetry.  Unit testing is also run, but extensive analysis is not performed.  All verification and validation per mission requirements is the responsibility of the mission (although attempts are made in the cFS Framework to provide a testing framework to facilitate the process).

The cFS Framework is a core subset of cFS.  There are additional OSALs, PSPs, and tools as listed below available from a variety of sources.

## References Documentation
  - cFE User's Guide: https://github.com/nasa/cFS/blob/gh-pages/cFE_Users_Guide.pdf
  - OSAL User's Guide: https://github.com/nasa/cFS/blob/gh-pages/OSAL_Users_Guide.pdf
  - cFE App Developer's Guide: https://github.com/nasa/cFE/blob/main/docs/cFE%20Application%20Developers%20Guide.md
  - Training documentation: https://ntrs.nasa.gov/citations/20205000691
  - cFS Overview: https://cfs.gsfc.nasa.gov/cFS-OviewBGSlideDeck-ExportControl-Final.pdf

## Release Notes

See [releases](https://github.com/nasa/cFS/releases) for release history and associated artifacts related to the cFS BUNDLE.

**Aquila: OFFICIAL RELEASE**:
  - Released under Apache 2.0
  - Includes cFE 6.7.0 (cFE, PSP, framework apps, and framework tools as marked) and OSAL 5.0.0

**cFS 6.6.0a Suite: OFFICIAL RELEASE**:
  - cFE 6.6.0a is released under Apache 2.0 license, see [LICENSE](https://github.com/nasa/cFE/blob/v6.6.0a/LICENSE-18128-Apache-2_0.pdf)
  - OSAL 4.2.1a is released under the NOSA license, see [LICENSE](https://github.com/nasa/osal/blob/osal-4.2.1a/LICENSE)
  - [Release notes](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_release_notes.md)
  - [Version description document](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_6_6_0_version_description.md)
  - [Test results](https://github.com/nasa/cFE/tree/v6.6.0a/test-and-ground/test-review-packages/Results)

Other elements listed below are released under a variety of licenses as detailed in their respective repositories.

## Known issues

Historical version description documents contain references to internal repositories and sourceforge, which is no longer in use.  Not all markdown documents have been updated for GitHub.

See related repositories for current open issues.

## Major future work

### Caelum (aka 7.0) Major release development plans (Targeting end of 2020 for release candidate)

  - Certification framework with automated build verification tests of framework requirements
    - Executable on real/emulated/simulated/ or dockerized targets
    - Add PSP coverage testing framework (nasa/psp#184, nasa/psp#174)
    - Add PSP and cFE functional testing framework for APIs  (nasa/cfe#779)
    - Scrub OSAL coverage and functional tests
    - Scrub cFE coverage tests
    - Add cFE API functional tests
    - NOTE: Command verification pending tool open source release
  - Documentation (updated traceability, APIs/ICDs, general update)
  - Framework for mission customization of core services
    - Header customization support (nasa/cFE#726)
  - Remove deprecated code
  - Cmd/Tlm structure scrub for alignment/padding/consistency
  - Library query and reporting and ES resource management (nasa/cFE#28, nasa/cFE#797)

### Other (may not make 7.0)
  - Open source automated build verification execution framework for emulated targets (likely docker based)
  - Deployment quality of life improvements (configuration, transition to CMake source selection vs compiler directives)
  - Update OS support (VxWorks 7, RTEMS 5)
  - Time services refactor
  - Symmetric multi-processing APIs
  - Electronic Data Sheet integration option and improvements to packet layouts for portability/consistency
  - Toolchain updates

## Getting Help

For best results, submit issues:questions or issues:help wanted requests to this repo.

Official cFS page: http://cfs.gsfc.nasa.gov

Community email list subscription request: https://lists.nasa.gov/mailman/listinfo/cfs-community

## Setup

Ensure the following software are installed: Make, CMake, GCC, and Git.  To setup the cFS BUNDLE directly from the latest set of interoperable repositories:

    git clone https://github.com/nasa/cFS.git
    cd cFS
    git submodule init
    git submodule update

Copy in the default makefile and definitions:

    cp cfe/cmake/Makefile.sample Makefile
    cp -r cfe/cmake/sample_defs sample_defs

## Build and Run

The cFS Framework including sample applications will build and run on the pc-linux platform support package (should run on most Linux distributions), via the steps described in https://github.com/nasa/cFE/tree/master/cmake/README.md.  Quick-start is below:

To prep, compile, and run on the host (from cFS directory above) as a normal user (best effort message queue depth and task priorities):

    make SIMULATION=native prep
    make
    make install
    cd build/exe/cpu1/
    ./core-cpu1

Should see startup messages, and CFE_ES_Main entering OPERATIONAL state.  Note the code must be executed from the build/exe/cpu1 directory to find the startup script and shared objects.

Note: The steps above are for a debug, permissive mode build and includes deprecated elements.  For a release build, recommendation is `make BUILDTYPE=release OMIT_DEPRECATED=true prep`.  Unit tests can be added with `ENABLE_UNIT_TESTS=true`, run with `make test`, and coverage reported with `make lcov`.

## Send commands, receive telemetry

The cFS-GroundSystem tool can be used to send commands and receive telemetry.  For details on using and setting up the Ground System, see the [Guide-GroundSystem](https://github.com/nasa/cFS-GroundSystem/blob/main/Guide-GroundSystem.md).  Note it depends on PyQt5 and PyZMQ:

1. Install PyQt5 and PyZMQ on your system.  Some systems may also require installing libcanberra-gtk-module.
       
2. Compile cmdUtil and start the ground system executable

       cd tools/cFS-GroundSystem/Subsystems/cmdUtil
       make
       cd ../..
       python3 GroundSystem.py

3. Select ""Start Command System""
4. Select ""Enable Tlm""
5. Enter IP address of system executing cFS, 127.0.0.1 if running locally

Should see telemetry, can send noops and see command counters increment.

## Compatible list of cFS apps

The following applications have been tested against this release:
  - TBD

## Other cFS related elements/tools/apps/distributions

The following list is user submitted, and not CCB controlled.  They are released by various organizations, under various licenses.

  - Distributions
    - cFS-101: Virtual machine distribution at https://github.com/nasa/CFS-101
    - OpenSatKit: Open source kit for satellite software at https://github.com/OpenSatKit/OpenSatKit
  - Other Ground station software
    - TBD
  - Other Apps
    - CS: Checksum application at https://github.com/nasa/CS
    - CF: CFDP application at https://github.com/nasa/CF
    - CI: Command Ingest application at https://github.com/nasa/CFS_CI
    - DS: Data Store application at https://github.com/nasa/DS
    - FM: File Manager application at https://github.com/nasa/FM
    - HK: Housekeeping application at https://github.com/nasa/HK
    - HS: Health and Safety application at https://github.com/nasa/HS
    - LC: Limit Checker application at https://github.com/nasa/LC
    - MD: Memory Dwell application at https://github.com/nasa/MD
    - MM: Memory Manager application at https://github.com/nasa/MM
    - SC: Stored Commands application at https://github.com/nasa/SC
    - SCA: Stored Command Absolute application at https://github.com/nasa/SCA
    - SCH: Scheduler application at https://github.com/nasa/SCH
    - TO: Telemetry Output application at https://github.com/nasa/CFS_TO
    - Skeleton App: A bare-bones application to which you can add your business logic at https://github.com/nasa/skeleton_app 
  - Other Interfaces
    - SIL: Simulink Interface Layer at https://github.com/nasa/SIL
    - ECI: External Code Interface at https://github.com/nasa/ECI
  - Other Libraries
    - cFS_IO_LIB: IO library at https://github.com/nasa/CFS_IO_LIB
    - cFS_LIB: at https://github.com/nasa/cfs_lib
  - Other Tools
    - CCDD: Command and Data Dictionary Tool at https://github.com/nasa/CCDD
    - Perfutils-java: Java based performance analyzer for cFS at https://github.com/nasa/perfutils-java
    - gen_sch_tbl: Tool to generated SCH app tables
  - Other OSALs
    - TBD
  - Other PSPs
    - TBD
  
"
10,nasa/ow_simulator,C++,"
# Ocean Worlds Autonomy Testbed for Exploration Research & Simulation (OceanWATERS)
[Overview](#overview) |
[Code Organization](#code-Organization) |
[Getting Started](#getting-started) |
[Contributing](#contributing) |
[License](#license)

## Overview
OceanWATERS is a physical and visual simulation of a lander on Europa. It is intended as a
testbed to aid in producing software that could fly on lander missions to ocean
worlds, such as Europa and Enceladus.


<p align=""center""><img width=""80%"" src=""oceanwaters/doc/lander_europa.jpg"" /></p>


## Code Organization

The [ow_simulator](https://github.com/nasa/ow_simulator) is the top level repository
for OceanWATERS. It primarily contains ROS/Gazebo packages related to visual and
physical simulation for OceanWATERS. It also contains workspace files for
setting up the rest of the OceanWATERS repositories:
- [ow_autonomy](https://github.com/nasa/ow_autonomy)
- [ow_europa](https://github.com/nasa/ow_europa)
- [irg_open](https://github.com/nasa/irg_open)

## Getting Started
- [Install software prerequisites](oceanwaters/doc/setup_dev_env.md)
- [Download and build OceanWATERS](oceanwaters/doc/setup_oceanwaters.md)
- [User Guide](https://github.com/nasa/ow_simulator/wiki/Using-OceanWATERS)

## Contributing
Please review [current bugs and features requests](https://github.com/nasa/ow_simulator/issues)
before submitting a new one. If we are unable to accomodate your request and you
want to contribute to this project yourself, follow these instructions:

Contributions must be the original work of the contributor with no conflicting
license or copyright restrictions. See our [license](LICENSE.txt) for more
details.

If you wish to contribute code or a bug fix please:
- Create your own fork of this repository. In the upper-right corner of the
[ow_simulator front page](https://github.com/nasa/ow_simulator) click `Fork`.
Your fork will be called `<your_username>/ow_simulator`.
- In your newly forked repository, create a branch with an appropriate name for
your feature or bug fix.
- Make changes to your new branch.
- Create a pull request against the `master` branch of `nasa/ow_simulator`.

## Citation
Coming soon

## License
OceanWATERS is open source software licensed under the
[NASA Open Source Agreement version 1.3](LICENSE.txt).

## Notices
Copyright ¬© 2020 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration.  All Rights Reserved.

## Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
11,nasa/NASA-Acronyms,JavaScript,"This expands over 25,000 NASA acronyms.

Search engine: https://nasaacronyms.com or https://nasaacronyms.com?[insert acronym]

Chrome: https://chrome.google.com/webstore/detail/nasa-acronyms/anpbkdhjbebjjkgdbglbcfaenjldbinf

Firefox: https://addons.mozilla.org/addon/nasa-acronyms

Authors: Joel Malissa & Logan Stafman
"
12,nasa/vscode-pvs,TypeScript,"# VSCode-PVS: An Integrated Development Environment for the Prototype Verification System

[![version](https://vsmarketplacebadge.apphb.com/version/paolomasci.vscode-pvs.svg)](https://marketplace.visualstudio.com/items?itemName=paolomasci.vscode-pvs)
[![installs](https://vsmarketplacebadge.apphb.com/installs-short/paolomasci.vscode-pvs.svg)](https://marketplace.visualstudio.com/items?itemName=paolomasci.vscode-pvs)
[![license](https://img.shields.io/badge/license-NASA-blue.svg)](https://opensource.org/licenses/NASA-1.3)
[![chat](https://img.shields.io/badge/Chat%20on-PVS%20Google%20Group-blue.svg)](https://groups.google.com/g/pvs-group)

VSCode-PVS is a new integrated development environment for creating, evaluating and verifying PVS specifications.
The environment redefines the way developers interact with PVS, and better aligns the PVS front-end to the functionalities provided by development environments used for programming languages such as C++ and Java.

<img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-screenshot.png"" width=""800"">

## Latest version
[vscode-pvs-1.0.47](releases/vscode-pvs-1.0.47.vsix)


## Documentation
- [Quick reference guide](https://github.com/nasa/vscode-pvs/blob/master/vscode-pvs/docs/USER-INTERFACE.md) for the VSCode-PVS User Interface
- [FAQs](https://github.com/nasa/vscode-pvs/blob/master/vscode-pvs/docs/FAQ.md) on VSCode-PVS, including troubleshooting techniques for basic problems.
- Are you new to PVS? Try out our [tutorial](https://github.com/nasa/vscode-pvs/blob/master/vscode-pvs/docs/TUTORIAL.md)!
- Join the new [PVS group on Google](https://groups.google.com/g/pvs-group)


## Requirements
- Linux or MacOS operating system
- NodeJS (v12.16.1 or greater) https://nodejs.org/en/download
- Visual Studio Code (v1.49.0 or greater) https://code.visualstudio.com

## Installation instructions
VSCode-PVS can be installed from the Visual Studio Code Marketplace or from GitHub.


**Automatic installation from Visual Studio Code Marketplace**
- Search `pvs` in https://marketplace.visualstudio.com and select `install`
<br><br><img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/how-to-install-vscode-pvs-from-marketplace.gif"" width=""600"">

**Manual installation from GitHub**
1. Download the .vsix file of VSCode-PVS from [github](https://github.com/nasa/vscode-pvs/raw/master/releases).
2. Click on the Extensions icon in the Activity Bar 
3. Click on the `...` menu in the title bar, and use `Install from VSIX` to select the downloaded .vsix file
<br><br><img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/how-to-install-vscode-pvs.gif"" width=""600"">

>Note: When installing VSCode-PVS for the first time, it will check if PVS Allegro v7.1.0 is present on your system. If not, VSCode-PVS will show a dialog that allows you to download PVS.

>Note: If you have used earlier releases of VSCode-PVS (`< 1.0.26`), you will need to download and install the latest version of PVS and NASALib. You can do this in VSCode-PVS, with the commands `M-x install-pvs` and `M-x install-nasalib`. 

**Manual installation of PVS**

Should VSCode-PVS fail to install PVS, please install PVS manually: Download PVS Allegro v7.1.0 for [MacOS](https://pvs.csl.sri.com/license.html?tgzfile=pvs7.1.0-ix86_64-Linux-allegro.tgz) or [Linux](https://pvs.csl.sri.com/license.html?tgzfile=pvs-6.0-ix86_64-Linux-allegro.tgz) and follow the installation instructions reported in the `INSTALL` file included in the downloaded package. Once PVS is installed on your machine, you can link up PVS and VSCode-PVS by indicating the location of the PVS executables in the VSCode-PVS settings.


## Updating VSCode-PVS
VSCode-PVS will be automatically updated every time we publish a new release in the marketplace.

All settings and preferences will be maintained when installing a new release.

If you would like to perform manual updates or try out nightly builds, you can download the .vsix file from [github](https://github.com/nasa/vscode-pvs/raw/master/releases).


## Publications
Paolo Masci and C√©sar Mu√±oz, [An Integrated Development Environment for the Prototype Verification System](https://dx.doi.org/10.4204/EPTCS.310.5), Electronic Proceedings in Theoretical Computer Science (EPTCS), Vol. 310, pp. 35-49, 2019 [[PDF](https://arxiv.org/pdf/1912.10632v1)]


## Functionalities
The main functionalities provided by the environment are as follows:
- **Syntax highlighting**: PVS keywords and library functions are automatically highlighted.
- **Autocompletion and code snippets**: Tooltips suggesting function names and language keywords are automatically presented in the editor when placing the mouse over a symbol name. Code snippets are provided for frequent modeling blocks, e.g., if-then-else. 
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-autocompletion.gif"" width=""600"">

- **Hover information for symbol definitions**: Hover boxes providing information about identifiers are automatically displayed when the user places the cursor over an identifier.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-hover.gif"" width=""600"">

- **Go-to definition**: Click on the name of the identifier while holding down the Ctrl key to jump to the location where the identifier is declared.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-goto-definition.gif"" width=""600"">

- **Live diagnostics**: Parsing is automatically performed in the background, and errors are reported in-line in the editor. Problematic expressions are underlined with red wavy lines. Tooltips presenting the error details are shown when the user places the cursor over the wavy lines.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-live-diagnostics.gif"" width=""600"">

- **Workspace Explorer**: Interactive tree view showing all theories in the current workspace, name and status of theorems and typecheck conditions.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-theory-explorer.gif"" width=""600"">

- **Proof Explorer + Prover Console**: Interactive tree view for viewing and editing the current proof. A prover console allows interaction with the theorem prover. Auto-completion is provided (using the TAB key) for prover commands, as well as access to the commands history.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-proof-explorer.gif"" width=""600"">

- **Plot Executable Functions**: Executable functions that return a list of real numbers can be rendered in a plot diagram.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-plot-expression.gif"" width=""600"">

- **Search NASALib**: Search definitions and theorems in [NASALib](https://github.com/nasa/pvslib), an extensive PVS library developed and maintained by the NASA Langley Formal Methods Team.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-search-nasalib.gif"" width=""600"">

- **Prototype Builder**: Build interactive visual prototypes to demonstrate the behavior of executable PVS specifications.
<br><br> <img src=""https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-rapid-prototyping.gif"" width=""600"">

<br>

## Structure
```
.
‚îú‚îÄ‚îÄ client                       // PVS Language Client (VSCode entry point)
‚îÇ   ‚îî‚îÄ‚îÄ src
‚îÇ       ‚îú‚îÄ‚îÄ providers            // Client-side service providers
‚îÇ       ‚îú‚îÄ‚îÄ views                // User interface components
‚îÇ       ‚îú‚îÄ‚îÄ common               // Utility functions 
‚îÇ       ‚îî‚îÄ‚îÄ pvsLanguageClient.ts // PVS Language Client implementation
‚îú‚îÄ‚îÄ icons                        // PVS icons theme
‚îú‚îÄ‚îÄ package.json                 // The extension manifest
‚îú‚îÄ‚îÄ syntax                       // Syntax highlighting
‚îú‚îÄ‚îÄ LICENSES                     // NASA Open Source License Agreement
‚îú‚îÄ‚îÄ Makefile                     // Makefile for building a .vsix image from the source code
‚îî‚îÄ‚îÄ server                       // PVS Language Server
    ‚îî‚îÄ‚îÄ src
        ‚îú‚îÄ‚îÄ providers                          // Server-side service providers
        ‚îÇ     ‚îú‚îÄ‚îÄ pvsCodeLensProvider.ts       // In-line actionable commands
        ‚îÇ     ‚îú‚îÄ‚îÄ pvsCompletionProvider.ts     // Auto-completion
        ‚îÇ     ‚îú‚îÄ‚îÄ pvsDefinitionProvider.ts     // Find definitions
        ‚îÇ     ‚îú‚îÄ‚îÄ pvsHoverProvider.ts          // Hover information 
        ‚îÇ     ‚îú‚îÄ‚îÄ pvsProofExplorer.ts          // Proof tree editor
        ‚îÇ     ‚îî‚îÄ‚îÄ pvsPackageManager.ts         // Installation manager 
        ‚îú‚îÄ‚îÄ parser               // Parser grammar and scripts      
        ‚îú‚îÄ‚îÄ common               // Utility functions           
        ‚îú‚îÄ‚îÄ pvsCli.ts            // PVS Command Line Interface
        ‚îú‚îÄ‚îÄ pvsProcess.ts        // PVS process wrapper
        ‚îú‚îÄ‚îÄ pvsLisp.ts           // Lisp reader for parsing PVS responses
        ‚îî‚îÄ‚îÄ pvsLanguageServer.ts // PVS Language Server implementation
```

<br>

## Notices
### Copyright 
Copyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
 
### Disclaimers
**No Warranty**: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY
  WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,
  INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE
  WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF
  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
  INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
  FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO
  THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER,
  CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT
  OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY
  OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.
  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
  REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
  AND DISTRIBUTES IT ""AS IS.""
 
**Waiver and Indemnity**: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS
  AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND
  SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF
  THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,
  EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM
  PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT
  SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
  STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
  PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE
  REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL
  TERMINATION OF THIS AGREEMENT.


## Contacts
* Paolo Masci (NIA) (paolo.masci@nianet.org)
* Cesar Munoz (NASA LaRC) (cesar.a.munoz@nasa.gov)

"
13,nasa/osal,C,"![Static Analysis](https://github.com/nasa/osal/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/osal/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : Operating System Abstraction Layer

This repository contains NASA's Operating System Abstraction Layer (OSAL), which is a framework component of the Core Flight System.

This is a collection of abstractio APIs and associated framework to be located in the `osal` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes build and execution instructions.

The autogenerated OSAL user's guide can be viewed at <https://github.com/nasa/cFS/blob/gh-pages/OSAL_Users_Guide.pdf>.

## Version History

### Development Build: v5.1.0-rc1+dev417

- Fixes infinite loop  in `UtPrintx()`. Adds the data's memory address to output. Note, UtPrintf displays the the file/line of the `UtPrintx` function, **not the actual test location**; it is better to call `UT_BSP_DoText` directly.
- Adds `OS_SocketShutdown()` API wrapper around BSD's socket shutdown() API. This allows a data transfer of a stream socket to be gracefully shut down prior to socket closure.
- See osal <https://github.com/nasa/osal/pull/979> and <https://github.com/nasa/cFS/pull/252>

### Development Build: v5.1.0-rc1+dev411

- [docs] Clarifies that that zero will be returned on EOF condition in the API documentation for OS_read/write/TimedRead/TimedWrite. In the case of the timed API calls, the `OS_ERR_TIMEOUT` status code will be returned if the timeout expired without the handle becoming readable/writable during that time.
- Addresses a shortcomings in the UT Assert hook functions. Namely the assumed return type of int32 which is not always the case.
- Adds the concept of a ""handler"" function to UT assert to replace hard-coded custom logic in UT assert. A handler is the custom logic that exists between the hook function and the return to the stub caller. The handler is directly responsible for setting all outputs.
- Adds a script to auto-generate stub functions that match this pattern. Given an API header file, the script extracts
the declarations, and generates a source file with stub definitions that rely on a separate handler to deal with the needed outputs.
- Refactors `os-shared-printf.h`) into two parts to improve the compatibility with the script method.
- Updates all existing stubs in OSAL to use the auto-generated stub logic from the script, created directly from the C header. This ensures that stubs will match the FSW implementation.
- [continuous-integration] Adds a local osal-specific makefile to help build unit tests. Adds a new github workflow that runs the unit tests in both the context of the bundle configuration and the local OSAL config. Verifies 100% line coverage.
- Fixes incorrect token use in `OS_SocketAccept`. Enables the `network-api-test` to handle multiple connections that re-use the same acceptor socket between them.
- Promotes the `OS_CONFIG_CONSOLE_ASYNC` option into the shared layer to remove duplicate implementation code and add more coverage testing.
- Adds an osconfig option to allow the user to elect this mode at configuration time.


### Development Build: v5.1.0-rc1+dev393

- Changes parameter names to avoid collisions. Renames `access` as `access_mode` in `osapi-file.h`. Renames `time` as `TimeSp` in `os-impl-posix-gettime.c`.
- Deletes the broken RTEMS `os-impl-shell.c` file so so OSAL builds with `OSAL_CONFIG_INCLUDE_SHELL=true`. RTEMS will always report `OS_ERR_NOT_IMPLEMENTED`.
- See <https://github.com/nasa/osal/pull/967> and <https://github.com/nasa/cFS/pull/248>

### Development Build: v5.1.0-rc1+dev387

- Replaces the separate ""Initialized"" and ""Shutdown"" flags with a single state flag. Creates a global single source of truth for the OSAL state. This enables users to run tests and OS_API_Init() multiple times without a reboot in the middle to reset the state.
  - Multiple invocations of OS_API_Init() are allowed - subsequent calls can be ignored
  - Deleting of any internal objects that did get created if OS_API_Init() fails (this leaves system in same state as when it started)
  - Allows Re-initialization of OSAL after OS_ApplicationShutdown() - may be relevant when running unit tests several times without rebooting.
- Adds OS_API_Teardown to complement OS_API_Init. This cleans up all OSAL resources ideally leaving the system in a state where `OS_API_Init()` may be invoked again.
- Reworks the shell unit test which was probably not working. Note this requires modifying the osal config to enable shell, otherwise test is skipped.
- See <https://github.com/nasa/osal/pull/956> and <https://github.com/nasa/cFS/pull/242>

### Development Build: v5.1.0-rc1+dev378

- Replaces nonstandard header file block comments and include guards. No behavior changes
- Removes `CLOCK_MONOTONIC` as the osal colck source since PSP no longer needs it. `OS_GetLocalTime()` and `OS_SetLocalTime()` will work as described.
- Replaces `shellName` with a specific `localShellName` that can be polled safely and changes its type to a char of `OS_MAX_API_NAME` length for safety.
- See <https://github.com/nasa/osal/pull/951> and <https://github.com/nasa/cFS/pull/238>

### Development Build: v5.1.0-rc1+dev367

- Removes `SOFTWARE_BIG_BIT_ORDER` and `SOFTWARE_LITTLE_BIT_ORDER` macros from `common_types.h`. These are not needed by OSAL and cannot handle all cases.  Application code with endianness dependency that was relying on these symbols may break. Users should leverage code in cFE: `cfe_endian.h`. See <https://github.com/nasa/cFE/pull/1218> for more details.
- Applies minor code and documentation cleanup: white space, typos, etc.
- Adds test to get full coverage of vxworks in `os-impl-bsd-socket.c` resulting in full line coverage for OSAL
- Adds more descriptive return codes if `OS_SymbolTableDump_Impl` does not do what is expected. Adds a new error `OS_ERR_OUTPUT_TOO_LARGE` if the size limit was insufficient. Return `OS_ERROR` if an empty file was written - this likely indicates some fundamental issue with the VxWorks symbol table. Returns `OS_ERR_NAME_TOO_LONG` if one of the symbol names was too long. Improves unit test to check for/verify these responses.
- Removes the unneeded `OS_TaskRegister()` and all references to it in code, tests, and documentation. No impact to behavior, but does affect API and has depenedencies
- Removes unused `-SCRIPT_MODE` flag in cmake
- Remove comparison between `osal_id_t` and `integers` to use the provided comparison function, `OS_ObjectIdDefined()`. System builds and runs again when using a type-safe/non-integer osal_id_t type.
- See <https://github.com/nasa/osal/pull/927>

### Development Build: v5.1.0-rc1+dev350

- Moves copyblock size to a macro and add comments. Defines `OS_CP_BLOCK_SIZE` and adds clear documentation that it could be adjusted for page size, performance, etc.
- Removes while loop
- Replaces all #includes of <os and <OSC_ matches with "" to match coding standard.
- Consolidates the duplicated switch in `OS_SocketOpen_Impl`
- Add const to input pointers for `OS_FdSet_ConvertIn_Impl` and `OS_ObjectIdTransactionFinish`
- Removes network prototypes defined in `osapi_sockets.h` that are also in `osapi_network.h`
- Removes `NULL` redefine from `common_types.h`
- Adds `Contributing.md` that points to bundle-level contribution guide
- Reports test cases that ""fail"" as ""not implemented"" with new `UtAssert_NA` macro instead of `UtPrintf`
- Calls to `OS_SelectSingle` and `OS_SelectMultiple` will fail if an FD within the set is outside the range of the underlying `FD_SETSIZE` from the C library.
- Fixes calculation used for the relative time interval in the `select()` call. Also adds a UT case that specifically exercises the carryover described. Fixes delay when this carry condition is hit
- Documents algorithm that provides application-controlled timeout on the connection initiation. Also adds a debug statement if the connect fails for a reason other than `EINPROGRESS`. No impact to normal behavior.
- Adds check for `EAGAIN` if the system fails to allocate kernel-internal resources.
- Adds a `CompileTimeAssert` to confirm that the size of the abstract buffer for socket addresses is large enough to store any of the enabled address types thereby removing the need for runtime tests.
- With this change, if `OS_SOCKADDR_MAX_LENis` not large enough for the address type, osal will fail to compile. This enforces that the abstract size is large enough for any/all enabled address types, regardless of what is actually used.
- Adds missing functional test for `OS_ShellOutputToFile`
- Add test for `fcntl()` error return of -1 and report errno. If setting `O_NONBLOCK` fails, then debug message is printed and blocking mode is used and timeouts will not work as a result.
- Improves error codes when attempting to seek on a pipe/socket. Translates the `OS_ERR_OPERATION_NOT_SUPPORTED` error rather than ""not implemented"". The `ESPIPE` errno means that seeking is not supported on the given file handle.
- Renames `OS_U32ValueWrapper_t` as `OS_VoidPtrValueWrapper_t` to better indicate its purpose. The point is to pass a value through a `void*`. Adds a compile-time assert to check that this is only used to directly pass values which have a size of less than or equal to sizeof(void*).
- Refactors the return statement for `OS_FileSys_FindVirtMountPoint()` so it is easier to read and adds some informational comments.
- Reports an error if calling `timer_gettime` after `timer_settime` fails.
- Returns `OS_ERROR` status to caller after an error on moduleInfoGet()
- Removes an extraneous/unreachable OS_ObjectIdDefined check and its accompanying debug statement. The only way this check could have been reached would be if the normal unlock process was bypassed such that the underlying OS mutex was unlocked but OSAL state still had it owned by a task. This condition never happens at runtime.
- Updates documentation for `OS_MAX_MODULE`
- See <https://github.com/nasa/osal/pull/917>

### Development Build: v5.1.0-rc1+dev297

- Fix #836, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/osal/pull/838>

### Development Build: v5.1.0-rc1+dev293

- Avoids various ""possible uninitialized variable"" warnings for routines that utilize this API.
- Renames `sockaddr*` structures to `sa*` to deconflict from structure name in `os-impl-bsd-sockets.c`. Adds `OS_NETWORK_SUPPORTS_IPV6` to `os-impl-bsd-sockets.c` compilation. Renames `bsd-select-stubs.c` to `sys-select-stubs.c`. Coverage now includes all currently possible files in VxWorks build
- Resolves CodeQL security warning by restricting permissions on file create.
- Changes comments using ""cpp"" comment style to ""c"" style
- Adds _new_ accessor functions APIs to get version strings and return the values of string macros defined in `osapi-version.h`.
  - The ""simple"" version currently `OS_VERSION` macro - this is the semantic version without any extra detail.  This is returned by `OS_GetVersion()`.
  - The ""descriptive"" version in `OS_VERSION_STRING` macro - this has extra detail like the most recent official release.  This is returned by `OS_GetVersionDescription()`.
  - The release code name, now returned by `OS_GetVersionDescription()`.  
- These accessor functions are the preferred way to get the OSAL version string, from now on users should avoid using the macro definitions as it is evaluated at OSAL library compile time, rather than application compile time, and thus will remain correct in the event that OSAL is relinked without recompiling the application.
Adds `osapi-version.c` to implement these 3 calls and associated coverage test. This allows the version.c file to be auto-generated in the future.
- See <https://github.com/nasa/osal/pull/835>

### Development Build: v5.1.0-rc1+dev280

- Makes tests skip after getting their first not implemented error.
- Updates stub helpers to match the behavior of calling the default implementation stub macro (NULL VA list)
- Removes redundant logic and assignment to fix static analysis warnings
- Truncates at the end of the logic flow for socket name as opposed to possibly 3 different locations. Fixes static analysis warning.
- Renames `timer_id` in unit tests to `local_timer_id` to avoid conflicts
- Removes all deprecated elements
- No behavior change. Renames `UT_Stub_CheckForceFail` to `UT_Stub_CheckDefaultReturnValue`, also only sets `Value` if not `NULL` (can pass in NULL value doesn't matter)
- See <https://github.com/nasa/osal/pull/830>

### Development Build: v5.1.0-rc1+dev262

- Adds test cases for `OS_ObjectIdFinalizeDelete`, `OS_DeleteAllObjects`, and others to get 100% line and function coverage on VxWorks and shared/portable layers.
- Ensures APIs check for `NULL` inputs or have documentation stating that a null value is allowed.
- Adds timeout to static analysis check and adds format check. Removes old .travis.yml and updates badges in readme.
- Adds Code QL analysis on push to main and pull requests (to main)
- Cleans commented-out code in tests to address static analysis warnings
- Initializes local variables to avoid returning uninitialized values from stubs and address static-analysis findings
- Replaces two local return codes defined as `uint32` with `int32` to resolve static-analysis warnings
- Simplifies switch statements based on previous checks. Removes unreachable, dead code to resolve static-analysis warnings
- Terminates  unit test macros variadic lists with `NULL` to address CWE-121 CodeQL warning
- Adds a check to send the semaphore to avoid unreachable code
- Adds a status return to `OS_ConsoleAPI_Init` so debug warnings will get reported correctly on errors.
- Declares `DummyVec` as static to avoid warning and returning stack allocated memory when returning `VecTbl` in `OSC_INUM_TO_IVEC` stub
- Updates types in `os-impl-no-symtab.c` to match latest APIs
- Updates types in `os-impl-no-symtab.c` to match latest APIs
- Fixes missing `NULL` terminations and applies the standard ""sizeof"" pattern where appropriate. No longer truncates filename in `OS_ModuleInfo`.
- Fixes `%u` conversion in RTEMS so to address build warning
- Create a wrapper around `memchr()` that mimics the non-C99 function `strnlen()` defined in POSIX-2008. Use this instead of `strlen()` whenever the string being checked either originates in or will be copied into a fixed-length array buffer. No behavior changes except if a bug causes strings to be unterminated.
- No behavior change, applies the standard formatting using `clang-format`
- See <https://github.com/nasa/osal/pull/774>

### Development Build: v5.1.0-rc1+dev221

- Fixes `printf` format to correctly build in RTEMS-5.
- **Deprecates `OS_fsBlocksFree()` and `OS_fsBytesFree()`** in favor of `OS_FileSysStatVolume()`.
- Adds `Security.md` with instructions to report vulnerabilities.
- Add `UtDebug` in `OS_printf` stub. Output the `OS_printf` input as a debug message from stub.
- Documentation: Add note on `UtTest_Add` API. Nesting `UtTest_Add` from within an added test fails without error.
- Unit Test: No more garbage characters written to test report log
- Fix typo in `osapi.h` affecting C++ build. No other functional change
- Unit Test: Rename `UT_ClearForceFail` as `UT_ClearDefaultValue`. Update the comments of `UT_SetDefaultReturnValue` to match the more general function.
- Unit Test: Add test teardown failures to the test summary and changed the printout to use the same style as startup failures.
- Unit Test: Removes no longer applicable `UT_CheckForOpenSockets` since the UT framework resets the state for each unit test.
- Changes the file-create operation to read-write permissions to work on RTEMS
- Unit Test: Fixes incorrect assertions in `network-api-test` to correctly check return values.
- Unit Test: Generalizes queue timeout test to also test message queue functionality to validate settings and permissions to work with mqueues.
- Implements `OS_time_t` with a single 64-bit tick counter rather than a split 32 bit seconds + 32 bit microseconds counter.
- Unit Test: Installs the modules used in unit testing and adds removal of post-test, left-over files.
- See <https://github.com/nasa/osal/pulls/767>

### Development Build: v5.1.0-rc1+dev184

- Address issues with OSAL global table management:
  - use iterators whenever possible
  - use an unlock key rather than task ID so OS_TaskExit() doesn't trigger a warning
  - general cleanup of lock/unlock impl and remove redundant logic
  - unlock global tables during create/delete
  - keep threads ""attached"" in POSIX, so they can be joined when deleted.
- No longer triggers warning with OS_TaskExit() on VxWorks (see #645)
- `OS_TaskDelete()` on POSIX does not return until the task has actually exited (see #642)
- The chmod test is now skipped on VxWorks rather than failing. The `OS_FileChmod_Impl()` function now returns `OS_ERR_NOT_IMPLEMENTED` when run on a file system that does not have permissions, which in turn causes the unit test to be skipped rather than fail.   
- Corrects a file handle leak.
-  Add parameter check to `OS_SocketSendTo` and adjust coverage test to validate.
- Replace `OS_fsBytesFree` and `OS_fsBlocksFree` with `OS_FileSysStatVolume`. This new API for getting stats on file system. Uses existing `OS_FileSysStatVolume_Impl` call and exposes it in the public API.
- When cleaning up for shutdown, delete resources that have a task/thread first, followed by other resource types. This helps avoid possible dependencies as running threads might be using the other resources. No detectable external impact; internally, the tasks are deleted first during shutdown, which only has an impact if/when tasks are actively using other OSAL resources.
- The mount/unmount *VxWorks* implementation was not adequately checking for and handling the `FS_BASED` pass -through mapping type - which should be mostly a no-op. Create a mount point directory if it does not already exist when using this mapping type for consistency with POSIX.
- Adds a documentation note to `OS_FileSysAddFixedMap()`: The virtual mount point cannot be empty - so `OS_FileSysAddFixedMap(.., ""/"", ""/"")` does not work but `OS_FileSysAddFixedMap(.., ""/"", ""/root"")` does work and allows one to open files in the root as `""/root/""` from OSAL applications. Mount-point directories do not need to previously exist when using OS_FileSysAddFixedMap
- store `taskTCB` return in a `void *`, then cast to `OS_impl_task_internal_record_t *` to avoid a strict alignment compiler error
- Removes the non-portable `OS_PACK` and `OS_ALIGNED` macros.
- Uses the POSIX dir implementation on VxWorks 6.9. The only incompatibility is the prototype for `mkdir()`which is missing the second argument; this is worked around with a compatibility macro for VxWorks 6.x builds.
- Translate and convert the VxWorks coverage test cases to the portable dir implementation, which benefits VxWorks7, RTEMS, and POSIX.
- Fixes prototypes so they run on RTEMS by replacing uint32 with size_t
- Adds` OS_CHECK_POINTER` macros to `OS_ConvertToArrayIndex` and `OS_TimeBaseGetFreeRun` so they can handle NULL pointers and return the correct error.
- Adds access functions to convert/extract different units from an OS_time_t value - so that other code in CFE/PSP/Apps can be updated to use the access functions and thereby not break when the internal time definition changes. Replaces the `int32` with `OS_time_t` in the ""stat"" structure used by the file module. Updates the pointer argument to `OS_SetLocalTime()` to be `const`. Prototype change of `OS_SetLocalTime()` should be backward compatible.
- See <https://github.com/nasa/osal/pulls/750>

### Development Build: v5.1.0-rc1+dev149

- Document UtAssert_Message parameters, also adds ""see also"" note for helper macros.
- Fix doxygen typo
- Replace `OS_BinSemFlush` with `OS_BinSemGive` to prevent a rare race condition. Change the port numbers to be different from network test for when tests are run in parallel.
- Fix doxygen format errors. Usersguide now builds without warnings.
- Suppress invalid cppcheck warning in `OS_WaitForStateChange`
- Add cppcheck static analysis workflow to osal CI
- See <https://github.com/nasa/osal/pull/744>

### Development Build: v5.1.0-rc1+dev132

- Convert the OSAL Configuration Guide from docx and pdf to a markdown file.
- Test Tasks do not run at 100%. Move all definitions and instantiations out of the core-test header file and reuse the already-existing single task definition.
- Break up `osapi-os-*.h` files into units that correspond to the implementation units. Kept old header file names for compatibility.
- Reworks the POSIX global lock implementation. Does not change the POSIX signal mask when locking/unlocking the global.
  - Fixes a race condition.
  - Adds a condition variable to the global lock structure. improves handling of tasks competing for access to the same object.
  - No longer changing signal masks repeatedly/unexpectedly. May be relevant to some BSP/driver developers.
- Checks return of sysconf for error and reports them. Only sets PageSize on success. If sysconf fails it provides a mechanism to avoid error propagation.
- Uses `errno` instead of status return from `clock_getres` with `strerror` reporting.
- Adds support for VxWorks 7
- See <https://github.com/nasa/osal/pull/690>

### Development Build: v5.1.0-rc1+dev109

- Add support for RTEMS 5.1 in the OSAL and provides defines and necessary ifdefs so RTEMS 4.11 can continue to be supported.
- Adds functional test for OS_chmod
- Refactor the table array access across OSAL. Use a token concept in combination with a macro to obtain the table entry instead of indexing arrays directly. All access is then done through this table pointer. Use the full object ID in the timer call back list. Update the timer sync callback prototype. Pass the entire OSAL ID to the sync function, not just the index. This is technically an API change.
- Replaces condition on forever loops to end on shutdown. Loops now exit on shutdown.
- Removes obsolete printf tests that didn't work
- See <https://github.com/nasa/osal/pull/680>


### Development Build: v5.1.0-rc1+dev91

- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing
- Add a 5th timer to TimerTest functional to test the one shot (zero-length time interval) case.
- Ensure all APIs use the proper type. Sizes are now size_t; these will now be 64 bits on a 64 bit platform.
- Fix build issue on VxWorks 6.9 by using the 3 argument form of `open()`. Passing `0` as the mode solves the build issue. This parameter is ignored when not creating a file.
-  The address calculations now use `unsigned long` instead of `long` to ensure that all rounding and base address adjustments behave the same way in the event that the addresses lie in the upper half of memory (i.e. start with a 1 bit) which would put it in the negative range of a long type.
- See <https://github.com/nasa/osal/pull/662>


### Development Build: v5.1.0-rc1+dev75

- Ensure that the handle is not NULL before invoking dlclose(). In particular the handle will be NULL for static modules. Shutdown after CTRL+C occurs normally (no segfault).
- Add a ""flags"" parameter to OS_ModuleLoad() to indicate the desired symbol visibility:
    - GLOBAL (0, the default, and matches current behavior)
    - LOCAL which hides from other modules and prevents other modules from binding to symbols in this module, thereby ensuring/preserving the ability to unload in the future
  - CFE should use LOCAL flag for apps, and GLOBAL flags for libraries.
- See <https://github.com/nasa/osal/pull/652>

### Development Build: v5.1.0-rc1+dev68

- When `OS_DEBUG` is enabled, this adds a message if mutex give/take actions occur outside the expected sequence. This informs the user (via the debug console) if a lock is taken more than once or if a lock is given by a different task than the one that originally took it:
```
OS_MutSemTake():216:WARNING: Task 65547 taking mutex 327685 while owned by task 65547
```
- Removes all FIXME comments
- Resolves security/filename race issue by opening file and acting on descriptor by adding fstat stub
- Squashed the minor recommended bugs
- UtAssert macros now accept variable string arguments.The `UtAssert_True` wrapper around call is no longer needed to accommodate dynamic string output, thus removing the double assert. UtAssert macros will now be able to offer more information by themselves.
- See <https://github.com/nasa/osal/pull/639>

### Development Build: v5.1.0-rc1+dev60

- Appliy standard formating, whitespace-only changes
- See <https://github.com/nasa/osal/pull/627>

### Development Build: v5.1.0-rc1+dev55

- Deprecate `OS_open` and `OS_creat` to and replaced them with by `OS_OpenCreate`, which implements both functions via flags, and follows the correct OSAL API patterns.
- Change use of uint32 for ID to the correct typedef. Also use ObjectIdFromInteger/ObjectIdToInteger where it is intended to convert these values to integers e.g. for the ""name"" fields in RTEMS.
- See <https://github.com/nasa/osal/pull/621>

### Development Build: v5.1.0-rc1+dev49

- Adds an event callback mechanism to certain state changes in OSAL. This allows the CFE PSP to be notified at these points, and therefore it can add platform-specific functionality.
- Correct issues involving recent OS_Milli2Ticks change.
- See <https://github.com/nasa/osal/pull/612>

### Development Build: v5.1.0-rc1+dev44

- Removes OS_Tick2Micros and internalize OS_Milli2Ticks.
- Adds ut_assert address equal macro.
- See <https://github.com/nasa/osal/pull/607>

### Development Build: v5.1.0-rc1+dev38

- Sets Revision to 99 for development builds
- See <https://github.com/nasa/osal/pull/600>

### Development Build: v5.1.0-rc1+dev34

- Move this existing function into the public API, as it is performs more verification than the OS_ConvertToArrayIndex function.
- The C library type is signed, and this makes the result check work as intended.
- See <https://github.com/nasa/osal/pull/596>


### Development Build: v5.1.0-rc1+dev16

 - In the next major OSAL release, this code will be no longer supported at all. It should be removed early in the cycle to avoid needing to maintain this compatibility code. This code was already conditional on the OSAL_OMIT_DEPRECATED flag and as such the CCB has already tested/verified running the code in this configuration as part of CI scripts. After this change, the build should be equivalent to the result of building with OMIT_DEPRECATED=true.
- See <https://github.com/nasa/osal/pull/582>

### Development Build: v5.1.0-rc1+dev12

- Removes internal functions that are no longer used or defined but whose prototypes and stubs were still present in OS_ObjectIdMap
- Removes repetitive clearing of the global ID and unlocking global table and replaces these with common implementation in the idmap source file. This moves deleting tables to be similar to creating tables and provides
a common location for additional table-deletion-related logic.
- Propagates return code from OS_TaskRegister_Impl(). If this routine fails then return the error to the caller, which also prevents the task from starting.
- See <https://github.com/nasa/osal/pull/576>

### Development Build: v5.1.0-rc1+dev5

- Adds OSAL network APIs missing functional tests as well as tests for OS_TimedRead and OS_TimedWrite
- Allows separate, dynamic registration of test setup and teardown routines which are executed before and after the normal test routine, which can create and delete any global/common test prerequisites.
- Adds FileSysAddFixedMap missing functional API test
- See <https://github.com/nasa/osal/pull/563>


### Development Build: 5.0.0+dev247

- `OS_SocketOpen()` sets `sock_id` and returns a status when successful.
- Changed timer-test to be able to use OS_MAX_TIMERS value on top of the hard-coded NUMBER_OF_TIMERS value. This will allow the test to be functional even if the OS_MAX_TIMERS value is reconfigured.
- Ensures that
  - All stub routines register their arguments in the context, so that the values will be available to hook functions.
  - The argument names used in stubs match the name in the prototype/documentation so the value can be retrieved by name.
- Adds back rounding up to PTHREAD_STACK_MIN and also adds rounding up to a system page size. Keeps check for zero stack at the shared level; attempts to create a task with zero stack will fail. Allows internal helper threads to be created with a default minimum stack size.
- Avoids a possible truncation in snprintf call. No buffer size/truncation warning when building with optimization enabled.
- Added new macros to `osapi-version` to report baseline and build number
- The coverage binaries are now correctly installed for CPU1 and CPU2 as opposed to installed twice to CPU2 but not at all for CPU1.
- Fixes a typo in ut_assert README and clarifies stub documentation.
- See <https://github.com/nasa/osal/pull/529>

### Development Build: 5.0.21

- Command line options in Linux are no longer ignored/dropped.
- No impact to current unit testing which runs UT assert as a standalone app. Add a position independent code (PIC) variant of the ut_assert library, which can be dynamically loaded into other applications rather than running as a standalone OSAL application. This enables loading
UT assert as a CFE library.
- Unit tests pass on RTEMS.
- Resolve inconsistency in how the stack size is treated across different OS implemntations. With this change the user-requested size is passed through to the underlying OS without an enforced minimum. An additional sanity check is added at the shared layer to ensure that the stack size is never passed as 0.
- Update Licenses for Apache 2.0
- See <https://github.com/nasa/osal/pull/521>

### Development Build: 5.0.20
-  Add ""non-zero"" to the out variable description for OS_Create (and related) API's.
- Increases the buffer for context info from 128 to 256 bytes and the total report buffer to 320 bytes.
- Add stub functions for `OS_TaskFindIdBySystemData()`, `OS_FileSysAddFixedMap()`, `OS_TimedRead()`, `OS_TimedWrite()`, and `OS_FileSysAddFixedMap()`
- Added the following wrappers macros around `UtAssert_True` for commonly-used asserts:
  - `UtAssert_INT32_EQ` - check equality as 32 bit signed int
  - `UtAssert_UINT32_EQ` - check equality as 32 bit unsigned int
  - `UtAssert_NOT_NULL` - check pointer not null
  - `UtAssert_NULL` - check pointer is null
  - `UtAssert_NONZERO` - check integer is nonzero
  - `UtAssert_ZERO` - check integer is zero
  - `UtAssert_STUB_COUNT` - check stub count
-  Using `unsigned long` instead of `uintmax_t` to fix support for VxWorks

- See <https://github.com/nasa/osal/pull/511> and <https://github.com/nasa/osal/pull/513>

### Development Build: 5.0.19

- Rename BSPs that can be used on multiple platforms.
`mcp750-vxworks` becomes `generic-vxworks`
`pc-linux` becomes `generic-linux`
- New features only, does not change existing behavior.
UT Hook functions now have the capability to get argument values by name, which is more future proof than assuming a numeric index.
- Add functional test for `OS_TimerAdd`
- Added functional tests for `OS_TimeBase Api` on `OS_TimeBaseCreate`, `OS_TimeBaseSet`, `OS_TimeBaseDelete`, `OS_TimeBaseGetIdByName`, `OS_TimeBaseGetInfo`, `OS_TimeBaseGetFreeRun`
- See <https://github.com/nasa/osal/pull/487> for details


### Development Build: 5.0.18

- Add functional tests for `OS_IdentifyObject`, `OS_ConvertToArrayIndex` and `OS_ForEachObject` functions.
- Fix doxygen warnings
- Unit test cases which use `OS_statfs` and run on an `RTEMS IMFS` volume will be skipped and categorized as ""NA"" due to `OS_ERR_NOT_IMPLEMENTED` response, rather than a failure.
- The device_name field was using the wrong length, it should be of `OS_FS_DEV_NAME_LEN` Also correct another length check on the local path name.
- For RTEMS, will not shutdown the kernel if test abort occurs.
- Unit tests work on RTEMS without BSP preallocating ramdisks
- If `OSAL_EXT_SOURCE_DIR` cache variable is set, this location will be checked first for a BSP/OS implementation layer.
- Implement `OS_GetResourceName()` and `OS_ForEachObjectOfType()`, which are new functions that allow for additional query capabilities. No impact to current behavior as the FSW does not currently use any of these new APIs.
- A functional test enhancement to `bin-sem-test` which replicates the specific conditions for the observed bug to occur. Deletes the task calling `OS_BinSemTake()` and then attempts to use the semaphore after this.
- Employ a `pthread` ""cleanup handler"" to handle the situation where a task is canceled during the `pthread_cond_wait()` call. This ensures that the `mutex` is unlocked as part of the cleanup, so other tasks may continue using the semaphore.    
- Change all initial `mutex` locking to be a finite ""timed"" wait rather than an infinite wait. In all cases, the condition variable is only held for brief periods of time and should be readily available. If a task blocks for a long time, this considers the mutex ""broken"" and aborts, thereby avoiding deadlock. This is a ""contingency"" fix in that if an exception or signal or other unknown/unhandled async event occurs that leaves the mutex permanently locked.
- Adds the mutex to protect the timer callback `timecb` resource table.
- See <https://github.com/nasa/osal/pull/482>

### Development Build: 5.0.17

-  `OS_QueueCreate()` will return an error code if the depth parameter is larger than the configured `OS_MAX_QUEUE_DEPTH`.
- See <https://github.com/nasa/osal/pull/477>

### Development Build: 5.0.16

- Resized buffers and added explicit termination to string copies. No warnings on GCC9 with strict settings and optimization enabled.
- New API to reverse lookup an OS-provided thread/task identifier back to an OSAL ID. Any use of existing OStask_id field within the task property structure is now deprecated.
- See <https://github.com/nasa/osal/pull/458>

### Development Build: 5.0.15

- Changes the build system.
- No more user-maintained osconfig.h file, this is now replaced by a cmake configuration file.
- Breaks up low-level implementation into small, separate subsystem units, with a separate header file for each one.
- See <https://github.com/nasa/osal/pull/444>

### Development Build: 5.0.14

- Adds library build, functional, and coverage test to CI
- Deprecates `OS_FS_SUCCESS, OS_FS_ERROR , OS_FS_ERR_INVALID_POINTER, OS_FS_ERR_NO_FREE_FDS , OS_FS_ERR_INVALID_FD, and OS_FS_UNIMPLEMENTED` from from `osapi-os-filesys.h`
- Individual directory names now limited to OS_MAX_FILE_NAME
- Fix tautology, local_idx1 is now compared with local_idx2
- Module files are generated when the `osal_loader_UT` test is built and run
- Consistent osal-core-test execution status
- See <https://github.com/nasa/osal/pull/440> for more details

### Development Build: 5.0.13

- Added coverage test to `OS_TimerCreate` for `OS_ERR_NAME_TOO_LONG`.
- Externalize enum for `SelectSingle`, ensures that pointers passed to `SelectFd...()` APIs are not null, ensures that pointer to `SelectSingle` is not null.
- Command to run in shell and output to fill will fail with default (not implemented) setting.
- Builds successfully using the inferred OS when only `OSAL_SYSTEM_BSPTYPE` is set. Generates a warning when `OSAL_SYSTEM_BSPTYPE` and `OSAL_SYSTEM_OSTYPE` are both set but are mismatched.
- See <https://github.com/nasa/osal/pull/433> for more details

### Development Build: 5.0.12

- Use the target_include_directories and target_compile_definitions functions from CMake to manage the build flags per target.
- Build implementation components using a separate CMakeLists.txt file rather than aux_source_directory.
- Provide sufficient framework for combining the OSAL BSP, UT BSP, and the CFE PSP and eliminating the duplication/overlap between these items.
- Minor updates (see <https://github.com/nasa/osal/pull/417>)

### Development Build: 5.0.11

- The more descriptive return value OS_ERR_NAME_NOT_FOUND (instead of OS_FS_ERROR) will now be returned from the following functions (): OS_rmfs, OS_mount, OS_unmount, OS_FS_GetPhysDriveName
- Wraps OS_ShMem* prototype and unit test wrapper additions in OSAL_OMIT_DEPRECATED
- Minor updates (see <https://github.com/nasa/osal/pull/408>)

### Development Build: 5.0.10

- Minor updates (see <https://github.com/nasa/osal/pull/401>)

  - 5.0.9: DEVELOPMENT

- Documentation updates (see <https://github.com/nasa/osal/pull/375>)

### Development Build: 5.0.8

- Minor updates (see <https://github.com/nasa/osal/pull/369>)

### Development Build: 5.0.7

- Fixes memset bug
- Minor updates (see <https://github.com/nasa/osal/pull/361>)

### Development Build: 5.0.6

- Minor updates (see <https://github.com/nasa/osal/pull/355>)

### Development Build: 5.0.5

- Fixed osal_timer_UT test failure case
- Minor updates (see <https://github.com/nasa/osal/pull/350>)

### Development Build: 5.0.4

- Minor updates (see <https://github.com/nasa/osal/pull/334>)

### Development Build: 5.0.3

- Minor updates (see <https://github.com/nasa/osal/pull/292>)

### Development Build: 5.0.2

- Bug fixes and minor updates (see <https://github.com/nasa/osal/pull/281>)

### Development Build: 5.0.1

- Minor updates (see <https://github.com/nasa/osal/pull/264>)

### **_OFFICIAL RELEASE: 5.0.0 - Aquila_**

- Changes are detailed in [cFS repo](https://github.com/nasa/cFS) release 6.7.0 documentation
- Released under the Apache 2.0 license

### **_OFFICIAL RELEASE: 4.2.1a_**

- Released under the [NOSA license](https://github.com/nasa/osal/blob/osal-4.2.1a/LICENSE)
- See [version description document](https://github.com/nasa/osal/blob/osal-4.2.1a/OSAL%204.2.1.0%20Version%20Description%20Document.pdf)
- This is a point release from an internal repository

# Quick Start:

Typically OSAL is built and tested as part of cFS as detailed in: [cFS repo](https://github.com/nasa/cFS)

OSAL library build pc-linux example (from the base osal directory):
```
mkdir build_osal
cd build_osal
cmake -DOSAL_SYSTEM_BSPTYPE=generic-linux ..
make
```

OSAL permissive build with tests example (see also [CI](https://github.com/nasa/osal/blob/master/.travis.yml))
```
mkdir build_osal_test
cd build_osal_test
cmake -DENABLE_UNIT_TESTS=true -DOSAL_SYSTEM_BSPTYPE=generic-linux -DOSAL_CONFIG_DEBUG_PERMISSIVE_MODE=TRUE ..
make
make test
```

See the [Configuration Guide](https://github.com/nasa/osal/blob/master/doc/OSAL-Configuration-guide.pdf) for more information.

See also the autogenerated user's guide: <https://github.com/nasa/cFS/blob/gh-pages/OSAL_Users_Guide.pdf>

## Known issues

See all open issues and closed to milestones later than this version.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
14,nasa/cmr-stac,HTML,"# CMR STAC API Proxy

An implementation of the [SpatioTemporal Asset Catalog API](https://github.com/radiantearth/stac-spec) on top of NASA's [Common Metadata Repository](https://cmr.earthdata.nasa.gov/search/).

Deployed at [https://cmr.earthdata.nasa.gov/stac/](https://cmr.earthdata.nasa.gov/stac/)

There is more detailed documentation in the [docs](docs/readme.md) folder of this repository.

## Development Quick Start

### Prerequisites

* `node.js`
* `nvm`: node version manager. Installing via nvm is the best way, see the appropriate version of node.js in `.nvmrc`.
* AWS CLI

### Setup

This application is a service that supports the STAC proxy. It is organized as an NPM module and will install all dependencies if you run the following command:

```bash
# checks for and installs if missiong the necessary version of node.js
nvm install
npm install
```

### Running locally

```bash
npm start
```

This will run the process in the current terminal session. Via browser or another terminal session you can find the entrypoint to the STAC collections:

```
open http://localhost:3000/dev/stac
```

### Deploying

- cd `search`
- `npm run deploy -- --stage <sit|uat|prod> --cmr-search-host <cmr-search-host> --cmr-search-protocol <http|https>`

## License

The full license can be found [here](./LICENSE.txt)
"
15,nasa/fprime,C++,"# F¬¥: A Flight-Proven, Multi-Platform, Open-Source Flight Software Framework

[![Language grade: C++](https://img.shields.io/lgtm/grade/cpp/g/nasa/fprime.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/fprime/context:cpp)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/nasa/fprime.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/fprime/context:python)
[![Language grade: JavaScript](https://img.shields.io/lgtm/grade/javascript/g/nasa/fprime.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/fprime/context:javascript)

**F¬¥ (F Prime)** is a component-driven framework that enables rapid development and deployment of spaceflight and other embedded software applications. Originally developed at the [Jet Propulsion Laboratory](https://www.jpl.nasa.gov/), F¬¥ has been successfully deployed on several space applications. It is tailored but not limited to small-scale spaceflight systems such as CubeSats, SmallSats, and instruments.

**Please Visit the F¬¥ Website:** [https://nasa.github.io/fprime/](https://nasa.github.io/fprime/).  This website contains project information, user guides, documentation, tutorials, and more!

F¬¥ comprises several elements: 

* An architecture that decomposes flight software into discrete components with well-defined interfaces
* A C++ framework that provides core capabilities such as message queues and threads
* Modeling tools for specifying components and connections and automatically generating code
* A growing collection of ready-to-use components
* Testing tools for testing flight software at the unit and integration levels.

## Quick Installation Guide

The following utilities are prerequisites to installing F¬¥:

- [cmake](https://cmake.org/)
- [git](https://git-scm.com/)
- [Python](https://www.python.org/) 3.6+ with pip

Once these utilities are installed, you can install F¬¥ Python dependencies. Installing dependencies in a Python virtual environment prevents issues at the system level, but installing in a virtual environment is not required. 

To install F¬¥ quickly, enter:

```
git clone https://github.com/nasa/fprime.git
cd fprime
pip install --upgrade wheel setuptools pip
pip install Fw/Python Gds/
```

For full installation instructions, including virtual environment creation and installation verification, see [INSTALL.md](./docs/INSTALL.md). 

## Example Deployments

F¬¥ comes with two example deployments. The deployments represent working F¬¥ applications to help you understand F¬¥. You can use these examples for reference, or clone them to start a new project. 

The next section links to more step-by-step tutorials, but it's a good idea to build and run at least the first example deployment to ensure that F¬¥ is installed correctly.

**Example one:** [Ref](./Ref/README.md)

   The standard reference application demonstrates how most of the system components should be wired together. The reference application can build on Linux or macOS, allowing you to get started immediately without the need for embedded hardware.

**Example two:** [RPI](./RPI/README.md)


This Raspberry PI application shows how to run F¬¥ in an embedded context by running on the Raspberry PI (a $35 embedded Linux computer). This application shows you how to get started on embedded projects with cross-compiling, drivers, and more. The Raspberry Pi was chosen because it is commercially available for a low price and runs Linux.

## Tutorials

F¬¥ provides several tutorials in order to help understand and develop within the framework. These tutorials cover basic component creation, system and topology design, tooling, and more. These tutorials are available at [docs/Tutorials/README.md](./docs/Tutorials/README.md).

## Getting Help with F¬¥

As F¬¥ becomes a community centered product line, there are more items available from the community at large. 

You can join the mailing list at [https://groups.google.com/d/forum/fprime-community](https://groups.google.com/g/fprime-community).

The F¬¥ community GitHub Organization contains third party contributions, more documentation of flight software development, and more! [https://github.com/fprime-community](https://github.com/fprime-community).

You can open issues with this repository at: [https://github.com/nasa/fprime/issues](https://github.com/nasa/fprime/issues)

## F¬¥ Features

F¬¥ has the following key features that enable robust embedded system design.

### Reusability

F¬¥'s component-based architecture enables a high degree of modularity and software reusability.

### Rapid Deployment

F¬¥ provides a complete development ecosystem, including modeling tools, testing tools, and a ground data system. Developers use the modeling tools to write high-level specifications, automatically generate implementations in C++, and fill in the implementations with domain-specific code. The framework and the code generators provide all the boilerplate code required in an F¬¥ deployment, including code for thread management, code for communication between components, and code for handling commands, telemetry, and parameters. The testing tools and the ground data system simplify software testing, both on workstations and on flight hardware in the lab.

### Portability

F¬¥ runs on a wide range of processors, from microcontrollers to multicore computers, and on several operating systems. Porting F¬¥ to new operating systems is straightforward.

### High Performance

F¬¥ utilizes a point-to-point architecture. The architecture minimizes the use of computational resources and is well suited for smaller processors.
	
### Adaptability

F¬¥ is tailored to the level of complexity required for small missions. This makes F¬¥ accessible and easy to use while still supporting a wide variety of missions.

### Analyzability

The typed port connections provide strong compile-time guarantees of correctness.

## F¬¥ Release Notes

#### Release 1.0: 

 * This is the initial release of the software to open source. See the license file for terms of use.

#### Release 1.01

 * Updated contributor list. No code changes. 

#### Release 1.1

 * Created a Raspberry Pi demo. Read about it [here](RPI/README.md)
 * Added a tutorial [here](docs/Tutorials/README.md)
 * Updated Svc/BufferManager with bug fix
 * Fixed a bunch of shell permissions
 
#### Release 1.2

* Better MagicDraw Plugin
* Prototype CMake build system. See: [CMake Documentation](./docs/UsersGuide/cmake/cmake-intro.md)
* Mars Helicopter Project fixes migrated in
* Python 3 support added
* Gse refactored and renamed to Gds
* Wx frontend to Gds
* UdpSender and UdpReceiver components added
* Purged inaccurate ITAR and Copyright notices
* Misc. bug fixes

#### Release 1.3

* New prototype HTML GUI
* Python packages Fw/Python and Gds
* Refined CMake and fprime-util helper script
* Better ground interface component
* Integration test API
* Baremetal components

#### Release 1.4

* Ref app no longer hangs on Linux exit
* GDS improvements:
  * File Uplink and Downlink implemented
  * GDS supports multiple active windows
  * Usability improvements for EVRs and commands
* CMake improvements:
  * Baremetal compilation supported
  * Random rebuilding fixed
  * Missing Cheetah templates properly rebuild
  * Separate projects supported without additional tweaks
* Updated MemAllocator to have:
  * ""recoverable"" flag to indicate if memory was recoverable across boots
  * size variable is now modifiable by allocator to indicate actual size
  * This will break existing code that uses MemAllocator
* Updated CmdSequencer
  * Uses new MemAllocator interface  

#### Release 1.5

* Documentation improvements
  * New user's guide containing considerable content: [https://nasa.github.io/fprime/UsersGuide/guide.html](https://nasa.github.io/fprime/UsersGuide/guide.html)
  * Auto-generated API documentation
  * Rewrites, edits, improvements across the board
* F¬¥ Project restructuring
  * Projects may now link to F¬¥ and F¬¥ library packages, without needing to keep the framework code in the same source tree
  * Usage of framework can be out-of-source
  * `settings.ini` Introduced
  * Example: [https://github.com/fprime-community/fprime-arduino](https://github.com/fprime-community/fprime-arduino)
* Refactored `fprim-util`
  * Replaced redundant targets with flags e.g. build-ut is now build --ut
  * Added `info` command
  * Bug and usability fixes
* GDS Improvements
  * Prototype GDS CLI tool
  * Project custom dashboard support
* Array, Enum type support and examples
* Code linting and bug fixes

"
16,nasa/cumulus-dashboard,JavaScript,"# Cumulus Dashboard

Code to generate and deploy the dashboard for the Cumulus API.

## Documentation

- [Configuration](#configuration)
- [Quick start](#quick-start)
- [Dashboard development](#dashboard-development)
- [Run the dashboard](#run-the-dashboard)
- [Deployment](#deployment)
- [Testing](#testing)
- [Create a Dashboard Release](#create-a-dashboard-release)


Other pages:
- [Usage](https://github.com/nasa/cumulus-dashboard/blob/master/USAGE.md)
- [Development Guide](https://github.com/nasa/cumulus-dashboard/blob/master/DEVELOPMENT.md)
- [Technical documentation on tables](https://github.com/nasa/cumulus-dashboard/blob/master/TABLES.md)

## Configuration

The dashboard is populated from data retrieved from the Cumulus API. The environment for the Cumulus API must be predetermined and set before the dashboard can be built and deployed. The information needed to configure the dashboard is found in `app/src/js/config/config.js`, but it is generally preferred to set environmental variables overriding the default values during the build process.

The following environment variables override the default values.

| Env Name | Description | Default |
| -------- | ----------- | -------- |
| APIROOT | The API URL. This must be set by the user. | *example.com* |
| AUTH_METHOD | The type of authorization method protecting the Cumulus API. [launchpad or earthdata] | *earthdata*  |
| AWS\_REGION | Region in which Cumulus API is running. | *us-west-2*  |
| DAAC\_NAME | An identifier: e.g. LPDAAC, | *Local* |
| ENABLE\_RECOVERY | If true, adds recovery options to the granule and collection pages. | *false* |
| ESROOT | \<optional\> Should point to an Elasticsearch endpoint. Must be set for distribution metrics to be displayed. | |
| ES\_PASSWORD | \<optional\> Elasticsearch password,needed when protected by basic authorization | |
| ES\_USER | \<optional\> Elasticsearch username, needed when protected by basic authorization | |
| HIDE\_PDR | Whether to hide (or show) the PDR menu. | *true* |
| KIBANAROOT | \<optional\> Should point to a Kibana endpoint. Must be set to examine distribution metrics details. | |
| LABELS | Choose `gitc` or `daac` localization. | *daac* |
| SHOW\_DISTRIBUTION\_API\_METRICS | \<optional\> Display metrics from Cumulus Distribution API.| *false* |
| SHOW\_TEA\_METRICS | \<optional\> Display metrics from Thin Egress Application (TEA). | *true* |
| STAGE | Identifier displayed at top of dashboard page: e.g. PROD, UAT | *development* |

## Quick start

### Get dashboard source code
The dashboard source is available on github and can be cloned with git.

```bash
  $ git clone https://github.com/nasa/cumulus-dashboard
```
The cloned directory `./cumulus-dashboard` will be refered as the root directory of the project and commands that are referenced in this document, should start from that directory.

### Build the dashboard using Docker and Docker Compose

It is easy to build a producution-ready, deployable version of the Cumulus dashboard without having to learn the complicated build process details.  A single script, `./bin/build_dashboard_via_docker.sh`, when combined with your dashboard's environment customizations, allows you to run the entire build process within a Docker container.

All of the environment variables in the [configuration](#configuration) section are available to override with custom values for your dashboard.  A recommended method is to store your variables in a sourceable environment file for each dashboard you are going to build and deploy.

If you are using bash, export the values for each configuration option. An example `production.env` could look like:
```sh
# production.env
export APIROOT=https://afakeidentifier.cloudfront.net
export DAAC_NAME=MY-DAAC
export STAGE=production
export HIDE_PDR=false
```

All values are optional except `APIROOT` which must point to the Cumulus API that the dashboard will connect to.

Set the environment and build the dashboard with these commands:
```sh
  $ source production.env && ./bin/build_dashboard_via_docker.sh
```

This script uses Docker Compose to build and copy the a compiled dashboard into the `./dist` directory. You can now deploy this directory to AWS behind [CloudFront](https://aws.amazon.com/cloudfront/).  If you are in NGAP, follow the instructions for ""Request Public or Protected Access to the APIs and Dashboard"" on the earthdata wiki page [Using Cumulus with Private APIs](https://wiki.earthdata.nasa.gov/display/CUMULUS/Cumulus+Deployments+in+NGAP).


### Run the dashboard locally via Docker Image

You can also create a Docker container that will serve the dashboard behind a simple nginx configuration. Having a runnable Docker image is useful for testing a build before deployment or for NGAP Sandbox environments, where if you configure your computer to [access Cumulus APIs via SSM](https://wiki.earthdata.nasa.gov/display/CUMULUS/Accessing+Cumulus+APIs+via+SSM), you can run the dashboard container locally against the live Sandbox Cumulus API.

The script `./bin/build_dashboard_image.sh` will build a docker image containing the dashboard bundle served behind a basic [nginx](https://www.nginx.com/) configuration. The script takes one optional parameter, the tag to name the generated image which defaults to cumulus-dashboard:latest.  The same customizations as described in the [previous section](#build-the-dashboard-using-docker-and-docker-compose) are available to configure your dashboard.

Example of building and running the project in Docker:
```bash
  $ source production.env && ./bin/build_dashboard_image.sh cumulus-dashboard:production-1
```

That command builds a Docker image with the name `cumulus-dashboard` and tag `production-1`. This image can be run in Docker to serve the Dashboard.

```bash
  $ docker run --rm -it -p 3000:80 cumulus-dashboard:production-1
```
In this example, the dashboard would be available at `http://localhost:3000/` in any browser.

--------

## Dashboard Development

### Build the dashboard

The dashboard uses node v12.18.0. To build/run the dashboard on your local machine, install [nvm](https://github.com/creationix/nvm) and run `nvm install v12.18.0`.

#### install requirements
We use npm for local package management, to install the requirements:
```bash
  $ nvm use
  $ npm install
```

To build a dashboard bundle<sup>[1](#bundlefootnote)</sup>:

```bash
  $ nvm use
  $ [SERVED_BY_CUMULUS_API=true] [DAAC_NAME=LPDAAC] [STAGE=production] [HIDE_PDR=false] [LABELS=daac] APIROOT=https://myapi.com npm run build
```
**NOTE**: Only the `APIROOT` environment variable is required and any of the environment varaibles currently set are passed to the build.

The compiled dashboard files (dashboard bundle) will be placed in the `./dist` directory.

#### Build dashboard to be served by CloudFront

If you wish to serve the dashboard from behind [CloudFront](https://aws.amazon.com/cloudfront/).  Build a `dist` with your configuration including `APIROOT` and ensure the `SERVED_BY_CUMULUS_API` variable is unset. For NGAP uers, follow the documentation to *Request Public or Protected Access to the APIs and Dashboard*, Step 5 of [Cumulus Deployments in NGAP](https://wiki.earthdata.nasa.gov/display/CUMULUS/Cumulus+Deployments+in+NGAP).

#### Build dashboard to be served by the Cumulus API.

It is possible to [serve the dashboard](https://nasa.github.io/cumulus-api/#serve-the-dashboard-from-a-bucket) with the Cumulus API. If you need to do this, you must build the dashboard with the environment variable `SERVED_BY_CUMULUS_API` set to `true`.  This configures the dashboard to work from the Cumulus `dashboard` endpoint.  This option should **only** be considered when you can't serve the dashboard from behind CloudFront, for example in an NGAP Sandbox environment. *NOTE: Your dashboard bucket must be in the bucket definitions in your Cumulus `terraform.tfvars`, otherwise you will not be able to access the bucket from the API.*


#### Build a specific dashboard version

`cumulus-dashboard` versions are distributed using tags in GitHub. You can select specific version in the following manner:

```bash
  $ git clone https://github.com/nasa/cumulus-dashboard
  $ cd cumulus-dashboard
  $ git fetch origin ${tagNumber}:refs/tags/${tagNumber}
  $ git checkout ${tagNumber}
```

Then follow the steps noted above to build the [dashboard locally](#build-the-dashboard) or [using Docker](#quick-start).

It is also possible to visit the repository at https://github.com/nasa/cumulus-dashboard/releases and download the source code bundle directly without cloning the repository.

## Run the dashboard

### Run the dashboard with hot reload
During development you can run the webpack development webserver to serve the dashboard while you are developing. When you run the dashboard this way, the compiled code in `./dist` is ignored, and the source code is served by the webpack-dev-server, which will watch for changes to the source and recompile as files are changed. Make sure you have [installed the requirements](#install-requirements) and then:

```bash
APIROOT=http://<myapi>.com npm run serve
```
The dashboard should be available at http://localhost:3000

### Run a built dashboard

To run a built dashboard, first [build the dashboard](#build-the-dashboard), then run:

```bash
  $ npm run serve:prod
```
This runs a node http-server in front of whatever exists in the `./dist` directory.  It's fast, but will not pick up any changes as you are working.

## Deployment

### Using S3

First, [build the dasbboard](#build-the-dashboard). Then deploy the `./dist` folder, the dashboard bundle, to an AWS bucket.

```bash
  $ aws s3 sync dist s3://my-bucket-to-be-used
```
If you are not in an NGAP environment, Look at the instructions for [Hosting a static website on Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html)
and [configuring a bucket as a static website](https://docs.aws.amazon.com/AmazonS3/latest/dev/HowDoIWebsiteConfiguration.html).

Otherwise, follow the instructions for building and deploying the dashboard for [cloudfront](#build-dashboard-to-be-served-by-cloudfront) or [the Cumulus API](#build-dashboard-to-be-served-by-the-cumulus-api).


## Testing

### Unit Tests

```bash
  $ npm run test
```

### Integration & Validation Tests

For the integration tests to work, you have to first run the localstack application, launch the localAPI and serve the dashboard first. Run the following commands in separate terminal sessions:

Run background localstack application.
```bash
  $ npm run start-localstack
```

Serve the cumulus API (separate terminal)
```bash
  $ npm run serve-api
```

Serve the dashboard web application (another terminal)
```bash
  $ [HIDE_PDR=false SHOW_DISTRIBUTION_API_METRICS=true ENABLE_RECOVERY=true ESROOT=http://example.com APIROOT=http://localhost:5001] npm run serve
```

If you're just testing dashboard code, you can generally run all of the above commands as a single docker-compose stack.
```bash
  $ npm run start-dashboard
```
This brings up LocalStack, Elasticsearch, the Cumulus localAPI, and the dashboard.

Run the test suite (yet another terminal window)
```bash
  $ npm run cypress
```

When the cypress editor opens, click on `run all specs`.


### Local API server

For **development** and **testing** purposes only, you can run a Cumulus API locally. This requires `docker-compose` in order to stand up the Docker containers that serve Cumulus API.  There are a number of commands that will stand up different portions of the stack.  See the [Docker Service Diagram](#dockerdiagram) and examine the `docker-compose*.yml` file in the `/localAPI/` directory to see all of the possible combinations. Described below are each of the provided commands for running the dashboard and Cumulus API locally.

*Important Note: These `docker-compose` commands do not build distributable containers, but are a provided as testing conveniences.  The docker-compose[-\*].yml files show that they work by linking your local directories into the container.*

In order to run the Cumulus API locally you must first [build the dashboard](#buildlocally) and then run the containers that provide LocalStack and Elasticsearch services.

These are started and stopped with the commands:
```bash
  $ npm run start-localstack
  $ npm run stop-localstack
```

After these containers are running, you can start a cumulus API locally in a terminal window `npm run serve-api`, the dashboard in another window. `[HIDE_PDR=false SHOW_DISTRIBUTION_API_METRICS=true ESROOT=http://example.com APIROOT=http://localhost:5001] npm run serve` and finally cypress in a third window. `npm run cypress`.

Once the Docker app is running, If you would like to see sample data you can seed the database. This will load the same sample data into the application that is used during cypress testing.
```bash
  $ npm run seed-database
```

If you prefer to stand up more of the stack in Docker containers, you can include the cumulus api in the docker-compose stack. To run the Cumulus API in a Docker container, (which still leaves running the dashboard and cypress up to you), just run the `cumulusapi` service.

The cumulusapi Docker service is started and stopped:
```bash
  $ npm run start-cumulusapi
  $ npm run stop-cumulusapi
```

the start command, will exit successfully long before the stack is actually ready to run.
The output looks like this:
```bash
> cumulus-dashboard@2.0.0 start-cumulusapi /Users/savoie/projects/cumulus/cumulus-dashboard
> docker-compose -f ./localAPI/docker-compose.yml -f ./localAPI/docker-compose-serve-api.yml up -d

Creating localapi_shim_1 ... done
Creating localapi_elasticsearch_1 ... done
Creating localapi_localstack_1    ... done
Creating localapi_serve_api_1     ... done
```
In order to find out that the stack is fully up and ready to receive requests, you can run the command `npm run view-docker-logs` to follow the progress of the stack.  When the Docker logs have shown the following:
```bash
serve_api_1      | Starting server on port 5001
```
and
```bash
localstack_1     | Ready.
```
you should be able to verify access to the local Cumulus API at http://localhost:5001/token


Then you can run the dashboard locally (without Docker) `[HIDE_PDR=false SHOW_DISTRIBUTION_API_METRICS=true ESROOT=http://example.com APIROOT=http://localhost:5001] npm run serve` and open cypress tests `npm run cypress`.

The Docker compose stack also includes a command to let a developer start all development containers with a single command.

Bring up and down the entire stack (the localAPI and the dashboard) with:
```bash
  $ npm run start-dashboard
  $ npm run stop-dashboard
```
This runs everything, the backing Localstack and Elasticsearch containers, the local Cumulus API and dashboard.  Edits to your code will be reflected in the running dashboard.  You can run cypress tests still with `npm run cypress`.  As a warning, this command takes a very long time to start up because the containers come up in a specific order and generally this should be reserved for use by Earthdata Bamboo or some other continuous intergration service.  But, if you are using it locally, **be sure to wait until all containers are fully up** before trying to visit the dashboard which is exposed at http://localhost:3000
The stack is ready when the `view-docker-logs` task shows:
```bash
dashboard_1      | > NODE_ENV=production http-server dist -p 3000 --proxy http://localhost:3000?
dashboard_1      |
dashboard_1      | Starting up http-server, serving dist
dashboard_1      | Available on:
dashboard_1      |   http://127.0.0.1:3000
dashboard_1      |   http://172.18.0.2:3000
dashboard_1      | Unhandled requests will be served from: http://localhost:3000?
dashboard_1      | Hit CTRL-C to stop the server
```


##### Troubleshooting Docker containers.

If something is not running correctly, or you're just interested, you can view the logs with a helper script, this will print out logs from each of the running docker containers.
```bash
  $ npm run view-docker-logs
```
This can be helpful in debugging problems with the docker application.

A common error is running the dashboard containers when the cumulus core unit-test-stack is running on your machine.  Just stop that stack and restart the dashboard stack to resolve.
```sh
ERROR: for localapi_shim_1  Cannot start service shim: driver failed programming external connectivity on endpoint localapi_shim_1 (7105603a4ff7fbb6f92211086f617bfab45d78cff47232793d152a244eb16feb): Bind for 0.0.0.0:9200 failed: port is already allocated

ERROR: for shim  Cannot start service shim: driver failed programming external connectivity on endpoint localapi_shim_1 (7105603a4ff7fbb6f92211086f617bfab45d78cff47232793d152a244eb16feb): Bind for 0.0.0.0:9200 failed: port is already allocated
```

#### Fully contained cypress testing.

You can run all of the cypress tests locally that Earthdata Bamboo runs with a single command:
```bash
  $ npm run e2e-tests
```
This stands up the entire stack as well as begins the e2e service that will run all cypress commands and report an exit code for their success or failure.  This is primarily used for CI, but can be useful to developers.


#### <a name=dockerdiagram></a> Docker Container Service Diagram.
![Docker Service Diagram](./ancillary/DashboardDockerServices.png)


## develop vs. master branches

The `master` branch is the branch where the source code of HEAD always reflects the latest product release. The `develop` branch is the branch where the source code of HEAD always reflects the latest merged development changes for the next release.  The `develop` branch is the branch where we should branch off.

When the source code in the develop branch reaches a stable point and is ready to be released, all of the changes should be merged back into master and then tagged with a release number.

## Create a Dashboard Release

### 1. Checkout `develop` branch

We will make changes in the `develop` branch.

### 2. Create a new branch for the release

Create a new branch off of the `develop` branch for the release named `release-vX.X.X` (e.g. `release-v1.3.0`).

### 3. Update the version number

When changes are ready to be released, the version number must be updated in `package.json`.

### 4. Update the minimum version of Cumulus API if necessary

See the `minCompatibleApiVersion` value in `app/src/js/config/index.js`.

### 5. Update CHANGELOG.md

Update the CHANGELOG.md. Put a header under the 'Unreleased' section with the new version number and the date.

Add a link reference for the GitHub ""compare"" view at the bottom of the CHANGELOG.md, following the existing pattern. This link reference should create a link in the CHANGELOG's release header to changes in the corresponding release.

Check to make sure there are `Breaking Changes` and `All Changes` section for the release if there are breaking changes. e.g. a new version of Cumulus API is required.

### 6. Update the version of the Cumulus API

If this release corresponds to a Cumulus Core package release, update the version of `@cumulus/api` to the latest package version so that the integration tests will run against that version.

### 7. Manual testing

Test the dashboard against a live API deployed with the latest Cumulus packages. The dashboard should be served from an S3 bucket through the [`/dashboard` API endpoint](https://nasa.github.io/cumulus-api/#serve-the-dashboard-from-a-bucket).

### 8. Create a pull request against the develop branch

Create a PR for the `release-vX.X.X` branch against the `develop` branch. Verify that the Earthdata Bamboo CI build for the PR succeeds and then merge to `develop`.

### 9. Create a pull request against the master branch

Create a PR for the `develop` branch against the `master` branch. Verify that the Earthdata Bamboo CI build for the PR succeeds and then merge to `master`.  Do not create a squash merge, but use a merge commit.

### 10. Create a git tag for the release

Push a new release tag to Github. The tag should be in the format `v1.2.3`, where `1.2.3` is the new version.

Create and push a new git tag:

```bash
  $ git checkout master
  $ git tag -a v1.x.x -m ""Release 1.x.x""
  $ git push origin v1.x.x
```

### 11. Add the release to GitHub

Follow the [Github documentation to create a new release](https://help.github.com/articles/creating-releases/) for the dashboard using the tag that you just pushed. Make sure to use the content from the CHANGELOG for this release as the description of the release on GitHub.

### 12. Create PR of master back into develop

Create a PR for the `master` branch back into `develop` to bring the merge commit back into develop.

It is likely that no branch plan will exist for the `master` branch.
#### Create a bamboo branch plan for the release
 - In the Cumulus Dashboard in Bamboo (<https://ci.earthdata.nasa.gov/browse/CUM-CDG>), click `Actions -> Configure Plan` in the top right.
 - Next to `Plan branch` click the rightmost button that displays `Create Plan Branch` upon hover.
 - Click `Create plan branch manually`.
 - Choose Branch Name `master` and then click `create`.
 - Verify that the build has started for this plan.





<a name=""bundlefootnote"">1</a>: A dashboard bundle is just a ready-to-deploy compiled version of the dashboard and environment.
"
17,nasa/gunns,C++,"[![Unit Tests](https://github.com/nasa/gunns/actions/workflows/unit_test.yml/badge.svg)](https://github.com/nasa/gunns/actions/workflows/unit_test.yml) [![Test Trick Sim](https://github.com/nasa/gunns/actions/workflows/sim_test.yml/badge.svg)](https://github.com/nasa/gunns/actions/workflows/sim_test.yml)

# General-Use Nodal Network Solver (GUNNS)

GUNNS is a set of generic C++ math models that combines nodal circuit analysis algorithms with the hydraulic-thermal-electric analogy to model flow circuits (fluid, thermal, electrical).  The models include generic physical components like pumps, valves, resistors & capacitors.  These components, called 'links', are connected to 'nodes' and configured with their characteristic values, creating a 'network' (circuit) of links and nodes.  The state of the network and its contained models are then propagated in the time domain.

### GunnsDraw, the Network Design GUI

Networks can be drawn up with a [Draw.io](https://www.draw.io) drawing, from which run-time C++ code is auto-generated that can be built into simulations.  This is called 'GunnsDraw'.  GunnsDraw includes custom Draw.io shapes for the generic physical components and Python scripts to auto-generate the run-time C++ code.
  
### Extensible

GUNNS is designed to be extensible and flexible.  It is written with object-oriented polymorphic interfaces between the network solver, links, nodes, and other extensible object types.  This allows users to develop new classes to model things that the GUNNS baseline doesn't cover.  Users can create custom GunnsDraw shapes by taking advantage of Draw.io shapes' extensible XML schema.  
  
GUNNS can also be extended to model other domains besides the above flow systems, as it can solve systems of the general form [A]{x} = {b} where [A] is a symmetric positive semi-definite matrix.  This is demonstrated in an included simple 6-DOF rigid body dynamics equations of motion model.

### Simulation Environment Not Included

GUNNS does not provide a complete simulation environment, as it is intended to be run in external environments.  GUNNS is optimized for the [NASA Trick](https://github.com/nasa/trick) simulation environment, but can be run in other environments with some additional work.

# The Gunnsmiths

GUNNS is managed by the [Simulation & Robotics Division, Simulation & Graphics Branch](https://www.nasa.gov/centers/johnson/engineering/robotics_simulation/index.html) at NASA Johnson Space Center.

GUNNS is developed by [CACI International Inc.](https://www.caci.com) under contract to NASA.  Contact info for our team can be found in the wiki.

<p align=left>
<img src=""https://raw.github.com/nasa/gunns/master/ER7_logo.png"" alt=""ER7 Logo"" height=75px>
<img src=""https://raw.github.com/nasa/gunns/master/CACI_International_logo.png"" alt=""CACI Logo"" height=75px>
</p>

# Wiki

See the [wiki](https://github.com/nasa/gunns/wiki) for more information including tutorials, user & developer guides.

# License

This software is released under the NASA Open Source Agreement Version 1.3 [license](https://github.com/nasa/gunns/blob/master/LICENSE).

# Notices

Copyright ¬© 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

# Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

"
18,nasa/mmt,Ruby,"# Metadata Management Tool Application
The Metadata Management Tool (MMT) and Draft Metadata Management Tool (dMMT) are web applications designed to assist users in managing metadata and interfacing with the CMR. The user‚Äôs guide for MMT can be found [here](https://wiki.earthdata.nasa.gov/display/ED/Metadata+Management+Tool+%28MMT%29+User%27s+Guide ""MMT User Guide"") and the user‚Äôs guide for dMMT can be found [here](https://wiki.earthdata.nasa.gov/display/ED/Draft+MMT+%28dMMT%29+User%27s+Guide ""dMMT User Guide""). Release notes for these applications can be found [here](https://wiki.earthdata.nasa.gov/display/ED/MMT+Release+Notes ""Release Notes"").

## Getting Started

### Requirements
 - Ruby 2.7.2

### Setup
Clone the Metadata Management Tool Git project:

    git clone https://github.com/nasa/mmt.git

Type the following command to install the necessary components:

    bundle install

Depending on your version of Ruby, you may need to install ruby rdoc/ri data:

    <= 1.8.6 : unsupported
     = 1.8.7 : gem install rdoc-data; rdoc-data --install
     = 1.9.1 : gem install rdoc-data; rdoc-data --install
    >= 1.9.2 : you're good to go!

#### Additional Install Steps
Some operating systems may require additional steps.

Mac OS X 10.14.6 moved some required libraries around which has been known to cause nokogiri to not install, if you have errors with that gem, you may need to run the following:

    open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg

Details can be found on nokogiri's [site](https://nokogiri.org/tutorials/installing_nokogiri.html#macos).

The libxml gem has also historically caused difficulty because it is a native library. If you are having issues installing libxml-ruby (cannot find libxml.h), you may need to configure it with the location of your libxml2 directory. You can do a:

    find / -name xmlversion.h

which may return something like the following:

    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/libxml2/libxml/xmlversion.h

then you can run `bundle config` with the location of libxml2 returned from the find command as in:

    bundle config build.libxml-ruby --with-xml2-include=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/libxml2

You should then be able to run `bundle install` as normal afterwards.

#### Database

Check your `/config/` directory for a `database.yml` file (with no additional extensions). If you do not have one, duplicate* the `database.yml.example` file and then rename it to `database.yml`.

*_Note: Do not simply rename the `database.yml.example` file as it is being tracked in Git and has its own history._

Next, create your database by running the standard rails command:

    rake db:create

And then to migrate the database schema, run the standard rails command:

    rake db:migrate

#### Other Steps

Finally, create an `application.yml` file in your `/config/` directory. The contents of this file will be supplied by an MMT developer

### Usage

*_Note: Before running this step, make sure you are **Running a local copy of CMR** as outlined below_

*_Note: If you want to run on http://localhost:3000 and just use Earthdata Login, you may need to modify entries in the `application.yml` file. Replace the 'urs...url' entries from 'https://mmt.localtest.earthdata.nasa.gov' to 'http://localhost:3000'_

*_Note: With Launchpad Integration, you will need to set up MMT to run locally with HTTPS. Please see `/doc/local_https_setup.md` for options and instructions_

To start the project, just type the default rails command:

    rails s

If you need to stop the server from running, hit `Ctrl + C` and the server will shutdown.

### Running a local copy of CMR
In order to use a local copy of the CMR you will need to download the latest file, set an environment variable, and run a rake task to set required permissions and ingest some data.

#### 1. Downloading the CMR file
If access to https://maven.earthdata.nasa.gov is possible, then the rake command `rake cmr:fetch` can be used to download the latest CMR jar. This task put the jar file in the `cmr` directory.
If this task fails for some reason, such as the maven repository is down, you can follow the instructions below to download and install manually from Bamboo:

Go to https://ci.earthdata.nasa.gov/browse/CN2-CSN2/latestSuccessful/artifact/, and download the `cmr-dev-system-uberjar.jar` file.
  * Note: It will rename itself to `cmr-dev-system-0.1.0-SNAPSHOT-standalone.jar`. This is the correct behavior. **DO NOT rename the file.**

In your root directory for MMT, create a folder named `cmr`. Place the `cmr-dev-system-0.1.0-SNAPSHOT-standalone.jar` file in the `cmr` folder.

#### 2. Setting the environment variable needed by the local CMR
Before running a local copy of the CMR, you will need to set a required environment variable. Add this line into your `.bash_profile`:

    export CMR_URS_PASSWORD=mock-urs-password

After adding the line and saving the file, don't forget to source the file

    source ~/.bash_profile

#### 3. Setting up local redis for CMR
CMR comes with redis in the jar, but it is not compiled to run on Macs.  If you need to run the CMR on a Mac, download it from

    https://redis.io/

CMR does not appear to be making significant configuration changes to redis, so a positive response from executing these commands in redis's root directory:

    make
    make test

should be sufficient to run CMR locally.  Run this command before starting CMR each session:

    path/to/redis/src/redis-server

The option '--daemonize yes' runs the server in the background.

Alternatively, you can install Redis with homebrew

The basics are

    brew update
    brew install redis
    brew services start redis

For more information, see one of these links

    https://www.devglan.com/blog/install-redis-windows-and-mac
    https://gist.github.com/tomysmile/1b8a321e7c58499ef9f9441b2faa0aa8



#### 4. Running the CMR rake tasks
To start the local CMR and load data*:

    rake cmr:start_and_load

After you see ""Done!"", you can load the app in your browser and use the local CMR. After you have started CMR, to just reload the data:

    rake cmr:load

To stop the locally running CMR, run this command:

    rake cmr:stop

You will need to stop the CMR before upgrading to a new CMR version. Note: stopping the running CMR for any reason will delete all data from the CMR. You will have to load the data again when you start it.

## Inserting Sample Drafts

You can insert sample drafts into your local database. These commands use the first user in the database (there should only be one), and add the drafts to your current provider, so make sure you login to the system and select a provider or the commands will fail.

To insert a sample draft that only has the required fields present:

    rake drafts:load_required

To insert a sample draft with every field completed:

    rake drafts:load_full

## Troubleshooting

### OpenSSL Issue

* If you receive a error from running `rake cmr:start_and_load` like

    Faraday::ConnectionFailed: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed

Try the following steps:

1. Ensure you are using RubyGems 2.0.3 or newer by typing `gem -v`. If it is older, type `gem update --system` to upgrade the RubyGems system.

2. Update the SSL certificates by running the following commands

    * `brew update`
    * `brew install openssl`
    * `brew link openssl --force`

3. Restart your terminal to refresh the OpenSSL version.

4. Check to ensure that OpenSSL version is 1.0.2 or newer with the command `openssl version`

5. Try running `rake cmr:start` and `rake cmr:load` as instructed above. If you still have issues, continue with these instructions below:

6. Uninstall Ruby 2.2.2. If you are using rvm, use the command `rvm remove 2.2.2`

7. Find out where your OpenSSL directory is by typing `which openssl`. An example directory you might get would be `/usr/local/bin/openssl`

8. Reinstall Ruby with the following command (if you are using rvm): `rvm install 2.2.2 --with-open-ssl-dir={DIRECTORY FROM STEP 7}`.

    * Using the example directory from above, it would be `rvm install 2.2.2 --with-open-ssl-dir=/usr/local/bin/openssl`.

9. Run `bundle install` to install any missing gems.

    * If your terminal tells you that it does not recognize the `bundle` command, run `gem install bundler`

9. Restart your terminal to refresh all settings.

10. Navigate to MMT directory and check to make sure Ruby and OpenSSL version are correct.

11. Run `rake cmr:start` and `rake cmr:load` again. If you still have issues, please reach out to a developer to help with troubleshooting.

### Earthdata Login Issue

* If you receive an error when logging into MMT using Earthdata Login such as

    JSON::ParserError at /urs_login_callback
    784: unexpected token at 'null'

Check your cmr.log file. It may show some errors and you need to restart your local copy of cmr.

### UMM JSON-Schema

You can view/download the latest UMM JSON-Schema here, https://git.earthdata.nasa.gov/projects/CMR/repos/cmr/browse/umm-spec-lib/resources/json-schemas

## Local Testing

#### JavaScript
MMT uses PhantomJS which allows us to run our Capybara tests on a headless WebKit browser. Before you're able to run tests locally you'll need to install it. The easiest way to accomplish this would be to use [Homebrew](http://brew.sh/) or a similar packager manager. If you're using Homebrew, run the following the command:

    brew install phantomjs

#### VCR
MMT uses [VCR](https://github.com/vcr/vcr) to record non-localhost HTTP interactions, it is configured in [spec/support/vcr.rb](spec/support/vcr.rb).

All calls to localhost are ignored by VCR and therefore will not be recorded.

This isn't an issue normally but with MMT we run a number of services locally while developing that we would like to be recorded.

#### CMR

For calls to CMR that are asynchronous, we do have a method of waiting for those to finish, synchronously. Within the [spec/helpers/cmr_helper.rb](spec/helpers/cmr_helper.rb) we have a method called `wait_for_cmr` that makes two calls to CMR and ElasticSearch to ensure all work is complete. This should ONLY be used within tests.

## ACLs
Access Control Lists (ACLs, aka Permissions) determine access to data and functionality in the CMR. See the [Access Control Documentation](https://cmr.earthdata.nasa.gov/access-control/site/docs/access-control/api.html) for technical information.

### Testing against ACLs
When testing functionality in the browser that requires specific permissions you'll need to ensure your environment is setup properly and you're able to assign yourself the permissions necessary. This includes:

1. Creating a Group
2. Add your URS account and the user 'typical' as a member of the group
3. Ensuring the group created has appropriate Provider Context, Group, and Provider Object ACLs permissions.

This provides access to the Provider Object Permissions pages.

4. Give your URS account and the user 'typical' access to the Administrators_2 group

This gives you permission to view System Level Groups and the System Object Permissions pages.

From here you'll need to modify appropriate permissions of the group so that you can test functionality associated with any of the permissions via the group show page, or the Provider or System Object Permissions pages.

##### Automating ACL Group Management
To run the above steps automatically there is a provided rake task to do the heavy lifting.

    rake acls:testing:prepare[URS_USERNAME]

Replacing URS_USERNAME with your own username. An example:

    $ rake acls:testing:prepare[username]
    [Success] Added username to MMT_2 Admin Group
    [Success] Added username to Administrators_2

Then you can manage the Provider Level permissions by clicking on the group on the Provider Object Permissions page or by clicking on the Provider Object Permissions for MMT_2 link on the group show page. If System Level permissions are required, you can click on the Administrators_2 group from the System Object Permissions page or click on the System Object Permissions link from the group show page.

Alternatively, if only one of provider level access or system level access is required, you can use the more specific rake task:

    rake acls:groups:mmt_2_users[username]

or

    rake acls:groups:admins[username]

### Draft MMT
The Draft MMT is intended for Non-NASA Users to propose new metadata records or changes to existing records in the CMR.  There are several steps required to run a local version of Draft MMT.

1. Enable https connections to the Draft MMT.     See the directions for configuring https [here](doc/local_https_setup.md)

2. Configure MMT to use the https connection.  In your application.yml file, make sure that `urs_login_callback_url`   is set to `https://mmt.localtest.earthdata.nasa.gov/urs_login_callback_url`.  

3. Create ACLs to give yourself permission to use Draft MMT. Access to the Draft MMT is controlled by the Non-NASA Draft User and Non-NASA Draft Approver ACLs. There is a rake task that will create the group and assign the ACL for you (make sure you use your own username):


    $ rake acls:proposal_mode:draft_user[URS_USERNAME]

or

    $ rake acls:proposal_mode:draft_approver[URS_USERNAME]

  * make sure you use your own username
  ***NOTE: Make sure that `proposal_mode` is set to 'false' in your `application.yml` file when you run this rake task. If you see `NotAllowedError: A requested action is not allowed in the current configuration.` when running this rake task, you missed this step.***

4. Change the app to the Draft MMT (aka proposal mode) by changing the `proposal_mode` environment variable in your `application.yml` file. Set `proposal_mode` to `true`.


5. Start the MMT app as usual with `bin/rails server -p 3000`  

6. Direct your browser to https://mmt.localtest.earthdata.nasa.gov .   Note that some browsers will give you a warning about the self-signed certificate that was created in step 1.  In that case,  use the browser controls to allow the certificate.

7. To return to normal MMT mode,  set `proposal_mode` to `false` in the application.yml file and restart the app.

### Replicating SIT Collections Locally
Often we need collections to exist in our local CMR that already exist in SIT for the purposes of sending collection ids (concept ids) as part of a payload to the ECHO API that doesn't run locally, but instead on testbed. In order to do this the collection concept ids have to match those on SIT so we cannot simply download and ingest them. A rake task exists to replicate collections locally for this purpose.

    $ rake collections:replicate

The task accepts two parameters

- **provider:** The provider id to replicate collections for *default: MMT_2*
- **page_size:** The number of collections to request *default: 25*

##### Examples

    $ rake collections:replicate[MMT_1,10]

Will download at most 10 collections from MMT_1.

    $ rake collections:replicate[SEDAC]

Will download at most 25 collections from SEDAC.

**NOTE** Some providers have permissions set on their collections and make require a token to view/download collections. You can set an ENV variable named

    CMR_SIT_TOKEN

that if set, will be provided to CMR when downloading collections. This variable is set by adding the following line to your **~/.bash_profile**

    export CMR_SIT_TOKEN=""""

After adding the line and saving the file, don't forget to source the file.

    source ~/.bash_profile
"
19,nasa/edsc-echoforms,JavaScript,"# Earthdata Search Components:  ECHO Forms

[![npm version](https://badge.fury.io/js/%40edsc%2Fechoforms.svg)](https://badge.fury.io/js/%40edsc%2Fechoforms)
![Build Status](https://github.com/nasa/edsc-echoforms/workflows/CI/badge.svg?branch=master)
[![codecov](https://codecov.io/gh/nasa/edsc-echoforms/branch/master/graph/badge.svg?token=4d8wFDtAc0)](https://codecov.io/gh/nasa/edsc-echoforms)

Try out the [online demo](http://nasa.github.io/edsc-echoforms/)

A React component implementing the
[ECHO Forms](https://earthdata.nasa.gov/files/ECHO_Forms_Specification_0.pdf)
specification. For a basic usage example and a testbed for changes,
see `example/src`.

The ECHO Forms component was developed as a component of
[Earthdata Search](https://github.com/nasa/earthdata-search).

For the jQuery version of this plugin see [this branch](https://github.com/nasa/edsc-echoforms/tree/jquery-plugin).

## Installation

    npm install @edsc/echoforms

## Usage

After installing you can use the component in your code.

```javascript
import EDSCEchoform from '@edsc/echoforms'

const Component = () => {
  return (
    <EDSCEchoform
      form={formXml}
      onFormModelUpdated={onFormModelUpdated}
      onFormIsValidUpdated={onFormIsValidUpdated}
    />
  )
}
```

### Props

| Prop | Type | Required | Default Value | Description
| ---- |:----:|:--------:|:-------------:| -----------
addBootstrapClasses | Boolean | false | false | Adds Bootstrap class names to elements. Bootstrap is **not** included in this package.
form | String | true | | ECHO Forms XML string.
hasShapefile | Boolean | false | false | Is a shapefile included in the search parameters. This is used to display help text about shapefile processing to users on shapefile form fields.
prepopulateValues | Object | false | | Values used to prepopulate fields through the form's `pre:prepopulate` extensions.
onFormModelUpdated | Function | true | | Callback function that returns `{ model, rawModel }`. `model` is the data model pruned of irrelevant fields. `rawModel` is the full data model.
onFormIsValidUpdated | Function | true | | Callback function that returns a Boolean value of the form's isValid property.

## Development

To compile:

    npm install

To start the example project for local testing:

    npm start

To run the tests:

    npm test

## Contributing

See CONTRIBUTING.md

## License

> Copyright ¬© 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
>
> Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>    http://www.apache.org/licenses/LICENSE-2.0
>
>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
"
20,nasa/cumulus-distribution-api,JavaScript,"## Cumulus Distribution API Documentation

[![CircleCI](https://circleci.com/gh/nasa/cumulus-distribution-api.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-distribution-api)

Cumulus Distribution API documentaion: https://nasa.github.io/cumulus-distribution-api

### Installation

     $ npm install

### Build

     $ npm run build

### Serve

     $ npm run serve

### Deploy

     $ npm run deploy
"
21,nasa/Open-Source-Catalog,JavaScript,"# Open-Source-Catalog

[![Build Status](https://travis-ci.org/nasa/Open-Source-Catalog.svg?branch=master)](https://travis-ci.org/nasa/Open-Source-Catalog)

## About

This GitHub repository is maintained by the [NASA OCIO Open Innovation Team](http://open.nasa.gov/about/) and contains a catalog of publicly available NASA open source projects that are published on [code.nasa.gov](http://code.nasa.gov). The catalog is persisted as a JSON file ```catalog.json``` and contains an array of projects.  As Code Sharing at NASA is a community effort, we encourage NASA developers to add a meta-record in to this catalog to publish their open source projects on [code.nasa.gov](http://code.nasa.gov/).

## Requirements
* Open Source software project approved for open source release by your [NASA Field Center SRA](http://code.nasa.gov/#/guide)
* Code Project hosted in a code repository (preferably GitHub.com) and visible to Internet Users
* Meta record of your software project; instantiate ```required_fields_project_template.json```

## Add/Edit your project
### OPTION 1
If you are behind the NASA firewall, we recommend you use the online form located [here](https://developer.nasa.gov/pages/OpenInnovation/code-submission-app/)
### OPTION 2
Create a project meta-record using the template from file required_fields_project_template.json:
  * Note that Category labels longer than 24 characters will be truncated.
```
{
    ""NASA Center"": ""Ames Research Center"",
    ""Contributors"": [
      ""jasonduley""
    ],
    ""Software"": ""My Software Project"",
    ""External Link"": ""https://github.com/nasa/my-software-project/wiki"",
    ""Public Code Repo"": ""https://github.com/nasa/my-software-project"",
    ""Description"": ""This is a description of the software project."",
    ""License"": [
      ""NASA Open Source""
    ],
    ""Categories"": [
      ""Framework"",
      ""Toolkit"",
      ""Web""
    ],
    ""Update_Date"": ""2014-09-23"",
    ""Labor_Hours"": 24
}
```

* Add your instantiated meta-record to the array in the catalog.json file via a pull request
* Once the merge is complete, your project will be published on [code.nasa.gov](http://code.nasa.gov/)

## Thanks
Special thanks goes out to [Chris Mattmann (NASA JPL)](https://github.com/chrismattmann), Sean Kelly (NASA JPL) and [Eric Whyne (DARPA)](https://github.com/ericwhyne) for their inspiration for this effort.
"
22,nasa/delta,Python,"**DELTA** (Deep Earth Learning, Tools, and Analysis) is a framework for deep learning on satellite imagery,
based on Tensorflow. DELTA classifies large satellite images with neural networks, automatically handling
tiling large imagery.

DELTA is currently under active development by the
[NASA Ames Intelligent Robotics Group](https://ti.arc.nasa.gov/tech/asr/groups/intelligent-robotics/).
Initially, it is mapping floods for disaster response, in collaboration with the
[U.S. Geological Survey](http://www.usgs.gov), [National Geospatial Intelligence Agency](https://www.nga.mil/),
[National Center for Supercomputing Applications](http://www.ncsa.illinois.edu/), and
[University of Alabama](https://www.ua.edu/).

Installation
============

1. Install [python3](https://www.python.org/downloads/), [GDAL](https://gdal.org/download.html),
   and the [GDAL python bindings](https://pypi.org/project/GDAL/). For Ubuntu Linux, you can run
   `scripts/setup.sh` from the DELTA repository to install these dependencies.

2. Install Tensorflow following the [instructions](https://www.tensorflow.org/install). For
   GPU support in DELTA (highly recommended) follow the directions in the
   [GPU guide](https://www.tensorflow.org/install/gpu).

3. Checkout the delta repository and install with pip:

```bash
git clone http://github.com/nasa/delta
python3 -m pip install delta
```

DELTA is now installed and ready to use!

Documentation
=============
DELTA can be used either as a command line tool or as a python library.
See the python documentation for the master branch [here](https://nasa.github.io/delta/),
or generate the documentation with `scripts/docs.sh`.

Example
=======

As a simple example, consider training a neural network to map clouds with Landsat-8 images.
The script `scripts/example/l8_cloud.sh` trains such a network using DELTA from the
[USGS SPARCS dataset](https://www.usgs.gov/core-science-systems/nli/landsat/spatial-procedures-automated-removal-cloud-and-shadow-sparcs),
and shows how DELTA can be used. The steps involved in this, and other, classification processes are:

1. **Collect** training data. The SPARCS dataset contains Landsat-8 imagery with and without clouds.

2. **Label** training data. The SPARCS labels classify each pixel according to cloud, land, water and other classes.

3. **Train** the neural network. The script `scripts/example/l8_cloud.sh` invokes the command

    ```
    delta train --config l8_cloud.yaml l8_clouds.h5
    ```

    where `scripts/example/l8_cloud.yaml` is a configuration file specifying the labeled training data and
    training parameters (learn more about configuration files below). A neural network file
    `l8_clouds.h5` is output.

4. **Classify** with the trained network. The script runs

    ```
    delta classify --config l8_cloud.yaml --image-dir ./validate --overlap 32 l8_clouds.h5
    ```

    to classify the images in the `validate` folder using the network `l8_clouds.h5` learned previously.
    The overlap tiles to ignore border regions when possible to make a more aesthetically pleasing classified
    image. The command outputs a predicted image and confusion matrix.

The results could be improved--- with more training, more data, an improved network, or more--- but this
example shows the basic usage of DETLA.

Configuration and Extensions
============================

DELTA provides many options for customizing data inputs and training. All options are configured via
YAML files. Some options can be overwritten with command line options (use
`delta --help` to see which). See the `delta.config` README to learn about available configuration
options.

DELTA can be extended to support custom neural network layers, image types, preprocessing operations, metrics, losses,
and training callbacks. Learn about DELTA extensions in the `delta.config.extensions` documentation.

Data Management
=============

DELTA integrates with [MLFlow](http://mlflow.org) to track training. MLFlow options can
be specified in the corresponding area of the configuration file. By default, training and
validation metrics are logged, along with all configuration parameters. The most recent neural
network is saved to a file when the training program is interrupted or completes.

View all the logged training information through mlflow by running::

```
  delta mlflow_ui
```

and navigating to the printed URL in a browser. This makes it easier to keep track when running
experiments and adjusting parameters.

Contributors
============
We welcome pull requests to contribute to DELTA. However, due to NASA legal restrictions, we must require
that all contributors sign and submit a
[NASA Individual Contributor License Agreement](https://www.nasa.gov/sites/default/files/atoms/files/astrobee_individual_contributor_license_agreement.pdf).
You can scan the document and submit via email. Thank you for your understanding.

Important notes for developers:

 * **Branching**: Active development occurs on `develop`. Releases are pushed to `master`.

 * **Code Style**: Code must pass our linter before merging. Run `scripts/linter/install_linter.sh` to install
   the linter as a git pre-commit hook.

 * **Unit Tests**: Code must pass unit tests before merging. Run `pytest` in the `tests` directory to run the tests.
   Please add new unit tests as appropriate.

 * **Development Setup**: You can install delta using pip's `-e` flag which installs in editable mode. Then you can
   run `delta` and it will use your latest changes made to the repo without reinstalling.

Licensing
=========
DELTA is released under the Apache 2 license.

Copyright (c) 2020, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.
"
23,nasa/isle,JavaScript,"ISLE - Inventory System for Lab Equipment
==========

![Demo](https://cloud.githubusercontent.com/assets/1322063/14293284/efe856d2-fb39-11e5-9765-7605555b06f8.gif)

## Created By
[Brandon Ruffridge](https://github.com/bruffridge)  
[Brent Gardner](https://github.com/bggardner)

## Brief Abstract
This web application allows inventories of assets to be managed. Assets along with their specifications are added to the system and then used by users via a check-in/check-out function. The inventory can be browsed by category or using search. Users are given various roles within the system to allow them to perform certain functions such as view-only, check-in/check-out, modify, and full-control. Inventory managers can add and track detailed information on all inventory assets including manufacturer, storage location, custom attributes, and relationships with other assets. Assets can be found by browsing by category, search, location, or current user. Assets are checked out to specified locations by users.

## Description of the Problem That Motivated ISLE's Development
One of our labs at the NASA Glenn Research Center wanted a way to track their inventory of over 350 pieces of equipment, who is using it, and where it is located. They also wanted to give lab users a way to see what equipment is available and see detailed specs on the equipment and check it out for use with their projects. This web based tool was developed to meet that objective.

## Technical Description

Developed using web standards and best practices such as Model-View-Controller architecture, Separation of Concerns, and Don't Repeat Yourself.  
Fast, intuitive UI featuring a custom application layout built using parts from Twitter Bootstrap, extensive AJAX and jQuery, and combined and minified Javascript and LESS CSS.  
Secure and 508 compliant.  
Features an innovative built-in bug reporting system to Pivotal Tracker.  
Deployed on the mature, open-source Linux, Apache, MySQL, and PHP (LAMP) technology stack.

## Get Started

* Have a Mac or Linux Box
  * Windows can run ISLE but can't run the bash scripts for [building static files](#building-static-files) or [syncing database for multiple developer teams](#keeping-database-in-sync-for-multiple-developer-teams)
* Have a webserver running PHP and MySQL.
  * You can also use the included Vagrant LAMP Box to deploy and run the application locally by following the steps below.
* Download and Install [Vagrant][1]
* Download and Install [VirtualBox][2]
* Clone ISLE ```git clone https://github.com/nasa/isle.git```
* Run ``` vagrant up ```
* Access Your Project at  [http://192.168.33.10/myinstance][3] or [http://192.168.33.10/myinstance2][11]

## Configuration

Search source code for ```config-todo:``` for things you may need to configure.  
**NOTE:** ISLE was modified to be easy to install and configure in a local development environment. Additional configuration steps would be needed to run ISLE in a secure production environment such as changing database credentials and moving them into a separate file and adding that file to .gitignore so the credentials don't go into source control.

## How to Contribute

[Check out our backlog](https://www.pivotaltracker.com/n/projects/1569431) of the things we want to add/fix. Fork the project, make your changes, test ( we don't have time to test for you ), then submit a pull request. Submit any new bugs or feature requests to the [issues page](https://github.com/nasa/isle/issues).

## Multiple Inventories

ISLE supports multiple ""instances"" so multiple inventories can be managed separately. Each instance has a unique url, but accesses the same php files and database. Data is kept separate by using different tables.  
The ```instances``` folder contains two example instances ```myinstance``` and ```myinstance2```.  
### It would be nice to have a bash script to automate creation of additional instances, however it is currently a manual process.  
To create additional instances duplicate the ```instances/myinstance``` folder and rename to whatever you want to call your instance. Delete the .log files in ```logs```. Then replace ```myinstance``` with whatever instance name you chose in all files within the duplicated folder. Also rename ```isle.local.myinstance.conf``` to ```isle.local.INSERT_YOUR_INSTANCE_NAME.conf```.  
Duplicate ```webroot/myinstance```. Delete any files in ```uploads``` except the .htaccess files.  
Edit ```isle-init.sh``` and copy and paste the following lines for running sql and enabling conf files. Replace ```myinstance``` with whatever instance name you chose.

```bash
mysql -uroot -p'root' -h localhost isle_dev < ""/var/www/instances/myinstance/init.sql""
mysql -uroot -p'root' -h localhost isle_dev < ""/var/www/instances/myinstance/data.sql""

cp /var/www/instances/myinstance/isle.local.myinstance.conf /etc/apache2/sites-available/isle.local.myinstance.conf
a2ensite isle.local.myinstance

cat <<EOT >> /etc/logrotate.d/isle-myinstance
/var/www/instances/myinstance/logs/*.log {
        yearly
        maxsize 2M
        rotate 5
        notifempty
        missingok
        su vagrant vagrant
}
EOT
```

Then run the following so the changes take effect.

```bash
vagrant destroy
vagrant up
```  

## Building Static Files

When you want to make changes to CSS or JS the files you want to edit are located in:  
**JS:** ```webroot/isle/cdn/scripts-dev```  
**CSS:** ```webroot/isle/cdn/styles/less```

Don't edit files in ```scripts``` or ```css-dev``` as those are created during the build process.

Build CSS and JS (combines and minifies)
* Make sure you have lessc installed.
* ``` cd PROJECT_FOLDER/webroot/isle/includes``` (this step is important or the build files will not be saved to the correct location)
* ```./build.sh```

## Keeping Database in Sync for Multiple Developer Teams

Change Workflow:  
* Make changes.
* ```./dbup.sh``` (option 1)
* git add, git commit, git push.

Update Workflow:  
* git pull
* ```./dbup.sh``` (option 2)

## Basic Vagrant Commands


### Start or resume your server
```bash
vagrant up
```

### Pause your server
```bash
vagrant suspend
```

### Delete your server
```bash
vagrant destroy
```

### SSH into your server
```bash
vagrant ssh
```



## Database Access

### MySQL 

- Hostname: localhost or 127.0.0.1
- Username: root
- Password: root
- Database: isle_dev

## Updating the Box

Although not necessary, if you want to check for updates, just type:

```bash
vagrant box outdated
```

It will tell you if you are running the latest version or not, of the box. If it says you aren't, simply run:

```bash
vagrant box update
```


## Setting a Hostname

If you're like me, you prefer to develop at a domain name versus an IP address. If you want to get rid of the some-what ugly IP address, just add a record like the following example to your computer's host file.

```bash
192.168.33.10 isle.local
```

Or if you want ""www"" to work as well, do:

```bash
192.168.33.10 isle.local www.isle.local
```

Technically you could also use a Vagrant Plugin like [Vagrant Hostmanager][4] to automatically update your host file when you run Vagrant Up. However, the purpose of Scotch Box is to have as little dependencies as possible so that it's always working when you run ""vagrant up"".

## Special Thanks To

* [Scotch Box][5]
* [Require.js][6]
* [Bootstrap][7]
* [jQuery][8]
* [Modernizr][9]
* [tag-it][10]
* [jquery-trap-input](https://github.com/julienw/jquery-trap-input)
* [jquery-dateFormat](https://github.com/phstc/jquery-dateFormat)
* [AppLayout](https://github.com/bruffridge/AppLayout)

 [1]: https://www.vagrantup.com/downloads.html
 [2]: https://www.virtualbox.org/wiki/Downloads
 [3]: http://192.168.33.10/myinstance
 [4]: https://github.com/smdahlen/vagrant-hostmanager
 [5]: https://box.scotch.io/
 [6]: http://requirejs.org/
 [7]: http://getbootstrap.com/
 [8]: https://jquery.com/
 [9]: https://modernizr.com/
 [10]: https://github.com/aehlke/tag-it
 [11]: http://192.168.33.10/myinstance2
"
24,nasa/openmct-map,JavaScript,"# Open MCT Map

## v0.3.0 TODO:
1. Retrieve timestamp from point.
2. Implement time based baselayers.

## Known Bugs
1. Changing time system will will probably break realtime data until view is reloaded.

A plugin for [Open MCT](https://nasa.github.io/openmct)
adding map style visualizations.  This plugin is experimental and not intended
for production usage.

## Usage


1. `npm install nasa/openmct-map`
2. include `node_modules/openmct-map/dist/openmct-map.js` and `node_modules/openmct-map/dist/openmct-map.css` in your `index.html`, or load with your favorite module loader.
    ```html
    <script src=""node_modules/openmct-map/dist/openmct-map.js""></script>
    <link rel=""stylesheet"" href=""node_modules/openmct-map/dist/openmct-map.css"" type=""text/css"" media=""screen"">
    ```
3. install plugin in OpenMCT before starting:
    ```javascript
    openmct.install(new OpenMCTMapPlugin());
    ```

## Build

```bash
$ npm install
```

A UMD module with associated source maps will be written to the
`dist` folder. When installed as a global, the plugin will be
available as `MapPlugin`.

## Configuration

The Map Plugin exposes three new types for OpenMCT:
* Location Combiner: Takes two telemetry points (one for x, one for y) and returns a location telemetry object.  For testing, use one of these with two sine wave generators to get a ""location"".
* Measurement Location Synthesizer: Takes two telemetry points (one for location, one for measurement), and returns a location measurement telemetry object.
* Traverse Map: The actual map for users.

The traverse map has a JSON field where you can specify layers to add.


## Usage

See [`index.html`](index.html) for an example of use.

## Developer Environment

You'll need to install nasa/openmct, and then run the simple dev server.

Rollup seems to fail to detect changes in files on some systems, so you might 
spend a lot of time restarting the dev server.

```bash
npm install nasa/openmct
npm run dev
```
"
25,nasa/nasapress,PHP,"# NASAPress (a WordPress Theme)

![screen shot 2018-06-15 at 12 31 39 pm](https://user-images.githubusercontent.com/1322063/41479194-64b3bab2-7098-11e8-81cf-ccf93f472f61.png)

## Sites using NASAPress

If you'd like your site to be added to this list please [create an issue](https://github.com/nasa/nasapress/issues/new) with your website name and URL.

* [NASA Glenn Research Center](https://www1.grc.nasa.gov)

## Features

* Built on [Sage 9.0.0-beta.3](https://github.com/roots/sage/releases/tag/9.0.0-beta.3)
* Sass for stylesheets
* ES6 for JavaScript
* [Webpack](https://webpack.github.io/) for compiling assets, optimizing images, and concatenating and minifying files
* [Browsersync](http://www.browsersync.io/) for synchronized browser testing
* [Laravel's Blade](https://laravel.com/docs/5.3/blade) as a templating engine
* [NASA Glenn Web Design System](https://nasa.github.io/nasawds-site/) based on the [U.S. Web Design System](https://designsystem.digital.gov) for CSS framework
* [Font Awesome](http://fontawesome.io/)

### Required plugins

* [Add Categories to Pages](https://wordpress.org/plugins/add-category-to-pages/)
* [Advanced Custom Fields](https://wordpress.org/plugins/advanced-custom-fields/)

### Recommended plugins

* [NASAPress Companion](https://github.com/nasa/nasapress-companion)
  * adds shortcodes for displaying nasa.gov news articles, spinoffs, and lists of pages.
* [Advanced TinyMCE Configuration](https://wordpress.org/plugins/advanced-tinymce-configuration/)
    * allows editors to add NASA Web Design Standards styles to elements in the visual text editor.
* [TinyMCE Advanced](https://wordpress.org/plugins/tinymce-advanced/)
* [Better WordPress External Links](https://wordpress.org/plugins/bwp-external-links/)
    * Plugin Settings:
      * Set External links' CSS class to `usa-external_link`
      * Uncheck 'Use CSS provided by this plugin?'
* [Disable Search](https://wordpress.org/plugins/disable-search/)
  * if using DigitalGov Search
* [Responsive Lightbox](https://wordpress.org/plugins/responsive-lightbox/)
    * for viewing images in a lightbox.
* [Yet Another Related Posts Plugin](https://wordpress.org/plugins/yet-another-related-posts-plugin/)
* [Yoast SEO](https://wordpress.org/plugins/wordpress-seo/)
  * for breadcrumbs
* [Gravity Forms](http://www.gravityforms.com/) and [Gravity Forms Survey Add-On](http://www.gravityforms.com/add-ons/survey/)
  * for site feedback form and other forms.
* [Popup Maker](https://wordpress.org/plugins/popup-maker/)
  * for displaying site feedback form in a popup window.
* [Hide YouTube Related Videos](https://wordpress.org/plugins/hide-youtube-related-videos/)
* [Broken Link Checker](https://wordpress.org/plugins/broken-link-checker/)

### Designed for use with these services

* [Digital Analytics Program](https://www.digitalgov.gov/services/dap/)
* [DigitalGov Search](https://search.digitalgov.gov/)

## Requirements

Make sure all dependencies have been installed before moving on:

* [WordPress](https://wordpress.org/) >= 4.7
* [PHP](http://php.net/manual/en/install.php) >= 5.6.4
* MySQL >= 5.6 or MariaDB >= 10.0
  * Earlier versions don't support FULLTEXT index for InnoDB engine required by YARPP plugin. See [this explanation of issue](https://easyengine.io/tutorials/mysql/yarpp-innodb/).
* [Composer](https://getcomposer.org/download/)
* [Node.js](http://nodejs.org/) >= 6.9.x
* [Yarn](https://yarnpkg.com/en/docs/install)

## Theme installation

Clone this repo into your WordPress themes directory.

Install Composer dependencies:

```shell
# @ app/themes/nasapress or wp-content/themes/nasapress
$ composer install
```

Run `yarn` from the theme directory to install dependencies. If you won't be making changes to the theme's static assets (css, javascript, images) then run `yarn install --production`.

Update `resources/assets/config.json` settings:
  * `devUrl` should reflect your local development hostname
  * `publicPath` should reflect your WordPress folder structure (`/wp-content/themes/nasapress` for non-[Bedrock](https://roots.io/bedrock/) installs)

## Theme setup

Search the theme folder for `todo-config`. These comments mark the locations where you'll likely need to make customizations for your site.

### Add top navigation

Create a menu and assign it to the 'Primary Navigation' location.

### Enable breadcrumbs

Install and activate the Yoast SEO plugin. Follow steps 1-5 [in this guide](https://kb.yoast.com/kb/implement-wordpress-seo-breadcrumbs/) to enable yoast breadcrumbs.

### Enable related pages

If you want to show related pages at the bottom of pages install and activate the YARPP plugin. On the plugin settings, you might see a message about 'consider titles' and 'consider bodies' being disabled due to InnoDB... If you are using MySQL 5.6 or greater, expand the message and click the 'Create FULLTEXT indices' button to enable them.

Under display options, select 'Pages', then click the Custom button and make sure 'You Might Also Like' is selected as the template file.

### Add NASA Web Design Standards styles to Visual Editor

todo

### Add site feedback form

todo

## Using the theme

### Page templates

#### Home page

Although not technically a template the theme expects a static front page and styles it differently than the others. Use the following as a starting point for this page.
```html
<div class=""usa-overlay""></div>

<section class=""usa-hero"">
  <div class=""usa-grid"">
    <div class=""usa-width-one-half"">
      <h1>Shaping the world of tomorrow</h1>
    </div>
  </div>
  <div class=""usa-grid"">
    <div class=""usa-width-one-half"">
      <p class=""usa-font-lead"">By developing technologies that will enable further exploration of the universe and revolutionize air travel</p>
    </div>
  </div>
  <div class=""usa-grid"">
    <div class=""usa-width-two-thirds"">
      <div class=""video-container"">
        https://www.youtube.com/watch?v=5VHPanW6F4E
      </div>
    </div>
  </div>
</section>
```

#### Landing Page

The landing page template features a large hero image with leading paragraph followed by text. Make sure your featured image is large enough to not pixellate too much at larger screen sizes.

#### Default template

The default template has no top hero section.

### On this page navigation

The default and landing page templates automatically convert h2, h3, and h4 tags into left 'in page' navigation. For shorter pages, this may not be desired, and can be turned off in the ""On this page"" settings on the edit page screen. In this section, you can also change which heading tags to convert to navigation.

### Setting NASA Official

A NASA Official can be added or changed on the edit page category screen. You can select from any users of your WordPress site.

## Theme structure

```shell
themes/your-theme-name/   # ‚Üí Root of your Sage based theme
‚îú‚îÄ‚îÄ app/                  # ‚Üí Theme PHP
‚îÇ   ‚îú‚îÄ‚îÄ lib/App/          # ‚Üí NASAPress functions
‚îÇ   ‚îú‚îÄ‚îÄ lib/Sage/         # ‚Üí Blade implementation, asset manifest
‚îÇ   ‚îú‚îÄ‚îÄ admin.php         # ‚Üí Theme customizer setup
‚îÇ   ‚îú‚îÄ‚îÄ filters.php       # ‚Üí Theme filters
‚îÇ   ‚îú‚îÄ‚îÄ helpers.php       # ‚Üí Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ setup.php         # ‚Üí Theme setup
‚îú‚îÄ‚îÄ composer.json         # ‚Üí Autoloading for `app/` files
‚îú‚îÄ‚îÄ composer.lock         # ‚Üí Composer lock file (never edit)
‚îú‚îÄ‚îÄ dist/                 # ‚Üí Built theme assets (never edit)
‚îú‚îÄ‚îÄ node_modules/         # ‚Üí Node.js packages (never edit)
‚îú‚îÄ‚îÄ package.json          # ‚Üí Node.js dependencies and scripts
‚îú‚îÄ‚îÄ resources/            # ‚Üí Theme assets and templates
‚îÇ   ‚îú‚îÄ‚îÄ assets/           # ‚Üí Front-end assets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.json   # ‚Üí Settings for compiled assets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build/        # ‚Üí Webpack and ESLint config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fonts/        # ‚Üí Theme fonts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images/       # ‚Üí Theme images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/      # ‚Üí Theme JS
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles/       # ‚Üí Theme stylesheets
‚îÇ   ‚îú‚îÄ‚îÄ functions.php     # ‚Üí Composer autoloader, theme includes
‚îÇ   ‚îú‚îÄ‚îÄ index.php         # ‚Üí Never manually edit
‚îÇ   ‚îú‚îÄ‚îÄ screenshot.png    # ‚Üí Theme screenshot for WP admin
‚îÇ   ‚îú‚îÄ‚îÄ style.css         # ‚Üí Theme meta information
‚îÇ   ‚îî‚îÄ‚îÄ views/            # ‚Üí Theme templates
‚îÇ       ‚îú‚îÄ‚îÄ layouts/      # ‚Üí Base templates
‚îÇ       ‚îî‚îÄ‚îÄ partials/     # ‚Üí Partial templates
‚îî‚îÄ‚îÄ vendor/               # ‚Üí Composer packages (never edit)
```

## Theme development

### Build commands

* `yarn run start` ‚Äî Compile assets when file changes are made, start Browsersync session
* `yarn run build` ‚Äî Compile and optimize the files in your assets directory
* `yarn run build:production` ‚Äî Compile assets for production

## Todos

Make site title customizable in wp-admin.
Make right side of footer customizable in wp-admin.
"
26,nasa/cumulus-message-adapter-js,JavaScript,"# @cumulus/cumulus-message-adapter-js

[![CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter-js.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-message-adapter-js)
[![npm version](https://badge.fury.io/js/%40cumulus%2Fcumulus-message-adapter-js.svg)](https://badge.fury.io/js/%40cumulus%2Fcumulus-message-adapter-js)

## What is Cumulus

Cumulus is a cloud-based data ingest, archive, distribution and management
prototype for NASA's future Earth science data streams.

Read the [Cumulus Documentation](https://nasa.github.io/cumulus)

## What is the Cumulus Message Adapter?

The Cumulus Message Adapter is a library that adapts incoming messages in the
Cumulus protocol to a format more easily consumable by Cumulus tasks, invokes
the tasks, and then adapts their response back to the Cumulus message protocol
to be sent to the next task.

## Installation

The cumulus-message-adapter-js can be installed via Node Package Manager (NPM) and the package is located [here](https://www.npmjs.com/package/@cumulus/cumulus-message-adapter-js).

The package can be added to your project by running `npm install @cumulus/cumulus-message-adapter-js --save`.

## Task definition

In order to use the Cumulus Message Adapter, you will need to create two
methods in your task module: a handler function and a business logic function.

The handler function is a standard Lambda handler function which takes three
parameters (as specified by AWS): `event`, `context`, and `callback`.

The business logic function is where the actual work of your task occurs. It
should take two parameters: `nestedEvent` and `context`.

The `nestedEvent` object contains two keys:

* `input` - the task's input, typically the `payload` of the message,
    produced at runtime
* `config` - the task's configuration, with any templated variables
    resolved

The `context` parameter is the standard Lambda context as passed by AWS.

The return value of the business logic function will be placed in the
`payload` of the resulting Cumulus message.

Expectations for input, config, and return values are all defined by the task,
and should be well documented. Tasks should thoughtfully consider their inputs
and return values, as breaking changes may have cascading effects on tasks
throughout a workflow. Configuration changes are slightly less impactful, but
must be communicated to those using the task.

## Cumulus Message Adapter interface

The Cumulus Message adapter for Javascript provides one method:
`runCumulusTask`. It takes five parameters:

* `taskFunction` - the function containing your business logic (as described
    above)
* `cumulusMessage` - the event passed by Lambda, and should be a Cumulus
    Message
* `context` - the Lambda context
* `callback` - the callback passed by Lambda
* `schemas` - JSON object with the locations of the task schemas

The `schemas` JSON should contain `input:`, `output:`, and `config:` with strings for each location. If the schema locations are not specified, the message adapter will look for schemas in a schemas directory at the root level for the files: input.json, output.json, or config.json. If the schema is not specified or missing, schema validation will not be performed.

## Example Cumulus task

```javascript
const cumulusMessageAdapter = require('@cumulus/cumulus-message-adapter-js');

function myBusinessLogic(nestedEvent, context) {
  console.log('Hello, example!');
  return { answer: 42 };
}

// The handler function should rarely, if ever, contain more than this line
function handler(event, context, callback) {
  cumulusMessageAdapter.runCumulusTask(myBusinessLogic, event, context, callback, schemas);
}
exports.handler = handler;
```

## Creating a deployment package

Tasks that use this library are just standard AWS Lambda tasks. Information on
creating release packages is available [here](https://docs.aws.amazon.com/lambda/latest/dg/deployment-package-v2.html).

## Usage in Cumulus Deployments

For documentation on how to utilize this package in a Cumulus Deployment, view the [Cumulus Workflow Documenation](https://nasa.github.io/cumulus/docs/workflows/input_output).

## Environment variables

There are two environment variables that can be used with this library:

* `CUMULUS_MESSAGE_ADAPTER_DISABLED=true`
  * Defaults to false. This env var disables Cumulus Message Adapter. This can be used to turn off the message adapter for tasks that adapt the message on their own, or for testing.
* `CUMULUS_MESSAGE_ADAPTER_DIR`
  * The default directory for Cumulus Message Adapter is the root directory of the lambda function.

## Development

### Running Tests

To run the tests for this package, run `npm run lint && npm test`

## Why use this approach

This approach has a few major advantages:

1. It explicitly prevents tasks from making assumptions about data structures
   like `meta` and `cumulus_meta` that are owned internally and may therefore
   be broken in future updates. To gain access to fields in these structures,
   tasks must be passed the data explicitly in the workflow configuration.
1. It provides clearer ownership of the various data structures. Operators own
   `meta`. Cumulus owns `cumulus_meta`. Tasks define their own `config`,
   `input`, and `output` formats.
1. The Cumulus Message Adapter greatly simplifies running Lambda functions not
   explicitly created for Cumulus.
1. The approach greatly simplifies testing for tasks, as tasks don't need to
   set up cumbersome structures to emulate the message protocol and can just
   test their business function.
"
27,nasa/pvslib,Common Lisp,"NASALib
=

NASALib is a continuing collaborative effort that has spanned over 3 decades,
to aid in research related to theorem proving  sponsored by NASA
(https://shemesh.larc.nasa.gov/fm/pvs/).
It consists of a collection of formal development (i.e.,
<i>libraries</i>) written in the Prototype Verification System
([PVS](http://pvs.csl.sri.com)), contributed by SRI, NASA,NIA, and the PVS community, and maintained by the
[NASA/NIA Formal Methods Team at LaRC](http://shemesh.larc.nasa.gov/fm).

# Release
The current version of NASALib is 7.1.0 (11/05/20) and requires [PVS 7.1](http://pvs.csl.sri.com/).

# Libraries

Currently, NASALib consists of 53 libraries and includes almost 30K lemmas.

| Library  | Description | 
| --- | --- | 
| [ACCoRD](./ACCoRD/README.md) | Framework for the analysis of air traffic conflict detection and resolution algorithms | 
| [affine_arith](./affine_arith/README.md) | Formalization of affine arithmetic and strategy for evaluating polynomial functions with variables on interval domains. |
| [algebra](./algebra/README.md) | Groups, monoids, rings, etc. |
| [analysis](./analysis/README.md) | Real analysis, limits, continuity, derivatives, integrals. |
| [ASP](./ASP/README.md) | Denotational semantics of Answer Set Programming. |
| [aviation](./aviation/README.md) | Support definitions and properties for aviation-related formalizations. |
| [Bernstein](./Bernstein/README.md) | Formalization of multivariate Bernstein polynomials. |
| [CCG](./CCG/README.md) | Formalization of diverse termination criteria. |
| [complex](./complex/README.md) | Complex numbers. |
| [complex_alt](./complex_alt/README.md) | Alternative formalization of complex numbers. |
| [complex_integration](./complex_integration/README.md) | Complex integration. |
| [co_structures](./co_structures/README.md) | Sequences of countable length defined as co-algebraic datatypes. |
| [digraphs](./digraphs/README.md) | Directed graphs: circuits, maximal subtrees, paths, DAGs. |
| [exact_real_arith](./exact_real_arith/README.md) | Exact real arithmetic including trig functions. |
| [examples](./examples/README.md) | Examples of application of the functionality provided by NASALib. |
| [extended_nnreal](./extended_nnreal/README.md) | Extended non-negative reals. |
| [fast_approx](./fast_approx/README.md) | Approximations of standard numerical functions. |
| [fault_tolerance](./fault_tolerance/README.md) | Fault tolerance protocols. |
| [float](./float/README.md) | Floating point numbers and arithmetic. |
| [graphs](./graphs/README.md) | Graph theory. |
| [groups](./groups/README.md) | Group theory. |
| [interval_arith](./interval_arith/README.md) | Interval arithmetic and numerical approximations. Includes automated strategies numerical for computing numerical approximations and interval for checking satisfiability and validity of simply quantified real-valued formulas. This development includes a formalization of Allen interval temporal logic. |
| [ints](./ints/README.md) | Integer division, gcd, mod, prime factorization, min, max. |
| [lebesgue](./lebesgue/README.md) | Lebesgue integral with connection to Riemann Integral. |
| [linear_algebra](./linear_algebra/README.md) | Linear algebra. |
| [lnexp](./lnexp/README.md) |  Logarithm, exponential and hyperbolic functions. & Foundational definitions of logarithm, exponential and hyperbolic functions. |
| [matrices](./matrices/README.md) | Executable specification of MxN matrices. This library includes computation of inverse and basic matrix operations such as addition and multiplication. |
| [measure_integration](./measure_integration/README.md) | Sigma algebras, measures, Fubini-Tonelli Lemmas. |
| [MetiTarski](./MetiTarski/README.md) | Integration of MetiTarski, an automated theorem prover for real-valued functions. |
| [metric_space](./metric_space/README.md) | Domains with a distance metric, continuity and uniform continuity. |
| [numbers](./numbers/README.md) | Elementary number theory. |
| [orders](./orders/README.md) | Abstract orders, lattices, fix points. |
| [power](./power/README.md) | Generalized Power function (without ln/exp). |
| [probability](./probability/README.md) | Probability theory. |
| [PVS0](./PVS0/README.md) | Formalization of fundamental computability concepts. |
| [PVSioChecker](./PVSioChecker/README.md) | Animation of PVS specifications. |
| [reals](./reals/README.md) | Summations, sup, inf, sqrt over the reals, absolute value, etc. |
| [Riemann](./Riemann/README.md) |  Riemann integral. |
| [scott](./scott/README.md) | Scott topology. |
| [series](./series/README.md) | Power series, comparison test, ratio test, Taylor's theorem. |
| [sets_aux](./sets_aux/README.md) | Power sets, orders, cardinality over infinite sets. Includes functional and relational facts based on Axiom of Choice and refinement relations based on equivalence relations. |
| [shapes](./shapes/README.md) | 2D-Shapes: triangle, parallelogram, rectangle, circular segment |
| [sigma_set](./sigma_set/README.md) | Summations over countably infinite sets. |
| [sorting](./sorting/README.md) | Sorting algorithms. |
| [structures](./structures/README.md) | Bounded arrays, finite sequences, bags, and several other structures. |
| [Sturm](./Sturm/README.md) |  Formalization of Sturm's theorem for univariate polynomials. Includes strategies `sturm` and `mono-poly` for automatically proving univariate polynomial relations over a real interval. |
| [Tarski](./Tarski/README.md) | Formalization of Tarski's theorem for univariate polynomials. Includes strategy tarski for automatically proving systems of univariate polynomial relations on the real line. |
| [topology](./topology/README.md) | Continuity, homeomorphisms, connected and compact spaces, Borel sets/functions. |
| [trig](./trig/README.md) | Trigonometry: definitions, identities, approximations. |
| [TRS](./TRS/README.md) | Term rewrite systems and Robinson unification algorithm. |
| [TU_games](./TU_games/README.md) | Cooperative TU-games. |
| [vect_analysis](./vect_analysis/README.md) | Limits, continuity, and derivatives of vector functions. |
| [vectors](./vectors/README.md) | 2-D, 3-D, 4-D, and n-dimensional vectors. |
| [while](./while/README.md) | Semantics for the programming language While. |

## Dependencies

Check the [NASALib dependency graph](docs/all-theories.svg ""Dependency Graph"").

# Scripts

NASALib also provides a collection of scripts that automates several tasks.

* [`proveit`](./Scripts.md#proveit) (*) - Runs PVS in batch mode 
* [`provethem`](./Scripts.md#provethem) (*) - Runs `proveit` on several libraries 
* [`pvsio`](./Scripts.md#pvsio) (*) - Command-line utility to run the PVSio ground evaluator.
* [`prove-all`](./Scripts.md#prove-all) - Runs `proveit` on each library
  in NASALib by wrapping `provethem` in order to provide a specific kind of run. 
* [`cleanbin-all`](./Scripts.md#cleanbin-all) - Clean `.pvscontext` and binary files from PVS libraries.
* [`find-all`](./Scripts.md#find-all) - Searches strings matching a given regular expressions in PVS libraries.
* [`dependencygraph`](./Scripts.md#dependencygraph) - Generates a library dependency graph for libraries in the current directory.
* [`dependency-all`](./Scripts.md#d#dependency-all) - Generates the dependency graphs for the PVS libraries in the current folder.

Click [here](docs/Scripts.md) for more details on these scripts.

(*) Already included in the PVS 7.1 distribution.

# Getting NASALib

## Via VSCode-PVS (recommended for new PVS users)

NASALib (v7.0.1) is fully compatible with
[VSCode-PVS](http://github.com/nasa/vscode-pvs), a modern graphical
interface to PVS based on
[Visual Studio Code](https://code.visualstudio.com). The latest
version of NASALib can be installed from VSCode-PVS.

## Development Version

For PVS advanced users, the development version is available from [GitHub](https://github.com/nasa/pvslib). 
To clone the development version, type the following command inside directory where PVS 7.0 is installed. Henceforth, that directory will be referred to as `<pvsdir>`. In the following commands, the dollar sign 
represents the prompt of the operating system.

```shell
$ git clone http://github.com/nasa/pvslib nasalib 
```

The command above will put a copy of the library in the directory `<pvsdir>/nasalib`.

### Major Recent Changes

* **The library `trig_fnd` is now deprecated**. It's still provided for backward compatibility, but it should be replaced by `trig`.  The new library `trig`, which used to be axiomatic, is now foundational. However, in contrast to `trig_fnd`, trigonometric definitions are based on infinite series, rather than integrals. This change considerably reduces the type-checking of theories involving trigonometric functions. The change from `trig_fnd` to `trig` should not have a major impact in your formal developments since names of definitions and lemmas are the same. However, theory importing may be slightly different.

* The PVS developments `TCASII`, `WellClear`,  and `DAIDALUS` are now [available](https://github.com/nasa/WellClear/tree/master/PVS) as part of the [GitHub WellClear distribution](https://github.com/nasa/WellClear). The PVS development `PRECiSA`  is now [available](https://github.com/nasa/PRECiSA/tree/master/PVS) as part of the [GitHub PRECiSA distribution](https://github.com/nasa/PRECiSA). The PVS development `PolyCARP`  is now [available](https://github.com/nasa/PolyCARP/tree/master/PVS) as part of the [GitHub PolyCARP distribution](https://github.com/nasa/PolyCARP).


# Manual Installation

The following instructions assume that NASALib is located in the directory `<pvsdir>/nasalib`.

## 1) Add this directory to the environment variable `PVS_LIBRARY_PATH`

If it does not exists, creates such variable and with the path of this directory as only content. It is usually very useful to have your shell systems creating this variable at startup. To this end, and depending upon your shell, you may want to add one of the following lines in your startup script.  For C shell (csh or tcsh), you may add this line in `~/.cshrc`:
```shell
setenv PVS_LIBRARY_PATH ""<pvsdir>/nasalib""
```
For Borne shell (bash or sh), add this line in either `~/.bashrc` or `~/.profile`:
```shell
export PVS_LIBRARY_PATH=""<pvsdir>/nasalib""
```

## 2) Additional steps to protect previous NASALib configurations (optional)

If you had a previous installation of NASALib, either remove the file `~/.pvs.lisp` or, if you have a special configuration in that file, remove the following line  
```lisp
(load ""<pvsdir>/nasalib/pvs-patches.lisp"") 
```
## 3) Install Scripts

Finally, go to the directory `<pvsdir>/nasalib` and run the following shell scripts (the dollar sign represents the prompt of the operating system).

The `install-scripts` command will update and install NASALib scripts as needed.
~~~shell
$ ./install-scripts
~~~

## Older Versions 
Older versions of NASALib are available from 
[http://shemesh.larc.nasa.gov/fm/ftp/larc/PVS-library](http://shemesh.larc.nasa.gov/fm/ftp/larc/PVS-library).

# Contributors

NASALib has grown over the years thanks to the contribution of several people, among them:

* [Aaron Dutle](http://shemesh.larc.nasa.gov/people/amd), NASA, USA
* Alfons Geser, HTWK Leipzig, Germany
* Amer Tahat, Michigan Technological University, USA
* Amy Isvik, Wartburg College, USA
* Ana Cristina Rocha Oliveira, University of Brasilia, Brazil
* Andr√© Galdino, Federal University of Goi√°s, Brazil
* Andreia Avelar Borges, University of Brasilia, Brazil
* Anthony Narkawicz, formerly at NASA, USA
* Ariane Alves Almeida, University of Brasilia, Brazil
* [Bruno Dutertre](http://www.csl.sri.com/users/bruno), SRI, USA
* Ben Di Vito, NASA (retired), USA
* [C√©sar Mu√±oz](http://shemesh.larc.nasa.gov/people/cam), NASA, USA
* Cl√©ment Blaudeau, EPFL, Switzerland and Ecole Polytechnique, France 
* Concepci√≥n Vidal, University of La Coru√±a, Spain
* David Griffioen,CWI, The Netherlands
* [David Lester](http://apt.cs.man.ac.uk/people/dlester), Manchester University, UK
* Dragan Stosic, Ireland
* [√ârik Martin-Dorel](http://erik.martin-dorel.org/), U. Montpellier 2 & U. of Perpignan (formerly), France
* Felicidad Aguado, University of La Coru√±a, Spain
* Flavio L.C. de Moura, University of Brasilia, Brazil
* [Gilles Dowek](https://who.rocq.inria.fr/Gilles.Dowek/index-en.html), INRIA, France
* [George Hagen](http://shemesh.larc.nasa.gov/people/geh), NASA, USA
* Gilberto Perez, University of La Coru√±a, Spain
* Gregory Anderson, University of Texas at Austin, USA
* Hanne Gottliebsen, formerly at NIA, USA
* Heber Herencia-Zapana, formerly at  NIA, USA
* J. Tanner Slagel, NASA, USA
* Jerry James, Utah State University, USA
* [Jeff Maddalon](http://shemesh.larc.nasa.gov/people/jmm), NASA, USA
* Jon Sjogren, Department of Defense, USA
* John Siratt, formerly at University of Arkansas at Little Rock, USA
* Katherine Cordwell, CMU, USA
* Kristin Rozier, formerly at NASA, USA
* [Lee Pike](http://corp.galois.com/lee-pike), formerly at Galois, USA
* [Marco A. Feli√∫](https://www.nianet.org/directory/research-staff/marco-feliu/), NIA, USA
* [Mariano Moscato](https://www.nianet.org/directory/research-staff/mariano-moscato/), NIA, USA
* [Mauricio Ayala-Rinc√≥n](http://www.mat.unb.br/~ayala), University of Brasilia, Brazil
* [Natarajan Shankar](http://www.csl.sri.com/users/shankar), SRI, USA
* Pablo Ascariz, formerly at University of La Coru√±a, Spain
* [Paul Miner](http://shemesh.larc.nasa.gov/people/psm), NASA, USA
* Pedro Cabalar, University of La Coru√±a, Spain
* Radu Siminiceanu, formerly at NIA, USA
* Ricky Butler, NASA (retired), USA
* [Silvie Boldo](https://www.lri.fr/~sboldo), INRIA, France
* [Sam Owre](http://www.csl.sri.com/users/owre), SRI, USA
* Thaynara de Lima, Federal University of Goi√°s, Brazil
* Thiago Mendon√ßa Ferreira Ramos, University of Brasilia, Brazil
* Thomas Norris
* V√≠ctor Carre√±o, NASA (retired), USA

If we have incorrectly attributed a PVS development or you have
contributed to NASALib and your name is not included here, please let
us know.

If you want to contribute please read this [guide](docs/DEVEL-GUIDE.md).

DISCLAIMER
--
NASALib is a collection of formal specifications most of
which have been in the public domain for several years. The Formal
Methods Team at NASA LaRC still
maintains these developments. For the developments originally made by
the Formal Methods Team, these
developments are considered fundamental research that do not
constitute software. Contributions made by others may have particular
licenses, which are listed in the file `top.pvs` in each
respective directory.  In case of doubt, please contact the developers
of each contribution, which are also listed in that file.

PVS patches, which are included in the directory `pvs-patches`, are part of the
PVS source code and they are covered by the PVS open source license.

Some proof strategies require third party research tools, e.g.,
MetiTarski and Z3. For convenience, they are included in this
repository with permission from their authors. Licenses for these
tools are also included as appropriate.

Enjoy it.

[The NASA/NIA Formal Methods Team at LaRC](http://shemesh.larc.nasa.gov/fm)

Contact: [C√©sar A. Mu√±oz (NASA)](http://shemesh.larc.nasa.gov/people/cam)
"
28,nasa/skeleton_app,C,"# Core Flight System : Framework : App : Sample

This repository contains a sample application (sample_app), which is a framework component of the Core Flight System.

This sample application is a non-flight example application implementation for the cFS Bundle. It is intended to be located in the `apps/sample_app` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes sample_app as a submodule), which includes build and execution instructions.

sample_app is an example for how to build and link an application in cFS.

## Version Notes
- 1.1.5  
  - Fix to build on RASPBIAN OS
  - Minor updates (see https://github.com/nasa/sample_app/pull/47)
- 1.1.4  
  - Fix for a clean build with OMIT_DEPRECATED
  - Minor updates (see https://github.com/nasa/sample_app/pull/44)
- 1.1.3
  - Minor updates (see https://github.com/nasa/sample_app/pull/34)
- 1.1.2
  - Minor updates (see https://github.com/nasa/sample_app/pull/20)
- 1.1.1
  - Minor updates (see https://github.com/nasa/sample_app/pull/15)
- **1.1.0 OFFICIAL RELEASE**:
  - Minor updates (see https://github.com/nasa/sample_app/pull/11)
  - Not backwards compatible with OSAL 4.2.1
  - Released as part of cFE 6.7.0, Apache 2.0
- **1.0.0a OFFICIAL RELEASE**:
  - Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

As a sample application, extensive testing is not performed prior to release and only minimal functionality is included.  Note discrepancies likely exist between this application and the example detailed in the application developer guide.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.

Official cFS page: http://cfs.gsfc.nasa.gov
"
29,nasa/ow_autonomy,C++,"Notices:
--------
Copyright ¬© 2020 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers
-----------
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.


ow_autonomy
===========

This package contains a candidate onboard autonomy component for an Ocean
Worlds lander, namely a ROS node (named `autonomy node`) embedding a PLEXIL plan
executive.


Contents
--------

src/plans directory contains the PLEXIL plans.

src/plexil-adapter contains the supporting code needed to run the PLEXIL plans,
and also the ROS node implementation.

See the README.md files in each subdirectory for more information.


Build
-----

See the ""Getting Started"" section in the parent repository's README.md file,
viewable at https://github.com/nasa/ow_simulator, for complete installation and
build instructions as well as hardware/software requirements.  This file recaps
a few key points and has some supplementary information.

A prerequisite for building and running this package is a working PLEXIL
installation, which has its own prerequisites. The environment variable
PLEXIL_HOME must be set to PLEXIL's installation pathname.

Your ROS environment should also first be set up:

```bash
 source <catkin-workspace>/devel/setup.sh
```

Assumed is this directory filed properly within an OceanWATERS ROS workspace
(see Conftluence for instructions).  Build the entire workspace with:

```bash
 catkin build
```

Build just the ow_autonomy package with:

```bash
 catkin build ow_autonomy
```

NOTE: If any new PLEXIL plans (.plp or .ple files) have been added since your
last build, a clean rebuild of ow_autonomy is needed.  See bottom of this file
for instructions.


Start the autonomy node
-----------------------

1. First you must start the simulator, e.g.

```bash
  roslaunch ow europa_terminator.launch
```
   NOTES:
    - to omit the Gazebo GUI for faster performance, add `gzclient:=false`
    - for alternate terrains, other launch files are available:
      atacama_y1a.launch, europa_terminator_workspace.launch,
      europa_test_dem.launch.

2. Next start the autonomy node.  Starting the autonomy node always runs a
   PLEXIL plan.  The simplest version is:

   `roslaunch ow_autonomy autonomy_node.launch`

   This invocation loads the default PLEXIL plan, Demo.plx.  A specific plan may
   be run by adding it to the command line, e.g.

   `roslaunch ow_autonomy autonomy_node.launch plan:=ReferenceMission1.plx`

   The argument given to the `plan` parameter must be a file found in :

   `<ow_workspace>/devel/etc/plexil`
   
   See `plans/README.md` for a description of the PLEXIL plans.


Fault Detection
---------------

A rudimentary version does nothing more than check relevant fault parameters for
each lander operation.  Run any plan you like, while setting fault parameters as
desired (see `ow_simulatyor/ow_faults/README.md` for instructions).  You can
also use `scripts/set-faults.py` which by default sets every fault; edit it as
needed.

Faults are simply reported, as ROS warnings.


Clean
-----

To clean (remove all build products from) just the ow_autonomy package:

 `cd <ow_workspace>/build`
 `rm -rf ow_autonomy`

To clean the entire ROS workspace (not needed if you only want to rebuild
ow_autonomy):

  `catkin clean`
"
30,nasa/nos3,C,"# NASA Operational Simulator for Small Satellites
The NASA Operational Simulator for Small Satellites (NOS3) is a suite of tools developed by NASA's Katherine Johnson Independent Verification and Validation (IV&V) Facility to aid in areas such as software development, integration & test (I&T), mission operations/training, verification and validation (V&V), and software systems check-out. 
NOS3 provides a software development environment, a multi-target build system, an operator interface/ground station, dynamics and environment simulations, and software-based models of spacecraft hardware.

### Known Issues
1. Not all cFS delivered apps are included and supported at this time. Currently supported are: CI, TO, SCH, SC, HK, CFS_LIB
2. CentOS support not included in this release
These issues will be addressed in future updates

### Documentation
The best source of documentation can be found at [NOS3](www.nos3.org), as well as a PDF Users Guide attached to this Release on Github

### Prerequisites
Each of the applications listed below are required prior to performing the installation procedure:
* [Git 1.8+](https://git-scm.com/)
* [Vagrant 2.2.3+](https://www.vagrantup.com/)
* [VirtualBox 6.1+](https://www.virtualbox.org/)

### Installing
1. Open a terminal
2. Navigate to the desired location for the repository
3. Clone the repository `git clone https://github.com/nasa/nos3.git`
4. Clone the submodules `git submodule init` and `git submodule update` 
5. Navigate to `/nos3/deployment`
6. Run `vagrant up` and wait to return to a prompt
	- This can take anywhere from 20 minutes to hours depending on internet speeds and host PC specs
  	- The VM will reboot multiple times in order to finish install packages for you automatically so wait for that prompt!
	- **Sometimes ansible does not seem to install and there is an error like ""Could not get lock /var/lib/apt/lists/lock"".  If this happens run `vagrant provision` to install ansible and provision.**
7. Login to the nos3 user using the password `nos3123!` and get to work!
7. Try building and running following the instructions below

### Getting started
It is recommended to share the nos3 repository into the virtual machine
1. Open a terminal
2. To build use the `make` command from the nos3 repo
3. To run nos3 use the `make launch` command from the nos3 repo
4. To halt nos3 use the `make stop` command from the nos3 repo

### Directory Layout
* `/nos3/deployment` contains the repository for generating the nos3 virtual environment
* `/nos3/fsw` contains the repositories needed to build cFS FSW
	- /apps - the open source cFS apps
	- /cfe - the core flight system (cFS) source files
	- /components - the hardware component apps
	- /osal - operating system abstraction layer (OSAL), enables building for linux and flight OS
	- /psp - platform support package (PSP), enables use on multiple types of boards
	- /tools - standard cFS provided tools
* `/nos3/gsw` contains the nos3 ground station files, and other ground based tools
	- /ait - Ammos Instrument Toolkit (Untested for 1.05.0)
	- /cosmos - COSMOS files
	- /OrbitInviewPowerPrediction - OIPP tool for operators
	- /scripts - convience scripts
* `/nos3/sims` contains the nos3 simulators and configuration files
	- /cfg - 42 files and NOS3 top level config file
	- /_sim - a component simulator
	- /nos_time_driver - time syncronization for all components
	- /sim_common - common files used by component simulators
	- /sim_server - NOS Engine Server config and build files
	- /sim_terminal - terminal for testing on NOS Engine busses

## Support
If this project interests you or if you have any questions, please feel free to contact any developer directly or email `support@nos3.org`.

## Reporting Issues
Please report issues to the tracking system on Github [NOS3 Issues](www.github.com/nasa/nos3/issues)

### Frequently Asked Questions
* A GUI environment hasn't shown up after an extended period (> 1.5 hours), what should I do?
  - Stop the provision, delete the existing VM (if it was created), and try again
  - `CTRL + C`, `vagrant destroy`, `y`, `vagrant up`, wait, `vagrant reload`
* What is the root username and password?
  - `vagrant` with password `vagrant`
* Why doesn't the shared clipboard work?
  - You will most likely need to re-install / update the guest additions and reboot for this to function properly
  - In the VirtualBox menu select: Devices -> Insert Guest Additions CD Image...
  - Follow the instructions provided
* How can I mount a shared folder so that I edit on my host and compile / run in the VM?
  - In the VirtualBox menu select: Devices -> Shared Folders -> Shared Folders Settings...
  - Select the folder with a plus sign to add your folder
	  * Provide the path, name, mount point inside the VM
		* Select `Auto-mount`, `Make Permanent`, and `OK`
* How can I run 42 without the GUI?
  - Edit the `/nos3/sims/cfg/InOut/Inp_Sim.txt` and set Graphics Front End to `FALSE` 
* NOS Engine Standalone server reports `NOSEngine.Uart - close uart port failed` error?
	- This isn't actually an error and is scheduled to be removed, proceed as usual.

### Versioning
We use [SemVer](http://semver.org/) for versioning. For the versions available, see the tags on this repository.

### License
This project is licensed under the NOSA (NASA Open Source Agreement) License. 

### Acknowledgments
* Special thanks to all the developers involved!

"
31,nasa/harmony-py,Python,"# harmony-py

Harmony-Py is a Python library for integrating with NASA's [Harmony](https://harmony.earthdata.nasa.gov/) Services.

Harmony-Py provides a Python alternative to directly using [Harmony's RESTful API](https://harmony.earthdata.nasa.gov/docs/api/). It handles NASA [Earthdata Login (EDL)](https://urs.earthdata.nasa.gov/home) authentication and optionally integrates with the [CMR Python Wrapper](https://github.com/nasa/eo-metadata-tools) by accepting collection results as a request parameter. It's convenient for scientists who wish to use Harmony from Jupyter notebooks as well as machine-to-machine communication with larger Python applications.

Harmony-Py is a work-in-progress, is not feature complete, and should only be used if you would like to test its functionality. We welcome feedback on Harmony-Py via [GitHub Issues](https://github.com/nasa/harmony-py/issues)

![Python package](https://github.com/nasa/harmony-py/workflows/Python%20package/badge.svg)

[![Documentation Status](https://readthedocs.org/projects/harmony-py/badge/?version=latest)](https://harmony-py.readthedocs.io/en/latest/?badge=latest)

# Using Harmony Py

## Prerequisites

* Python 3.7+


## Installing

The library is available from [PyPI](https://pypi.org/project/harmony-py/) and can be installed with pip:

        $ pip install -U harmony-py

This will install harmony-py and its dependencies into your current Python environment. It's recommended that you install harmony-py into a virtualenv along with any other dependencies you may have.


# Running Examples & Developing on Harmony Py

## Prerequisites

* Python 3.7+
* (optional,recommended) [pyenv](https://github.com/pyenv/pyenv) and [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv)


## Installing Development & Example Dependencies

First, it's recommended that you create a Python virtualenv so that Harmony Py and its dependencies are isolated in their own environment. To do so, you can either [create and activate a Python virtual environment with venv](https://docs.python.org/3/tutorial/venv.html), or--if you have pyenv and pyenv-virtualenv installed--use pyenv to create and activate one for you (`harmony-py`). There are `make` targets for both of these options--choose one.

1a. Create a virtualenv with venv:

        $ make venv-setup
        $ source .venv/bin/activate

To deactivate it:

        $ source deactivate

1b. Use pyenv & pyenv-virtualenv. This will install Python 3.9 & create a virtualenv using that version of Python. Important: if this is your first time using pyenv to install Python, be sure that you have the [Python build requirements installed](https://github.com/pyenv/pyenv/wiki#suggested-build-environment) first.

        $ make pyenv-setup

If you've setup pyenv with your shell properly, it should automatically activate the environment. You can check if it's activated by:

        $ pyenv version

It should show `harmony-py`. Pyenv does auto-activation by creating a `.python-version` file in the project directory. Most shells can be setup to automatically activate & deactivate virtual environments when cd'ing into & out of directories by using the value found in `.python-version`. This is convenient since it ensures that the correct virtualenv has been activated (and deactivated) when starting work on a project. See the pyenv docs for more details. If you need to manually activate & deactivate:

        $ pyenv activate harmony-py
        $ pyenv deactivate

2. Install dependencies:

        $ make install

3. Optionally register your local copy with pip:

        $ pip install -e ./path/to/harmony_py


## Running the Example Jupyter Notebooks

Jupyter notebooks in the `examples` subdirectory show how to use the Harmony Py library. Start up the Jupyter Lab notebook server and run these examples:

The Jupyter Lab server will start and [open in your browser](http://localhost:8888/lab). Double-click on a notebook in the file-browser sidebar and run the notebook. Note that some notebooks may have cells which prompt for your EDL username and password. Be sure to use your UAT credentials since most of the example notebooks use the Harmony UAT environment.

        $ make examples


## Developing

### Generating Documentation

Documentation on the Read The Docs site is generated automatically. It is generated by using `sphinx` with reStructuredText (.rst) and other files in the `docs` directory. To generate the docs locally and see what they look like:

        $ make docs

You can then view the documentation in a web browser under `./docs/_build/html/index.html`.

IMPORTANT: The documentation uses a notebook from the `examples` directory rendered as HTML. If you've modified that notebook (see `Makefile` for notebook that is currently rendered), you will need to run `make docs` locally. You will see a change to the `docs/user/notebook.html` file after doing so. This file should be committed to the git repo since it is used when the latest docs are pushed to the Read The Docs site (it can't currently be generated as part of the build).

### Running the Linter & Unit Tests

Run the linter on the project source:

        $ make lint

Run unit tests and test coverage. This will display terminal output and generate an HTML coverage report in the `htmlcov` directory.

        $ make test

For development, you may want to run the unit tests continuously as you update tests and the code-under-test:

        $ make test-watch


### Generating Request Parameters

The `harmony.Request` constructor can accept parameters that are defined in the [Harmony OGC API schema](). If this schema has been changed and the `Request` constructor needs to be updated, you may run the generator utility. This tool reads the Harmony schema and generates a partial constructor signature with docstrings:

        $ python internal/genparams.py ${HARMONY_DIR}/app/schemas/ogc-api-coverages/1.0.0/ogc-api-coverages-v1.0.0.yml

Either set `HARMONY_DIR` or replace it with your Harmony project directory path. You may then write standard output to a file and then use it to update the `harmony.Request` constructor and code.

## CI

Harmony-py uses [GitHub
Actions](https://github.com/nasa/harmony-py/actions) to run the Linter
& Unit Tests. The test coverage output is saved as a build artifact.

## Building and Releasing

If a new version of Harmony-Py will be released then the `master` branch should be tagged with an updated version:

        $ git checkout master
        $ git tag -a v1.2.3    # where v1.2.3 is the next version number

In order to generate new package and wheel files, do the following:

        $ make build

`make` reads the current version number based on git tag, populates the version in `harmony/__init__.py`, and `setup.py` reads the version number from `harmony/__init__.py` for packaging purposes.

This leaves us with a modifed __init\__.py which must be committed and pushed to `master`.

        $ git add harmony/__init__.py
        $ git commit -m ""Version bump to v1.2.3""
        $ git tag -f
        $ git push

Then, provided API tokens are in order, the following runs the build target and publishes to PyPI:

        $ make publish
"
32,nasa/zarr-eosdis-store,Python,"zarr-eosdis-store
=================

The zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently
by the `Zarr Python library <https://zarr.readthedocs.io/en/stable/index.html>`_, provided they
have a sidecar DMR++ metadata file generated.

Installation
============

This module requires Python 3.8 or greater::

    $ python --version
    Python 3.8.2

Install from PyPI::

    $ pip install zarr-eosdis-store

To install the latest development version::

    $ pip install pip install git+https://github.com/nasa/zarr-eosdis-store.git@main#egg=zarr-eosdis-store

Earthdata Login
===============

To access EOSDIS data, you need to sign in with a free NASA Earthdata Login account, which you can obtain at
`<https://urs.earthdata.nasa.gov/>`_.

Once you have an account, you will need to add your credentials to your ``~/.netrc`` file::

    machine urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD

If you are accessing test data, you will need to use an account from the Earthdata Login test system at
`<https://uat.urs.earthdata.nasa.gov/>`_ instead, adding a corresponding line to your ``~/.netrc`` file::

    machine uat.urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD


Usage
=====

To use the library, simply instantiate ``eosdis_store.EosdisStore`` with the URL to the data file you would
like to access, pass it to the Zarr library as you would with any other store, and use the Zarr API as with any
other read-only Zarr file.  Note: the URL to the data file will typically end with an HDF5 or NetCDF4 extension,
not .zarr.

.. code-block:: python

   from eosdis_store import EosdisStore
   import zarr

   # Assumes you have set up .netrc with your Earthdata Login information
   f = zarr.open(EosdisStore('https://example.com/your/data/file.nc4'))

   # Read metadata and data from f using the Zarr API
   print(f['parameter_name'][0:0:0])

If the data has _FillValue (to flag nodata), scale_factor, or add_offset set (defined in metadata using CF-conventions)
they can be retrieved from the parameter attributes.

.. code-block:: python

  import numpy as np

  scale_factor = f['parameter_name].scale_factor
  add_offset = f['parameter_name].add_offset
  nodata = f['parameter_name]._FillValue

  arr = f['parameter_name'][] * scale_factor + add_offset

  nodata_locs = np.where(arr == nodata)


A better way to handle these is to use XArray. Rather than reading the data immediately when a slice is requested, XArray
defers the read until the data is actually accessed. With the Zarr backend to XArray, the scale and offset can be set so that
when the data is accessed it will apply those values. This is more efficient if the data is going to be used in other operations.

The scale_factor and get_offset will be used if specified in the NetCDF/HDF5 file.

.. code-block:: python

  import xarray

  store = EosdisStore('https://example.com/your/data/file.nc4')

  f = xarray.open_zarr(store)

  # the data is not read yet
  xa = f['parameter_name'][<slice>]

  # convert to numpy array, data is read
  arr = xa.values

The resulting array will have had scale and offset applied, and any element that is equal to the _FillValue attribute will be
set to numpy `nan`. To use XArray without apply the scale and offset or setting the nodata to `nan`, supply the `mask_and_scale`
keyword to xarray.open_zarr to False:

.. code-block:: python

  store = EosdisStore('https://example.com/your/data/file.nc4')

  f = xarray.open_zarr(store, mask_and_scale=False)


Technical Summary
=================

We make use of a technique to read NetCDF4 and some HDF5 files that was prototyped by The HDF Group and USGS, described
`here <https://medium.com/pangeo/cloud-performant-reading-of-netcdf4-hdf5-data-using-the-zarr-library-1a95c5c92314)>`_.

To allow the technique to work with EOSDIS data, we have extended it and optimized access in the following key ways:

* The ``EosdisStore`` reads a DMR++ file generated by OPeNDAP to present its metadata and determine byte offsets to the
  Zarr library. By reusing these, we avoid needing to generate new metadata sidecar files to support new data.

* The store uses HTTPS and authenticates with a ``.netrc`` entry, rather than the S3 API, making it compatible with
  EOSDIS access patterns and requirements

* The store caches redirect URLs for a period of time set by the Cache-Control header.  Doing this avoids the overhead
  of repeated redirects when accessing parts of files.

* The store uses a parallel API that allows it to make more efficient access optimizations:
*
  * When the Zarr library accesses data that requires reading multiple near-sequential bytes in the file, the store combines
    these smaller requests into a single larger request.

  * After an initial request to cache any authentication and redirect information, the store runs subsequent requests in
    parallel.

Development
===========

Clone the repository, then ``pip install`` its dependencies::

    pip install -r requirements.txt
    pip install -r requirements-dev.txt

To check code coverage and run tests::

    coverage run -m pytest

To check coding style::

    flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

To build documentation, generated at ``docs/_build/html/index.html``::

    cd docs && make html
"
33,nasa/Common-Metadata-Repository,Clojure,"# Common Metadata Repository

Visit the CMR at [https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository](https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository)

## About

The Common Metadata Repository (CMR) is an earth science metadata repository
for [NASA](https://www.nasa.gov/) [EOSDIS](https://earthdata.nasa.gov) data. The CMR
Search API provides access to this metadata.

## Client-facing Components

- Search
  - Allows the user to search by collections, granules, and concepts with a
    myriad of different query types
  - API Docs: https://cmr.earthdata.nasa.gov/search/site/search_api_docs.html

- Ingest
  - Ingest refers to the process of validating, inserting, updating, or
    deleting metadata in the CMR system. It affects only the metadata for the
    specific Data Partner. The CMR allows Data Partners to ingest metadata.
    records through a RESTful API
  - API Docs: https://cmr.earthdata.nasa.gov/ingest/site/ingest_api_docs.html

- Access Control
  - Access Control Lists (ACLs) are the mechanism which grants users
    access to perform different operations in the CMR. CMR ACLs follow the same
    design as ECHO ACLs, which are a superset of the generic ACL
    design pattern used in many other systems. An ACL is a
    mapping of actors (subjects) to resources (object) to operations
    (predicate).
  - Two quick examples of a CMR ACL could be:
    - All registered users have READ access to ASTER data
    - A provider's operations team may ingest data for that provider
  - API Docs: https://cmr.earthdata.nasa.gov/access-control/site/access_control_api_docs.html

## Our Development Environment

- Mac OSX
- Atom: https://atom.io/
- Proto-Repl: https://atom.io/packages/proto-repl
  - Installed and configured according to this guide: https://git.io/atom_clojure_setup

## Prerequisites

- Java 1.8.0 (a.k.a. JAVA8) only; higher versions are not currently supported.
- Leiningen (https://leiningen.org) 2.5.1 or above.
  - We've had success with Homebrew and with the install script on the
    Leiningen website.
- Ruby (used to support two legacy apps)
- Maven (https://maven.apache.org/install.html)
    - Mac OS X devs can use `brew install maven`
    - Linux devs can use `sudo apt-get install maven`
- GCC and libc
- Docker

## Obtaining the Code

You can get the CMR source code by cloning the repository from Github:

```
$ git clone git@github.com:nasa/Common-Metadata-Repository.git cmr
```

## Building and Running the CMR

The CMR is a system consisting of many services. The services can run
individually or in a single process. Running in a single process makes
local development easier because it avoids having to start many different
processes. The dev-system project allows the CMR to run from a single REPL
or Jar file. If you're developing a client against the CMR you can build and
run the entire CMR with no external dependencies from this Jar file and use
that instance for local testing. The sections below contain instructions for
running the CMR as a single process or as many processes.

#### Using the `cmr` CLI Tool

This project has its own tool that is able to do everything from initial setup to
running builds and tests on the CI/CD infrastructure. To use the tool
as we do below, be sure to run the following from the top-level CMR directory:

```
export PATH=$PATH:`pwd`/bin
source resources/shell/cmr-bash-autocomplete
```

(If you use a system shell not compatible with Bash, we'll accept a PR with
auto-complete for it.)

To make this change permanent:

```
echo ""export PATH=\$PATH:`pwd`/bin"" >> ~/.profile
echo ""source `pwd`/resources/shell/cmr-bash-autocomplete"" >> ~/.profile
```

#### Oracle Dependencies

Even if you're not going to develop against a local Oracle database,
you still need to have the Oracle libraries locally installed to use the
CMR.

Here are the steps to do so:

1. Ensure you have installed on your system the items listed above in the
   ""Prerequisites"" section.
1. Download the Oracle JDBC JAR files into `./oracle-lib/support` by
   following instructions in `./oracle-lib/README.md`. (The CMR must have these
   libraries to build but it does not depend on Oracle DB when running
   locally. It uses a local in-memory database by default.) If you're reading this
   guide on the web, [here is a handy link to the instructions.](https://github.com/nasa/Common-Metadata-Repository/tree/master/oracle-lib)
1. With the JAR files downloaded to the proper location, you're now ready
   to install them for use by the CMR:"" `cmr install oracle-libs`

#### Building and Running CMR Dev System in a REPL with CMR CLI tool

1. `cmr setup profile` and then update the new `./dev-system/profiles.clj` file.
   it will look something like this:
   ``` clojure
   {:dev-config {:env {:cmr-metadata-db-password ""<YOUR PASSWORD HERE>""
                       :cmr-sys-dba-password ""<YOUR PASSWORD HERE>""
                       :cmr-bootstrap-password ""<YOUR PASSWORD HERE>""
                       :cmr-ingest-password ""<YOUR PASSWORD HERE>""
                       :cmr-urs-password ""<YOUR PASSWORD HERE>""}}}
   ```

2. `cmr setup dev`
3. `cmr start repl`
4. Once given a Clojure prompt, run `(reset)`

Note that the `reset` action could potentially take a while, not only due to
the code reloading for a large number of namespaces, but for bootstrapping
services as well as starting up worker threads.

#### Building and Running CMR Dev System from a Jar

Assuming you have already run the above steps (namely `cmr setup dev`), to
build and run the default CMR development system (`dev-system`) from a
`.jar` file:

1. `cmr build uberjars`
2. `cmr build all`
3. `cmr start uberjar dev-system` will run the dev-system as a background task

See CMR Development Guide to read about specifying options and setting
environment variables

#### Building and Running separate CMR Applications

The following will build every application but will put each jar into the
appropriate `target` directory for each application. The command shown in step
3 is an example. For the proper command to start up each application, see the
`Applications` section below. Note: You only need to complete steps 1 and 2 once.

1. `cmr build uberjar APP`
2. `cmr run uberjar APP`

Where `APP` is any supported CMR app. You can touble-tap the `TAB` key on
your keyboard to get the `cmr` tool to show you the list of available apps
after entering `uberjar` in each step above.

Note: building uberjars will interfere with your repl. If you want to use your repl post-build you will need to,
`rm -f ./dev-system/target/`

## Checking Dependencies, Static Analysis, and Tests

There are several `lein` plugins within the CMR for performing
various tasks either at individual subproject levels or at the top-level for
all subprojects.

#### Dependency Versions

To check for up-to-date versions of all project dependencies, you can use
`cmr test versions PROJ`, where `PROJ` is any CMR sub-project under the
top-level directory.

You may run the same command without a project to check for all projects:
`cmr test versions`.

Note that this command fails with the first project that fails. If many
subprojects are failing their dependency version checks and you wish to see
them all, you may use your system shell:

```sh
for PROJ in `ls -1d */project.clj|xargs dirname`
do
  ""Checking $PROJ ...""
  cmr test versions $PROJ
  cd - &> /dev/null
done
```

#### Dependency Ambiguities and `.jar` File Conflicts

To see if the JVM is having problems resolving which version of a
dependency to use, you can run `cmr test dep-tree PROJ`. To perform this
against all projects: `cmr test dep-trees`.

#### Static Analysis and Linting

To perform static analysis and linting for a project, you can run
`cmr test lint PROJ`. As above with dependency version checking, by
not passing a project, you can run for all projects: `cmr test lint`.

#### Dependency Vulnerability Scanning

You can see if your currently installed version of CMR has any reported Common Vulnerabilities and Exploits (CVEs) by running the helpful alias `lein check-sec` that you can use in each application, or at the root folder to scan all CMR apps together.

You will find the vulnerability summary in `./target/dependency-check-report.html` in each application.

#### Testing CMR

Test files in CMR should follow the naming convention of ending in `-test`.

There are two modes of testing the CMR:

* From the REPL
* Utilizing the CI/CD script to run against an Oracle database

For the first, the steps are as follows:

1. Ensure you have set up your development environment in `dev-system`
2. If you have built any `.jar` files, run `cmr clean PROJ` (for a given
   project) or `cmr clean` to clean all projects.
3. Start the REPL: `cmr start repl`
4. Once in the REPL, start the in-memory services: `(reset)`
5. Run the tests: `(run-all-tests)` or `(run-all-tests-future)`

You have the option of substituting the last step with `(run-suites)`. This
uses a third-party tool to display clear test results which are
easier copy/paste should you want to run them on an individual basis.
These results also contain easier to read
exception messages/stacktraces. Here's an excerpt:

```
   cmr.system-int-test.ingest.provider-ingest-test
     update-provider-test
       assertion 1 ........................................................ [OK]
       assertion 2 ........................................................ [OK]
       assertion 3 ........................................................ [OK]
       assertion 4 ........................................................ [OK]
     delete-provider-test
       assertion 1 ........................................................ [OK]
       assertion 2 ........................................................ [OK]
       assertion 3 ........................................................ [OK]
       assertion 4 ........................................................ [OK]
       assertion 5 ........................................................ [OK]
       assertion 6 ........................................................ [OK]
       assertion 7 ........................................................ [OK]
       assertion 8 ........................................................ [OK]
       assertion 9 ........................................................ [OK]
```

For non-terminal based dev, depending upon your IDE/editor, you may have
shortcuts available to you for starting/restarting the services and/or running the
tests. To find out what these are you can contact a CMR core dev.

To run the tests against an Oracle database, we recommend that
you use an Oracle VM built for this purpose. You will also need
configuration and authentication information that will be set as environment
variables. Be sure to contact a CMR core dev for this information.

To run only certain types of tests, you may run the following:

##### Unit Tests

``` sh
lein modules utest
```

##### Integration Tests

If running CMR with the in-memory database (default)
``` sh
lein modules itest --skip-meta :oracle

```

If running CMR with an external database
``` sh
lein modules itest --skip-meta :in-memory-db
```

If you want to run tests against Oracle, bring up the Oracle VM and execute
the following to create the users and run the migrations:

``` sh
cmr setup db
```

Then, in the CMR REPL:

```clj
user=> (reset :db :external)
...
user=> (run-all-tests)
...
```

Those tests will take much longer to run than when done with the in-memory
database (~25m vs. ~6m). To switch back to using the in-memory database,
call `(reset :db :in-memory)`.

There is also a different, optional test runner you can use. For more details
see the docstring for `run-suites` in `dev-system/dev/user.clj`.
It will contain usage instructions
#### Testing in the CI Environment

Throughout the modules, in the `project.clj` files there are additional `lein` aliseses for 
executing the tests in the CI/CD environment. They are 
* ci-itest
* ci-utest

These run the integration and unit tests, respectively, in the CI environment. The difference
between `itest` and `ci-itest` or `utest` and `ci-utest` are the settings passed to the
kaocha test runner.

In the CI environment, color is omitted, and certain tests that require an internal memory
database are excluded. The aliases may be used locally as well. 

To see the differnce in detail, inspect the `tests.edn` files for each module to see the
profile in use in the CI environment. Kaocha supports the use of profiles so more may 
be added as necessary.

### Test Development

CMR uses the [Kaocha](https://github.com/lambdaisland/kaocha) test library.
It provides plugins and grouping capabilities. Tests are organized in each module
with the standard being `:unit` and `:integration`.

Not all modules will contain `:integration` tests.

## Code structure

The CMR comprises several small services called microservices. These are
small purposed-based services that do a small set of things well.

- For more reading on microservices: https://martinfowler.com/articles/microservices.html

### The Microservices

Each microservice has a `README` file in its root directory, which provides a
short overview of the service's functionality. There are many main
applications, as well as several libraries and support applications.

#### Applications:

- access-control-app
  - The mechanism which grants users access to perform different
    operations in the CMR. It also maintains groups and access control rules.
    Note that ECHO and URS provide user access as an external dependency.
    The mock-echo application implements both of the necessary interfaces
    for local testing.
  - Main method: cmr.access_control.runner

- bootstrap-app
  - Contains APIs for performing various bulk actions in the CMR
  - Main method: cmr.bootstrap.runner
  - See `/bootstrap-app/README.md` for a list of lein and uberjar commands

- dev-system
  - An app that combines the separate microservices of the CMR into a single
  application. We use this to simplify development
  - Main method: cmr.dev_system.runner

- indexer-app
  - This handles indexing collections, granules, and tags in Elasticsearch
  - Maintains the set of indexes in elasticsearch for each concept
  - Main method: cmr.indexer.runner

- ingest-app
  - The Ingest app handles collaborating with metadata db and indexer systems.
  This maintains the lifecycle of concepts coming into the CMR
  - Main method: cmr.ingest.runner

- search-app
  - Provides a public search API for concepts in the CMR
  - Main method: cmr.search.runner

- search-relevancy-test
  - Tests to measure and report the effectiveness of CMR's search relevancy algorithm

- system-int-test
  - Black-box, system-level tests to ensure functionality of the CMR

- virtual-product-app
  - Adds the concept of Virtual Products to the CMR. Virtual Products represent
  products that a provider generates on demand from users. This takes place when
   a user places an order or downloads a product through a URL
  - Main method: cmr.virtual_product.runner

- metadata-db-app
  - A database that maintains revisioned copies of metadata for the CMR
  - Main method: cmr.metadata_db.runner

- mock-echo-app
  - This mocks out the ECHO REST API and the URS API as well. Its purpose is to
  make it easier to integration test the CMR system without having to run a full
   instance of ECHO. It will only provide the parts necessary to enable
   integration testing. You should not expect a perfect or complete
   implementation of ECHO.
  - Main method: cmr.mock_echo.runner

#### Libraries:

- acl-lib
  - Contains utilities for retrieving and working with ACLs

- common-app-lib
  - Contains utilities used within many CMR applications

- common-lib
  - Provides common utility code for CMR projects

- elastic-utils-lib
  - A library that handles most of the interfacing with Elasticsearch

- es-spatial-plugin
  - An Elasticsearch plugin that enables spatial search within elastic

- oracle-lib
  - Contains utilities for connecting to and manipulating data in Oracle

- orbits-lib
  - Clojure wrapper of a Ruby implementation of the Backtrack Orbit Search
    Algorithm (BOSA)

- message-queue-lib
  - A library for interfacing with RabbitMQ, AWS SQS, and an in-memory message queue

- spatial-lib
  - The spatial libraries provide utilities for working with spatial areas in the CMR

- transmit-lib
  - The Transmit Library defines functions for invoking CMR services

- umm-lib
  - This is the old source of UMM schemas and translation code. Since the
    advent of umm-spec-lib we are planning to remove it

- umm-spec-lib
  - The UMM Spec lib contains JSON schemas that define the Unified Metadata
    Model, as well as mappings to other supported formats, and code to migrate
    collections between any supported formats.

## Further Reading

- CMR Client Partner User Guide: https://wiki.earthdata.nasa.gov/display/ED/CMR+Client+Partner+User+Guide
- CMR Data Partner User Guide: https://wiki.earthdata.nasa.gov/display/CMR/CMR+Data+Partner+User+Guide
- CMR Client Developer Forum: https://wiki.earthdata.nasa.gov/display/CMR/CMR+Client+Developer+Forum

## License

Copyright ¬© 2014-2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
"
34,nasa/cumulus-api,JavaScript,"## Cumulus API Docs

[![CircleCI](https://circleci.com/gh/nasa/cumulus-api.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-api)

Cumulus API documentaion: https://nasa.github.io/cumulus-api

### Installation

     $ npm install 

### Build

     $ npm run build

### Serve

     $ npm run serve

### Deploy

     $ npm run deploy"
35,nasa/cFE,C,"![Static Analysis](https://github.com/nasa/cfe/workflows/Static%20Analysis/badge.svg)

# Core Flight System : Framework : Core Flight Executive

This repository contains NASA's Core Flight Executive (cFE), which is a framework component of the Core Flight System.

This is a collection of services and associated framework to be located in the `cfe` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes build and execution instructions.

The detailed cFE user's guide can be viewed at <https://github.com/nasa/cFS/blob/gh-pages/cFE_Users_Guide.pdf>.

## Version History

### Development Build: v6.8.0-rc1+dev559

- Adds tests for nominal use cases of the ES Critical Data Store API.
- Adds functional tests for nominal uses of FS Header API.
- Adds functional tests for Time Current API.
- [docs] Makes comment blocks in source and header files more consistent for all internal, CFE core APIs. Moves information about the function behavior to its prototype in the header in doxygen format. Comment blocks on the function implementation refer back to the prototype, it does not duplicate the info. Local helper functions that are not separately prototyped, are exceptions to this pattern. Adds intended scope to all functions: global, application-internal, or file/local.
- See <https://github.com/nasa/cFE/pull/1481> and <https://github.com/nasa/cFS/pull/252>

### Development Build: v6.8.0-rc1+dev540

- Changes the type of pointer for `MemPtr` in `CFE_ES_PoolCreateNoSem` API from uint8* to void* to be more consistent and easier to use. Should be backward compatible.
Updates the doxygen documentation for this parameter, as it was incorrectly specifying a 32-bit alignment requirement.
- Adds new functional tests for ES Child Task API. Does not check edge cases. Fixed spelling mistake in `UtAssert_ResourceID_Undifeined` name
- Removes BUILDDIR reference and an old comment. No behavior changes
- Moves and renames `cfe_resourceid_basevalue.h` to `cfe_resourceid_basevalue.h`. Since it is is assumed/required that resource IDs follow the ""osal compatible"" pattern. Perhaps in a future version this could change, but
- Ensures that the `CFE_SUCCESS` constant is of the `CFE_Status_t` type. Since an explicit cast is required on all error codes that are expected to be negative values.
- Removes unused error codes: `CFE_ES_ERR_SHELL_CMD` and `CFE_SB_NO_MSG_RECV`. No impact to behavior.
- When a startup file has a line with too many tokens the build script will generate a concise warning including an indicator of which line is causing the problem.
- Confirm that the call to `CFE_ES_CDS_CachePreload` returns `CFE_SUCCESS` before continuing. No behavior changes. Now shows up as untested lines in the coverage report since error condition cannot be exercised through coverage.
- [docs] Clarify that `CFE_ES_DeleteCDS` does not wipe or erase the block, it only returns resources to the pool for re-use.
- [docs] Adds comments in `CFE_ES_RunExceptionScan` describing the logic when an exception cannot be traced back to a specific app, in that it should fall back to the PSP reset.
- `CFE_ES_GenPoolInitialize` now returns ` CFE_ES_BAD_ARGUMENT` error if the `AlignSize` passed-in value is not actually a power of two instead of ""fixing"" the alignment mask,
- Replace internal `CFE_ES_SYSLOG_APPEND` macro with the `CFE_ES_WriteToSysLog()` API since coding standards discourage use of multi-line macros.
- [docs] Improve Resource IDs documentation. Specifically on use of the various helper functions and common patterns Documents that the ""IsMatch()"" functions accept NULL pointers so they can be used with initial validation (gatekeeper). All other helper functions assume a non-NULL pointer.
- Compiler will catch if the `CFE_RESOURCEID_MAX` value changes in such a way that makes it not usable as a bit mask as intended. Add a compile time assert to ensure that `CFE_RESOURCEID_MAX` value is one less than a power of two  (i.e. an LSB-justified bit mask). Notes in the comments that it serves as both a numeric limit and a mask.
- See <https://github.com/nasa/cFE/pull/1431> and <https://github.com/nasa/cFS/pull/250>


### Development Build: v6.8.0-rc1+dev509

- Separates the list of CFE core interface modules (e.g. core_api) from the list of CFE core implementation modules (e.g. msg). This allows the content of core_api to be expanded to locally include any additional modules the user has added to cFE core via the `MISSION_CORE_MODULES` list.
- Adds documentation for `CFE_ES_RegisterCDS()` regarding clearing.
- Removes the separate CFE ""testrunner"" module and moves the logic associated with running a test into cfe_assert library. Converts the ""testcase"" module from a library into an app, by calling into the runner logic that is now inside cfe_assert. Each functional test is a separate app, not a library, so it can be started and stopped via ES command like any other app.
- Removes check on `ENABLE_UNIT_TESTS=true` when building ""cfe_assert"", it should be built all the time.
- See <https://github.com/nasa/cfe/pull/1406> and <https://github.com/nasa/cfs/pull/248>

### Development Build: v6.8.0-rc1+dev498

- Reports test failures as CFE events. Test status messages are now sent as Events rather than Syslog. This allows for more processing capability, and allows failures to be received externally (e.g. ground system).
- See <https://github.com/nasa/cFE/pull/1295> and <https://github.com/nasa/cFS/pull/242>

### Development Build: v6.8.0-rc1+dev494

- Adds new tests for the ES Info APIs
- See <https://github.com/nasa/cFE/pull/1284> and <https://github.com/nasa/cFS/pull/238>

### Development Build: v6.8.0-rc1+dev490

- Removes `PspConfig` member from the `GLOBAL_CONFIGDATA` object. Updates the only remaining reference to this object inside the CFE_PSP_VERSION and uses the new Version API instead. Updates the OSAL version print to use the version API call and CFE uses the macro directly.
- Replaces duplicate mention of the removed `CFE_SB_ZeroCopyGetPtr` with the correct removal target of `CFE_SB_ZeroCopyReleasePtr`
- See <https://github.com/nasa/cFE/pull/1279> and <https://github.com/nasa/cFS/pull/233>

### Development Build: v6.8.0-rc1+dev484

- Removes cases in `cfe_es_apps.c` and `cfe_tbl_internal.c` that could never hit the alternate condition since the condition was already checked
- Removes all APIs deprecated in #777 and #998
- Resolves CodeQL warnings on uninitialized variables.
- Refactors a small portion of `CFE_TIME_UnregisterSynchCallback` and initializes variables to resolve ""uninitialized variable"" false alarms.
- Fixes a typo in initialization in `CFE_TBL_Validate( CFE_TBL_Handle_t TblHandle )`
- Initializes `TotalMsgSize` as 0 to avoid static analysis warning of ""use before initialized""
- Increments the `CreatePipeErrorCounter` for all create pipe errors to eliminate a trivial static analysis warning
- Removes redundant or unreachable assignments and checks
- Updates header guards to standard format. Converts some file-scope block comments to a doxygen format to include a summary of the file.
- Enables the internal helper functions that determine table slot availability to handle NULL pointers.
- Resolves static analysis warnings by removing redundant check for `CFE_SUCCESS` in `CFE_EVS_EarlyInit`
- Moves the invocation of `CFE_PSP_AttachExceptions()` from the registration function to the pre-entry function and removes all references to task registration in code, docs, and tests. **This API change affects cFS apps.**
- Renames `CFE_TestRunner_AppMain` as `CFE_TR_AppMain` so it is less than 20 characters long. Updates App file names in documentation for `cfe_es_startup.scr`.
- Replace the call to `CFE_SB_MessageStringGet()` with the new filename-aware function `CFE_FS_ParseInputFileName()` for commands that contain file names like `CFE_ES_StopPerfDataCmd`. The default pathname/extension logic is now applied here too and only a ""basename"" is strictly necessary, although if a full/absolute path is given, it will be used as is.
- Removes the now-unnecessary `CFE_SB_ZeroCopyHandle_t` type and all APIs that refer or require it .Replaces `CFE_SB_ZeroCopyGetPtr()` and `CFE_SB_ZeroCopyReleasePtr()` with two new simplified functions `CFE_SB_AllocateMessageBuffer()` and `CFE_SB_ReleaseMessageBuffer()` , respectively.  These new functions do not use a separate handle. Updates the `CFE_SB_TransmitBuffer()` API to also remove the handle. **This breaks public APIs**.
- Internal cleanup localized to ES implementation. Consolidate all ES global variables under the `CFE_ES_Global` struct. Removes the separate `CFE_ES_TaskData` and some random pointers that were stored at global scope. Adjusts all references to the deprecated items accordingly (search and replace).
- Adds PSP version info to ES Housekeeping TLM messages. Changes both PSP and OSAL version info assignments on HK TLM to use the new version info API.
- Fixes check for ""NumBuckets"" member to use `<=` instead of `<`. `CFE_ES_GenPoolValidateState()` now returns `true` if using the max number of buckets (17 by default) and the pool structure using max value will correctly validate
- Replaces remaining `CFE_ES_ERR_BUFFER` with `CFE_ES_BAD_ARGUMENT` for when functions receive an invalid null-pointer argument. Adds null pointer checks in `cfe_es_api.c`.
- Adds branch coverage to html report when running `make lcov`
- See <https://github.com/nasa/cFE/pull/1258>

### Development Build: v6.8.0-rc1+dev436

- Adds a local definition of SOFTWARE_BIG/LITTLE_BIT_ORDER directly inside cfe_endian.h to provide a compatible symbol for apps that still require this. This allows CFE to build and run successfully when OSAL stops providing this in `common_types.h`.
- Removes incorrect statements from Application Developers Guide
- Fixes truncation handling on vsnprintf error by adding a cast to avoid implicit conversion
- Clarify the documentation on SB MsgId regarding requirements for command and telemetry messages
- Avoids undefined behavior and resolves static analysis warnings by casting isspace input to unsigned char.
- Updates message module and msgid v1, CFE_MSG_SetMsgId, to use mask instead of cast to alter value. Resolves static analysis warning.
- Updates CFE_ES_FileWriteByteCntErr to report status, not a size_t actual since OS_write returns int32. Use int16 for local type from CFE_TBL_FindTableInRegistry since it's an index, not a status.
- Replaces <> with "" in local #includes
- Adds CONTRIBUING.md that links to the main cFS contributing guide.
- See <https://github.com/nasa/cFE/pull/1243>

### Development Build: v6.8.0-rc1+dev412

- Apply standard code style and format
- Add new continuous integration workflow to enforce this format
- See <https://github.com/nasa/cFE/pull/1219>

### Development Build: v6.8.0-rc1+dev402

- HOTFIX 20210312, updates to work with older CMake
- Fix #972, reorganize directory structure
- HOTFIX IC 2021-03-05: Correct static app build issue
- See <https://github.com/nasa/cFE/pull/1222>

### Development Build: v6.8.0-rc1+dev392

- Fix #665, update pipe name documentation.
- Fix #1165, remove configs about shells
- Fix #1094, Update CRC documentation
- Fix #979, add stack size and priority to task info ‚Ä¶
- Fix #1170, refactor target config objects ‚Ä¶
- Fix #1207, Add wrapper targets to simplify app builds ‚Ä¶
- Fix #1211, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/cFE/pull/1213>

### Development Build: v6.8.0-rc1+dev382

- Refactors the SB buffer descriptor object `CFE_SB_BufferD_t` and simplify the zero-copy buffer paradigm. Combines the zero-copy and the normal CFE buffer descriptor into a single unified `CFE_SB_BufferD_t` object. Results in a simpler zero-copy design that is similarto the the standard, non-zero-copy message path. All message descriptor objects are now tracked in a list by SB. All changes are internal to SB. This does not affect API or behavior of any existing APIs (but see note). Corrects a minor issue where the `MsgSendErrorCounter` would get incremented if there were no subscribers, but only in the zero copy API.  
- Replaces `int_32` with `CFE_Status_t` for all error message codes  
- Removes references to `cfeesugshellsrv` from user guide
- Adds null pointer checks and size checks to validate method parameters. Returning the input instead of an error code
- Removes use of `LogEnabled` element in HK telemetry for EVS logic since log is always enabled now. On failures, reset area or semaphore will panic.
-  Fixes various build warnings when `BUILDTYPE=release`.
- See <https://github.com/nasa/cFE/pull/1196>

### Development Build: v6.8.0-rc1+dev365

- Implements a generic FS facility to perform file writes as a background job. Applications wanting to use this need to instantiate a state object (metadata) in global memory and two callback APIs, one to get a data record, another to send events. The following file requests are changed to use this facility:
  - ES ER Log dump
  - SB Pipe Info
  - SB Message Map
  - SB Route Info
  - TBL Registry Dump
- Changes the internal SB member names for consistency and thus fixes propagation of `Depth` and `CurrentDepth` into files:
  - `MaxQueueDepth` for maximum depth at queue creation time (previously was QueueDepth or Depth depending on context)
  - `CurrentQueueDepth` for the running count (previously was InUse or CurrentDepth depending on context)
  - `PeakQueueDepth` for the highest ""watermark"" (previously was PeakInUse or PeakDepth depending on context)
- Encapsulates all parameters for apps and tasks into a structure object. Cleans up internal APIs to pass this new object rather than individual parameters. Adds details to the relevant record (i.e. a task record has all relevant task details) which eliminates the need to traverse the app record to find some data.
- Enables items in `FILELIST` to be in a target name directory as well as symlinks. `arch_build.cmake` now checks a name-based subdirectory under `${MISSION_DEFS}` for files listed in the `FILELIST` for that target. If file is a symlink, the link should be followed so the correct content is installed, not a symlink.
- Adds documentation on  inclusion presence of null terminators for length parameters.
- Shortened `CFE_PLATFORM_ES_DEFAULT_TASK_LOG_FILE` name so it is within the `OSAL_MAX_FILE_NAME` size limit. Will now output task info to default filename if no filename is provided in command.
- Replaces `UT_Stub_SetForceFail` with `UT_Stub_SetDefaultReturnValue`. No behavior change.
- See <https://github.com/nasa/cFE/pull/1171>

### Development Build: v6.8.0-rc1+dev348

- Corrects reference to PSP header file location. Build now succesfully completes the build succeeds again when using `add_psp_module()` in custom CMakeLists file.
- Replace ""send"" with ""write"" in names for commands that write files. For example, `CFE_SB_**SEND**_ROUTING_INFO_CC` is now `CFE_SB_**WRITE**_ROUTING_INFO_CC`. Updates function names, command code names and comments.
- Removes incorrectly implemented deferred return code of `-1` for `CFE_SB_ReceiveBuffer` from software bus setup in `UT_InitData`.
- Implements more informative **assert messages** by making `SETUP, TEARDOWN, ASSERT` print `0x%lx` while `ASSERT_EQ` now prints both `%lf` and `0x%lx` format for the inputs
- Updates continuous-integration badges in `ReadMe.md`. The badges now reflect the success status of different runs.
- Remove `Test_SB_Cmds_SubRptUnexpCmdCode` which was a duplicate of `Test_SB_Cmds_CmdUnexpCmdCode` and did not implement any new tests.
- Initializes status in `CFE_ES_WaitForSystemState` and adds missing success test case so the function doesn't return an uninitialized `Status`.
- Removes the `HkPacket` and `TblRegPacket` message initializations from `CFE_TBL_EarlyInit` since they are initialized in `CFE_TBL_InitData`. Moves the `NotifyMsg` message initialization to `CFE_TBL_InitData` and sets the message ID each time it's sent from `CFE_TBL_SendNotificationMsg`. Possibly results in small performance improvement since the message isn't initialized every call.
- Removes unimplemented `CFE_ES_AppGetList` and `CFE_ES_AppDumpAllInfo` prototypes.
- Adds a 15-minute timeout to continuous integration workflows to prevent excess resource utilization.
- Makes debug subscription events only print the Pipe ID, not a name, in the debug events.
- Updates the documentation and verification for `CFE_PLATFORM_SB_HIGHEST_VALID_MSGID` to allows the full range of values.
- Clarifies the difference between ""restart"" and ""reload"" in API/cmd and user's guide documentation for `CFE_ES_RESTART_APP_CC`.
- Switches throttle indexes to use `CFE_SB_RouteId_Atom_t` and combines helper function given that msgid was removed due to being a resource hog. Resolves static analysis warning.
- `CFE_ES_RestartApp` now checks for file existence as part of command processing and does not remove the app if the file doesn't exist (just avoids one error case). it also rejects the command and increments command error counter if file is missing.
- Removes `CFE_PLATFORM_SB_MAX_PIPE_DEPTH` in favor of `OS_QUEUE_MAX_DEPTH`. This depth parameter in command is now checked prior to attempting OSAL call.
- Filters pointer now `const` in API and reports truncation when registering filters with `CFE_EVS_Register`.
- Removes the ability to disable the log by not defining `CFE_PLATFORM_EVS_LOG_ON` so users are no longer able to disable log completely. For minimum memory use define `CFE_PLATFORM_EVS_LOG_MAX = 1`. Note: This could remove control based on LogEnabled, panic on reset area fail and limp along if ""sem create"" fails.
- Removes the remnants of the table service exclusion logic and documentation: `EXCLUDE_CFE_TBL` no longer available, even if defined, table services will still start.
- Set ES and EVS pipe message limit to defaults as opposed to the custom, unjustified, `CFE_SB_SubscribeEx`. This change might queue additional HK messages, but SCH loads after ES anyways.
- Replaces `CFE_SB_Default_Qos` with `CFE_SB_DEFAULT_QOS` macro that avoids global variable exposure. Removes SB-internal defines that are not implemented nor used.
- Explicity `memset` the task data to zero at the start of EarlyInit. Standardize the global typdef/variable names.
- Moves all functions, macros, types, and other definitions related to resource IDs and generic resource management into a separate module, like `CFE MSG`, `SBR`, etc. This allows a mission to elect ""strict"" implementations of these objects, where every ID type is unique, and assigning between them or `uint32` results in a compiler error. **API now has separate types for each resource type (Apps, Tasks, Libs, Counters, etc).** The user can elect at the mission level whether this is a simple typedef (all uint32, all interchangeable) or a wrapper type (separate/strict type, cannot be interchanged). The former is backward compatible but the latter is not - must use proper types.
- Adds Code QL analysis to continuous integration workflow.
- See <https://github.com/nasa/cFE/pull/1150>

### Development Build: v6.8.0-rc1+dev290

- Documentation: Add Security.md with instructions to report vulnerability
- Documentation: Update cpuname/MISSION_CPUNAMES documentation
- Fixes `UT_CheckEventHistoryFromFunc()` helper routine to read the correct number of IDs so it checks the correct number of events. Also correct bad event checks in TBL UT.
- Adds `OS_printf` to `CFE_ES_SYSLOG_APPEND` so it matches `CFE_ES_WriteToSysLog`
- Removes unused `SenderReporting` and `CFE_PLATFORM_SB_DEFAULT_REPORT_SENDER`
- Tests pass when debug events are enabled via `CFE_PLATFORM_EVS_DEFAULT_TYPE_FLAG` in platform config.
- Removes references to `UT_CheckForOpenSockets` which is no longer applicable since the UT framework resets the state for each unit test.
- Rename `UT_ClearForceFail` as `UT_ClearDefaultValue` given change in https://github.com/nasa/osal/issues/724
- Adds checks that ensure `CFE_SB_GetUserData` works with all payload data types.
- Adds header padding to 64-bit so that `CFE_SB_GetUserData` will work for message structures with elements up to 64 bit
- For primary-only header config: telemetry header required to 64 bit boundary (affects all receivers)
- For primary and extended header config: command header required padding to 64 bit boundary (affects all senders)
- Refactor `CFE_TIME_RegisterSynchCallback` to only have one return point and eliminates ""possible uninitialized variable"" static analysis warning
- None of these changes are expected to cause problematic.
- Addresses message delivery issues due to inconsistent locking by reworking cFE-SB API implementation. Ensures all events are generated and counters are incremented consistently by avoiding early returns in functions and using the `PendingEventID` register to record what event ID should be sent per the current operation.
- Employs the `CFE_ES_ResourceID_t` type and related patterns for managing the SB Pipe IDs.
- Will break code which directly accessed these items without going through the lookup function.
- **`CFE_SB_PipeId_t` type is no longer usable as a direct array index**, increased in size from 8 to 32 bits, and is now consistent with all other ID types in both behavior and size.
- **The ""pipe stats"" structure in the Pipe TLM is also changed**. This structure contained a `CFE_SB_PipeId_t` value, hence why it had to be updated because the type is now bigger. The spare bytes are also moved to the end of the struct.
- Removes `OS_printf` checks of stub calls in unit tests and checks for specific format string in history instead to confirm the right path was taken.
- Removes `CFE_MISSION_REV` from platform config.
- Removes the rest of the references and uses of `CFE_PLATFORM_ES_PERF_MAX_IDS` in favor of `CFE_MISSION_ES_PERF_MAX_IDS`
- Remove uses of strncpy and other minor hardcoded references
- Cleanup unit tests to reflect size changes in `CFE_MISSION_MAX_API_LEN` and `CFE_MISSION_MAX_PATH_LEN`.
- Moved ES pipe name and lengths to defines
- Removed PipeName and PipeDepth variables from app global
- Removed unnecessary (char *) casts
- Simplified `&stingname[0]` to `stringname` where observed
- Enables projects that have OSs with different limits to maintain a standard cmd/tlm and have unit tests pass.
- Make `CFE_ES_WriteToSysLog` stub unit test more informative by adding `UtDebug` output
- See <https://github.com/nasa/cFE/pull/1109>

### Development Build: v6.8.0-rc1+dev248

- Replace `OS_FileSysStatVolume()` with`OS_fsBlocksFree()` which will be deprecated. This call reports the number of total blocks, not just the free blocks, making the check more accurate and removing the need for a workaround for desktop machines.
- Instead of accessing `OS_time_t` values directly, use the OSAL-provided conversion and access methods. This provides independence and abstraction from the specific `OS_time_t` definition and allows OSAL to transition to a 64 bit value.
- Removes the spurious `CFE_SB_TimeOut_t` typedef from `cfe_sb.h`. May affect any apps that inappropriately rely on the private typedef.
- Removes unused `network_includes.h`. Not used by the framework anywhere,  apps should use OSAL Socket APIs instead.   
- Fixes deprecation directive typos
- See <https://github.com/nasa/cFE/pull/1088>

### Development Build: v6.8.0-rc1+dev236

- Resolved doxygen warnings for osalguide and updated header file references
- Corrects the documentation for the CFE_SB_GetPipeName() unit test stub function.
- Adds a new github actions workflow file to run cppcheck
- See <https://github.com/nasa/cFE/pull/1066>

### Development Build: v6.8.0-rc1+dev228

- Remove use of `osapi-os-loader.h` from ES UT.
- Use volatile `sig_atomic_t` for system state to avoid race issue if uint32 isn't atomic on a system
- Set the flags parameter on the OS_ModuleLoad() properly to allow an app to be properly unloaded, which in turn allows the reload command to work as expected. Fixes problem where unload comand resulted in continuous restarting of the same app code.
- Replaced `Test_MSG_PrintMsg` with `UT_DisplayPkt`. Also removed unused `Test_MSG_Sum`.
- See <https://github.com/nasa/cFE/pull/1047>

### Development Build: v6.8.0-rc1+dev218

- Adds `CFE_SB_TransmitMsg`, `CFE_SB_TransmitBuffer`, `CFE_SB_ReceiveBuffer`
  - Main change is to utilize `CFE_SB_Buffer_t` and `CFE_MSG_Message_t` in a consistent manner to facilitate alignment
  - Deprecates multiple `CFE_SB_*` items
  - Redefines `CFE_MSG_Size_t` as `size_t` to minimize duplicated work and facilitate transition to `size_t`
- Use a generic void* as the interface type for the pool buffer pointers. This reduces the need for local type casting in the apps and makes it generally easier to use.
- Remove  reference to CEXP in RTEMS 4.11 i686 toolchain. Add an RTEMS 5.1 i686 toolchain file.
- See <https://github.com/nasa/cFE/pull/1045>


### Development Build: v6.8.0-rc1+dev204

- Backward compatible API change. Replace many uses of generic uint16 and uint32 with a more purpose-specific type. Replace all sizes with size_t across the API.
- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing
- Deprecates many SB Elements and replaces them with the new MSG module. See https://github.com/nasa/cFE/issues/777 for list.
-  App and Lib info telemetry structures no longer contain the `ModuleId` value from OSAL.
- Add an extra write of a null char after `strncpy` which squelches a warning and appease compiler warning logic.
- Uses `CFE_PLATFORM_ES_DEFAULT_STACK_SIZE` as a default instead of a minimum. Affects the Start App command; if stack size is specified as zero, then the default stack size value from platform config is used. Otherwise the value in the command will be passed through and used as-is.
- Changes the type of the AppID parameter on ""Cleanup"" routines from `uint32` to `CFE_ES_ResourceID_t`.
- See <https://github.com/nasa/cFE/pull/1027>

### Development Build: v6.8.0-rc1+dev179

- Adds macros for more compact calls to `CFE_EVS_SendEvent`, making the type be part of the function name.
- The sample configs leap seconds default value is now up to date. (As of Oct 2020)
- Removed the clear=false logic (and clear parameter) `CFE_MSG_Init()` now always zeroes entire message and sets defaults.
- Adds flags parameter to calls to `OS_ModuleLoad()`. Initially just pass 0 (GLOBAL) to maintain old behavior.
- Updates `CFE_ES_RegisterCDSEx` stub to match current signature
- Includes `cfe_private.h` for stubs that implement related elements.
- See <https://github.com/nasa/cFE/pull/1008>

### Development Build: v6.8.0-rc1+dev164

- Keeps task names under 16 chars to make more debugger friendly, regardless
of the OSAL limit. Task name shows up as `ES_BG_TASK`
- Move ES typedefs shared across API and telemetry messages into the `cfe_es_extern_typedefs.h`.
- Move all ES typedefs that define the telemetry interface and structures that define the output of commands that write data files into this group (query all apps, query all tasks, query all CDS).
- Remove some localized definitions and replace with MISSION scope definitions where appropriate/necessary.
- Adjust `strncpy()` call to avoid compiler warning
- Cast fixed width types to the type used in the `printf` call. Removes `printf` type warnings on the 32-bit RTEMS build.
- See <https://github.com/nasa/cFE/pull/991>

### Development Build: v6.8.0-rc1+dev150

- Provide new Library API similar to App API
- Allows the existing CFE_ES_AppInfo_t structure to be extended to libraries as well as applications by introducing a new value (3) for the Type field.
- Allows Libraries to be queried via API calls similar to App API.
- Extends the Query All/Query One commands to operate on Libraries or Applications.
- Breaks up the monolithic AppCreate and LoadLibrary functions and have these call subroutines that operate on the common components.
- Fix race conditions in app request processing state machine.
- Adds SBR module which includes message map and routing table. The access APIs are on the SB side which still owns the destination logic
- Removes passing of route index or pointers being. Everything is now based on route and message id
- Oversized the hash message map (4x) to minimize collisions
- Hash designed for 32 bit, a change in CFE_SB_MsgId_Atom_t size may require implementation updates
- Adds debug event for collisions during add
- Dropped routing push/pop, dropped ""key"" in direct implementation
- Deletes unused code CFE_SB_FindGlobalMsgIdCnt
- Fixes variable declaration violations of coding standard
- Individual events for deleting destinations when deleting a pipe removed to avoid a race condition around a 10-20% performance hit to hash via rough comparison on a linux box, no memory impact
- See <https://github.com/nasa/cFE/pull/975>

### Development Build: v6.8.0-rc1+dev139

- For all resource types which have names, IDs are not re-issued after deletion, helping ensure safety as previously deleted IDs will not validate. Provides a consistent Name-ID translation API for all resource types. Enforces consistent argument/name validation on all resource types, and also enforces name uniqueness where relevant.
- Enhancement to use the full 16 bits of resource ID space, which avoids repeating ID values that have already been used. This significantly decreases the likelihood that a previously deleted ID will alias a newly allocated/valid ID.
- See <https://github.com/nasa/cFE/pull/959>

### Development Build: v6.8.0-rc1+dev129

- Rather than having a second pool implementation only for CDS, use the generic pool implementation. This also uses the abstract resource identifiers to identify CDS blocks, rather than a direct reference.
- Add the system-specific module suffix (.o, .so, .obj, etc) and the default CFE core executable name to the configdata structure.
- See <https://github.com/nasa/cFE/pull/944>

### Development Build: v6.8.0-rc1+dev122

- Adds the field `UnregAppID` to track whether an ""unregistered"" event was generated, un-overloading the EventCount field to serve its primary purpose of counting actual events generated from a valid/registered AppID.
- Move the AppID lookup execution to be early in the `CFE_SB_SendMsgFull` implementation. This avoids double locking between SB+ES and avoids a block-scope local variable.
- Instead of identifying a memory pool by its memory address, use a resource ID. IDs are a constant size, regardless of whether the host machine is 32 or 64 bits.
  - IDs can be put into commands/telemetry and maintain a more consistent format with consistent alignment requirements.
  - IDs can be independently verified without dereferencing memory. Previously the only way to validate a memory pool was to read the address pointed to, which results in a segfault if the address was bad.
- Change from `OS_MAX*` defines to appropriately-scoped CFE defines for array sizing
- This creates the new `CFE_Status_t` typedef for function's return status codes. Also adds a note to `CFE_TBL_GetStatus` since its behavior will likely change in the future in the hopes of not having a non-zero ""info"" status.
- See <https://github.com/nasa/cFE/pull/936>

### Development Build: v6.8.0-rc1+dev109

- Add a new typedef `CFE_ES_ResourceID_t` that can replace `uint32` for all ID storage and manipulation. Initially this is just an alias to `uint32` for backward compatibility.
- See <https://github.com/nasa/cFE/pull/916>

### Development Build: v6.8.0-rc1+dev105

- Removes dependency on CCSDS version define.
- Removes old name and id defines.
- CFE_ES_CalculateCRC default stub behavior.
- Replaces calls to OS_open and OS_creat
- Replaces UT_Text with UtPrintf
- Updates variable checks in read_targetconfig

- See <https://github.com/nasa/cFE/pull/912>

### Development Build: v6.8.0-rc1+dev91

- Sets Revision to 99 for development build.
- Installs unit test to target directory.
- Returns processor ID to default to unbreak toolchain
- Applies the appid/taskid/counterid pattern to Library resources.
- See <https://github.com/nasa/cFE/pull/891>

### Development Build: v6.8.0-rc1+dev81

- Deconflict CFE_ES_LIB_ALREADY_LOADED and CFE_ES_ERR_SYS_LOG_TRUNCATED EIDs
- Scrub all CFE references/uses of OSAL IDs to use the proper osal_id_t type. Any place that an OSAL ID is stored in memory or passed in an API call are changed to the osal_id_t type, rather than uint32. Conversions between this and other types (e.g. bare integer) is done using the OSAL-supplied conversion helpers.
- After the changes implemented in #101, there may be routing table entries with no subscribers (RoutePtr->ListHeadPtr would be NULL.) This could cause a seg-fault. Also, even if there are entries in the routing table, there will be no event generated if the unsubscribe does not find a matching route entry.
- Adds debug message.
- Applies the appid/taskid pattern to Generic Counter resources.
- Adds test for SB subscribe/unusubscribe/unsubscribe.
- See <https://github.com/nasa/cFE/pull/876>


### Development Build: v6.8.0-rc1+dev65

- In the next major CFE release, this code will be no longer supported at all. It should be removed early in the cycle to avoid needing to maintain this compatibility code.
- The CFE_ES_FindCDSInRegistry function had an unusual loop control structure with mixed types of signed and unsigned. This has the possibility of being infinite if the MaxNumRegEntries is zero due to the way the end condition is structured. Simplify to be like other loops and use unsigned int control variable.
- Fixes the cast-align error (use the aligned Msg since it's available already).
- HOTFIX-20200902 - Fix sb unit test setup issue.
- HOTFIX 20200902 - Update documentation links for deprecated symbols.
- HOTFIX 20200902 - Fix SB Test_CleanupApp_API AppID.
- See <https://github.com/nasa/cFE/pull/861>

### Development Build: v6.8.0-rc1+dev42

- Removes reference from documentation.
- CFE_SB_SendMsg stub now behaves the same as CFE_SB_TimeStampMsg (copies message pointer from local). No longer need to emulate CFE_SB_InitMsg from test code, set the API/stub data buffers directly.
- Removed iterator modification from within the loop... replaced with break.
- Resolves loop iterator size too small for comparison.
- Replaced CFE_MISSION_SPACECRAFT_ID use with CFE_PSP_GetSpacecraftId() and updated unit test
- See <https://github.com/nasa/cFE/pull/849>

### Development Build: v6.8.0-rc1+dev28

- Add msg stubs, update SB_UT to use them, and remove msg module include from unit tests
- Collapses time options down to just 32 bit second, 16 bit subsecond, always big endian. Removes old defines, and triggers an error if the configuration is set to a format that was removed.
- Enables source selection and out-of-tree mission-defined overrides in the msg directory
- Unit tests added from within unit tests will not execute, replaced this pattern with direct calls to the main subtest setup routine.
- See <https://github.com/nasa/cFE/pull/833>

### Development Build: v6.8.0-rc1+dev13

- Deprecates `CFE_SB_GetLastSenderId()` API by introducing new `CFE_OMIT_DEPRECATED_6_8` tag
- Documentation update remove deleted requiremements
- Add a new *cfe_assert* module for functional testing by making it possible to load the UT assert object code as a CFE library. These are compiled as separate, independent modules and only loaded on demand. Also includes a draft example for cFE testing, which calls some basic ES AppId functions.
- See <https://github.com/nasa/cFE/pull/816>

### Development Build: v6.7.0+dev292

- Add missing include path to the target/h and wrn/coreip directory.
Set and clarify difference between WIND_HOME and WIND_BASE variables.
Remove unrelated comment about CEXP (remnant from RTEMS). No more errors about missing headers.
- Version reporting is does not span multiple lines.
- See <https://github.com/nasa/cFE/pull/792>

### Development Build: v6.7.0+dev289

- Update `CFE_SB_TimeStampMsg` to save the message pointer argument `UT_Stub_CopyFromLocal` so that unit tests can check it
- Only affects build system. Fully backward compatible. The defaults are applied if a user has nothing specifically configured in their `targets.cmake`. The defaults will select osal, cfe-core, and psp as before. The user now has the option to explicitly configure and control the inclusion of these modules and also provide mission-specific search paths to override them as desired.
- Note this only affects UT stubs. Change the internal names of some stub arguments to match prototype. Ensure that:
  - All functions in the CFE public API have a stub function implemented
  - All parameters to the stub function are registered in the context object, so the values will be available to hook functions.
  - The names of all parameters match the prototype/documentation, so hook functions that use name-based argument value retrieval will work.
-  Adds to table search path in `arch_build.cmake`
- Calls to OS_open() now use the OSAL-defined symbol, not the POSIX symbol.
-  Defines new macros to report the build number and build baseline and new strings containing the version number of cFE and a combined string with the version number for OSAL, PSP, and CFE.
- Allow explicitly setting of the processor ID in `targets.cmake`. The `TGTx_PROCESSOR_ID` setting will be passed to the final build/link of CFE core as the CPU ID. If unspecified, then the CMake index value is used instead (backward compatible).
- `cmake` now detects conditions where no files were present to fulfill an config include file requirement and reports an error during `make prep` lists the files it checked for rather than generating an empty file.
- See <https://github.com/nasa/cFE/pull/765>

### Development Build: 6.7.21

- If a string is exactly the size of the field when using the `CFE_TBL_FILEDEF()` macro it will produce a compiler error
- Added cFE User's Guide Reference to README.md
- Removes old license
- See <https://github.com/nasa/cFE/pull/743>

### Development Build: 6.7.20

- SB Unit use of the UT assert framework is closer to original design intent
- See <https://github.com/nasa/cFE/pull/743>

### Development Build: 6.7.19

- API Change: cFE ut_sb_stubs now has CFE_SB_DeletePipe available.
Behavior Change: App unit tests requiring this will not fail to build due to undefined reference to CFE_SB_DeletePipe
- Hook functions may now use the va_list form and obtain the full set of variable arguments passed to CFE_EVS_SendEvent and variants.
- Replace all direct references to data types defined in ccsds.h with the abstract type defined in cfe_sb.h.
- See <https://github.com/nasa/cFE/pull/729> for details.

### Development Build: 6.7.18

- Using ut stubs CFE_EVS_SendEvent and CFE_ES_WriteToSysLog, the register buffer will have the correct size. access to register buffer element will exist
- Both the main task and the child task(s) are successfully deleted and restarted after the exception occurs.
- Fixes doxygen warnings for the tbl subsystem.
- No compiler warnings or errors on cross build.
- Changes Message Key from uint16 to uint32 to avoid rollover and system hang
- See <https://github.com/nasa/cFE/pull/712> for more details

### Development Build: 6.7.17

- No longer automatically decompresses apps/libraries as part of load
- Deletes now unused CFE_ES_CountObjectCallback and CFE_ES_ListResourcesDebug. Flags were unused
- Removes all conditional preprocessing blocks related to CFE_ARINC653.
- Ensure clean build, no warnings on string operations using GCC 9.3.0.
- When OMIT_DEPRECATED = true attempt to send output to shell command will result in command error counter increment (unrecognized function code)
- SBN will need to init command with new MID
- Documentation links and references will now work properly
- API CFE_ES_ProcessCoreException is removed, replaced with async event.
- Removed duplicate prototype in cfe_time_utils.h
- Removes unused defines and adds documentation to TBL event defines.
- Deprecates CFE_TIME_CFE2FSSeconds and CFE_TIME_FS2CFESeconds.
- Unit tests now build and run when MESSAGE_FORMAT_IS_CCSDS_VER_2 is configured.
- Build now works with both extended headers and OMIT_DEPRECATED options set.
- No more alignment warnings
- Adds new unit test macros
- See <https://github.com/nasa/cFE/pull/692> for more details

### Development Build: 6.7.16

- Users must now select OSAL options via the CMake file in their defs directory, rather than the osconfig.h file.
- See <https://github.com/nasa/cFE/pull/672> for more details

### Development Build: 6.7.15

- Upon power on reset, default system log mode set to overwrite. Upon processor reset, default system log mode set to discard.
- No longer locks while locked (no issue observed on linux/posix, but user reported issue on FreeRTOS 10)
- Internal `CFE_TBL_LoadFromFile()` API changed slightly to add AppName as a parameter. Return value from `LoadFromFile()` no longer relevant for event generation.
- Updates `CFE_TBL_CleanUpApp` such that it now checks the 'used flag' prior to calling `CFE_TBL_RemoveAccessLink` for a given TblHandle. Also sets the AppId to `CFE_TBL_NOT_OWNED` after removing the access descriptor link from linked list.
- Removed `OS_FS_SUCCESS, OS_FS_ERROR , OS_FS_ERR_INVALID_POINTER, OS_FS_ERR_NO_FREE_FDS , OS_FS_ERR_INVALID_FD, and OS_FS_UNIMPLEMENTED` from `osapi-os-filesys.h`
- See <https://github.com/nasa/cFE/pull/649> for more details

### Development Build: 6.7.14

- Exposes the `CFE_SB_IsValidMsgId()` for application usage.
- `CFE_SB_GetLastSenderID` will now detect if it is being called prior to a message being sent on a given pipe.
- Mismatches between PSP/BSP/OS are now detected and warned about during make prep. Only the `CFE_SYSTEM_PSPNAME` is actually required to be specified for a CFE build now. Others can be omitted.
- See <https://github.com/nasa/cFE/pull/635> for more details

### Development Build: 6.7.13

- RTEMS builds without error.
- Use the INTERFACE_COMPILE_DEFINITIONS and INTERFACE_INCLUDE_DIRECTORIES properties from the osal target and apply them to the entire CFE build as a directory-scope property. No impact until these are set in OSAL.
- Minor other updates (see <https://github.com/nasa/cFE/pull/615>)

### Development Build: 6.7.12

- Cmd code (and checksum) are always in the same place (matches GSFC spec for command secondary header)
- No impact to behavior. Previously the perf log dump file frequently contained errors due to out of order or otherwise corrupted entries, which is now fixed.
- Minor other updates (see <https://github.com/nasa/cFE/pull/586>)

### Development Build: 6.7.11

- Improve documentation
- Update makefile to report branch coverage
- Minor other updates (see <https://github.com/nasa/cFE/pull/566>)

### Development Build: 6.7.10

- Fix potential unit test problems with name collisions
- Improve documentation
- Minor other updates (see <https://github.com/nasa/cFE/pull/563>)

### Development Build: 6.7.9

- No longer requires sed ""hack"" to change the setting in default_config.h
- Minor other updates (see <https://github.com/nasa/cFE/pull/544>)

### Development Build: 6.7.8

- Updates and cleaned up documentation and requirements
- Fixes incorrect debug messages
- Decrease child task count when one is deleted
- Minor other updates (see <https://github.com/nasa/cFE/pull/530>)

### Development Build: 6.7.7

- Adds a new function, CFE_SB_GetPipeIdByName, which retrieves the pipe ID given a name of a pipe.
- Improvement in error reporting when using a pipe name that is already in use, or when the queue limit has been reached.
- Added userguide and osalguide to the local target list to avoid makefile warning
- Minor other updates (see <https://github.com/nasa/cFE/pull/511>)

### Development Build: 6.7.6

- Adds OMIT_DEPRECATED prep flag
- Adds and enforces strict warnings
- Software Bus now increments sequence counter even if there are no subscribers
- Warning, comment, and code coverage cleanup (see <https://github.com/nasa/cFE/pull/490>)

### Development Build: 6.7.5

- Added CI static analysis check
- Resolved static analysis warnings
- Minor other updates (see <https://github.com/nasa/cFE/pull/479>)

### Development Build: 6.7.4

- Minor updates (see <https://github.com/nasa/cFE/pull/448>)

### Development Build: 6.7.3

- Minor updates (see <https://github.com/nasa/cFE/pull/413>)

### Development Build: 6.7.2

- Minor bugs and enhancements (see <https://github.com/nasa/cFE/pull/388>)

### Development Build: 6.7.1

- Fix strlen in CFE_ES_TaskInit <https://github.com/nasa/cFE/pull/23>
- Minor bug fixes (see <https://github.com/nasa/cFE/pull/378>)

### **_OFFICIAL RELEASE: v6.7.0 - Aquila_**

- This is a point release from an internal repository
- Changes are detailed in [cFS repo](https://github.com/nasa/cFS) release documentation
- Apache 2.0

### **_OFFICIAL RELEASE: v6.6.0a_**

- This is a point release from an internal repository
- Apache 2.0
- Additional release notes are found in [release notes](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_release_notes.md)
- See the [version description document](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_6_6_0_version_description.pdf) for the full document
- Test results can be found in [test results](https://github.com/nasa/cFE/tree/v6.6.0a/test-and-ground/test-review-packages/Results)

## Known issues

See all open issues and closed to milestones later than this version.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
36,nasa/harmony,TypeScript,"# Harmony

Services.  Together.

Harmony has two fundamental goals in life:
1. **Services** - Increase usage and ease of use of EOSDIS' data, especially focusing on opportunities made possible now that data from multiple DAACs reside in AWS.  Users should be able to work seamlessly across data from different DAACs in ways previously unachievable.
2. **Together** - Transform how we, as a development community, work together to accomplish goal number 1.  Let's reuse the simple, but necessary components (e.g. EDL, UMM, CMR and Metrics integration) and let's work together on the stuff that's hard (and fun) like chaining, scaling and cloud optimizations.

For general project information, visit the [Harmony wiki](https://wiki.earthdata.nasa.gov/display/Harmony). Harmony discussion and collaboration occurs in the EOSDIS #harmony Slack channel.


## Table of Contents

1. [Development Prerequisites](#Development-Prerequisites)
    1. [Earthdata Login Application Requirement](#Earthdata-Login-Application-Requirement)
    2. [Software Requirements](#Software-Requirements)
2. [Running Harmony](#Running-Harmony)
    1. [Set Up Environment Variables](#Set-Up-Environment-Variables)
    2. [Run Tests](#Run-Tests)
    3. [Set Up A Database](#Set-Up-A-Database)
    4. [Set Up and Run Argo, Localstack](#Set-Up-and-Run-Argo,-Localstack)
    5. [Add A Service Backend](#Add-A-Service-Backend)
    6. [Run Harmony](#Run-Harmony)
    7. [Connect A Client](#Connect-A-Client)
3. [Local Development Of Workflows Using Visual Studio Code](#Local-Development-Of-Workflows-Using-Visual-Studio-Code)
4. [Running in AWS](#Running-in-AWS)
5. [Contributing to Harmony](#Contributing-to-Harmony)
6. [Additional Resources](#Additional-Resources)

## Development Prerequisites

For developing Harmony on Windows follow this document as well as the information in [docs/dev_container/README.md](docs/dev_container/README.md).

### Earthdata Login Application Requirement

To use Earthdata Login with a locally running Harmomy, you must first set up a new application in the Earthdata Login UAT environment using the Earthdata Login UI.  https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+An+Application.  This is a four step process:

1. Request and receive permission to be an Application Creator
2. Create a local/dev Harmony Application in the EDL web interface
3. Add the necessary Required Application Group
4. Update .env with credentials

You must select ""401"" as the application type for Harmony to work correctly with command line clients and clients like QGIS.  You will also need to add the ""eosdis_enterprise"" group to the list of required application groups in order for CMR searches issued by Harmony to be able to use your Earthdata Login tokens.  Update `OAUTH_CLIENT_ID` and `OAUTH_PASSWORD` in .env with the information from your Earthdata Login application. Additional information including other OAUTH values to use when creating the application can be found in the example/dotenv file in this repository.


### Software Requirements

Required:
* A local copy of this repository.  Using `git clone` is strongly recommended
* Node.js version 12.  We strongly recommend installing [NVM](https://github.com/nvm-sh/nvm) to add and manage node versions.
* Mac OSX, Linux, or similar command line tooling.  Harmony is tested to run on OSX >= 10.14 and Amazon Linux 2.  Command-line instructions and bash helper files under [bin/](bin/) are tested on OSX >= 10.14.
* [git](https://git-scm.com) - Used to clone this repository
* A running [Docker Desktop](https://www.docker.com/products/developer-tools) or daemon instance - Used to invoke docker-based services
* [Docker compose](https://docs.docker.com/compose/) version 1.20.0 or greater; preferably the latest version, which is v1.26 or greater.
* The [AWS CLI](https://aws.amazon.com/cli/) - Used to interact with both localstack and real AWS accounts
* [SQLite3 commandline](https://sqlite.org/index.html) - Used to create the local development and test databases. Install using your OS package manager, or [download precompiled binaries from SQLite](https://www.sqlite.org/download.html)
* PostgreSQL (required by the pg-native library) - `brew install postgresql` on OSX
* Earthdata Login application in UAT (Details below in the 'Set up Earthdata Login application for your local Harmony instance' section)
* [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) - A command-line application for interfacing with a Kubenetes API.


Highly Recommended:
* An Amazon Web Services account - Used for testing Harmony against object stores and running Harmony in AWS
* An editor with syntax awareness of TypeScript.  If you do not have this or any preference, consider [Visual Studio Code](https://code.visualstudio.com)


Optional:
* [awscli-local](https://github.com/localstack/awscli-local) - CLI helpers for interacting with localstack
* [Python](https://www.python.org) version 3.7 - Useful for locally running and testing harmony-docker and other backend services

## Running Harmony

### Set up Environment
If you have not yet cloned the Harmony repository, run
```
$ git clone https://github.com/nasa/harmony.git
```

Ensure node is available and is the correct version, 12.x.y, where ""x"" >= 14.

```
$ node --version
v12.22.1
```

Ensure npm is available and is version 7 or later.
```
$ npm --version
7.11.2
```

If either are not the correct versions and you are using NVM, install them and ensure your `PATH` is up-to-date by running:

```
$ nvm install && nvm use && npm install -g npm@7
```

The output should include node 12 and npm 7.
```
Now using node v12.22.1 (npm v7.11.2)
```

Be sure to **verify the version on the final line** to make sure the NVM binary appears first in your `PATH`.

From the harmony project root, install library dependencies:
```
$ npm install
```

Recommended: Add `./node_modules/.bin` to your `PATH`.  This will allow you to run binaries from installed node modules.  If you choose not to do this, you will need to prefix node module calls with `npx`, e.g. `npx mocha` instead of just `mocha`

### Set Up Environment Variables

Harmony uses environment variables for managing much of its configuration. Most of the variables can be defaulted, and harmony provides those defaults suitable for local development in the `env-defaults` file. In order to set up the remaining variables, run the following from the harmony project root:

```
$ bin/create-dotenv
```

The script will create a file named `.env` in the root project directory containing only those parameters that cannot be defaulted. Open the file and update the values for any of the variables that are currently blank. Detailed information for the environment variables can be found in the `env-defaults` file.

Harmony reads both the `env-defaults` and `.env` files at startup to determine the configuration. To override any default values, set the desired value in the `.env` file. There is no need to duplicate parameters in the `.env` file if using the default value.

Note: The defaults are suitable for running locally with Mac OS X. If running on Linux there are a couple of parameters that will also need to be overridden and are documented as such in the `env-defaults` file.

### Run Tests

To run the linter, tests, and coverage checks as the CI environment will, run

```
$ npm test
```

Harmony uses [eslint](https://eslint.org) as a linter, which can be invoked as `$ npx eslint` (or `$ eslint` if you have set up your `PATH`).  It uses [mocha](https://mochajs.org) for tests, `$ npx mocha`, and [nyc](https://istanbul.js.org) for code coverage, `$ npx nyc mocha`.

#### Test Fixtures
Rather than repeatedly perform the same queries against the CMR, our test suite
uses [node-replay](https://github.com/assaf/node-replay) to record and play back
HTTP interactions.  All non-localhost interactions are recorded and placed in files
in the [fixtures directory](fixtures/).

By default, the test suite will playback interactions it has already seen and
record any new interactions to new files.  This behavior can be changed by setting
the `REPLAY` environment variable, as described in the
[node-replay README](https://github.com/assaf/node-replay).

To re-record everything, remove the fixtures directory and run the test suite. This should be done to cull the recordings when a code change makes many of them obsolete, when CMR adds response fields that Harmony needs to make use of, and periodically to ensure no impactful CMR changes or regressions.

### Set Up A Database

To setup a sqlite3 database with the correct schema for local execution, run

```
$ bin/create-database development
```

This should be run any time the versioned contents of the `db/db.sql` file change.

This will create a file, `db/development.sqlite3`, which will contain your local data.  You can delete the above file to remove
all existing development data.

In production environments, we use PostgreSQL and use database migrations to modify the schema.  If you have a PostgreSQL
database, you can create and/or migrate your database by setting `NODE_ENV=production` and
`DATABASE_URL=postgresql://your-postgres-connection-url` and running:
```
$ npx knex --cwd db migrate:latest
```

### Set Up and Run Argo, Localstack

Harmony uses [Argo Workflows](https://github.com/argoproj/argo) to manage job executions.  In development, we use [Localstack](https://github.com/localstack/localstack) to avoid allocating AWS resources.

#### Prerequisites

* Mac:
  * Install [Docker Desktop] https://www.docker.com/products/docker-desktop. Docker Desktop comes bundled with Kubernetes and `kubectl`.
    If you encounter issues running `kubectl` commands, first make sure you are running the version bunedled with Docker Desktop.
  * Run Kubernetes in Docker Desktop by selecting Preferences -> Kubernetes -> Enable Kubernetes
  * Install the [Argo CLI](https://github.com/argoproj/argo/releases/tag/v2.9.5), the command line interface to Argo
* Linux / Generic:
  * Install [minikube](https://kubernetes.io/docs/tasks/tools/install-kubectl/), a single-node kubernetes cluster useful for local development
  * Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/), a command line interface to kubernetes.
  * Install the [Argo CLI](https://github.com/argoproj/argo/releases/tag/v2.9.5), the command line interface to Argo

#### Installing and running Argo and Localstack on Kubernetes

```
$ ./bin/start-argo
```

This will install Argo and forward port 2746 to localhost. It will take a few minutes the first time you run it. You will know when it has completed when it prints

```
Handling connection for 2746
```

You can then connect to the Argo Server UI at `http://localhost:2746'.

You can change the startup port by adding the `-p` option like so for port 8080:

```
$ ./bin/start-argo -p 8080
```

`minikube` will default to using the `docker` driver. You can change the driver used by minikube by using the `-d` option with `start-argo` like so

```
$ ./bin/start-argo -d DRIVER
```

where `DRIVER` is one of the supported VM drivers found [here](https://kubernetes.io/docs/setup/learning-environment/minikube/#specifying-the-vm-driver).

#### Deleting applications and stopping Kubernetes

To delete the argo and localstack deployment, run:

```
$ kubectl delete namespaces argo
```

`minikube` users can stop Kubernetes by pressing `ctrl-C` on the `bin/start-argo` process or run `minikube stop`.  Docker Desktop users will
need to close Docker or disable Kubernetes support in the UI.  Note that the latter uninstalls `kubectl`.

#### (minikube only) Configuring the callback URL for backend services

You can skip this step if you are using the default docker driver for minikube and set CALLBACK_URL_ROOT as described in the example dotenv file. If you are using a different driver such as virtualbox you may need to execute the following command to get the IP address minikube has bridged to localhost:

```bash
minikube ssh grep host.minikube.internal /etc/hosts | cut -f1
```

This should print out an IP address. Use this in your .env file to specify the `CALLBACK_URL_ROOT` value, e.g., `CALLBACK_URL_ROOT=http://192.168.65.2:4001`.

### Add A Service Backend

Clone the Harmony service example repository into a peer directory of the main Harmony repo
```
$ cd ..
$ git clone https://github.com/nasa/harmony-service-example.git
```

(minikube only) From the harmony-service-example project root, run
```bash
eval $(minikube docker-env)
```

This will set up the proper environment for building the image so that it may be used in minikube and Argo. Next run the following command to build and locally install the image:

```bash
./bin/build-image
```

This may take some time, but ultimately it will produce a local docker image tagged `harmony/gdal:latest`.  You may choose to use another service appropriate to your collection if you have [adapted it to run in Harmony](docs/adapting-new-services.md).

### Run Harmony

To run Harmony locally such that it reloads when files change (recommended during development), run

```
$ npm run start-dev
```

In production, we use `$ npm run start` which does the same but does not add the file watching and reloading behavior.

You should see messages about the two applications listening on two ports, ""frontend"" and ""backend.""  The frontend application receives requests from users, while the backend application receives callbacks from services.

### Connect A Client

You should now be able to view the outputs of performing a simple transformation request.  Harmony has its own test collection
set up for sanity checking harmony with the harmony-gdal backend.  This will fetch a granule from that collection converted to GeoTIFF:
[http://localhost:3000/C1233800302-EEDTEST/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?granuleId=G1233800343-EEDTEST](http://localhost:3000/C1233800302-EEDTEST/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?granuleId=G1233800343-EEDTEST)

You can also set up a WMS connection in [QGIS](https://qgis.org/en/site/about/index.html), for example, by placing the
`http://localhost:3000/C1233800302-EEDTEST/wms` as the ""URL"" field input in the ""Connection Details""
dialog when adding a new WMS connection.  Thereafter, expanding the connection should provide a list of layers obtained through a
GetCapabilities call to the test server, and double-clicking a layer should add it to a map, making a WMS call to retrieve an appropriate
PNG from the test server.

You can also use the Argo dashboard at http://localhost:2746 to visualize the workflows that were kicked off from your Harmony transformation requests.


## Local Development Of Workflows Using Visual Studio Code

This section describes a VS Code based approach to local development. The general ideas are, however, applicable to other editors.

There are two components to local development. The first is mounting your local project directory to a pod in a workflow so that changes to your code are automatically picked up whenever you run the workflow. The second is attaching a debugger to code running in a pod container (unless you prefer the print-debug method, in which case you can use the logs).

#### Prerequisites
* [Visual Studio Code](https://code.visualstudio.com/) and the [Kubernetes plugin](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-kubernetes-tools)

### Mounting a local directory to a pod running in a workflow

This is accomplished in two steps. The first step is to mount a local directory to a node in your `kubernetes/minikube` cluster. On a mac using the `virtualbox` driver the `/Users` directory is automatically mounted as `/Uses` on the single node in `minikube`. On Linux using the `virtualbox`driver the `/home` directory is automatically mounted at `/hosthome`. Other options for mounting a local directory can be found [here](https://minikube.sigs.k8s.io/docs/handbook/mount/).

The second step is to mount the directory on the node to a directory on the pod in your workflow. This can be done using a [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) volume defined in your workflow template. The following snippet creates a volume using the `/Users/username/project_folder` directory from the `node` on which the pod runs, _not directory from the local filesystem_. Again, on a mac using `virtualbox` the local `/Users` folder is conveniently mounted to the `/Users` folder on the node.

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-
spec:
  volumes:
  - name: test-volume
    hostPath:
      path: /Users/username/project_folder
  ...
```

You can then mount the volume in your pod using a `volumeMounts` entry in you container configuration:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-
spec:
  volumes:
  - name: test-volume
    hostPath:
      path: /Users/username/project_folder
  entrypoint: hello
  arguments:
    parameters:
    - name: message
      value: James
  templates:
  - name: hello
    inputs:
      parameters:
      - name: message
    container:
      image: node
      volumeMounts:
      - mountPath: /test-mount
        name: test-volume
```

Now the pod will be able to access local code directly in the `/test-mount` directory. Updates to code in the developers local project will immediately show up in workflows.

### Attaching a debugger to a running workflow

Argo Workflow steps run as kubernetes [jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/), which means that the containers that run them are short-lived. This complicates the process of attaching a debugger to them somewhat. In order to attach the debugger to code running in a container in a workflow you have to start the code in a manner that will pause the code on the first line when it runs and wait for a debugger to attach.

For NodeJS code this is easily done by passing the `--inspect-brk` option to the `node` command. workflow template building on our previous example is given here

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: hello-world-
spec:
  volumes:
  - name: test-volume
    hostPath:
      path: /Users/username/project_folder
  entrypoint: hello
  arguments:
    parameters:
    - name: message
      value: James
  templates:
  - name: hello
    inputs:
      parameters:
      - name: message
    container:
      image: node
      volumeMounts:
      - mountPath: /test-mount
        name: test-volume
      command: [node]
      args: [""--inspect-brk"", ""/test-mount/index.js"", ""{{inputs.parameters.message}}""]
```

In this example the starting point for the step is in the `index.js` file.

Similar approaches are available for Python and Java, although they might require changes to the code.

Once you launch your workflow it will pause at the step (wait for the icon in the UI to change from yellow to blue and spinning), and you can attach the debugger. For VS Code this is easily done using the `kubernetes` plugin.

Open the plugin by clicking on the `kubernetes` icon in the left sidebar. Expend the `CLUSTERS` tree to show the pods in `CLUSTERS>minikube>Nodes>minikube` then ctrl+click on the pod with the same name as the step in your workflow, e.g., `hello-world-9th8k` (you may need to refresh the view). Select `Debug (Attach)` from the menu, then selecting the `wait` container (not `main`), and select the runtime environment (java, nodejs, or python).

At this point the editor should open the file that is the starting point for your applications and it should be stopped on the first line of code to be run. You can then perform all the usual debugging operations such as stepping trough code and examining variables.

## Running in AWS

Note: It is currently easiest to allow the CI/CD service to deploy the service remotely; it is deployed to the sandbox after each merge to `master`.
As the deployment simply uploads the code, sets environment variables, kills the old server and runs `$ npm run start`, at present, there is not
typically much to be gained by running remotely during development.

When setting up a new environment, the first two steps need to be performed, but the CI environment should be set up to run the deployment rather than having
it done manually.

#### Prerequisites
* Once per account, run `$ bin/account-setup` to create a service linked role for ECS.
* Upload the harmony/gdal Docker image somewhere accessible to an EC2 deployment.  This should be done any time the image changes.  The easiest way is to create an ECR in your account and push the image there.  Running `$ bin/build-image && bin/push-image` from the harmony-gdal repository will perform this step..

### Stop here and set up CI/CD

Deploying the code should be done using the harmony-ci-cd project from Bamboo rather than manually.  Apart from that project and CI/CD setup,
we do not yet have automation scripts for (re)deploying to AWS manually, as it is typically not needed during development.

### Deploy the code to AWS

Note: The harmony-ci-cd repository contains automation code to do the following, usable from Bamboo.  You may use it locally by setting all
relevant environment variables in a `.env` file, running `$ bin/build-image` in the root directory of the harmony-ci-cd project, and then
running the **harmony-ci-cd** `bin/deploy` script from your **harmony** codebase's root directory.

1. `scp` the Harmony codebase to the remote instance
2. `ssh` into the remote instance
3. Run `$ $(aws ecr get-login --region=$AWS_DEFAULT_REGION --no-include-email)` where `AWS_DEFAULT_REGION` is the region containing your harmony-gdal ECR instance.
Skip this step if harmony-gdal is not in an ECR.
4. Run `$ if pgrep node; then pkill node; fi` to stop any existing server that may be running
5. Run `$ nohup npm start >> ../server.log 2>&1 &` to start harmony
6. Run `$ docker pull $GDAL_IMAGE` to fetch harmony-gdal changes, where `GDAL_IMAGE` is the EC2-accessible location of your harmony-gdal Docker image. Repeat for any other docker images you want to use.

### Connecting a client to an AWS instance

This process is identical to ""Connect a client"" above, except instead of `http://localhost:3000`, the protocol and host should be that of your
load balancer, e.g. `https://your-load-balancer-name.us-west-2.elb.amazonaws.com`.  Retrieve the precise load balancer details from the
AWS console.

### Updating development resources after pulling new code

Once up and running, if you update code, you can ensure dependencies are correct, Argo is deployed, and necessary Docker images are built
by running

```
$ npm run update-dev
```

## Contributing to Harmony

We welcome Pull Requests from developers not on the Harmony
team. Please follow the standard ""Fork and Pull Request"" workflow
shown below.

### Submitting a Pull Request

If you are a developer on another team and would like to submit a Pull
Request to this repo:

1. Create a fork of the harmony repository.
2. In the fork repo's permissions, add the `edc_snyk_user`
   with `Read` access
3. In the `#harmony-service-providers` Slack channel, ask a Harmony
   team member to import your fork repo into Snyk (see below).
4. When ready, submit a PR from the fork's branch back to the harmony
   master branch. Ideally name the PR with a Jira ticket name (e.g.,
   HARMONY-314)
5. The PR's 'build' tab should not show errors

### Importing a Fork Repo Into Snyk

To run Snyk on a fork of the repo (see above), the developer's
fork needs to be imported into Snyk:

1. Open [Snyk](https://app.snyk.io/org/esdis-cumulus-core-gibs-cmr-etc./reports/)
2. Click [Integrations](https://app.snyk.io/org/esdis-cumulus-core-gibs-cmr-etc./integrations)
   on the navbar at the top of the page
3. Click the integration type based on where the repo is hosted. E.g.:
   Bitbucket Server, GitHub, etc.
4. Search for 'harmony' using the search box
5. Click the checkbox on the developer's newly-created fork repo
6. Click the 'Import selected repositories' button

This import should be done before the developer submits a PR. If it
hasn't, the PR 'build' will fail and the PR will be blocked. In this
situation, the project can still be imported into Snyk, but then the
PR will need to be declined and resubmitted.

## Additional Resources

* [Adapting new services to Harmony](docs/adapting-new-services.md)
* [Harmony message schemas](app/schemas/data-operation)
* [EOSS protocol OpenAPI Specification](app/schemas/eoss)
* [Harmony NetCDF to Zarr service repository](https://github.com/nasa/harmony-netcdf-to-zarr)
* [Harmony GDAL example service repository](https://github.com/nasa/harmony-service-example)
* [Linux Container-based development with Harmony](docs/dev_container/README.md) (n.b. Windows users)
"
37,nasa/CARA_Analysis_Tools,MATLAB,"# CARA_Analysis_Tools
The tools provided here are free and open-source.

**Conjunction Consequence Assessment:**
> Risk is properly considered as the combination of likelihood and consequence; but conjunction assessment has usually limited itself to the consideration of only collision likelihood. When considered from an orbital regime protection perspective, the focus shifts to the question of the amount of debris that a collision might produce (the ‚Äúconsequence‚Äù). An operational algorithm for determining the expected amount of debris production should a conjunction result in a collision, has been proposed and previously validated.

**Monte Carlo from TCA:**
> A method that may be employed as a method of determining the probability of collision is to perform a Monte Carlo simulation of both the primary and secondary object states at the time of close approach and statistically determine the probability of collision based on the number of trials which violate a predetermined proximity threshold.
A Monte Carlo simulation is a computational technique that allows a probabilistic process to be modelled using random sampling from a known multivariate distribution. As the process of orbit determination yields both a best estimate of a satellite state and its associated uncertainty, the problem of collision probability lends itself well to using Monte Carlo sampling methodology.

**Single Covariance Max Pc:**
> Frisbee (2015) proposed a method by which the maximum possible probability of collision could be determined for a close approach event for which only one object has position uncertainty information. This is of particular use in determining whether an encounter may be of risk to an asset, as the maximum probability of collision may be below an actionable threshold. To determine the maximum probability of collision, the covariance ellipsoid of the object possessing a covariance matrix is mapped to the conjunction plane and distended so that the Mahalanobis distance between the two objects is equal to one. To do this, the covariance of the secondary object is oriented along a one dimensional position uncertainty along the miss vector between the two objects.

**Two-Dimension Pc:**
> One method that may be employed as a method of determining the probability of collision is by transforming a close approach event from a three dimensional problem to a two dimensional problem which greatly simplifies the calculation of the probability of collision. This calculation is widely used to characterize and analyze close approach events and determine resultant probabilities of collision as a result of mitigation actions.
"
38,nasa/NPSS-Power-System-Library,PLSQL,"# NPSS - Power System Library

## NASA - Glenn Research Center

## Introduction

This repository contains a power system library (PSL) for the Numerical
Propulsion System Simulation (NPSS) software framework.
It contains circuit components such as resistors and capacitors, electric
machines, and ports that are used to link them together.
The library also includes bus components that allow more than two elements to be
connected to a single node.

Several examples are included in the library to demonstrate the use of the
various components, and how they may be connected to an NPSS gas-turbine engine
model.

## Usage

If you have your NPSS environment set up, you can run a model like so:

```bat
runnpss-psl run\[file_name].run
```

> NOTE: You must run this command from the project root
directory (containing ""src"", ""model"", etc).

## Example

If you run the baseline example, ""baseline.run,"" you should get an output that
looks like this in the file ""engine.viewOut"":

```txt
*******************************************************************************                                                                 
NCP                   NPSS_2.7.1    model:         Baseline   run by:     glthoma1   solutionMode= STEADY_STATE     converge=    1    CASE:    0
time:  0.0000   timeStep:0.05000    therm_package:   GasTbl   Mode:         DESIGN   itr/pas/Jac/Bry=  15/  20/  1/ 13    run: 12/01/20 12:33:00

                                        FLOW STATION DATA                                                                               
                                W        Pt        Tt       ht     FAR       Wc        Ps        Ts      rhos     Aphy      MN      gamt
St0-St1   Atm.Fl_O         219.74    14.696    518.67   123.95  0.0000   219.74     0.000      0.00  0.000000      0.0  0.0000   1.40052
St1-St2   Prop.Fl_O        219.74    20.574    574.97   137.46  0.0000   165.25     0.000      0.00  0.000000      0.0  0.0000   1.39944
St00-St10 TurbineAtm.Fl>   500.00   500.000   2200.00   574.10  0.0200    30.27     0.000      0.00  0.000000      0.0  0.0000   1.30774
St10-End0 Turb.Fl_O        500.00   476.711   2177.85   567.66  0.0200    31.58     0.000      0.00  0.000000      0.0  0.0000   1.30837
St2-End   Noz.Fl_O         219.74    20.574    574.97   137.46  0.0000   165.25    14.000    515.05  0.073366    508.3  0.7625   1.39944

                                        ELECTRICAL PORT DATA                                          
                                   Complex Power Data           |               Misc Data             
             |S|, kVA  /_S, deg    P, kW  Q, kVAR     Power Type  Power Factor  frequency, Hz         
Gen1.EP_O     3332.51    11.567  3264.83   668.19            AC3        0.9797        400.000         
Cable1.EP_I   3332.51    11.567  3264.83   668.19            AC3        0.9797        400.000         
Cable1.EP_O   3328.84    11.478  3262.27   662.43            AC3        0.9800        400.000         
EM1.EP_I      3328.84    11.478  3262.27   662.43            AC3        0.9800        400.000         
                                                                                                      
                                   Complex Voltage Data  (V_LL)   |     Complex Current Data  (I_Line)
                 |V|, V  /_V, deg      V.r      V.j      |I|, A  /_I, deg      I.r      I.j           
Gen1.EP_O       1200.00     0.000  1200.00     0.00     1603.35   -41.567  1199.61  ----.--           
Cable1.EP_I     1200.00     0.000  1200.00     0.00     1603.35   -41.567  1199.61  ----.--           
Cable1.EP_O     1198.68    -0.088  1198.68    -1.85     1603.35   -41.567  1199.61  ----.--           
EM1.EP_I        1198.68    -0.088  1198.68    -1.85     1603.35   -41.567  1199.61  ----.--           

                                        ELECTRICAL POWER SYSTEM COMPONENT DATA  
            eff  Mass, kg  Loss_r, kW  Loss_j, kVAR  Q_heat, BTU/s              
Gen1    1.00000    251.14      136.03          0.00         128.94              
Cable1  0.99921    452.55        2.57          5.76           5.98              
EM1     1.00000    240.91      130.49        662.43         639.93              
                                                                                
                            ELECTRICAL POWER SYSTEM -- COMPONENT SPECIFIC DATA  
                                                                                
AeroCable Data                                                                  
           R, Ohms  L, Henries  X, Reactance  cable_size   #parallel    ampacity
Cable1   3.33E-004   2.97E-007     7.47E-004         2/0       6.000    1608.000
```
For more information, see the NPSS Power System Library [wiki page](../../wiki/Home/).
"
39,nasa/prog_models,Python,"# Prognostics Model Python Package

The NASA Prognostic Model Package is a python modeling framework focused on defining and building models for prognostics (computation of remaining useful life) of engineering systems, and provides a set of prognostics models for select components developed within this framework, suitable for use in prognostics applications for these components.

This is designed to be used with the [Prognostics Algorithms Package](https://github.com/nasa/prog_algs).

## Installation 
`pip3 install prog_models`

## Documentation
See documentation [here](https://nasa.github.io/prog_models/)
 
## Repository Directory Structure 
Here is the directory structure for the github repository 
 
`prog_models/` - The prognostics model python package<br />
&nbsp;&nbsp; |-`models/` - Example models<br /> 
&nbsp;&nbsp; |-`prognostics_model.py` - Physics-based model superclass of degraded system behavior<br />
&nbsp;&nbsp; |-`visualize.py` - Visualization tools<br />
`docs/` - Project documentation<br />
`sphinx_config/` - Configuration for automatic documentation generation<br />
`examples/` - Example python scripts using prog_models<br />
`tests/` - Tests for prog_models<br />
`README.md` - The readme (this file)<br />
`requirements.txt` - python library dependiencies required to be met to use this package. Install using `pip install -r requirements.txt`<br />
`prog_model_template.py` - Template for Prognsotics Model<br />
`tutorial.ipynb` - Tutorial (Juypter Notebook)

## Citing this repository
Use the following to cite this repository:

```
@misc{2020_nasa_prog_models,
    author    = {Christopher Teubert and Matteo Corbetta and Chetan Kulkarni and Matthew Daigle},
    title     = {Prognostics Models Python Package},
    month     = Apr,
    year      = 2021,
    version   = {1.0},
    url       = {https://github.com/nasa/prog_models}
    }
```

The corresponding reference should look like this:

C. Teubert, M. Corbetta, C. Kulkarni, and M. Daigle, Prognostics Models Python Package, v1.0, Apr. 2021. URL https://github.com/nasa/prog_models.

## Acknowledgements
The structure and algorithms of this package are strongly inspired by the [MATLAB Prognostics Model Library](https://github.com/nasa/PrognosticsModelLibrary). We would like to recognize Matthew Daigle and the rest of the team that contributed to the Prognostics Model Library for the contributions their work on the MATLAB library made to the design of prog_models

## Notices

Copyright ¬© 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

## Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
40,nasa/harmony-netcdf-to-zarr,Python,"# harmony/netcdf-to-zarr

A Harmony service to convert NetCDF4 files to Zarr files.  Takes conventional Harmony messages and translates
their input granules to Zarr using xarray.

This library intentionally does very little checking of the input files and file extensions.  It is designed
to work on NetCDF granules.  It ought to work with any other file type that can be opened with
[xarray.open_mfdataset](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html) using the
`h5netcdf` driver.  This includes some HDF5 EOSDIS datasets.  Individual collections must be tested to ensure
compatibility.


## Development

### Setup

#### Docker

It is possible to develop and run this service locally using only Docker.  This is the recommended option
for validation and small changes. Install [Docker](https://www.docker.com/get-started) on your development
machine.

#### Environment file

This service uses the
[harmony-service-lib-py](https://github.com/nasa/harmony-service-lib-py),
and requires that certain environment variables be set, as shown in the Harmony Service Lib README. For example,
`STAGING_BUCKET` and `STAGING_PATH` are required, and `EDL_USERNAME` and `EDL_PASSWORD` are required for any
data behind Earthdata Login. For local testing (not integrated into Harmony in a dev environment or AWS
deployment), use the example `.env` file in this repo:

    $ cp example/dotenv .env

and update the `.env` with the correct values.

#### Python & Project Dependencies (Optional)

If you would like to do local development outside of Docker, install Python, create a Python virtualenv,
and install the project dependencies.

If you have [pyenv](https://github.com/pyenv/pyenv) and
[pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv) installed (recommended), install Python and
create a virtualenv:

    $ pyenv install 3.7.4
    $ pyenv virtualenv 3.7.4 harmony-ntz
    $ pyenv activate harmony-ntz
    $ pyenv version > .python-version

The last step above creates a local .python-version file which will be automatically activated when cd'ing into the
directory if pyenv-virtualenv has been initialized in your shell (See the pyenv-virtualenv docs linked above).

Install project dependencies:

    $ pip install -r requirements/core.txt -r requirements/dev.txt

NOTE: All steps in this README which install dependencies need to be performed while on the NASA VPN
in order to download and install the Harmony Service Lib, which is published on the
[Nexus artifact repository](https://maven.earthdata.nasa.gov/).

### Development with Docker

#### Testing & Running the Service Independently

To run unit tests, coverage reports, or run the service on a sample message _outside_ of the
entire Harmony stack, start by building new runtime and test images:

*IMPORTANT*: Be sure to do these steps in a shell in which has *not* been updated to point to
the Minikube Docker daemon. This is usually done via a shell `eval` command. Doing so will
cause tests and the service to fail due to limitations in Minikube.

    $ bin/build-image
    $ bin/build-test-image

Run unit tests and generate overage reports. This will mount the local directory into the
container and run the unit tests. So all tests will reflect local changes to the service.

    $ bin/test-in-docker

You may be concurrently making changes to the Harmony Service Lib. To run the unit tests using
the local clone of that Harmony Service Lib and any changes made to it:

    $ LOCAL_SVCLIB_DIR=../harmony-service-lib-py bin/test-in-docker

Finally, run the service using an example Harmony operation request
([example/harmony-operation.json](example/harmony-operation.json) as input.  This will reflect
local changes to this repo, but will not include local changes to the Harmony Service Lib.

    $ bin/run-in-docker example/harmony-operation.json

To run the example and also include local Harmony Service Lib changes:

    $ LOCAL_SVCLIB_DIR=../harmony-service-lib-py bin/run-in-docker example/harmony-operation.json

#### Testing & Running the Service in Harmony

*Without local Harmony Service Lib changes*:

Be sure your environment is pointed to the Minikube Docker daemon:

    $ eval $(minikube docker-env)

Build the image:

    $ bin/build-image

You can now run a workflow in your local Harmony stack and it will execute using this image.

*With local Harmony Service Lib changes*:

To run this service in Harmony *with* a local copy of the Service Lib, build
the image, but specify the location of the local Harmony Service Lib clone:

    $ LOCAL_SVCLIB_DIR=../harmony-service-lib-py bin/build-image

You can now run a workflow in your local Harmony stack and it will execute using this image.
Note that this also means that the image needs to be rebuilt (using the same command) to
include any further changes to the Harmony Service Lib or this service.

### Development without Docker

#### Testing & running the Service Independently

Run tests with coverage reports:

    $ bin/test

Run an example:

    $ dotenv run python3 -m harmony_netcdf_to_zarr --harmony-action invoke --harmony-input ""$(bin/replace.sh example/harmony-operation.json)""

#### Installing `harmony-service-lib-py` in Development Mode

You may be concurrently developing on this service as well as the `harmony-service-lib-py`. If so, and you
want to test changes to it along with this service, install the `harmony-service-lib-py` in 'development mode'.
Install it using pip and the path to the local clone of the service library:

```
pip install -e ../harmony-service-lib-py
```

Now any changes made to that local repo will be visible in this project when you run tests, etc.

Finally, you can test & run the service in Harmony just as shown in the `Development with Docker` section above.
"
41,nasa/cmr-csw,Ruby,"# [CMR CATALOGUE SERVICE FOR THE WEB (CSW)](https://cmr.earthdata.nasa.gov/csw)

Visit NASA's CSW based on the EOSDIS COMMON METADATA REPOSITORY (CMR) at
[https://cmr.earthdata.nasa.gov/csw](https://cmr.earthdata.nasa.gov/csw)

[![Build Status](https://travis-ci.org/nasa/cmr-csw.svg?branch=master)](https://travis-ci.org/nasa/cmr-csw)

## About
The CMR CSW is a web application developed by [NASA](http://nasa.gov) [EOSDIS](https://earthdata.nasa.gov)
to enable data discovery, search, and access across EOSDIS' Earth Science data holdings.
It provides an interface compliant with the [OpenGIS Catalogue Service Implementation Specification v 2.0.2](http://portal.opengeospatial.org/files/?artifact_id=20555)
by taking advantage of NASA's [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/) APIs for data discovery and access.

## License

> Copyright ¬© 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
>
> Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>    http://www.apache.org/licenses/LICENSE-2.0
>
> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

## Third-Party Licenses

See public/licenses.txt

## Installation

* Ruby 2.5.3
* A Ruby version manager such as [RVM](http://rvm.io/) or [rbenv](https://github.com/rbenv/rbenv) is strongly recommended.

### Initial setup
Once the repository is cloned locally and Ruby 2.5.3 is installed, you must install the dependencies.
If you don't have the [bundler](http://bundler.io/) gem already installed, execute the command below in the project root directory:
   
    gem install bundler   

or if you wish to install the bundler without documentation:

    gem install bundler --no-rdoc --no-ri

Install all the gem dependencies:

    bundle install    

### Set up the required environment
The application requires the environment variables below to be set.

String that uniquely identifies your specific CMR CSW installation:
    
    client_id = <your client identifier>
    
### Run the automated [Rspec](http://rspec.info/) tests
Execute the command below in the project root directory:

    bin/rspec

All tests should pass.

### Run the application
Execute the command below in the project root directory:

    rails server

Open `http://localhost:3000/csw` in a local browser.
"
42,nasa/harmony-service-lib-py,Python,"# harmony-service-lib

A library for Python-based Harmony services to parse incoming messages, fetch data, stage data, and call back to Harmony

## Installing

### Using pip

Install the latest version of the package from PyPI using pip:

    $ pip install harmony-service-lib

### Other methods:

The package is installable from source via

    $ pip install git+https://github.com/harmony/harmony-service-lib-py.git#egg=harmony-service-lib

If using a local source tree, run the following in the source root directory instead:

    $ pip install -e .

## Usage

Services that want to work with Harmony can make use of this library to ease
interop and upgrades.  To work with Harmony, services must:

1. Receive incoming messages from Harmony.  Currently the CLI is the only
supported way to receive messages, though HTTP is likely to follow.  `harmony.cli`
provides helpers for setting up CLI parsing while being unobtrusive to non-Harmony
CLIs that may also need to exist.
2. Extend `harmony.BaseHarmonyAdapter` and implement the `#invoke` to
adapt the incoming Harmony message to a service call and adapt the service
result to call to one of the adapter's `#completed_with_*` methods. The adapter
class provides helper methods for retrieving data, staging results, and cleaning
up temporary files, though these can be overridden or ignored if a service
needs different behavior, e.g. if it operates on data in situ and does not
want to download the remote file.

A full example of these two requirements with use of helpers can be found in
[example/example_service.py](example/example_service.py)

## Environment

The following environment variables can be used to control the behavior of the
library and allow easier testing:

REQUIRED:

* `STAGING_BUCKET`: When using helpers to stage service output and pre-sign URLs, this
       indicates the S3 bucket where data will be staged
* `STAGING_PATH`: When using helpers to stage output, this indicates the path within
       `STAGING_BUCKET` under which data will be staged
* `ENV`: The name of the environment.  If 'dev' or 'test', callbacks to Harmony are
       not made and data is not staged unless also using localstack
* `OAUTH_UID`, `OAUTH_PASSWORD`: Used to acquire a shared EDL token
       needed for downloading granules from EDL token-aware data
       sources. Services using data in S3 do not need to set this.

       NOTE: If `FALLBACK_AUTHN_ENABLED` is set to True (CAUTION!)
       these credentials will be used to download data *as* the EDL
       application user. This may cause problems with metrics and can
       result in users getting data for which they've not approved a
       EULA.
* `OAUTH_CLIENT_ID`: The Earthdata application client ID.
* `OAUTH_HOST`: Set to the correct Earthdata Login URL, depending on
       where the service is being deployed. This should be the same
       environment where the `OAUTH_*` credentials are valid. Defaults
       to UAT.
* `OAUTH_REDIRECT_URI`: A valid redirect URI for the EDL application.
* `SHARED_SECRET_KEY`: The 32-byte encryption key shared between Harmony and backend services.
       This is used to encrypt & decrypt the `accessToken` in the Harmony operation message.
       In a production environment, this should be injected into the container running the service
       Docker image. When running the service within Harmony, the Harmony infrastructure will
       ensure that this environment variable is set with the shared secret key, and the Harmony
       service library will read and use this key. Therefore, the service developer need not
       be aware of this variable or its value.

OPTIONAL:

* `APP_NAME`: Defaults to first argument on commandline. Appears in log records.
* `AWS_DEFAULT_REGION`: (Default: `""us-west-2""`) The region in which S3 calls will be made
* `USE_LOCALSTACK`: (Development) If 'true' will perform S3 calls against localstack rather
       than AWS
* `LOCALSTACK_HOST`: (Development) If `USE_LOCALSTACK` `true` and this is set, will
       establish `boto` client connections for S3 & SQS operations using this hostname.
* `TEXT_LOGGER`: (Default: True) Setting this to true will cause all
       log messages to use a text string format. By default log
       messages will be formatted as JSON.
* `HEALTH_CHECK_PATH`: Set this to the path where the health check file should be stored. This
       file's mtime is set to the current time whenever a successful attempt is made to to read the
       message queue (whether or not a message is retrieved). This file can be used by a container's
       health check command. The container is considered unhealthy if the mtime of the file is old -
       where 'old' is configurable in the service container. If this variable is not set the path
       defaults to '/tmp/health.txt'.

OPTIONAL -- Use with CAUTION:

* `FALLBACK_AUTHN_ENABLED`: Default: False. Enable the fallback authentication that
  uses the EDL application credentials. See CAUTION note above.
* `EDL_USERNAME`: The Earthdata Login username used for fallback authn.
* `EDL_PASSWORD`: The Earthdata Login password used for fallback authn.

## Development Setup

Prerequisites:
  - Python 3.7+, ideally installed via a virtual environment such as `pyenv`
  - A local copy of the code

Install dependencies:

    $ make develop

Run linter against production code:

    $ make lint

Run tests:

    $ make test

Build & publish the package:

    $ make publish

## Releasing

GitHub release notes will automatically be generated based on pull request subjects.
Pull request subject lines should therefore concisely emphasize library
user-facing behavior and updates they should appear in the changelog.  If more
information is needed for release notes, note that in the PR content.
"
43,nasa/koviz,C++,"# Get the Code
```sh
git clone https://github.com/nasa/koviz.git
```
# Qt Prerequisites

`koviz` is built upon the Qt framework.

## Redhat 7+

```sh
sudo yum install qt5-qtbase-devel gcc gcc-c++ make flex bison
```

## Redhat 8+

```sh
sudo dnf install -y qt5-qtbase-devel bison flex make gcc gcc-c++
```

## Ubuntu latest
```sh
sudo apt-get install qtbase5-dev qt5-default
```

# Build

## Redhat 7 & 8

```sh
qmake-qt5
make
```
## Ubuntu

```sh
qmake
make
```

# Run

```sh
bin/koviz -h                  # for usage
bin/koviz /path/to/RUN_dir    # View trick run
bin/koviz /path/to/MONTE_dir  # View trick MONTE dir (set of runs)
```
"
44,nasa/icarous,C++,"![](ICAROUS-logo.jpeg """")

Independent Configurable Architecture for Reliable Operations of
Unmanned Systems (ICAROUS)
========

ICAROUS (Independent Configurable Architecture for Reliable Operations of
Unmanned Systems) is a software architecture that enables the robust integration
of mission specific software modules and highly assured core software
modules for building safety-centric autonomous unmanned aircraft
applications. The set of core software modules includes formally
verified algorithms that detect, monitor, and control conformance
to safety criteria; avoid stationary obstacles and maintain a safe
distance from other users of the airspace; and compute resolution
and recovery maneuvers, autonomously executed by the autopilot, when
safety criteria are violated or about to be violated. ICAROUS is implemented using the
NASA's core Flight Systems (cFS) middleware. The aforementioned functionalities are implemented as
cFS applications which interact via a publish/subscribe messaging
service provided by the cFS Software Bus.

### User Guide

https://nasa.github.io/icarous/

### Current Releases

- ICAROUS  V-2.2.5 - March 8, 2021

### Pycarous

Refer to [Python/pycarous/README.md](Python/pycarous/README.md) for more information about the Icarous python framework.


### License

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. 

### Contact

[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.

### Detect and Avoid (DAA) and Geofencing Capabilities

ICAROUS integrates NASA's open source software packages [DAIDALUS](http://shemesh.larc.nasa.gov/fm/DAIDALUS)
(Detect and Avoid Alerting Logic for Unmanned Systems) and
[PolyCARP](http://shemesh.larc.nasa.gov/fm/PolyCARP) (Algorithms and Software
for Computations with Polygons). DAIDALUS provides detect and avoid
capabilities, while PolyCARP provides geofencing capabilities.

## Logo

The ICAROUS logo was designed by 
[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC), NASA Langley Research Center.

### Copyright Notice

Copyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
45,nasa/eo-metadata-tools,Python,"# EO Metadata Tools #

This repository is for an upcoming project to provide tools for interfacing with
NASA metadata systems. Each directory possibly written in different programing
languages.

## Projects ##

* [CMR Python Wrapper](CMR/python)

## Community

Please review the following files:

* [Code of Conduct](CODE_OF_CONDUCT.md)
* [Contributing](CONTRIBUTING.md)
* [License](LICENSE)
* [Security](SECURITY.md)
"
46,nasa/nasa.github.io,HTML,"# NASA's Public GitHub Organization

## Purpose
The github.com/nasa organizational account is intended to publicly host NASA code that has been SRA-approved for open source. 

# IF YOU ARE LOOKING FOR INFORMATION, PLEASE CHECK OUT THE GITHUB PAGES PAGE CREATED BY THIS REPO https://nasa.github.io/

<!-- <b><i>If you are a NASA Staff member, please check out these instructions for adding a code repository or getting made a collaborator: https://nasa.github.io/</i></b> -->

## Visualizations of NASA Code
An exploration of user engagement with NASA code repositories. Use this to find out which code has active development and might be good place to submit a pull request! https://observablehq.com/@justingosses/public-engagement-with-nasas-open-source-code-projects-on-g?collection=@justingosses/nasa-metadata

A collection of data explorations on observable using NASA open-source code metadata: https://observablehq.com/collection/@justingosses/nasa-metadata

## Owners
We currently have two active owners in the org:
+ Taylor Yates (evan.t.yates@nasa.gov)
+ Justin Gosses (justin.c.gosses@nasa.gov)

Please reach out to us if you have any questions not covered in https://nasa.github.io/ or https://code.nasa.gov/#/guide

<!-- ## Related Information & Sites

#### Please make sure any repos added here are also tracked in code.nasa.gov! 
In addition to being a congressional mandate, these will then get harvested into [code.gov](https://code.gov/) enabling tracking of government written code provided to the public. 

#### Other Related Sites
- [code.nasa.gov](https://code.nasa.gov)
- [data.nasa.gov](https://data.nasa.gov)
- [api.nasa.gov](https://api.nasa.gov)
- [open.nasa.gov](https://open.nasa.gov)
- [nasa.gov/data](https://nasa.gov/data) -->
"
47,nasa/trick,C++,"<p align=center>
<a href=""https://nasa.github.io/trick"">
<img src=""https://raw.github.com/nasa/Trick/master/TrickLogo.png"" alt=""Trick Logo"" height=150px>
</a>
</p>

<p align=left>
<a href=""https://github.com/nasa/trick/actions?query=workflow%3ALinux"">
<img src=""https://github.com/nasa/trick/workflows/Linux/badge.svg?branch=master"" alt=""Linux"" height=30px>
</a>
</p>

<p align=left>
<a href=""https://github.com/nasa/trick/actions?query=workflow%3AmacOS"">
<img src=""https://github.com/nasa/trick/workflows/macOS/badge.svg?branch=master"" alt=""macOS"" height=30px>
</a>
</p>

<p align=left>
<a href=""https://github.com/nasa/trick/actions?query=workflow%3A32-bit"">
<img src=""https://github.com/nasa/trick/workflows/32-bit/badge.svg?branch=master"" alt=""macOS"" height=30px>
</a>
</p>

<p align=justify>
The Trick Simulation Environment, developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development. Trick expedites the creation of simulations for early vehicle design, performance evaluation, flight software development, flight vehicle dynamic load analysis, and virtual/hardware in the loop training. Trick's purpose is to provide a common set of simulation capabilities that allow users to concentrate on their domain specific models, rather than simulation-specific functions like job ordering, input file processing, or data recording.
</p>

<table>
    <col width=""33%"">
    <col width=""33%"">
    <col width=""33%"">
    <thead>
        <tr>
            <th><a href=""https://nasa.github.io/trick/documentation/install_guide/Install-Guide"">Install Guide</a></th>
            <th><a href=""https://nasa.github.io/trick/tutorial/Tutorial"">Tutorial</a></th>
            <th><a href=""https://nasa.github.io/trick/documentation/Documentation-Home"">Documentation</a></th>
        </tr>
    </thead>
    <tbody>
        <tr align=""center"">
            <td>Follow the installation guide to properly configure Trick on your operating system.</td>
            <td>Complete the tutorial to become familiar with the basics.</td>
            <td>Visit the documentation for a more complete understanding of Trick.</td>
        </tr>
    </tbody>
</table>

<table>
    <col width=""33%"">
    <col width=""33%"">
    <col width=""33%"">
    <thead>
        <tr>
            <th><a href=""https://nasa.github.io/trick/related_projects/Related-Projects"">Related Projects</a></th>
            <th><a href=""https://nasa.github.io/trick/faq/FAQ"">Frequently Asked Questions</a></th>
            <th><a href=""https://nasa.github.io/trick/howto_guides/How-To-Guides"">How-To Guides</a></th>
        </tr>
    </thead>
    <tbody>
        <tr align=""center"">
            <td>View some of the many projects that use Trick.</td>
            <td>Read some of the most frequently asked questions pertaining to Trick.</td>
            <td>See a collection of how-to guides detailing common Trick processes.</td>
        </tr>
    </tbody>
</table>

---

Trick is released under the NASA Open Source Agreement Version 1.3 [license](https://github.com/nasa/trick/blob/master/LICENSE).
"
48,nasa/ow_europa,GLSL,"Notices:
--------
Copyright ¬© 2020 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers
-----------
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.


ow_europa
============
This repository contains models of mission sites on Europa. It is a separate
repository because it will potentially contain very large amounts of data that
we do not want to mix with smaller code repositories.

"
49,nasa/cmr-opensearch,Ruby,"# [Common Metadata Repository (CMR) OpenSearch](https://cmr.earthdata.nasa.gov/openseaarch)

Visit NASA's EOSDIS CMR OpenSearch at:
[https://cmr.earthdata.nasa.gov/opensearch](https://cmr.earthdata.nasa.gov/opensearch)

The CMR OpenSearch documentation page is at:
[https://cmr.earthdata.nasa.gov/opensearch/home/docs](https://cmr.earthdata.nasa.gov/opensearch/home/docs)

[![Build Status](https://travis-ci.org/nasa/cmr-opensearch.svg?branch=master)](https://travis-ci.org/nasa/cmr-opensearch)

## About
CMR OpenSearch is a web application developed by [NASA](http://nasa.gov) [EOSDIS](https://earthdata.nasa.gov)
to enable data discovery, search, and access across Earth Science data holdings by using an open standard.
It provides an interface compliant with the [OpenSearch 1.1 (Draft 5) specification](http://www.opensearch.org/Home)
by taking advantage of NASA's [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/) APIs for data discovery and access.

## License

> Copyright ¬© 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
>
> Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>    http://www.apache.org/licenses/LICENSE-2.0
>
> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

## Third-Party Licenses

See public/licenses.txt

## Installation

* Ruby 2.5.3
* A Ruby version manager such as [RVM](http://rvm.io/) or [rbenv](https://github.com/rbenv/rbenv) is strongly recommended.

### Initial setup
Once the repository is cloned locally and Ruby 2.5.3 is installed, you must install the dependencies.
If you don't have the [bundler](http://bundler.io/) gem already installed, execute the command below in the project root directory:

    gem install bundler   

or if you wish to install the bundler without documentation:

    gem install bundler --no-rdoc --no-ri

Install all the gem dependencies:

    bundle install    

In some cases, depending on your operating system type and/or version, the above command will fail while trying to install
the libv8 and therubyracer gems.  While there might be lots of causes for the errors and lots of
solutions to fix the errors, we found that on some versions of OS X, you can overcome the problem by trying to use the existing
operating system version of the libv8 library, rather than trying to build a new one during the normal gem install.
We found the following workarounds to the _**bundle install**_ failures due to libv8:

    $ brew install v8@3.15
    $ bundle config build.libv8 --with-system-v8
    $ bundle config build.therubyracer --with-v8-dir=$(brew --prefix v8@3.15)
    $ bundle install

Local problems with mimemagic on MACOSX?
    brew install shared-mime-info

### Set up the required environment
The application requires the environment variables below to be set in order to run the web application:  

URL of the internal / back-end CMR API instance endpoint.  In a hosted environment, the application
takes advantage of the direct access back-end interal URLs for increased performance in comparison to
the public CMR API instance endpoint. For local installs or installs in non-CMR hosting environments,
the _catalog_rest_endpoint_ and the _public_catalog_rest_endpoint_ should both point to the public
CMR search API endpoint.

    catalog_rest_endpoint = <internal endpoint for the CMR Search API instance used by the application>

URL of your specific CMR OpenSearch install:

    opensearch_url = <CMR OpenSearch application URL>

URL of the public CMR search API instance used in OpenSearch results links in the response ATOM feed:

    public_catalog_rest_endpoint = <public endpoint for the CMR Search API instance>

URL for the release page of the CMR OpenSearch application.
The release page references appear on the user interface as well as in the search results:

    release_page = <CMR OpenSearch EarthData release page>

The ATOM feed author email to be used for CMR entries in the matching ATOM results feed:

    contact = <atom feed author email for each feed entry>

The environment specific postfix (such as dev,PT, TB etc.) for the internally generated client_id sent to echo in
order to obtain an echo token:

    mode = <postfix for client_id string used in obtaining echo tokens>

The ECHO REST endpoint used for obtaining user access tokens:

    echo_rest_endpoint = <echo REST endpoint>

A CMR token with CMR collection tagging permissions in the respective CMR environment that the
_public_catalog_rest_endpoint_ points to:

    CMR_ECHO_SYSTEM_TOKEN = <value>

URL for the CMR documentation page which appears in the OpenSearch web application footer:

    documentation_page = <CMR documentation page URL>

Email for the NASA official responsible for the CMR OpenSearch application:

    organization_contact_email = <email>

Full name for the NASA official responsible for CMR OpenSearch application:

    organization_contact_name = <full name>

We provide default values for the above environment variables to enable running of the automated tests during
CI (continuous integrations) builds.
The application first looks for the configuration file:

    config/application.yml

If the file exists, the application loads the values of variables in the file in the Rails environment.  Having
a local _config/application.yml_ file is an effective way to populate the environment variables
that the application needs in order to run.  A sample _config/application.yml_ file is below:

    current: &current
        opensearch_url: http://localhost:3000/opensearch
        catalog_rest_endpoint: https://cmr.earthdata.nasa.gov/search/
        echo_rest_endpoint: https://api.echo.nasa.gov/echo-rest/
        contact: echodev@echo.nasa.gov
        mode: dev
        public_catalog_rest_endpoint: https://cmr.earthdata.nasa.gov/search/
        release_page: https://wiki.earthdata.nasa.gov/display/echo/Open+Search+API+release+information
        documentation_page: https://wiki.earthdata.nasa.gov/display/CMR/Common+Metadata+Repository+Home
        organization: Sample Organization Name
        organization_contact_email: contact@example.com
        organization_contact_name: ContactFirstName ContactLastName

    development:
        <<: *current
        CMR_ECHO_SYSTEM_TOKEN: ""CMR system token with tagging permissions in the CMR environment that development uses""

    production:
        <<: *current
        CMR_ECHO_SYSTEM_TOKEN: ""CMR system token with tagging permissions in the CMR PROD environment""

    test: &test
        # test values are already defaulted to enable CI automated Rspec and cucumber tests

### Run the automated [Rspec](http://rspec.info/) and [cucumber](https://github.com/cucumber/cucumber-rails) tests
Execute the commands below in the project root directory:

    bundle exec rspec
    bundle exec cucumber

All tests should pass in less than 2 minutes.

### Run the application
Execute the command below in the project root directory:

    rails server

Open `http://localhost:3000/opensearch` in a local browser.
"
50,nasa/harmony-service-example,Python,"# harmony-gdal

A demonstration of a subsetter capability to be used with Harmomy.

## Prerequisites

For building & pushing the image locally:

1. [Docker](https://www.docker.com/get-started)

For local development:

1. [pyenv](https://github.com/pyenv/pyenv)
2. [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv)

## Development

### Install dependencies

1. Install & use miniconda:

        $ pyenv install miniconda3-4.7.12
        $ pyenv local miniconda3-4.7.12

2. Create and activate a conda environment containing the development dependencies:

        $ conda env create -n hgdal -f environment.yml
        $ pyenv activate miniconda3-4.7.12/envs/hgdal

3. (Optional) To use an unreleased version of the `harmony-service-lib-py`, e.g., when testing changes to it, install it as a dependency from the filesystem:

        $ git clone https://git.earthdata.nasa.gov/projects/HARMONY/repos/harmony-service-lib-py/browse ../harmony-service-lib-py
        $ pip3 install ../harmony-service-lib-py/ --target deps/harmony-service-lib-py

### Run unit tests:

        # Run the tests once:
        $ pytest --ignore deps

        # Run the tests continuously in watch mode:
        $ ptw -c --ignore deps

### Manually building & deploying

1. Build the Docker image:

        $ bin/build-image

2. Deploy (publish) the Docker image to Amazon ECR:

        $ bin/push-image

### Building from Dev Container

If you plan to build the Docker image from a container, in addition to the above instructions, you'll want to create a .env file and populate it with the following:

```
# Harmony-GDAL Environment Variables

# Set to 'true' if running Docker in Docker and the docker daemon is somewhere other than the current context
DIND=true

# Indicates where docker commands should find the docker daemon
DOCKER_DAEMON_ADDR=host.docker.internal:2375
```

## CI

The project has a [Bamboo CI job](https://ci.earthdata.nasa.gov/browse/HARMONY-HG) running
in the Earthdata Bamboo environment.
"
51,nasa/apod-api,Python,"# Astronomy Picture of the Day (APOD) microservice

A microservice written in Python with the [Flask micro framework](http://flask.pocoo.org).

## NOTES: 
### Code re-organization has occurred [2020-05-04]!
Code was reorganized to make it work more easily on AWS's Elastic Beanstalk service.

The changes over previous version were :
1. Moved main code out of the APOD folder and into the top level directory as Elastic Beanstalk had a hard time finding the initial python file unless it was in the top-level folder. 
2. Changed service.py to application.py
3. Changed references to app in application.py to application

You can find a frozen version of the previous code in the branch called <a href=""https://github.com/nasa/apod-api/tree/prevCodeOrganization"">""prevCodeOrganization""</a>

#### API Reliability
A very large number of people use the instance of this API that NASA has set up. If you need a extremely reliable version of this API, you likely want to stand up your own version of the API. You can do that with this code! All information that this API returns is actually just grabbed from the <a href='https://apod.nasa.gov/apod/astropix.html'>Astronomy Photo of the Day Website</a> (APOD).

#### Content Related Issues
No one watching this repository has anything to do with Astronomy Photo of the Day website, so we're unable to deal with issues directly related to their content. Please contact them directly.


# Table of contents
1. [Getting Started](#getting_started)
    1. [Standard environment](#standard_env)
    2. [`virtualenv` environment](#virtualenv)
    3. [`conda` environment](#conda)
2. [Docs](#docs)
3. [APOD parser](#TheAPODParser)
4. [Deployed](#Deployed)
5. [Feedback](#feedback)
6. [Author](#author)

&nbsp;
## Getting started <a name=""getting_started""></a>

### Standard environment <a name=""standard_env""></a>

1. Clone the repo
```bash
git clone https://github.com/nasa/apod-api
```
2. `cd` into the new directory
```bash
cd apod-api
```
3. Install dependencies into the project's `lib`
```bash
pip install -r requirements.txt -t lib
```
4. Add `lib` to your PYTHONPATH and run the server
```bash
PYTHONPATH=./lib python application.py
```
&nbsp;
### `virtualenv` environment <a name=""virtualenv""></a>

1. Clone the repo
```bash
git clone https://github.com/nasa/apod-api
```
2. `cd` into the new directory
```bash
cd apod-api
```
3. Create a new virtual environment `env` in the directory
```bash
python -m virtualenv env
```
4. Activate the new environment
```bash
source env/bin/activate
```
5. Install dependencies in new environment
```bash
pip install -r requirements.txt
```
6. Run the server locally
```bash
python application.py
```
&nbsp;
### `conda` environment <a name=""conda""></a>

1. Clone the repo
```bash
git clone https://github.com/nasa/apod-api
```
2. `cd` into the new directory
```bash
cd apod-api
```
3. Create a new virtual environment `env` in the directory
```bash
conda create --prefix ./env
```
4. Activate the new environment
```bash
conda activate ./env
```
5. Install dependencies in new environment
```bash
pip install -r requirements.txt
```
6. Run the server locally
```bash
python application.py
```

### Run it in Docker

1. Clone the repo
```bash
git clone https://github.com/nasa/apod-api.git
```
2. `cd` into the new directory
```bash
cd apod-api
```
3. Build the image
```bash
docker build . -t apod-api
```
4. Run the image
```bash
docker run -p 5000:5000 apod-api
```

&nbsp;
## Docs <a name=""docs""></a>

### Endpoint: `/<version>/apod`

There is only one endpoint in this service which takes 2 optional fields
as parameters to a http GET request. A JSON dictionary is returned nominally.

**Fields**

- `date` A string in YYYY-MM-DD format indicating the date of the APOD image (example: 2014-11-03).  Defaults to today's date.  Must be after 1995-06-16, the first day an APOD picture was posted.  There are no images for tomorrow available through this API.
- `concept_tags` A boolean indicating whether concept tags should be returned with the rest of the response.  The concept tags are not necessarily included in the explanation, but rather derived from common search tags that are associated with the description text.  (Better than just pure text search.)  Defaults to False.
- `hd` A boolean parameter indicating whether or not high-resolution images should be returned. This is present for legacy purposes, it is always ignored by the service and high-resolution urls are returned regardless.
- `count` A positive integer, no greater than 100. If this is specified then `count` randomly chosen images will be returned in a JSON array. Cannot be used in conjunction with `date` or `start_date` and `end_date`.
- `start_date` A string in YYYY-MM-DD format indicating the start of a date range. All images in the range from `start_date` to `end_date` will be returned in a JSON array. Cannot be used with `date`.
- `end_date` A string in YYYY-MM-DD format indicating that end of a date range. If `start_date` is specified without an `end_date` then `end_date` defaults to the current date.
- `thumbs` If set to `true`, the API returns URL of video thumbnail. If an APOD is not a video, this parameter is ignored.

**Returned fields**

- `resource` A dictionary describing the `image_set` or `planet` that the response illustrates, completely determined by the structured endpoint.
- `concept_tags` A boolean reflection of the supplied option.  Included in response because of default values.
- `title` The title of the image.
- `date` Date of image. Included in response because of default values.
- `url` The URL of the APOD image or video of the day.
- `hdurl` The URL for any high-resolution image for that day. Returned regardless of 'hd' param setting but will be omitted in the response IF it does not exist originally at APOD.
- `media_type` The type of media (data) returned. May either be 'image' or 'video' depending on content.
- `explanation` The supplied text explanation of the image.
- `concepts` The most relevant concepts within the text explanation.  Only supplied if `concept_tags` is set to True.
- `thumbnail_url` The URL of thumbnail of the video. 
- `copyright` The name of the copyright holder.
- `service_version` The service version used.

**Example**

```bash
localhost:5000/v1/apod?date=2014-10-01&concept_tags=True
```
<details><summary>See Return Object</summary>
<p>

```jsoniq
{
    resource: {
        image_set: ""apod""
    },
    concept_tags: ""True"",
    date: ""2013-10-01"",
    title: ""Filaments of the Vela Supernova Remnant"",
    url: ""http://apod.nasa.gov/apod/image/1310/velafilaments_jadescope_960.jpg"",
    explanation: ""The explosion is over but the consequences continue. About eleven
    thousand years ago a star in the constellation of Vela could be seen to explode,
    creating a strange point of light briefly visible to humans living near the
    beginning of recorded history. The outer layers of the star crashed into the
    interstellar medium, driving a shock wave that is still visible today. A roughly
    spherical, expanding shock wave is visible in X-rays. The above image captures some
    of that filamentary and gigantic shock in visible light. As gas flies away from the
    detonated star, it decays and reacts with the interstellar medium, producing light
    in many different colors and energy bands. Remaining at the center of the Vela
    Supernova Remnant is a pulsar, a star as dense as nuclear matter that rotates
    completely around more than ten times in a single second."",
    concepts: {
        0: ""Astronomy"",
        1: ""Star"",
        2: ""Sun"",
        3: ""Milky Way"",
        4: ""Hubble Space Telescope"",
        5: ""Earth"",
        6: ""Nebula"",
        7: ""Interstellar medium""
    }
}
```

</p>
</details>


```bash
https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&count=5
```

<details><summary>See Return Object</summary>
<p>


```jsoniq
[
  {
    ""copyright"": ""Panther Observatory"",
    ""date"": ""2006-04-15"",
    ""explanation"": ""In this stunning cosmic vista, galaxy M81 is on the left surrounded by blue spiral arms.  On the right marked by massive gas and dust clouds, is M82.  These two mammoth galaxies have been locked in gravitational combat for the past billion years.   The gravity from each galaxy dramatically affects the other during each hundred million-year pass.  Last go-round, M82's gravity likely raised density waves rippling around M81, resulting in the richness of M81's spiral arms.  But M81 left M82 with violent star forming regions and colliding gas clouds so energetic the galaxy glows in X-rays.  In a few billion years only one galaxy will remain."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/0604/M81_M82_schedler_c80.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Galaxy Wars: M81 versus M82"",
    ""url"": ""https://apod.nasa.gov/apod/image/0604/M81_M82_schedler_c25.jpg""
  },
  {
    ""date"": ""2013-07-22"",
    ""explanation"": ""You are here.  Everyone you've ever known is here. Every human who has ever lived -- is here. Pictured above is the Earth-Moon system as captured by the Cassini mission orbiting Saturn in the outer Solar System. Earth is the brighter and bluer of the two spots near the center, while the Moon is visible to its lower right. Images of Earth from Saturn were taken on Friday. Quickly released unprocessed images were released Saturday showing several streaks that are not stars but rather cosmic rays that struck the digital camera while it was taking the image.  The above processed image was released earlier today.  At nearly the same time, many humans on Earth were snapping their own pictures of Saturn.   Note: Today's APOD has been updated."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/1307/earthmoon2_cassini_946.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Earth and Moon from Saturn"",
    ""url"": ""https://apod.nasa.gov/apod/image/1307/earthmoon2_cassini_960.jpg""
  },
  {
    ""copyright"": ""Joe Orman"",
    ""date"": ""2000-04-06"",
    ""explanation"": ""Rising before the Sun on February 2nd, astrophotographer Joe Orman anticipated this apparition of the bright morning star Venus near a lovely crescent Moon above a neighbor's house in suburban Phoenix, Arizona, USA. Fortunately, the alignment of bright planets and the Moon is one of the most inspiring sights in the night sky and one that is often easy to enjoy and share without any special equipment. Take tonight, for example. Those blessed with clear skies can simply step outside near sunset and view a young crescent Moon very near three bright planets in the west Jupiter, Mars, and Saturn. Jupiter will be the unmistakable brightest star near the Moon with a reddish Mars just to Jupiter's north and pale yellow Saturn directly above. Of course, these sky shows create an evocative picture but the planets and Moon just appear to be near each other -- they are actually only approximately lined up and lie in widely separated orbits. Unfortunately, next month's highly publicized alignment of planets on May 5th will be lost from view in the Sun's glare but such planetary alignments occur repeatedly and pose no danger to planet Earth."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/0004/vm_orman_big.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Venus, Moon, and Neighbors"",
    ""url"": ""https://apod.nasa.gov/apod/image/0004/vm_orman.jpg""
  },
  {
    ""date"": ""2014-07-12"",
    ""explanation"": ""A new star, likely the brightest supernova in recorded human history, lit up planet Earth's sky in the year 1006 AD. The expanding debris cloud from the stellar explosion, found in the southerly constellation of Lupus, still puts on a cosmic light show across the electromagnetic spectrum. In fact, this composite view includes X-ray data in blue from the Chandra Observatory, optical data in yellowish hues, and radio image data in red. Now known as the SN 1006 supernova remnant, the debris cloud appears to be about 60 light-years across and is understood to represent the remains of a white dwarf star. Part of a binary star system, the compact white dwarf gradually captured material from its companion star. The buildup in mass finally triggered a thermonuclear explosion that destroyed the dwarf star. Because the distance to the supernova remnant is about 7,000 light-years, that explosion actually happened 7,000 years before the light reached Earth in 1006. Shockwaves in the remnant accelerate particles to extreme energies and are thought to be a source of the mysterious cosmic rays."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/1407/sn1006c.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""SN 1006 Supernova Remnant"",
    ""url"": ""https://apod.nasa.gov/apod/image/1407/sn1006c_c800.jpg""
  },
  {
    ""date"": ""1997-01-21"",
    ""explanation"": ""In Jules Verne's science fiction classic A Journey to the Center of the Earth, Professor Hardwigg and his fellow explorers encounter many strange and exciting wonders. What wonders lie at the center of our Galaxy? Astronomers now know of some of the bizarre objects which exist there, like vast dust clouds,\r bright young stars, swirling rings of gas, and possibly even a large black hole. Much of the Galactic center region is shielded from our view in visible light by the intervening dust and gas. But it can be explored using other forms of electromagnetic radiation, like radio, infrared, X-rays, and gamma rays. This beautiful high resolution image of the Galactic center region in infrared light was made by the SPIRIT III telescope onboard the Midcourse Space Experiment. The center itself appears as a bright spot near the middle of the roughly 1x3 degree field of view, the plane of the Galaxy is vertical, and the north galactic pole is towards the right. The picture is in false color - starlight appears blue while dust is greenish grey, tending to red in the cooler areas."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/9701/galcen_msx_big.gif"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Journey to the Center of the Galaxy \r\nCredit:"",
    ""url"": ""https://apod.nasa.gov/apod/image/9701/galcen_msx.jpg""
  }
]
```

</p>
</details>



```bash
https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&start_date=2017-07-08&end_date=2017-07-10
```

<details><summary>See Return Object</summary>
<p>

```jsoniq
[
  {
    ""copyright"": ""T. Rector"",
    ""date"": ""2017-07-08"",
    ""explanation"": ""Similar in size to large, bright spiral galaxies in our neighborhood, IC 342 is a mere 10 million light-years distant in the long-necked, northern constellation Camelopardalis. A sprawling island universe, IC 342 would otherwise be a prominent galaxy in our night sky, but it is hidden from clear view and only glimpsed through the veil of stars, gas and dust clouds along the plane of our own Milky Way galaxy. Even though IC 342's light is dimmed by intervening cosmic clouds, this sharp telescopic image traces the galaxy's own obscuring dust, blue star clusters, and glowing pink star forming regions along spiral arms that wind far from the galaxy's core. IC 342 may have undergone a recent burst of star formation activity and is close enough to have gravitationally influenced the evolution of the local group of galaxies and the Milky Way."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/1707/ic342_rector2048.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Hidden Galaxy IC 342"",
    ""url"": ""https://apod.nasa.gov/apod/image/1707/ic342_rector1024s.jpg""
  },
  {
    ""date"": ""2017-07-09"",
    ""explanation"": ""Can you find your favorite country or city?  Surprisingly, on this world-wide nightscape, city lights make this task quite possible.  Human-made lights highlight particularly developed or populated areas of the Earth's surface, including the seaboards of Europe, the eastern United States, and Japan.  Many large cities are located near rivers or oceans so that they can exchange goods cheaply by boat.  Particularly dark areas include the central parts of South America, Africa, Asia, and Australia.  The featured composite was created from images that were collected during cloud-free periods in April and October 2012 by the Suomi-NPP satellite, from a polar orbit about 824 kilometers above the surface, using its Visible Infrared Imaging Radiometer Suite (VIIRS)."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/1707/EarthAtNight_SuomiNPP_3600.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Earth at Night"",
    ""url"": ""https://apod.nasa.gov/apod/image/1707/EarthAtNight_SuomiNPP_1080.jpg""
  },
  {
    ""date"": ""2017-07-10"",
    ""explanation"": ""What's happening around the center of this spiral galaxy? Seen in total, NGC 1512 appears to be a barred spiral galaxy -- a type of spiral that has a straight bar of stars across its center.  This bar crosses an outer ring, though, a ring not seen as it surrounds the pictured region. Featured in this Hubble Space Telescope image is an inner ring -- one that itself surrounds the nucleus of the spiral.  The two rings are connected not only by a bar of bright stars but by dark lanes of dust. Inside of this inner ring, dust continues to spiral right into the very center -- possibly the location of a large black hole. The rings are bright with newly formed stars which may have been triggered by the collision of NGC 1512 with its galactic neighbor, NGC 1510."",
    ""hdurl"": ""https://apod.nasa.gov/apod/image/1707/NGC1512_Schmidt_1342.jpg"",
    ""media_type"": ""image"",
    ""service_version"": ""v1"",
    ""title"": ""Spiral Galaxy NGC 1512: The Nuclear Ring"",
    ""url"": ""https://apod.nasa.gov/apod/image/1707/NGC1512_Schmidt_960.jpg""
  }
]
```


</p>
</details>

## The APOD Parser<a name=""TheAPODParser""></a>

<i>The APOD Parser is not part of the API itself. </i> Rather is intended to be used for accessing the APOD API quickly with Python without writing much additional code yourself. It is found in the apod_parser folder.

### Usage

1. First import the `apod_object_parser.py` file.

2. Now use the `get_data` function and pass your API key as the only argument. You can get the API key <a href=""https://api.nasa.gov/#signUp"">here</a>

```python
response = apod_object_parser.get_data(<your_api_key>)
```

3. Now you can use the following functions:

-> `apod_object_parser.get_date(response)`

-> `apod_object_parser.get_explaination(response)`

-> `apod_object_parser.get_hdurl(response)`

-> `apod_object_parser.get_media_type(response)`

-> `apod_object_parser.get_service_version(response)`

-> `apod_object_parser.get_title(response)`

-> `apod_object_parser.get_url(response)`

**for full docs and more functions visit the readme of  the apod parser by clicking <a href=""apod_parser/apod_parser_readme.md"">here</a>**

## Deployed <a name=""Deployed""></a>
The deployed version of this API is based on the `eb` branch. The version that was deployed before that is in the `eb_previous` branch. The `master` branch is used as development as that's where most of the pull requests will come into anyways.

This API is deployed on AWS using elastic beanstalk due to large number of people who use the service. However, if you're planning on using it just yourself, it is small enough to be stood up on a single micro EC2 or any other small size cloud compute machine.

## Feedback <a name=""feedback""></a>

Star this repo if you found it useful. Use the github issue tracker to give
feedback on this repo.

## Author <a name=""author""></a>
- Brian Thomas (based on code by Dan Hammer) 
- Justin Gosses (made changes to allow this repository to run more easily on AWS Elastic Beanstalk after heroku instance was shut-down)
- Please checkout the <a href=""https://github.com/nasa/apod-api/graphs/contributors"">contributers</a> to this repository on the righthand side of this page. 

## Contributing
We do accept pull requests from the public. Please note that we can be slow to respond. Please be patient. 

Also, **the people with rights on this repository are not people who can debug problems with the APOD website itself**. If you would like to contribute, right now we could use some attention to the tests. 

"
52,nasa/GlennOPT,Jupyter Notebook,"# GlennOPT
## Objective
The objective of this project is to develop a standalone optimization tool that can be easily integrated into openmdao at a later stage. 
This tool will be entirely written in python making it compatible with windows and linux. 

## Why this tool?
This tool overcomes some of the limitations of gradient based optimization where F(x) = y. This isn't always true for all simulations. If you need gradient optimization then go with OpenMDAO.

## Summary
Many optimization packages seem like a compile of past tools written in other langauges, they lack unverisal features described above that can make big data really happen at Glenn

## Project Goals and Tasks
Key Features
*  Keeps track of each evaluation
*  Restarts from an Population folder
*  Execution timeouts (If CFD is stuck, kill it, but keep eval folder)
*  Exports all results to tecplot, csv 
*  Track performance parameters
*  Apply constraints to performance parameters 
Future Features
*  Addition of other optimization algorithms
*  Incorporate metamodels (machine learning, kriging) directly in the optimization


# Tutorials
[Multi-Objective Kursawe Function](https://colab.research.google.com/github/nasa/GlennOPT/blob/main/test/KUR/multi_objective_example.ipynb)

[Single Objective Rosenbrock](https://colab.research.google.com/github/nasa/GlennOPT/blob/main/test/Rosenbrock/RosenbrockExample.ipynb)

[Multi and Single Objective Probe Placement](https://colab.research.google.com/github/nasa/GlennOPT/blob/main/test/ProbePlacement_multi/ProbePlacementExample.ipynb)

# Contributors 
|                   | Position      | Dates            | Responsibility                      |   |
|-------------------|---------------|------------------|-------------------------------------|---|
| Justin Rush       | LERCIP Intern | Fall 2020        | CCD DOE                             |   |
| Nadeem Kever      | LERCIP Intern | Summer-Fall 2020 | Single Objective DE                 |   |
| Trey Harrison     | Researcher    | Fall 2020-       | Probe Optimization test case        |   |
| Paht Juangphanich | Researcher    | Spring 2020-     | Architect                           |   |


"
53,nasa/cmr-metadata-review,Ruby,"# CMR Metadata Review
The CMR Metadata Review tool is used to curate NASA EOSDIS collection and granule level metadata in CMR for correctness, completeness and consistency.

## Synopsis

The CMR Metadata Review tool is designed to aid in the process of performing quality checks on metadata records in the Common Metadata Repository (CMR). A series of initial automated metadata quality checks are performed when ingesting a selected record into the tool, after which the record is available for manual review. Metadata reviewers perform manual quality checks in the tool, with the option to flag identified issues as high (red), medium (yellow), or low (blue) priority. Additionally, the tool allows reviewers to leave specific recommendations on how to resolve identified issues. Metrics are tracked for all of the data entered into the tool (e.g. number of high, medium, and low priority issues flagged) allowing for the generation of reports on the fly. Detailed reports can be generated for individual record reviews, which summarize the number of red, yellow and blue flagged elements along with recommendations made on how to resolve identified issues. Higher level reports, summarizing metrics across all records in the tool or from a certain data provider, can also be generated. Reports can be shared directly in the tool with other users.

## Motivation

This tool was specifically developed for the Analysis and Review of CMR (ARC) Team, which is responsible for conducting quality evaluations of NASA's metadata records in the Common Metadata Repository (CMR). The overarching goal of this project is to ensure that NASA data in the CMR is discoverable and accessible, and that the user experience is consistent when exploring data holdings across NASA's multiple data centers. Achieving this goal involves evaluating metadata for completeness, correctness, and consistency; a process largely supported by this tool.

## Development Environment Set Up

This can be found (with pictures) [here](https://wiki.earthdata.nasa.gov/display/CMRARC/Dev+Environment+Set+Up).

## Setting Up Local Development Project

All development work is done on local copies of the code base.  Follow the steps below to prepare a local copy on your machine.

1. Install Ruby on your machine.  The production version of the project uses Ruby v 2.7.2. Other versions may work, but only 2.7.2 is currently fully supported.

- You can check which version of ruby you are running with the command `ruby -v`, and the location with `which ruby`

- If the version of ruby that comes up is not 2.7.2 and you are using rvm use the command `rvm list` to list which ruby versions you have. To set the ruby version for the current directory use the command `rvm use 2.7.2`, or to set your default use `rvm --default use 2.7.2`

2. Download the source code from this github repo.

3. The user will then need to setup a local Postgres DB to connect to the app. _\*NOTE: make sure to install Postgres 9.6, because 10.\* is known to cause issues with running tests._

- Macs come pre-installed with PSQL (and a common way to manage it is with [PGAdmin](https://www.pgadmin.org/download/).

- _From the command line/Terminal_ - enter the psql command line with the command `psql`

- You will then be prompted for your password

- If you run into the issue of `password authentication failed for user` ‚Äì try the command `psql -U postgres -h localhost` and enter in the password for your computer.  If that command does not work, you may find it easier to alter the pg_hba.conf file.  The location of this file varies based on where the postgres is trying to run from.  It should be located in the data directory.  Using `ps aux | grep postgres` should list the active processes, one of which should contain something vaguely like: ' /usr/local/opt/postgresql@9.6/bin/postgres -D /usr/local/var/postgresql@9.6'.  The location after the -D is the data directory for the instance that is currently running.  After you alter that file, you will need to restart postgres for the changes to take effect.  

- If you run into the issue of `psql: FATAL: database ""<user>"" does not exist` - see [this link](https://stackoverflow.com/questions/17633422/psql-fatal-database-user-does-not-exist)

- Some people may find they need to create an initial database to connect to with `/usr/local/opt/postgresql@9.6/bin/createdb -h localhost`

4. From the PSQL console, you will need to add a `SUPERUSER` matching the username values shown in `config/database.yml` within the project repo. The development & test DB's both are in local Postgres. There are many ways to create a postgres db and superuser.  These links can help ‚Äì  [here](https://launchschool.com/blog/how-to-install-postgresql-on-a-mac) and [here](https://www.postgresql.org/docs/9.1/static/sql-createrole.html).

- With user information from `config/database.yml`, the commands will look like: `CREATE ROLE <user> WITH SUPERUSER PASSWORD '<password>';`

- Create both the development and test users

- To see if you have a successfully created a user, use the command `\du` in the psql console

- If you have error connecting with the two  accounts, run the following command to grant login right: `ALTER ROLE ""<user>"" WITH LOGIN;`

- After you have finished creating the users, exit out of the psql console with the command `\q`

5. Start the Postgres server on your machine. This command will depend on how you installed.

6. The user should then navigate into the project directory and run the 'bundle install' command to install all needed dependencies (you might need to do the command `gem install bundler` to get the bundler gem). Sometimes there are build conflicts on the local machine during this process.  Most solutions can be found on stack overflow. If you encounter any bundle install failures, please post the error notice and solution here so they can be updated in the source directory.

	1. If Puma is problematic (as observed on the Mac OS 10.13) , try the following: `gem install puma -v '3.6.2' -- --with-opt-dir=/usr/local/opt/openssl`.

7. Once installation of gems is complete, to create the Database the user should run the commands `rake db:create`, `rake db:migrate`, and rake `db:seed` in that order. These commands will create the db required to run the project.

- The `seeds.rb` file currently will only seed some user data required for testing.

8. To ensure that you've created the proper databases ‚Äì you can go back into the psql console and use the command `\l`

9. An application.yml file will be needed before starting the server.  This file can only be obtained from a teammate on the project, it does not reside on the repo.  Once received, copy the file into your config folder. The application.yml is set to be ignored by git.  However, if somehow you accidentally commit the file or send the file to the cloud repo, let someone on the team know immediately.  All env variables will need to be reset across platforms to ensure safety of the system.

10. The last piece of software that needs to be installed is python. The production version uses 2.7. Using pip you'll need to install the ""requests"" package, e.g,:
/usr/bin/pip install requests

11. The application uses react:rails which has a dependency on having [yarn](https://tecadmin.net/install-yarn-macos/) installed on your system.  Yarn is a package manager used to install Javascript dependencies needed by the frontend.   To install these dependencies, issue the command: `yarn install` from the root directory of the application.
    
11. Now the project should be ready to go. Navigate to the home directory and execute `rails s` to start the server.

12. The homepage will be served at the address `localhost:3000` in your browser. To use the tool locally you will need to:

- Register for an [Earthdata Login account](https://sit.urs.earthdata.nasa.gov/) for the SIT environment.

- Request ACL permissions to access the tool in the SIT environment by emailing [Earthdata Support](mailto:support@earthdata.nasa.gov). In order to Ingest collections into the tool, you may need Admin or Arc Curator permissions as well..  Also see section below entitles ""Authentication""

## Quick Start Guide - Installation on Mac

## Changing Python to default to 2.7
    echo ‚Äòexport PATH=‚Äú/System/Library/Frameworks/Python.framework/Versions/2.7/bin/:$PATH‚Äù‚Äô >> ~/.zshrc
    source ~/.zshrc
    
## Installing pip (if necessary)
    curl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py
    python get-pip.py --user
    pip install requests

## Installing rvm
    \curl -sSL https://get.rvm.io | bash
    rvm install 2.7.2
    rvm use 2.7.2 --default 

### Installing Postgresql 9.6
    brew install postgresql@9.6
    echo 'export PATH=""/usr/local/opt/postgresql@9.6/bin:$PATH""' >> ~/.zshrc
    createuser -s postgres
    createuser -s cmrdash
    createuser -s cmrdash_test

### Installing source code
    git clone https://[userid]@github.com/nasa/cmr-metadata-review.git
    cd cmr-metadata-review
    rake db:create:all
    rake db:migrate
    cp ~/application.yml config (yml is supplied by another developer)

### Install Gems
    gem install bundle
    bundle install

### Install reactjs deps
    brew install yarn
    yarn install

### Startup the server
    rails s

## Authentication
The application uses [devise](https://github.com/heartcombo/devise) which is a flexible authentication solution for 
Rails.  The authorization mechanism uses [omniauth](https://github.com/omniauth/omniauth) which is a library/module that standardizes 
multi-provider authentication for web applications.  Any developer can create strategies for OmniAuth that can 
authenticate users via disparate systems.  

One strategy, called [Developer](https://github.com/omniauth/omniauth/blob/master/lib/omniauth/strategies/developer.rb), is included with OmniAuth and provides a completely 
insecure, non-production-usable strategy that directly prompts an user for authentication information and then 
passes it straight through.    If the server is run in ""development"" mode, you will see a button called ""Login with Developer"",
you can click that, provide your name and email, and it will log you in using the information you provide.  It will bypass all
typical authorizations you would get with production and assign you the role of `admin`.

The second strategy, called [URS](https://git.earthdata.nasa.gov/projects/CMRARC/repos/omniauth-edl/browse), provides 
a production-usable strategy that directly authenticates with Earth Data Login.   You can register for a account for 
[SIT](https://sit.urs.earthdata.nasa.gov/home), [UAT](https://uat.urs.earthdata.nasa.gov/home), and [PROD](https://urs.earthdata.nasa.gov/home).  Please note, SIT and UAT both require PIV access.   If you want to configure authentication with the production, you'll need to modify these 2 lines 
in `cmr-metadata-review/config/environments/development.rb`:

    config.cmr_base_url = 'https://cmr.earthdata.nasa.gov'
    config.kms_base_url = 'https://gcmd.earthdata.nasa.gov'

This will tell the authorization mechanism to access production CMR for access control rather than SIT. 

## Importing Database 

    psql -U postgres -c 'drop database cmrdash'
    psql -U postgres -c 'create database cmrdash'
    psql --user cmrdash cmrdash < sit-db-dump-2018-09-27.sql
(Note you may see a bunch of ERROR like role ""arcdashadmin"" does not exist, this can be ignored)

After the import is complete, you'll need to run `db:migrate` again.

Additional note: If you login and view the dashboard home page (with all the sections) and the sections are empty, check the server logs, if you see lots of:

    Record without DAAC: #<Record id: 4489, recordable_id: 1891, recordable_type: ""Collection"", revision_id: ""30"", closed_date: nil, format: ""echo10"", state: ""in_daac_review"", associated_granule_value: nil, copy_recommendations_note: nil, released_to_daac_date: ""2021-03-27 17:00:12"", daac: nil, campaign: [], native_format: nil>

It means one of the migration scripts did not run properly (not entirely sure why), but if you
redo the migration for that one script, it will work:

    rake db:migrate:redo VERSION=20200227150658

## Other Known Issues

During the RAILS 5.2 upgrade, there was an issue with the CSRF authenticity tokens.   Namely, this specific
workflow:  if a user clicks See Review Details, then clicks Curation Home, then clicks Revert Record,
they will get a Invalid Authenticity Token.   Workaround is to tell form_with it should not auto include the
token, rather we should explicitly include it ourselves. i.e.,
`<%= hidden_field_tag :authenticity_token, form_authenticity_token %>`
Note: The above work-around is not necessary on GET requests, only POST, PUT, and DELETE.

See [https://bugs.earthdata.nasa.gov/browse/CMRARC-484] and [https://github.com/rails/rails/issues/24257]
for more details.
"
54,nasa/concept-tagging-training,Python,"# Concept Tagging Training

This software enables the creation of concept classifiers, to be utilized by an 
accompanying [service](https://github.com/nasa/concept-tagging-api). If you don't have your own data to train, you can use the pretrained models <a href=""https://data.nasa.gov/Software/STI-Tagging-Models/jd6d-mr3p"">described here</a>. This project was written about [here](https://strategy.data.gov/proof-points/2019/05/28/improving-data-access-and-data-management-artificial-intelligence-generated-metadata-tags-at-nasa/) for the Federal Data Strategy Incubator Project.

### What is Concept Tagging
By concept tagging, we mean you can supply text, for example:` Volcanic activity, or volcanism, has played a significant role in the geologic evolution of Mars.[2] Scientists have known since the Mariner 9 mission in 1972 that volcanic features cover large portions of the Martian surface.` and get back predicted keywords, like `volcanology, mars surface, and structural properties`, as well as topics like `space sciences, geosciences`, from a standardized list of several thousand NASA concepts with a probability score for each prediction.

## Requirements

You can see a list of options for this project by navigating to the root of the project and executing `make` or `make help`.

This project requires:
* [docker](https://docs.docker.com/install/) -- [tested with this version](docker-versions.txt)
* [GNU Make](https://www.gnu.org/software/make/) -- tested with 3.81 built for i386-apple-darwin11.3.0

## Index:
1. [installation](#installation)
2. [how to run](#how-to-run)
3. [managing experiments](#managing-experiments)
4. [advanced usage](#advanced-usage)

## installation
You have several options for installing and using the pipeline. 
1) [pull existing docker image](#pull-existing-docker-image)
2) [build docker image from source](#build-docker-image-from-source)
3) [install in python virtual environment](#install-in-python-virtual-environment)
 
### pull existing docker image
You can just pull a stable docker image which has already been made:
```bash
docker pull storage.analytics.nasa.gov/abuonomo/concept_trainer:stable
```
In order to do this, you must be on the NASA network and able to connect to the <https://storage.analytics.nasa.gov> docker registry.
\* <sub> There are several versions of the images. You can see them [here](https://storage.analytics.nasa.gov/repository/abuonomo/rat_trainer). 
If you don't use ""stable"", some or all of this guide may not work properly. </sub>


### build docker image from source
To build from source, first clone this repository and go to its root.

Then build the docker image using:
```bash
docker build -t concept_trainer:example .
```
Substitute `concept_trainer:example` for whatever name you would like. Keep this image name in mind. It will be used elsewhere. 

\* If you are actively developing this project, you should look at the `make build` in [Makefile](Makefile). This command automatically tags the image with the current commit url and most recent git tag. The command requires that [setuptools-scm](https://pypi.org/project/setuptools-scm/) is installed.

### install in python virtual environment
\* tested with python3.7
First, clone this repository. 
Then create and activate a virtual environment. For example, using [venv](https://docs.python.org/3/library/venv.html):
```bash
python -m venv my_env
source my_env/bin/activate
```
Next, while in the root of this project, run `make requirements`.


## how to run
The pipeline takes input document metadata structured like [this](data/raw/STI_public_metadata_records_sample100.jsonl) and a config file like [this](config/test_config.yml). The pipeline produces interim data, models, and reports.

1. [using docker](#using-docker) -- if you pulled or built the image
2. [using python in virtual environment](#using-python-in-virtual-environment) -- if you are running in a local virtual environment

### using docker
First, make sure `config`, `data`, `data/raw`, `data/interim`, `models`, and `reports` directories. If they do not exist, make them (`mkdir config data models reports data/raw`). These directories will be used as docker mounted volumes. If you don't make these directories beforehand, they will be created by docker later on, but their permissions will be unnecessarily restrictive.  

Next, make sure you have your input data in the `data/raw/` directory. [Here](data/raw/STI_public_metadata_records_sample100.jsonl) is an example file with the proper structure. You also need to make sure the `subj_mapping.json` file [here](data/interim/subj_mapping.json) is in `data/interim/` directory.

Now, make sure you have a config file in the `config` directory. [Here](config/test_config.yml) is an example config which will work with the above example file.

With these files in place, you can run the full pipeline with this command:
```bash
docker run -it \
     -v $(pwd)/data:/home/data \
     -v $(pwd)/models:/home/models \
     -v $(pwd)/config:/home/config \
     -v $(pwd)/reports:/home/reports \
    concept_trainer:example pipeline \
        EXPERIMENT_NAME=my_test_experiment \
        IN_CORPUS=data/raw/STI_public_metadata_records_sample100.jsonl \
        IN_CONFIG=config/test_config.yml
```
Substitute `concept_trainer:example` with the name of your docker image.
You can set the `EXPERIMENT_NAME` to whatever you prefer.
`IN_CORPUS` and `IN_CONFIG` should be set to the paths to the corpus and to the configuration file, respectively.

\* Developers can also use the `container` command in the [Makefile](Makefile). Note that this command requires [setuptools-scm](https://pypi.org/project/setuptools-scm/). Note that this command will use the image defined by the `IMAGE_NAME` variable and version number equivalent to the most recent git tag. 


### using python in virtual environment

Assuming you have cloned this repository, files for testing the pipeline should be in place. In particular, `data/raw/STI_public_metadata_records_sample100.jsonl` and `config/test_config.yml` should both exist. Additionally, you should add the `src` directory to your `PYTHONPATH`:
```
export PYTHONPATH=$PYTHONPATH:$(pwd)/src/
``` 

Then, you can run a test of the pipeline with: 
```
make pipeline \
    EXPERIMENT_NAME=test \
    IN_CORPUS=data/raw/STI_public_metadata_records_sample100.jsonl \
    IN_CONFIG=config/test_config.yml
```
If you are not using the default values, simply substitute the proper paths for `IN_CORPUS` and `IN_CONFIG`. Choose whatever name you prefer for `EXPERIMENT_NAME`.

## managing experiments

If you have access to the `hq-ocio-ci-bigdata` moderate s3 bucket, you can sync local experiments with those in the s3 bucket.

For example, if you created a local experiment with `EXPERIMENT_NAME=my_cool_experiment`, you can upload your local results to the appropriate place on the s3 bucket with:
```bash
make sync_experiment_to_s3 EXPERIMENT_NAME=my_cool_experiment PROFILE=my_aws_profile
```
where `my_aws_profile` is the name of your awscli profile which has access to the given bucket.

Afterwards, you can download the experiment interim files and results with:
```bash
make sync_experiment_from_s3 EXPERIMENT_NAME=my_cool_experiment PROFILE=my_aws_profile
```
## use full sti metadata records
If you have access to the moderate bucket and you want to work with the full STI metadata records, you can download them to the `data/raw` folder with:
```bash
make sync_raw_data_from_s3 PROFILE=my_aws_profile
``` 
When using these data, you will want to use a config file which is different from the test config file. You can browse previous experiments at `s3://hq-ocio-ci-bigdata/home/DataSquad/classifier_scripts/` to see example config files. You might try:
```yaml
weights:  # assign weights for term types specified in process section
  NOUN: 1
  PROPN: 1
  NOUN_CHUNK: 1
  ENT: 1
  ACRONYM: 1
min_feature_occurrence: 100
max_feature_occurrence: 0.6
min_concept_occurrence: 500
```
See [config/test_config.yml](config/test_config.yml) for details on these parameters.

## advanced usage
For more advanced usage of the project, look at the [Makefile](Makefile) commands and their associated scripts. You can learn more about these python scripts by them with help flags. For example, you can run `python src/make_cat_models.py -h`. 

"
55,nasa/pigans-material-ID,Python,"# Inverse Estimation of Elastic Modulus Using Physics-Informed Generative Adversarial Networks

This repo contains the code and data used to generate the results in the report:
```
J. Warner, J. Cuevas, G. Bomarito, P. Leser, and W. Leser. Inverse Estimation of Elastic Modulus 
Using Physics-Informed Generative Adversarial Networks. NASA/TM-2020-5001300. May 2020.
```

## Dependencies
*  Python 3
*  Tensorflow 2

All additional modules required that can be found in requirements.txt

```python
pip install -r requirements.txt
```

## Usage

### 1) Dataset Generation
The training/testing data used for the report can be found in the `data/` directory, 
so this step is optional. If you would like to create your own data (change the
number of sensors, collocation points, etc. used), see the **generate_pigan.py** script.

### 2) Training

Use the **train_pigan.py** file to train the PIGAN using the generated training data.
To visualize realtime metrics, use Tensorboard. Use the command:
```
 tensorboard --logdir ""data/tensorboard/""
```
and then follow the link provided in Chrome to see the graphs.

### 3) Generating Predicitons and Analyzing Results

To generate samples with the trained PIGAN and assess accuracy versus the testing data, 
use the **evaluate_pigan.py** script. Be sure to specify the location of your trained models, 
or use the models in the paper, which reside in the `data/paper_models/` folder.
This python script will generate data using the model and recreate many of the 
report. After the script runs, the plots will be saved as pdfs in `data/plots/`.


## PI-GAN Architecture

### The Generator

The Generator is composed of two feed forward deep neural networks (DNNs) that generate values for u and E, respectively. <br>
It learns to estimate the probability distribution through 3 seperate loss terms: <br>

**WGAN Loss** is the loss associated with the Discriminator:<br> <br>
<img src=""images/wgan_gen_loss.png"" title=""Equation 7"" alt=""Generator Loss"" width=""auto"" height=""30""/> <br>


**PDE Loss** is found by evaluating the generated samples at collocation points and computing the PDE's residual: <br><br>
<img src=""images/residual.png"" title=""Equation 11"" alt=""Residual"" width=""auto"" height=""30""/> <br> <br>
The residual is then averaged over all samples and collocation points: <br><br>
<img src=""images/loss_pde.png"" title=""Equation 13"" alt=""PDE Loss"" width=""auto"" height=""70""/> <br>

The implementation of PDE loss can be found in [pde.py](pigan/components/pde.py).<br>

**Boundary Condition Loss** is calculated similarly. First generated samples are evaulated at collocation points on the boundary: <br><br>
<img src=""images/boundary.png"" title=""Equation 12"" alt=""Boundary"" width=""auto"" height=""30""/> <br><br>
The residual is then averaged over all samples and boundary collocation points: <br><br>
<img src=""images/loss_bc.png"" title=""Equation 14"" alt=""Boundary Loss"" width=""auto"" height=""70""/> <br><br>

The implementation of Boundary Condition loss can be found [boundary_conditions.py](pigan/components/boundary_conditions.py).

These losses are combined to find the **Total Loss**: (Note: Our results equally weighted all three loss types)  <br><br>
<img src=""images/gen_tot_loss.png"" title=""Equation 15"" alt=""Total Loss"" width=""auto"" height=""40""/> <br>


### The Discriminator
The Discriminator looks at the samples of u being generated and scores them on how close they are to the real expected values of u. <br>

It also enforces the gradient penalty, which encourages the discriminator gradients towards unity:

<img src=""images/wgan_disc_loss.png"" title=""Equation 8"" alt=""Discriminator Loss"" width=""auto"" height=""65""/> 

<br>
See the report for more details on the PIGAN formulation.

## Acknowledgements

* The authors thank Theodore Lewitt (USC) for his help in preparing the code in this repo.

"
56,nasa/openmct-hello,JavaScript,"# Index

Basic starter plugin for Open MCT using webpack.

Includes basic linting rules, jasmine + karma for testing.

# Running

```
npm install

npm start
```
"
57,nasa/cumulus-template-deploy,HCL,"# Cumulus Template Deployment Project
## How To Deploy
Documentation for how to use the Cumulus template project can be read online:

[https://nasa.github.io/cumulus/docs/deployment/deployment-readme](https://nasa.github.io/cumulus/docs/deployment/deployment-readme)"
58,nasa/EMTAT,MATLAB,"# EMTAT
Electrical Modeling and Thermal Analysis Toolbox<br>
<meta name=""keywords"" content=""T-MATS, TMATS, EMTAT, Control System, Numerical Methods, Newton-Raphson, Jacobian Calculation, Propulsion, Aircraft Engine, Jet, Turbofan, Turbojet, Compressor, Turbine, Nozzle, Inlet, open source, simulation, modeling, NASA, electrical, EAP, power system, hybrid, thermodynamics, turbomachinery, MATLAB, Simulink, jet, engine,  etc."">
<b> <a href= ""https://github.com/nasa/EMTAT/releases"" >Click Here</a> for stable release download</b> <br> <br>
<b>Introduction</b> <br>
The Electrical Modeling and Thermal Analysis Toolbox 
is a Simulink toolbox intended for use in the modeling and simulation of electrical 
systems and their controls. EMTAT contains generic electrical 
components that may be combined with a variable input iterative solver and optimization 
algorithm to create complex systems to meet the needs of a developer. Development of this tool
was initiated on behalf of the NASA Transformative Aeronautics Concepts Program (TACP)/
Transformational Tools and Technologies (TTT) projects.
<br><br>
<b>Description</b> <br>
This simulation toolbox has been developed to model the electrical portions of Electrified Aircraft  Propulsion   (EAP)   systems,   including   self-
heating   of   electrical   systems.   Dynamic models of EAP systems  often  include turbomachinery, an electrical  power  system, and a thermal 
management system. These portions of the propulsion system can operate on timescales   covering   six   orders   of   magnitude,   which   poses   a   
challenging   modeling   task. Assumptions and approximations are made in an effort to capture the dynamics relevant to engine,   electrical   
system,   and   thermal   system   interactions,   while   still   allowing   for   fast simulations. These assumptions and approximations enable the electrical 
component models to run at the time scale of the turbomachinery, neglecting the very high frequency electrical dynamics, while demonstrating 
representative behavior of the end-to-end system. Thus the models execute in an acceptable time frame for what would otherwise be an 
extremely stiff simulation.   The   resulting   simulations   of   EAP   systems   are   appropriate   for   system-level control  design and analysis. 
<br><br>
EMTAT is written in MATLAB/Simulink (The Mathworks, Inc.), is open source, 
and is intended for use by industry, government, and academia. All EMTAT equations 
were developed from public sources and all default maps and constants provided in the 
EMTAT software package are nonproprietary and available to the public. The software 
is released under the Apache V2.0 license agreement. 
<br><br>
<b>Getting Started</b> <br>
Stable releases of EMTAT are located under the <a href= ""https://github.com/nasa/EMTAT/releases"" >releases tab</a>. It is encouraged that a user
download the most up to date version using the appropriate software download button (green button). 
Installation instructions are detailed in the user's guide which is included within the package (Documentation/TM-20205008125.pdf). 
<br><br>
EMTAT encourages open collaboration and if a user wishes to become a developer the software 
may be forked at any time via the main page link.
"
59,nasa/XPlaneConnect,C,"# X-Plane Connect
The X-Plane Connect (XPC) Toolbox is an open source research tool used to
interact with the commercial flight simulator software X-Plane. XPC allows users
to control aircraft and receive state information from aircraft simulated in
X-Plane using functions written in C, C++, Java, MATLAB, or Python in real time over the
network. This research tool has been used to visualize flight paths, test control
algorithms, simulate an active airspace, or generate out-the-window visuals for
in-house flight simulation software. Possible applications include active control
of an XPlane simulation, flight visualization, recording states during a flight,
or interacting with a mission over UDP.

### Architecture
XPC includes an X-Plane plugin (xpcPlugin) and clients written in several
languages that interact with the plugin.

#### Quick Start
To get started using X-Plane Connect, do the following.

1. Purchase and install X-Plane 9, 10 or 11.
2. Download the `XPlaneConnect.zip` file from the latest release on the [releases](https://github.com/nasa/XPlaneConnect/releases) page.
3. Copy the contents of the .zip archive to the plugin directory (`[X-Plane Directory]/Resources/plugins/`)
4. Write some code using one of the clients to manipulate X-Plane data.

Each client is located in a top-level directory of the repository named for the
client's language. The client directories generally include a `src` folder
containing the client source code, and an `Examples` folder containing sample
code demonstrating how to use the client.

#### Additional Information
For detailed information about XPC and how to use the XPC clients, refer to the
[XPC Wiki](https://github.com/nasa/XPlaneConnect/wiki).

#### Capabilities
The XPC Toolbox allows the user to manipulate the internal state of X-Plane by
reading and setting DataRefs, a complete list of which can be found on the
[X-Plane SDK wiki](http://www.xsquawkbox.net/xpsdk/docs/DataRefs.html).

In addition, several convenience functions are provided, which allow the user to
efficiently execute common commands. These functions include the ability to set
the position and control surfaces of both player and multiplayer aircraft. In
addition, the pause function allows users to easily pause and un-pause X-Plane's
physics simulation engine.

### Compatibility
XPC has been tested with the following software versions:
* Windows: Vista, 7, & 8
* Mac OSX: 10.8-10.14
* Linux: Tested on Red Hat Enterprise Linux Workstation release 6.6
* X-Plane: 9, 10 & 11

### Contributing
All contributions are welcome! If you are having problems with the plugin, please
open an issue on GitHub or email [Chris Teubert](mailto:christopher.a.teubert@nasa.gov).
If you would like to contribute directly, please feel free to open a pull request
against the ""develop"" branch. Pull requests will be evaluated and integrated into
the next official release.


### Notices
Copyright ¬©2013-2018 United States Government as represented by the Administrator
of the National Aeronautics and Space Administration.  All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY
KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY
WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED
WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY
WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE.
THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT
AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE,
SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT
SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND
DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

#### X-Plane API
Copyright (c) 2008, Sandy Barbour and Ben Supnik  All rights reserved.
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in the
Software without restriction, including without limitation the rights to use,
copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the
Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.
* Neither the names of the authors nor that of X-Plane or Laminar Research may
  be used to endorse or promote products derived from this software without
  specific prior written permission from the authors or Laminar Research,
  respectively.

X-Plane API SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS
IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"
60,nasa/HyperInSPACE,Python,"# Hyperspectral In situ Support for PACE
<html lang=""en"">

<center><img src=""Data/banner2.png"" alt=""Banner""></center>

HyperInSPACE is designed to provide hyperspectral in situ support for the <a href='https://pace.gsfc.nasa.gov/'>PACE mission</a> by processing automated, above-water, hyperspectral ocean color radiometry data using state-of-the-art methods and protocols for quality assurance, uncertainty estimation/propagation, sky/sunglint correction, convolution to satellite wavebands, and ocean color product retrieval. Data output are formatted to text files for submission to the SeaBASS database and saved as comprehensive HDF5 records with automated processing reports. The package is designed to facilitate rigorous, flexible, and transparent data processing for the ocean color remote sensing community, particularly PIs funded by NASA to submit such radiometric data to SeaBASS. Radiometry processed in HyperInSPACE are used for water optical characterization, ocean color product retrieval algorithm development, and orbital platform validation. 

Currently, HyperInSPACE supports <a href='https://www.seabird.com/'>Sea-Bird Scientific</a> HyperSAS packages with and without SolarTracker or pySAS platforms. If you are interested in integrating support for your platform, contact us at the email address below the copyright.

## Version 1.0.7 (see Changelog.md)

---
```
                 NASA Goddard Space Flight Center (GSFC) 
         Software distribution policy for Public Domain Software

 The HyperInSPACE code is in the public domain, available without fee for 
 educational, research, non-commercial and commercial purposes. Users may 
 distribute this code to third parties provided that this statement appears
 on all copies and that no charge is made for such copies.

 NASA GSFC MAKES NO REPRESENTATION ABOUT THE SUITABILITY OF THE SOFTWARE
 FOR ANY PURPOSE. IT IS PROVIDED ""AS IS"" WITHOUT EXPRESS OR IMPLIED
 WARRANTY. NEITHER NASA GSFC NOR THE U.S. GOVERNMENT SHALL BE LIABLE FOR
 ANY DAMAGE SUFFERED BY THE USER OF THIS SOFTWARE.

 Copyright ¬© 2020 United States Government as represented by the Administrator 
 of the National Aeronautics and Space Administration. All Rights Reserved.
```
---
Primary Author: Dirk Aurin, USRA @ NASA Goddard Space Flight Center <dirk.a.aurin@nasa.gov>\
Acknowledgements: N.Vandenberg (PySciDON; https://ieeexplore.ieee.org/abstract/document/8121926)

## Requirements and Installation

Clone this repository (branch: ""master"") to a convenient directory on your computer. When HyperInSPACE/Main.py is launched for the first time, sub-directories will be created and databases downloaded and moved into them as described below. No system files will be changed.

Requires Python 3.X installed on a Linux, MacOS, or Windows computer. The <a href='https://www.anaconda.com/'>Anaconda</a> distribution is encouraged. (If you are unfamiliar with Anaconda, a nice walkthrough can be found here: https://youtu.be/YJC6ldI3hWk. If you are running a minimal, bootstrap distribution such as Miniconda, several additional packages may be needed including: scipy, matplotlib, pyqt5, h5py, fpdf, and requests.)

HDF5 data files will be read and written using the h5py module (2.9.0 at the time of writing) included in a standard Anaconda Python package. The Zhang et al. (2017) sky/sunglint correction also requires Xarray, which requires installation (complete details here: http://xarray.pydata.org/en/stable/installing.html). To install Xarray with Anaconda:
```
prompt$ conda install xarray dask netCDF4 bottleneck
```
 Utilization of <a href='https://gmao.gsfc.nasa.gov/'>GMAO</a> atmospheric models for use in the Zhang et al. (2017) glint correction and filling in environmental conditions not otherwise provided in field logs will require a user account on the <a href='https://earthdata.nasa.gov/'>NASA EARTHDATA</a> server. These are the same credentials used to download satellite imagery from oceancolor.gsfc.nasa.gov. New profiles can be created here: https://urs.earthdata.nasa.gov/users/new (but HyperInSPACE will prompt and link you when run). The Pysolar module (https://anaconda.org/conda-forge/pysolar) must be installed unless the raw data includes solar geometries (e.g. SolarTracker with SATNAV and associated device file as described below). To install Pysolar with Anaconda:
```
prompt$ conda install -c conda-forge pysolar
```
To generate PDF reports for each file processed at Level-2, the package fpdf will need to be installed:
```
prompt$ pip install fpdf
```
The new (v1.0.5) Anomaly Analysis tool requires pyqtgraph (http://www.pyqtgraph.org/) which can be installed with pip:
```
prompt$ pip install pyqtgraph
```
or conda
```
prompt$ conda install -c conda-forge pyqtgraph
```
---
## Launching

<center><img src=""Data/banner.jpg"" alt=""banner""></center>
<!-- ![test](Data/banner.jpg) -->

HyperInSPACE is a Main-View-Controller Python package with a pyQt5 GUI that can be launched in several ways, such as by navigating to the program folder on the command line and typing the following command after the prompt:
```
prompt$ python Main.py
```
However you launch the GUI, *watch for important feedback at the command line terminal* in addition to informational GUI windows.

The following sub-directories will be created automatically (if not present) when you first run the program: 

- Config - Configuration and instrument files (by subdirectory - auto-created), SeaBASS header configuration files, Main view configuration file
- Logs - Most command line output messages generated during processing are captured for later reference in .log text files here
- Plots - A variety of optional plotting routines are included which create name-appropriate sub-directories (i.e. 'L1C_Anoms', 'L1D', 'L1E', 'L2', 'L2_Spectral_Filter'). As with the Data, this path for outputting plots is optional and will be overwritten by choosing an alternate Data/Plots parent directory (see below).
- Data - This directory now comes unpacked in the distribution. By default, it contains only Pope & Fry/Smith & Baker water absorption properties, Thuillier and satellite spectral response functions, banner images for the GUI, and the Zhang glint correction database. This is also the optional fallback location for input and/or output radiometry data, though setting up separate locations for field data is highly recommended (see below).
- Source - This directory (which comes unpacked with the distribution) holds the majority of the Python source code.

(Note: Data, Plots, and Logs directories are not tracked with git.)

### Database download
When you first launch the software, it will need to download a large (2.3 GB) database (Zhang_rho_db.mat) for use in glint correction. If this database is not found in Data, a dialog window will appear before the Main.py GUI with guidance on how to proceed. If this download should fail for any reason, further instructions will be given at the command line terminal where Main.py was launched.

---
## Guide

### Quick Start Overview
1. Identify the research cruise, relevant calibration files, and ancillary data files to be used in processing
2. Launch HyperInSPACE and set up the Main window for data directories and the ancillary file
3. Create a new Configuration (or edit and existing Configuration)
4. Add and enable only *relevant* calibration and instrument files to the Configuration; there is no such thing as a standard instrument package
5. Choose appropriate processing parameters for L1A-L2 (do not depend on software defaults; there is no such thing as a standard data collection)
6. HDF files will be produced at each level of processing, plus optional SeaBASS text files for radiometry at L1E and L2. Plots can be produced at L1D, L1E, and L2. Processing logs and plots are aggregated into PDF Reports at L2 (covering all processing from RAW to L2) written to a dedicated Reports directory in the selected Output directory.


### Main Window

The Main window appears once Main.py is launched. It has options to specify a configuration file, input/output directories, ancillary input files (i.e. environmental conditions and relevant geometries in a SeaBASS file format), single-level processing, and multi-level processing. For batch processing, pop-up windows from ""failed"" (no output due to either corrupt raw binary data or stringent quality control filtering) files can be suppressed. Producing no output file at a given processing level is often a normal result of quality control filtering, so this option allows batches to continue uninterrupted.

The 'New' button allows creation of a new configuration file. 'Edit' allows editing the currently selected configuration file. 'Delete' is used to delete the currently selected configuration file *and* corresponding auto-created calibration directories (see Configuration). After creating a new configuration file, select it from the drop-down menu, and select 'Edit' to launch the Configuration module and GUI. 

The 'Input...' and 'Output Data/Plots Parent Directory' buttons are self explanatory and allow optional selection of data and directories from any mounted/mapped drive. Note that output data and plot sub-directories (e.g. for processing levels) are also auto-created during processing as described below. The parent directory is the directory containing the sub-directories for processing levels (e.g. ""/L1A"", ""/L1B"", etc.) If no input or output data directories are selected, '/Data' and '/Plots' under the HyperInSPACE directory structure will be used by default as the parent directories.

Ancillary data files for environmental conditions and relevant geometries used in L2 processing must be text files in SeaBASS format with columns for date, time, lat, and lon. See https://seabass.gsfc.nasa.gov/ for a description of SeaBASS format. Optional data fields include station number, ship heading, relative sensor azimuth, aerosol optical depth, cloud cover, salinity, water temperature, and wind speed. An example ancillary file is included for use as a template. It is recommended that ancillary files are checked with the 'FCHECK' utility as described on the SeaBASS website. They will be interpreted using the included SB_support.py module from NASA/OBPG. 

In case environmental conditions were not logged in the field, or for filling in gaps in logged data, they will be retrieved from GMAO models as described below. The ancillary data file is optional (though strongly advised for adding wind speed at a minimum) provided the sensor suite is equipped with a SolarTracker or equivalent to supply the relevant sensor/solar geometries. If no SolarTracker-type instrument is present to report the relative sensor/solar geometries, the ancillary file must be provided with at least the ship heading and relative angle between the bow of the ship and the sensor azimuth as a function of time. 

### Configuration

Launch the configuration module and GUI (ConfigWindow.py) from the Main window by selecting/editing a configuration file or creating a new one. This file will be instrument-suite-specific, and is also deployment-specific according to which factory calibration files are needed, as well as how the instrument was configured on the platform or ship. Some cruises (e.g. moving between significantly different water types) may also require multiple configurations to obtain the highest quality ocean color products at Level 2. Sharp gradients in environmental conditions could also warrant multiple configurations for the same cruise (e.g. sharp changes in temperature may effect how data deglitching is parameterized, as described below).

##### Calibration & Instrument Files:
***NOTE: IT IS IMPORTANT THAT THESE INSTRUCTIONS FOR SELECTING AND ACTIVATING CALIBRATION AND INSTRUMENT FILES ARE FOLLOWED CAREFULLY OR PROCESSING WILL FAIL***

**Note: You do not need to move/copy/paste your calibration and instrument files; HyperInSPACE will take care of that for you.**

In the 'Configuration' window, click 'Add Calibration Files' to add the *relevant* calibration or instrument files (date-specific HyperOCR factory calibrations or ancillary instrument Telemetry Definition Files; i.e. the '.cal' and '.tdf' files). *Only add and enable those calibration and instrument files that are relevant to the cruise/package you wish to process (see below).* Each instrument you add here -- be it a radiometer or an external data instrument such as a tilt-heading sensor -- requires at least one .cal or .tdf file for raw binary data to be interpreted. Two .cal files are required in the case of radiometers calibrated seperately for shutter open (light) and shutter closed (dark) calibrations, as is typical with Satlantic/Seabird HyperOCRs. Instruments with no calibrations (e.g. GPS, SolarTracker, etc.) still require a Telemetry Definition File (.tdf) to be properly interpreted. 

Adding new .cal and .tdf files will automatically copy these files from the directory you identify on your machine into the HyperInSPACE directory structure once the Configuration is saved. 

The calibration or instrument file can now be selected using the drop-down menu. Enable (in the neighboring checkbox) only the files that correspond to the data you want to process with this configuration. You will need to know which .cal/.tdf files correspond to each sensor/instrument, and which represent light and dark shutter measurements. For example:

- SATMSG.tdf > SAS Solar Tracker status message string (Frame Type: Not Required)

- SATTHSUUUUA.tdf > Tilt-heading sensor (Frame Type: Not Required) (Note: Use of built-in flux-gate compass is extremely inadviseable on a steel ship or platform. Best practice is to use externally supplied heading data from the ship's NMEA datastream or from a seperate, external dual antenna GPS incorporated into the SolarTracker. DO NOT USE COURSE DATA FROM SINGLE GPS SYSTEMS.)

- SATNAVVVVA.tdf > Solar Tracker (Frame Type: Not Required)

- GPRMC_NMEAWWW.tdf > GPS (Frame Type: Not Required)

- SATPYR.tdf > Pyrometer (Frame Type: Not Required)

- HEDXXXAcal > Es (Frame Type: Dark)

- HSEXXXA.cal > Es (Frame Type: Light)

- HLDYYYA.cal > Li (Frame Type: Dark)

- HSLYYYA.cal > Li (Frame Type: Light)

- HLDZZZA.cal > Lt (Frame Type: Dark)

- HSLZZZA.cal > Lt (Frame Type: Light)

where UUUU, VVV, WWW, XXX, YYY, and ZZZ are the serial numbers of the individual instruments, which are followed where appropriate by factory calibration codes (usually A, B, C, etc. associated with the date of calibration) ***Be sure to choose the factory calibration files appropriate to the date of data collection.***

Selections:  
-Add Calibration Files - Allows loading calibration/instrument files (.cal/.tdf) into HyperInSPACE. Once loaded the drop-down box can be used to select the file to enable the instrument and set the frame type.
-Enabled checkbox - Used to enable/disable loading the file in HyperInSPACE.
-Frame Type - ShutterLight/ShutterDark/Not Required can be selected. This is used to specify shutter frame type (ShutterLight/ShutterDark) for dark correction.

For each calibration file:  
Click 'Enable' to enable the calibration file. Select the frame type used for dark data correction, light data, or 'Not Required' for navigational and ancillary data. in version 1.0.4 (which only fully supports Satlantic/Seabird HyperSAS) each radiometer will require two calibration files (light and dark). Data from the GPS and SATNAV instruments, etc. are interpreted using the corresponding Telemetry Definition Files ('.tdf').

Once you have created your new Configuration, CAL/TDF files are copied from their chosen locations into the /Config directory HyperInSPACE directory structure within an automatically created sub-directory named for the Configuration (i.e. a configuration named ""KORUS"" creates a KORUS.cfg configuration file in /Config and creates the /Config/KORUS_Calibration/directory with the chosen calibration & TDF files.

Level 1A through Level 2 processing configurations are adjusted in the Configuration window. If you are reading this for the first time, the Configuration Window is a good reference to accompany the discussion below regarding processing. *The values set in the configuration file should be considered carefully. They will depend on your viewing geometry and desired quality control thresholds. Do not use default values without consideration.* Level 1d includes a module that can be launched from the Configuration window to assist with data deglitching parameter selection ('Anomaly Analysis'). Spectral filters are also plotted in L2 to help with filter parameterization factors. More details with citations and default setting descriptions are given below. A separate module to assist in the creation of SeaBASS output files is launched in Level 1E processing, and applied to L1E and L2 SeaBASS output as described below.

Click 'Save/Close' or 'Save As' to save the configuration file. As of version 1.0.4, SeaBASS headers will be updated automatically to reflect your selection in the Configuration window. The configuration file is saved to the /Config directory under the HyperInSPACE main directory with a .cfg extension.

### Processing Overview

It will be helpful to set your 'Input Data Parent' and 'Output Data Parent' directories from the Main window. As an example, one could use a cruise directory containing RAW HyperSAS data as the Input Parent Directory, and then create another directory to use as the Output Parent Directory when processing from L0 (raw binary). Files will be automatically sorted by processing level in the automatically created sub-directories (i.e. the software automatically creates and looks for L1A, L1B, L1C, L1D, L1E, and L2 directories under the parent directory). If not selected, the Input/Output parent directories will default to the /Data directory within HyperInSPACE. Your Main window set-up (including configuration file, Input/Output directories, and Ancillary File) will be saved in Config/main.config using the Save button or upon closing the Main window, and reopened the next time you launch Main.py.

Process the data by clicking on one of the buttons for single-level or multi-level processing. A file selection dialogue will appear. Multiple data files can be processed together (successively) by selecting them together in the GUI (e.g. Shift- or Ctrl- click, or Ctrl-A for all, depending on your platform). Input files will be checked for match to expected input level (e.g. L1A file input for for L1B processing). Multi-level processing works the same as single-level by processing each input raw file through all levels before moving on to the next raw file. However, it will only continue with a given file if the preceding level was created immediately (within 1 minute) prior. In other words, if -- due to changes in QA/QC parameterization -- a file is entirely discarded at a given level, but an old file of the same name still exists in that directory, it will be ignored, and processing for that file will be terminated for higher levels. 


*Bug: Very rarely, when running the program for the first time, the first RAW binary data file opened for processing is not read in properly. Processing will fail with the error message: [filename] does not match expected input level for outputing L2. The file will process properly if run a second time (assuming it is a healthy file). Cause unknown.*

#### Level 1A - Preprocessing

Process data from raw binary (Satlantic HyperSAS '.RAW' collections) to L1A (Hierarchical Data Format 5 '.hdf'). Calibration files and the RawFileReader.py script allow for interpretation of raw data fields, which are read into HDF objects.

**Solar Zenith Angle Filter**: prescreens data for high SZA (low solar elevation) to exclude files which may have been collected post-dusk or pre-dawn from further processing. *Triggering the SZA threshold will skip the entire file, not just samples within the file, so do not be overly conservative with this selection, particularly for files collected over a long period.* Further screening for SZA min/max at a sample level is available in L2 processing. This option is currently only applied when using Satlantic SolarTracker (SATNAV) raw data; it is available again for all platforms at L2.
**Default: 60 degrees (e.g. Brewin et al., 2016)**

#### Level 1B

Process data from L1A to L1B. Factory calibrations are applied and data arranged in a standard HDF5 format.

L1B Format:
*Data being processed from non-HyperSAS SOLARTRACKER systems should be formatted in HDF5 to match L1B in order to be successfully processed to L2 in HyperInSPACE. HyperSAS data with no SolarTracker can be processed from raw data, provided the relative geometries are included in the ancillary data file as described above and below.* An example of an L1B HDF file is provided in /Data for reference. Datasets are grouped by instrument and contain their data in an array referenced '.data'. For example, latitude data is stored in the group 'GPS' under 'LATPOS.data'. Data should also contain 'dtype', a list of column headers strings (e.g. the wavelength of sample '304.37', or for many instruments simply 'NONE' if the data is already well described in the group name; see example file). The following datasets and attributes and groups are generally required:

Root level attributes:

- 'CAL_FILE_NAMES' - list of TDF and CAL file names
- 'ES'/'LI'/'LT_UNITS' - calibrated data units for Es, Li, and Lt
- 'FILE_CREATION_TIME' - DD-MMM-YYYY HH:MM:SS in UTC
- 'PROCESSING_LEVEL' - '1b'
- 'WAVELENGTH UNITS' - 'nm'

Group id: 'GPS', attributes: 'CalFileName' (TDF file), 'Frametag' ('$GPRMC' or '$GPGGA'), 'InstrumentType' ('GPS')

datasets: (where y is the number of samples in the file)

- 'LATPOS' - y-length vector of latitude, DDMM.MM (D-Degree(positive), M-Decimal minute)
- 'LATHEMI' - y-length vector of latitude hemisphere 'N' or 'S'
- 'LONPOS' - y-length vector of longitude, DDDMM.MM (D-Degree(positive) , M-Decimal minute)
- 'LONHEMI' - y-length vector of longitude hemisphere 'W' or 'E'
- 'UTCPOS' - y-length vector of UTC, HHMMSS.SS (no leading zeroes, e.g. 12:01:00AM is 100.0)
- Several optional datasets including, 'COURSE', 'SPEED', etc.

Group id: 'ES_DARK', attributes: 'CalFileName' (CAL file), 'Frametag' ('SAT'+instrument code+serial number), 'FrameType' ('ShutterDark'), 'InstrumentType' ('Reference')
        
datasets: 

- 'DATETAG'* - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2'* - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'ES' - Array x by y, where x is waveband, y is series. Calibrated Es dark shutter data

*Note that HDF5 does not support Python datetime, so datetime is calculated for use at each level using Datetag and Timetag2.

Group id: 'ES_LIGHT', attributes: 'CalFileName' (CAL file), 'Frametag' ('SAT'+instrument code+serial number), 'FrameType' ('ShutterLight'), 'InstrumentType' ('Reference')
        
datasets: 

- 'DATETAG' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'ES' - Array x by y, where x is waveband, y is series. Calibrated Es light shutter data

Group id: 'LI_DARK', attributes: 'CalFileName' (CAL file), 'Frametag' ('SAT'+instrument code+serial number), 'FrameType' ('ShutterDark'), 'InstrumentType' ('SAS')
        
datasets: 

- 'DATETAG' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'LI' - Array x by y, where x is waveband, y is series. Calibrated Li dark shutter data

Group id: 'LI_LIGHT', attributes: 'CalFileName' (CAL file), 'Frametag' ('SAT'+instrument code+serial number), 'FrameType' ('ShutterLight'), 'InstrumentType' ('SAS')
        
datasets: 

- 'DATETAG' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'LI' - Array x by y, where x is waveband, y is series. Calibrated Li light shutter data

Group id: 'LT_DARK', attributes: 'CalFileName' (CAL file), 'Frametag' ('SAT'+instrument code+serial number), 'FrameType' ('ShutterDark'), 'InstrumentType' ('SAS')

datasets: 

- 'DATETAG' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'LT' - Array x by y, where x is waveband, y is series. Calibrated Lt dark shutter data

Group id: 'LT_LIGHT', attributes: 'CalFileName' (CAL file), 'Frametag' ('SAT'+instrument code+serial number), 'FrameType' ('ShutterLight'), 'InstrumentType' ('SAS')

datasets: 

- 'DATETAG' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'LT' - Array x by y, where x is waveband, y is series. Calibrated Lt light shutter data

Group id: 'SOLARTRACKER', attributes: 'CalFileName' (CAL file), 'Frametag' ('SATNAV'+instrument code+serial number), 'FrameType' ('SATNAVNNNN'), 'InstrumentType' ('SAS')

datasets:

- 'DATETAG' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year
- 'TIMETAG2' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)
- 'AZIMUTH' - Vector of length y, Solar Azimuth Angle (dtype 'SUN')
- 'ELEVATION' - Vector of length y, Solar Elevation (dtype 'SUN')
- 'HEADING' - Array 2 by y, True Sensor Azimuth (dtype 'SAS_TRUE'), Ship Heading (dtype 'SHIP_TRUE', optional)
- 'PITCH' - Vector of lenght y, Ship pitch
- 'ROLL' - Vector of lenght y, Ship roll


#### Level 1C

Process data from L1B to L1C. Data are filtered for vessel attitude (pitch, roll, and yaw when available), viewing and solar geometry. *It should be noted that viewing geometry should conform to total radiance (Lt) measured at about 40 degrees from nadir, and sky radiance (Li) at about 40 degrees from zenith* **(Mobley 1999, Mueller et al. 2003 (NASA Protocols))**. Unlike other approaches, HyperInSPACE eliminates data flagged for problematic pitch/roll, yaw, and solar/sensor geometries *prior* to deglitching the time series (L1D), thus increasing the relative sensitivity of deglitching for the removal of non-environmental anomalies.

**SolarTracker**: Select when using the Satlantic SolarTracker package. In this case sensor and solor geometry data will come from the SolarTracker (i.e. SATNAV**.tdf). If deselected, solar geometries will be calculated from GPS time and position with Pysolar, while sensor azimuth (i.e. ship heading and sensor offset) must either be provided in the ancillary data or (eventually) from other data inputs. Currently, if SolarTracker is unchecked, the Ancillary file chosen in the Main Window will be read in, subset for the relevant dates/times, held in the ANCILLARY_NOTRACKER group object, and carried forward to subsequent levels (i.e. the file will not need to be read in again at L2). If the ancillary data file is very large (e.g. for a whole cruise at high temporal resolution), this process of reading in the text file and subsetting it to the radiometry file can be slow.

**Rotator Home Angle Offset**: Generally 0. This is the offset between the neutral position of the radiometer suite and the bow of the ship. This *should* be zero if the SAS Home Direction was set at the time of data collection in the SolarTracker as per Satlantic SAT-DN-635. If no SolarTracker was used, the offset can be set here if stable (e.g. pointing angle on a fixed tower), or in the ancillary data file if changeable in time. Without SolarTracker, L1C processing will require at a minimum ship heading data in the ancillary file. Then the offset can be given in the ancillary file (dynamic) or set here in the GUI (static). *Note: as SeaBASS does not have a field for this angle between the instrument and the bow of the ship, the field ""relaz"" (normally reserved for the relative azimuth between the instrument and the sun) is utilized for the angle between the ship heading (NOT COG) and the sensor.*

**Rotator Delay**: Seconds of data discarded after a SolarTracker rotation is detected. Set to 0 to ignore. Not an option without SolarTracker.
**Default: 60 seconds (Vandenberg 2017)**

**Pitch & Roll Filter** (optional): Data outside these thresholds are discarded if this is enabled in the checkbox. Not currently an option without SolarTracker. 
*{To Do: see what other accelerometer data are being collected and accommodate.}*
**Default: 5 degrees (IOCCG Draft Protocols; Zibordi et al. 2019; 2 deg ""ideal"" to 5 deg ""upper limit"")**

**Absolute Rotator Angle Filter** (optional): Angles relative to the SolarTracker neutral angle beyond which data will be excluded due to obstructions blocking the field of view. These are generally set in the SolarTracker software when initialized for a given platform. Not an option without SolarTracker.
**Default: -40 to +40 (arbitrary)**

**Relative Solar Azimuth Filter** (optional): Relative azimuth angle in degrees between the viewing Li/Lt and the sun.  
**Default: 90-135 deg (Mobley 1999, Zhang et al. 2017); 135 deg (Mueller 2003 (NASA Protocols)); 90 deg unless certain of platform shadow (Zibordi et al. 2009, IOCCG Draft Protocols)**


#### Level 1D

Process data from L1C to L1D. Light and dark data are screened for electronic noise (""deglitched"" - see Anomaly Analysis), which is then removed from the data (optional, but strongly advised). Shutter dark samples are then subtracted from shutter light frames after dark data have been interpolated in time to match light data. 
**(e.g. Brewin et al. 2016, Sea-Bird/Satlantic 2017)**

Resulting dark-corrected spectra for Es, Li, and Lt can optionally be plotted, and will be carried forward into the PDF processing report produced at L2.

*{To Do: Allow provisions for above water radiometer set-ups that do not have a dark shutter correction.}*
*{To Do: discard dark data outside thresholds as hinted at in Zibordi 2009 (no actual threshold or methods given)}*
*Currently, spectra with anomalies in any band are deleted in their entirety, but this might be overkill -- certainly it is very conservative. It may be sufficient to set the anomalous values to NaNs, and only delete the entire spectrum if more than, say, 25% of wavebands are anomalous. TBD.*

##### Anomaly Analysis (optional)

Deglitching the data (which must follow after L1C processing, as it is evaluated on L1C files) is highly sensitive to the deglitching parameters described below, as well as some environmental conditions not controlled for in L1C and the variability of the radiometric data itself. Therefore, a separate module was developed to tune these parameters for individual files, instruments, and/or field campaigns and conditions. A sharp temperature change, shutter malfunction, or heavy vibration, for example, could impact ""glitchy-ness"" and change the optimal deglitching parameterization. 

Due to high HyperOCR noise in the NIR, deglitching is currently hard-coded to only perform deglichting between 350 - 850 nm. Deglitching is conservative: i.e. if a value in any waveband within a timeseries is flagged, all data for that timestamp are removed.

The tool is launched pressing the Anomaly Analysis button in the Configuration Window. A dialog will appear to select an L1C file for deglitching, after which a GUI will display timeseries plots of the light (shutter open) and dark (shutter closed) data for a given waveband. Metadata including date, time, wind, cloud, waves, solar and sensor geometry are shown in the top of the window. In addition, the software allows the user to define the file naming scheme of photographs collected in the field, presuming they are named with date and time. The software will look in a directory called /Photos in the designated input directory structure and match all photos within 90 minutes of the mean collection time for the file. Matched photos can by scanned using the button on the right to launch the viewer. The slider below the metadata allows for adjustment of the wavelength to be screened (the Update button will update the figures for any changes in sensor or parameterization), and radio buttons allow selection between Es, Li, or Lt sensors. Sensors can be parameterized independently of each other, and seperately for the light and dark signals. Plots are interactive and can be explored in higher detail by panning with the left mouse button or zooming with the right mouse button (a small ""A"" box in the bottom left of the plot restores it to all data, or right-click for more options).

For each waveband of each sensor, and for both light and dark shutter measurements, the time series of radiometric data are low-pass filtered with a moving average over time using discrete linear convolution of two one dimensional sequences with adjustable window sizes (number of samples in the window). For darks, a *STATIONARY** standard deviation anomaly (from the moving average in time) is used to assess whether data are within an adjustable ""sigma factor"" multiplier within the window. For lights, a *MOVING* standard deviation anomaly (from the moving average of separately adjustable window size) is used to assess whether data are within a separately adjustable sigma. The low-band filter is passed over the data twice. First and last data points for light and dark data cannot be accurately filtered with this method, and are discarded.  

Adjust the window size and sigma parameters for each instrument and hit Update (or keyboard Enter) to see which data (black circles) are retained or discarded (red 'x' or '+' for first and second pass, respectively). Move the slider and hit update to see how these factors impact data in various portions of the spectrum. The field '% Loss (all bands)' shows how application of the current parameterization decimates the entire spectral/temporal dataset for the given sensor.

In addition to the low-pass filters, light and dark data from each sensor can be filtered with a high and low value threshold. These are chosen by selecting the desired band (and hit Set Band) independently for light and dark data, and choosing a minimum and/or maximum threshold value in the appropriate boxes. Leave value as ""None"" if a particularly threshold should be ignored. For example, to filter only Li data on thresholds only for a high threshold for dark data based on 555 nm, select the Threshold checkbox, select the Li Radio button, move the slider to 555 nm, and hit Update. Now, you can enter a value (e.g. 1.0) into the lefthand ""Max"" textbox and hit ""Update"" (or keyboard Enter). The filtered data should show in blue. Keep in mind, they will only show in the waveband for which they were set, but like the low-pass filter, if they fall outside the thresholds in that band, that timestamp will be deleted for all bands.

Currently, to threshold data from any of the three instruments, Threshold must be left checked, but leaving the min/max values as None in the other sensors will still work to ignore thresholding those sensors.

To see the results plotting when reviewing the threshold parameters on a file, make sure the waveband slider is on the appropriate waveband (and hit Update).

Once the parameters have been adjusted for each sensor, they can be saved (Save Sensor Params button) to the current software configuration and to a backup configuration file for later use. This means that once you have 'tuned' these parameters for a given file, the software will be able to load the file (from the Config directory) to reference those parameters. This is useful for reprocessing; *you should only need to tune these once for each file.* If you find that a given set of deglitching parameterizations is working sufficiently well for all your L1C files for a given cruise, simply save them once, save the Configuration from the Configuration Window, and the software configuration will reuse them for all files (i.e. it only applies alternate values for files that were specifically saved). Saved file-specific parameterization can be viewed/editted in the CSV file named after the Configuration in the Config directory (e.g. ""KORUS_anoms.csv"" for the ""KORUS.cfg"").

For record keeping and the PDF processing report, plots of the delitching (similar to those shown in realtime) can be saved to disk. Select the waveband interval at which to save plots (e.g. at 3.3 nm resolution and 20 interval, plots are produced every 66 nm, or 48 PNG files for a typical HyperSAS system), and click Save Anomaly Plots. Results of the anomaly detection are saved to [output_directory]/Plots/L1C_Anoms. Data flagged for removal given the parameterizations chosen in the Configuration window are shown for the filter first pass (red box) and second pass (blue star) and thresholds (red circles only shown in the band for which they were chosen).

For convenience a shortcut to processing the currently active L1C file to L1D is provided (Process to L1D). 

To save the current values from the Anomaly Analysis tool as the defaults for the given cruise, Save Sensor Params > Close > Save/Close the Configuration Window.

*{KNOWN BUG: the pyqtgraph GUI interface does not always update as expected on macOS Catalina. Hitting Update again or switching the sensor radio button generally resolves the issue.}*

**Defaults: TBD; experimental**
**(Abe et al. 2006, Chandola et al. 2009)**  
**(API Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html)**

*{A problem with instrument sensitivity to integration time was recently revealed, and a patch to adjust the response to the integration time is under development. This may be applied here, or alternatively in L1B.}*

#### Level 1E

Process data from L1D to L1E. Interpolates radiometric data to common timestamps and wavebands, optionally generates temporal plots of Li, Lt, and Es, and ancillary data to show how data were interpolated. L1E also optionally outputs text files (see 'SeaBASS File and Header' below) containing the data and metadata for submission to the SeaWiFS Bio-optical Archive and Storage System (SeaBASS; https://seabass.gsfc.nasa.gov/)

Each HyperOCR collects data at unique time intervals and requires interpolation for inter-instrument comparison. Satlantic ProSoft 7.7 software interpolates radiometric data between radiometers using the OCR with the fastest sampling rate (Sea-Bird 2017), but here we use the timestamp of the slowest-sampling radiometer (typically Lt) to minimize perterbations in interpolated data (i.e. interpolated data in HyperInSPACE are always closer in time to actual sampled data) **(Brewin et al. 2016, Vandenberg 2017)**. 

Each HyperOCR radiometer collects data in a unique set of wavebands nominally at 3.3 nm resolution. For merging, they must be interpolated to common wavebands. Interpolating to a different (i.e. lower) spectral resolution is also an option. No extrapolation is calculated (i.e. interpolation is between the global minimum and maximum spectral range for all HyperOCRs). Spectral interpolation is linear by default, but has an option for univariate spline with a smoothing factor of 3 (see ProcessL1e.interpolateL1e in ProcessL1e.py).
**(API: https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html)**

*Note: only the datasets specified in ProcessL1e.py in each group will be interpolated and carried forward. For radiometers, this means that ancillary instrument data such as SPEC_TEMP and THERMAL_RESP will be dropped at L1E and beyond. See ProcessL1e.py at Perform Time Intepolation comment.*

Optional plots of Es, Li, and Lt of L1E data can be generated which show the temporal interpolation for each parameter and each waveband to the slowest sampling radiometer timestamp. They are saved in [output_directory]/Plots/L1E. Plotting is time and memory intensive, and so may not be adviseable for processing an entire cruise.

*{To Do: Allow provision for above water radiometers that operate simultaneously and/or in the exact same wavebands.}*

##### SeaBASS File and Header

To output SeaBASS formatted text files, check the box. A SeaBASS subfolder within the L1E directory will be created, and separate files generated for Li, Lt, and Es hyperspectral data.

An eponymous, linked module allows the user to collect information from the data and the processing configuration (as defined in the Configuration window) into the SeaBASS files and their headers. The module is launched by selecting the 'Edit SeaBASS Header' button in the Configuration window. A SeaBASS header configuration file is automatically stored in the /Config directory with the name of the Configuration and a .hdr extension. Instructions are given at the top of the SeaBASS Header window. Within the SeaBASS Header window, the left column allows the user to input the fields required by SeaBASS. Calibration files (if they have been added at the time of creation) are auto-populated. In the right hand column, the HyperInSPACE parameterizations defined in the Configurations window is shown in the 'Config Comments' box, and can be editted (though this should rarely ever be necessary). Additional comments can be added in the second comments field, and the lower fields are autopopulated from each data file as it is processed. To override auto-population of the lower fields in the right column, enter the desired value here in the SeaBASS Header window.

*{To Do: Populate the left column using values in the Ancillary file, if present.}*

#### Level 2

Process L1E to L2. Further quality control filters are applied to data, and data are then averaged within optional time interval ensembles prior to calculating the remote sensing reflectance within each ensemble. A typical field collection file for the HyperSAS SolarTracker is one hour, and the optimal ensemble periods within that hour will depend on how rapidly conditions and water-types are changing, as well as the instrument sampling rate. While the use of ensembles is optional (set to 0 to avoid averaging), it is highly recommended, as it allows for the statistical analysis required for Percent Lt calculation (radiance acceptance fraction; see below) within each ensemble, rather than %Lt across an entire (e.g. one hour) collection, and it also improves radiometric uncertainty estimation.

Prior to ensemble binning, individual spectra may be filtered out for:
**Lt(NIR)>Lt(UV)**
Spectra with Lt higher in the UV (average from 780-850) than the UV (350-400) are eliminated.
*{Unable to find citiation for the Lt(NIR)> Lt(UV) filter...}*

**Maximum Wind Speed**.  
**Default 7 m/s (IOCCG Draft Protocols 2019; D'Alimonte pers.comm 2019); 10 m/s Mueller et al. 2003 (NASA Protocols); 15 m/s (Zibordi et al. 2009);**

**Solar Zenith Angle** may be filtered for minimum and maximum values.  
**Default Min: 20 deg (Zhang et al 2017); Default Max: 60 deg (Brewin et al. 2016)**

**Spectral Outlier Filter** may be applied to remove noisy data prior to binning. This simple filter examines only the spectra of Es, Li, and Lt from 400 - 700 nm, above which the data are noisy in the HyperOCRs. Using the standard deviation of the normalized spectra for the entire sample ensemble, together with a multiplier to establish an ""envelope"" of acceptable values, spectra with data outside the envelop in any band are rejected. Currently, the arbitrary filter factors are 5.0 for Es, 8.0 for Li, and 3.0 for Lt. Results of spectral filtering are saved as spectral plots in [output_directory]/Plots/L2_Spectral_Filter. The filter can be optimized by studying these plots for various parameterizations of the filter.

**Meteorological flags** based on **(Ruddick et al. 2006, Mobley, 1999, Wernand et al. 2002, Garaba et al. 2012, Vandenberg 2017)** can be optionally applied to screen for undesirable data. Specifically, data are filtered for cloud cover, unusually low downwelling irradiance at **480 nm < default 2.0 uW cm^-2 nm^-1** for data likely to have been collected near dawn or dusk, or **(Es(470)/Es(680) < 1.0**), and for data likely to have high relative humidity or rain (**Es(720)/Es(370) < 1.095**). Cloud screening (**Li(750)/Es(750) >= 0.05**) is optional and not well parameterized. Clear skies are approximately 0.02 (Mobley 1999) and fully overcast are of order 0.3 (Ruddick et al. 2006). However, the Ruddick skyglint correction (below) can partially compensate for clear versus cloudy skies, so to avoid eliminating non-clear skies prior to glint correction, set this high (e.g. 1.0). Further investigation with automated sky photography for cloud cover is warranted.

**Ensembles**
**Extract Cruise Stations** can be selected if station information is provided in the ancillary data file identified in the Main window. If selected, only data collected on station will be processed, and the output data/plot files will have the station number appended to their names. At current writing, stations must be numeric, not string-type. If this option is deselected, all automated data (underway and on station) will be included in the ensemble processing.

**Ensemble Interval** can be set to the user's requirements depending on sampling conditions and instrument rate (**default 300 sec**). Setting this to zero avoids temporal bin-averaging, preserving the common timestamps established in L1E. Processing the data without ensenble averages can be very slow, as the reflectances are calculated for each spectrum collected (i.e. nominally every 3.3 seconds of data for HyperSAS). The ensemble period is used to process the spectra within the lowest percentile of Lt(780) as defined/set below. The ensemble average spectra for Es, Li, and Lt is calculated, as well as variability in spectra within the ensemble, which is used to help estimate sample uncertainty.

**Percent Lt Calculation** Data are optionally limited to the darkest percentile of Lt data at 780 nm within the sampling interval (if binning is performed; otherwise across the entire file) to minimize the effects of surface glitter from capillary waves. The percentile chosen is sensitive to the sampling rate. The 5% default recommended in Hooker et al. 2002 was devised for a multispectral system with rapid sampling rate.
**Default: 5 % (Hooker et al. 2002, Zibordi et al. 2002, Hooker and Morel 2003); <10% (IOCCG Draft Protocols)**.

**Skyglint/Sunglint Correction (rho)**
Use of the Ruddick et al. 2006 and the Zhang et al. 2017 glint corrections require wind data, and Zhang (2017) also requires aerosol optical depth, salinity, and sea surface temperature. Since most field collections of above water radiometry are missing some or all of these anchillary parameters, an embedded function allows the user to download model data from the NASA EARTHDATA server. These data are generated by the NASA Global Modeling and Assimilation Office (GMAO) as hourly, global 'MERRA2' HDF files at 0.5 deg (latitude) by 0.625 deg (longitude) resolution (https://gmao.gsfc.nasa.gov/reanalysis/MERRA-2/). Two files will be downloaded for each hour of data processed (total ~8.3 MB for one hour of field data) and stored in /Data/Anc. Global ancillary data files from GMAO will be reused, so it is not recommended to clear this directory unless updated models are being released by GMAO. Details for how these data are applied to above water radiometry are given below.

As of January 2020, access to these data requires a user login and password, which can be obtained here (https://oceancolor.gsfc.nasa.gov/registration). A link to register is also provided in the Configuration window. When the user selects 'Download Ancillary Models', pop-up windows will allow the user to enter a login and password. Once this has been done once, canceling the login pop-up dialog will force the program to use the current configuration (i.e. it is only necessary to re-enter the password if it has changed.)

The default value for sea-surface reflectance (**Rho_sky**, sometimes called the Fresnel factor) is set by default to 0.0256 based on **(Mobley 1999, Mueller et al. 2003 (NASA Protocols))**, which can be optionally adjusted for wind speed and cloud cover using the relationship found in **(Ruddick et al. 2006)** (i.e. Li(750)/Es(750)< 0.05 for clear skies, Rho_sky = 0.0256 + 0.00039* U + 0.000034* U^2, else Rho_sky = 0.0256). The default wind speed (U) should be set by the user depending on in situ conditions, for instances when the ancillary data and models are not available (more information is given below on how ancillary wind data are applied). This correction does not account for the spectral dependence **(Lee et al. 2010, Gilerson et al. 2018)** or polarization sensitivity **(Harmel et al. 2012, Mobley 2015, Hieronymi 2016, D'Alimonte and Kajiyama 2016, Foster and Gilerson 2016, Gilerson et al. 2018)** in Rho_sky. Uncertainty in rho is estimated from Ruddick et al. 2006 at +/- 0.003.

The third option provided for glint correction is based on **Zhang et al. 2017**. This model explicitly accounts for spectral dependence in rho, separates the glint contribution from the sky and the sun, and accounts for polarization in the skylight term. This approach requires knowledge of environmental conditions during sampling including: wind speed, aerosol optical depth, solar and sensor azimuth and zenith angles, water temperature and salinity. To accomodate these parameters, HyperInSPACE uses either the ancillary data file provided in the main window, GMAO models, or the default values set in the Configuration window as follows: field data ancillary files are screened for wind, water temperature, and salinity. These are each associated with the nearest timestamps of the radiometer suite to within one hour. Radiometer timestamps still lacking wind and aerosol data will extract it from the GMAO models, if available. Otherwise, the default values set in the Configuration window will be used as a last resort.

*{To Do: Work to optimize the processing of Zhang rho. Currently, this proceeds far more slowly in Python than Matlab. See notes in ZhangRho.py for gen_vec_quad and prob_reflection near L165}*
*{To Do: Include additional skylight/skyglint corrections, such as Groetsch et al. 2017/2020}*
*{To Do: Include a bidirectional correction to Lw based on, e.g. Lee 2011, Zibordi 2009 (for starters; ultimately the BRDF will need to be better parameterized for all conditions and water types.)}*
*{To Do: Improve uncertainty estimates (e.g. Zibordi et al. 2009). The uncertainty in rho within the Zhang model is not well constrained.}*

Remote sensing reflectance is calculated as Rrs = (Lt - rho_sky* Li) / Es (e.g. **(Mobley 1999, Mueller et al. 2003, Ruddick et al. 2006)**). Normalized water leaving radiance (nLw) is calculated as Rrs*F0, where F0 is the top of atmosphere incident radiation adjusted for the Earth-Sun distance on the day sampled.

Uncertainties in Li, Lt, and Es are estimated using the standard deviation of spectra in the ensemble or full-file average. Uncertainty in rho is estimated at +/- 0.003 from Ruddick et al. 2006. Uncertainty in Rrs and nLw are estimated using propagation of uncertainties from Li, Lt, Es, and rho assuming random, uncorrelated error.

Additional glint may be removed from the Rrs and nLw by subtracting the value in the NIR from the entire spectrum **(Mueller et al. 2003 (NASA Protocols))**. This approach, however, assumes neglible water-leaving radiance in the 750-800 nm range (not true of turbid waters), and ignores the spectral dependence in sky glint, and **should therefore only be used in the clearest waters and with caution**. Here, a minimum in Rrs(750-800) or nLw(750-800) is found and subtracted from the entire spectrum.

An alternate NIR residual correction can be applied based on **Ruddick et al. 2005, Ruddick et al. 2006**. This utilizes the spectral shape in water leaving reflectances in the NIR to estimate the residual glint correction for turbid waters with NIR reflectances from about 0.0001 to 0.03

Negative reflectances can be removed as follows: any spectrum with any negative reflectances between 380 nm and 700 nm is removed from the record entirely. Negative reflectances outside of this range (e.g. noisy data deeper in the NIR) are set to 0.

Spectral wavebands for a few satellite ocean color sensors can be optionally calculated using their spectral weighting functions. These will be included with the hyperspectral output in the L2 HDF files. Spectral response functions are applied to convolve the (ir)radiances prior to calculating reflectances. **(Burgghoff et al. 2020)**.

Plots of processed L2 data from each radiometer and calculated reflectances can be created and stored in [output_directory]/Plots/L2. Uncertainties are shown for each spectrum as shaded regions, and satellite bands (if selected) are superimposed on the hyperspectral data.

Select the ""Derived L2 Ocean Color Products"" button to choose, calculate, and plot derived biochemical and inherent optical properties using a variety of ocean color algorithms. Algorithms largely mirror those available in SeaDAS with a few additions. They include OC3M, PIC, POC, Kd490, iPAR, GIOP, QAA, and the Average Visible Wavelength (Vandermuellen et al. 2020) and GOCAD-based CDOM/Sg/DOC algorithms (Aurin et al. 2018), as well as the Rrs spectral QA score (Wei et al 2016).

To output SeaBASS formatted text files, check the box. A subfolder within the L2 directory will be created, and separate text files will be made for Li, Lt, Es, and Rrs hyperspectral data and satellite bands, if selected. Set-up for the SeaBASS header is managed with the 'Edit/Update SeaBASS Header' in the L1E configuration.


**PDF Reports**

Upon completion of L2 processing for each file (or lower level if that is the terminal processing level), a PDF summary report will be produced and saved in [output_directory]/Reports. This contains metadata, processing parameters, processing logs, and plots of QA analysis, radiometry, and derived ocean color products. These reports should be used to evaluate the choices made in the configuration and adjust them if necessary.


## References
- Abe, N., B. Zadrozny and J. Langford (2006). Outlier detection by active learning. Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. Philadelphia, PA, USA, Association for Computing Machinery: 504‚Äì509.
- Brewin, R. J. W., G. Dall'Olmo, S. Pardo, V. van Dongen-Vogels and E. S. Boss (2016). ""Underway spectrophotometry along the Atlantic Meridional Transect reveals high performance in satellite chlorophyll retrievals."" Remote Sensing of Environment 183: 82-97.
- Burggraaff, O. (2020). ""Biases from incorrect reflectance convolution."" Optics Express 28(9): 13801-13816.
- Chandola, V., A. Banerjee and V. Kumar (2009). ""Anomaly detection: A survey."" ACM Comput. Surv. 41(3): Article 15.
- D‚ÄôAlimonte, D. and T. Kajiyama (2016). ""Effects of light polarization and waves slope statistics on the reflectance factor of the sea surface."" Optics Express 24(8): 7922-7942.
- Foster, R. and A. Gilerson (2016). ""Polarized transfer functions of the ocean surface for above-surface determination of the vector submarine light field."" Applied Optics 55(33): 9476-9494.
- Garaba, S. P., J. Schulz, M. R. Wernand and O. Zielinski (2012). ""Sunglint Detection for Unmanned and Automated Platforms."" Sensors 12(9): 12545.
- Gilerson, A., C. Carrizo, R. Foster and T. Harmel (2018). ""Variability of the reflectance coefficient of skylight from the ocean surface and its implications to ocean color."" Optics Express 26(8): 9615-9633.
- Harmel, T., A. Gilerson, A. Tonizzo, J. Chowdhary, A. Weidemann, R. Arnone and S. Ahmed (2012). ""Polarization impacts on the water-leaving radiance retrieval from above-water radiometric measurements."" Applied Optics 51(35): 8324-8340.
- Hieronymi, M. (2016). ""Polarized reflectance and transmittance distribution functions of the ocean surface."" Optics Express 24(14): A1045-A1068.
- Hooker, S. B., G. Lazin, G. Zibordi and S. McLean (2002). ""An Evaluation of Above- and In-Water Methods for Determining Water-Leaving Radiances."" Journal of Atmospheric and Oceanic Technology 19(4): 486-515.
- Hooker, S. B. and A. Morel (2003). ""Platform and Environmental Effects on Above-Water Determinations of Water-Leaving Radiances."" Journal of Atmospheric and Oceanic Technology 20(1): 187-205.
- Lee, Z., Y.-H. Ahn, C. Mobley and R. Arnone (2010). ""Removal of surface-reflected light for the measurement of remote-sensing reflectance from an above-surface platform."" Optics Express 18(25): 26313-26324.
- Mobley, C. D. (1999). ""Estimation of the remote-sensing reflectance from above-surface measurements."" Applied Optics 38(36): 7442-7455.
- Mobley, C. D. (2015). ""Polarized reflectance and transmittance properties of windblown sea surfaces."" Applied Optics 54(15): 4828-4849.
- Mueller, J. L., A. Morel, R. Frouin, C. O. Davis, R. Arnone, K. L. Carder, Z. P. Lee, R. G. Steward, S. B. Hooker, C. D. Mobley, S. McLean, B. Holbert, M. Miller, C. Pietras, K. D. Knobelspiesse, G. S. Fargion, J. Porter and K. J. Voss (2003). Ocean Optics Protocols for Satellite Ocean Color Sensor Validation, Revision 4, Volume III. Ocean Optics Protocols for Satellite Ocean Color Sensor Validation. J. L. Mueller. Greenbelt, MD, NASA Goddard Space Flight Center.
- Ruddick, K., V. De Cauwer and B. Van Mol (2005). Use of the near infrared similarity reflectance spectrum for the quality control of remote sensing data, SPIE.
- Ruddick, K. G., V. De Cauwer, Y.-J. Park and G. Moore (2006). ""Seaborne measurements of near infrared water-leaving reflectance: The similarity spectrum for turbid waters."" Limnology and Oceanography 51(2): 1167-1179.
- Vandenberg, N., M. Costa, Y. Coady and T. Agbaje (2017). PySciDON: A python scientific framework for development of ocean network applications. 2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM).
- Wernand, M. R. (2002). GUIDELINES FOR (SHIP BORNE) AUTO-MONITORING
OF COASTAL AND OCEAN COLOR. Ocean Optics XVI. S. Ackleson and C. Trees. Santa Fe, NM, USA.
- Zhang, X., S. He, A. Shabani, P.-W. Zhai and K. Du (2017). ""Spectral sea surface reflectance of skylight."" Optics Express 25(4): A1-A13.
- Zibordi, G., S. B. Hooker, J. F. Berthon and D. D'Alimonte (2002). ""Autonomous Above-Water Radiance Measurements from an Offshore Platform: A Field Assessment Experiment."" Journal of Atmospheric and Oceanic Technology 19(5): 808-819.
- Zibordi, G., F. M√©lin, J.-F. Berthon, B. Holben, I. Slutsker, D. Giles, D. D‚ÄôAlimonte, D. Vandemark, H. Feng, G. Schuster, B. E. Fabbri, S. Kaitala and J. Sepp√§l√§ (2009). ""AERONET-OC: A Network for the Validation of Ocean Color Primary Products."" Journal of Atmospheric and Oceanic Technology 26(8): 1634-1651.
- Zibordi, G., K. J. Voss, B. Johnson and J. L. Mueller (2019). Protocols for Satellite Ocean Colour Data Validation: In Situ Optical Radiometry. IOCCG Ocean Optics and Biogeochemistry Protocols for Satellite Ocean Colour Sensor Validation. IOCCG. Dartmouth, NS, Canada, IOCCG.
"
61,nasa/ominas,IDL,"## OMINAS:  Open-source Multiple INstrument Analysis Software

See the full OMINAS documentation here:  https://nasa.github.io/ominas/guides/userguide.html


###Current release: v1.0.5


https://github.com/nasa/ominas/releases/latest
"
62,nasa/JHU-PIV-data,HTML,"# JHU-PIV-data

## REMEMBER: Pull Remote Repo changes frequently using this command:
```bash
git pull
```

## Upload changes to this repo

1. Change directories in your Command Prompt or Terminal application
```bash
cd \Users\blucci\Desktop\piv
```
2. See changed files (they should show up as red)
```bash
git status
```
3. Add all locally changed files to the staging area
```bash
git add *
```
4. Commit this set of file changes with a message describing the changes made
```bash
git commit -m ""<a description of the changes>""
```
5. Push the new commits to your remote repository
```bash
git push
```
"
63,nasa/sample_lib,C,"![Static Analysis](https://github.com/nasa/sample_lib/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/sample_lib/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : App : Sample Lib

This repository contains a sample library (sample_lib), which is a framework component of the Core Flight System.

This sample library is a non-flight example library implementation for the cFS Bundle. It is intended to be located in the `apps/sample_lib` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes sample_lib as a submodule), which includes build and execution instructions.

sample_lib implements SAMPLE_Function, as an example for how to build and link a library in cFS.

## Version History

### Development Build: v1.2.0-rc1+dev34

- Replace direct ref to ArgPtr with `UT_Hook_GetArgValueByName` macro. Reading the pointer directly is not advised.
- See <https://github.com/nasa/sample_lib/pull/61> and <https://github.com/nasa/cFS/pull/250>

### Development Build: v1.2.0-rc1+dev30

- Replace <> with "" for local includes
- Adds CONTRIBUTING.md that links to the main cFS contributing guide.
- See <https://github.com/nasa/sample_lib/pull/55>

### Development Build: v1.2.0-rc1+dev24

- Fix #46, simplify build to use wrappers and interface libs
- Fix #48, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/sample_lib/pull/50>

### Development Build: v1.2.0-rc1+dev10

- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing
- See <https://github.com/nasa/sample_lib/pull/38>

### Development Build: v1.2.0-rc1+dev8

- No behavior changes. All identifiers now use the prefix `SAMPLE_LIB_`. Changes the name of the init function from SAMPLE_LibInit to SAMPLE_LIB_Init which affects the CFE startup script.
- Set REVISION to ""99"" to indicate development version status
- See <https://github.com/nasa/sample_lib/pull/35>

### Development Build: v1.2.0-rc1+dev3

- Installs unit test to target directory.
- See <https://github.com/nasa/sample_lib/pull/32>

### Development Build: 1.1.0+dev27

- Install unit test as part of cmake recipe. Sample lib test runner now shows up in expected install directory
- Add build number and baseline to version reporting
- See <https://github.com/nasa/sample_lib/pull/28>

### Development Build: 1.1.4

- Apply code style
- See <https://github.com/nasa/sample_lib/pull/24>

### Development Build: 1.1.3

- Coverage data `make lcov` includes the sample_lib code
- See <https://github.com/nasa/sample_lib/pull/22>

### Development Build: 1.1.2

- Added coverage test and a stub library to facilitate unit test
- See <https://github.com/nasa/sample_lib/pull/16>

### Development Build: 1.1.1

- See <https://github.com/nasa/sample_lib/pull/14>

### ***OFFICIAL RELEASE: 1.1.0 - Aquila***

- Released as part of cFE 6.7.0, Apache 2.0
- See <https://github.com/nasa/sample_lib/pull/6>

### ***OFFICIAL RELEASE: 1.0.0a***

- Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

As a lab library, extensive testing is not performed prior to release and only minimal functionality is included.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.

Official cFS page: http://cfs.gsfc.nasa.gov
"
64,nasa/CFL3D,Fortran,"## CFL3D

CFL3D 
is a structured-grid, cell-centered, upwind-biased, Reynolds-averaged Navier-Stokes (RANS) code. It can be run
in parallel on multiple grid zones with point-matched, patched, overset, or embedded connectivities. Both
multigrid and mesh sequencing are available in time-accurate or steady-state modes.

The most up-to-date information can be found on the web at:

https://cfl3d.larc.nasa.gov

-------------

Copyright 2001 United States Government as represented by the Administrator
of the National Aeronautics and Space Administration. All Rights Reserved.

The CFL3D platform is licensed under the Apache License, Version 2.0 
(the ""License""); you may not use this file except in compliance with the 
License. You may obtain a copy of the License at 
http://www.apache.org/licenses/LICENSE-2.0. 

Unless required by applicable law or agreed to in writing, software 
distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT 
WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the 
License for the specific language governing permissions and limitations 
under the License.
"
65,nasa/edsc-timeline,JavaScript,"# Earthdata Search Components: Timeline

[![npm version](https://badge.fury.io/js/%40edsc%2Ftimeline.svg)](https://badge.fury.io/js/%40edsc%2Ftimeline)
![Build Status](https://github.com/nasa/edsc-timeline/workflows/CI/badge.svg?branch=master)
[![codecov](https://codecov.io/gh/nasa/edsc-timeline/branch/master/graph/badge.svg?token=KQNTU9DTFD)](https://codecov.io/gh/nasa/edsc-timeline)

Try out the [online demo](http://nasa.github.io/edsc-timeline/)

A React plugin implementing a timeline view of data, allowing
time range selection as well as keyboard and touch interaction
For a basic usage example and a testbed for changes,
see `example/src`

The edsc-timeline plugin was developed as a component of
[Earthdata Search](https://github.com/nasa/earthdata-search).

## Installation

    npm install @edsc/timeline

## Usage

After installing you can use the component in your code.

```javascript
import EDSCTimeline from '@edsc/timeline'

const Component = () => {
  const data = [
    {
      id: 'row1',
      title: 'Example title',
      intervals: [
        [
          new Date('2019-08-12').getTime(), // Start of the interval
          new Date('2019-12-20').getTime() // End of the interval
        ],
        [
          new Date('2020-01-04').getTime(),
          new Date('2020-05-20').getTime()
        ]
      ]
    }
  ]

  return (
    <EDSCTimeline
      data={data}
    />
  )
}
```

### Props

| Prop | Type | Required | Default Value | Description
| ---- |:----:|:--------:|:-------------:| -----------
data | array | true | | Array of rows to be displayed on the timeline
center | number | | new Date().getTime() | Center timestamp of the timeline
minZoom | number | | 1 | Minimum zoom level
maxZoom | number | | 5 | Maximum zoom level
zoom | number | | 3 | Active zoom level
temporalRange | object | | {} | Temporal range ({ start, end }) that is displayed on the timeline
focusedInterval | object | | {} | Focused interval ({ start, end }) that is displayed on the timeline
onFocusedSet | function | | | Callback function that returns the focused interval when it is set
onTemporalSet | function | | | Callback function that returns the temporal range when it is set
onTimelineMove | function | | | Callback function called when the timeline is moved
onTimelineMoveEnd | function | | | Callback function called when the timeline is finished moving
onArrowKeyPan | function | | | Callback function called when arrow keys are used to change the focused interval
onButtonPan | function | | | Callback function called when buttons are used to change the focused interval
onButtonZoom | function | | | Callback function called when buttons are used to change the zoom level
onDragPan | function | | | Callback function called when the timeline is panned using dragging
onFocusedIntervalClick | function | | | Callback function called when a focused interval is clicked
onScrollPan | function | | | Callback function called when the mouse wheel is used to pan the timeline
onScrollZoom | function | | | Callback function called when the mouse wheel is used to change the zoom level

### Callback function return value

Every callback function returns this object

```javascript
{
  center,
  focusedEnd,
  focusedStart,
  temporalEnd,
  temporalStart,
  timelineEnd,
  timelineStart,
  zoom
}
```

## Development

To compile:

    npm install

To start the example project for local testing:

    npm start

To run the Jest tests:

    npm test

To run the Cypress tests:

    npm run cypress:run

## Contributing

See CONTRIBUTING.md

## License

> Copyright ¬© 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
>
> Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
> You may obtain a copy of the License at
>
>    http://www.apache.org/licenses/LICENSE-2.0
>
>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
"
66,nasa/ipv6_python,Python,"Advanced IPv6 Socket Manipulation for Python

Description:

This rather simplistic extension module is intended to allow for more advanced
manipulation of IPv6 sockets in Python.  In particular, Python did not have an
easy means to interact with and obtain the flow label of a particular IPv6
socket.  This extension currently allows for flow labels to be enabled on a
socket and a random flow label can be requested from the kernel.  In the future,
additional options may be added to facilitate additional flow label actions.

A collection of tools are also included which can be used to generate test
traffic under strict patterns, similar to how many telemetry systems operate.
See the documentation under tools for more details.

Installation:

This package is installed using distutils.  The most common approach is to run:

  python setup.py install
    or
  python3 setup.py install

Usage:

  import ipv6

You can then pass an IPv6 socket to the function get_flow_label() to
apply a random flow label assigned by the kernel.  The first parameter
is the socket object or the integer file descriptor.  The remaining
parameters are optional depending on the state of your socket.  Here is
an example:

  sockaddr = ipv6.get_flow_label(sock,*sockaddr)

See https://docs.python.org/2/library/socket.html for info on sockaddr

The get_flow_label call returns a new sockaddr structure with the flowinfo
field set to newly created flow label assigned by the kernel.  You can then
send data as normal.

License:

This code is released under the NASA Open Source Agreement version 1.3

Simple Example:

# Send a single UDP packet
import socket
import ipv6
host = ""::1""
port = 3300
data = ""NASA""
resolve = socket.getaddrinfo(host, port, socket.AF_INET6, socket.SOCK_DGRAM)
(family, socktype, proto, _, sockaddr) = resolve[0]
sock = socket.socket(family, socktype, proto)
sockaddr = ipv6.get_flow_label(sock,*sockaddr)
print ""Flow Label:"",hex(sockaddr[2])
sock.sendto(data,sockaddr)

"
67,nasa/cFS-GroundSystem,Python,"![Static Analysis](https://github.com/nasa/cFS-GroundSystem/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/cFS-GroundSystem/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : Tool : Ground System

This repository contains NASA's Lab Ground System (cFS-GroundSystem), which is a framework component of the Core Flight System.

This lab application is a non-flight utility ground system to interact with the cFS. It is intended to be located in the `tools/cFS-GroundSystem` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes cFS-GroundSystem as a submodule), which includes build and execution instructions.

See [Guide-GroundSystem.md](https://github.com/nasa/cFS-GroundSystem/blob/master/Guide-GroundSystem.md) for more information.

## Version History

### Development Build: v2.2.0-rc1+dev46

- Changes executable command from 'startg' to 'cFS-GroundSystem'
- Changes version to be the version stated in version.py
- Adds executable installation instructions to Guide-GroundSystem.md
- See <https://github.com/nasa/cFS-GroundSystem/pull/178> and <https://github.com/nasa/cFS/pull/248>

### Development Build: v2.2.0-rc1+dev41

- Corrects values in sb and tbl hk-tlm.txt to allow the TBL and SB tlm pages to open.
- Adds a contributing guide that links to the main cFS contributing guide.
- See <https://github.com/nasa/cfs-groundsystem/pull/171>

### Development Build: v2.2.0-rc1+dev33

- Fix #163, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/cfs-groundsystem/pull/167>

### Development Build: v2.2.0-rc1+dev18

- Documentation: Add `Security.md` with instructions to report vulnerabilities
- **Breaking change**, CmdUtil, Rounds header up to match <https://github.com/nasa/cFE/pull/1077>
- **Breaking change**, GUI, Rounds header up to match <https://github.com/nasa/cFE/pull/1077>
- See <https://github.com/nasa/cFS-GroundSystem/pull/150>

### Development Build: v2.2.0-rc1+dev11

- Updated CHeaderParser.py to address specific issues.
- See <https://github.com/nasa/cFS-GroundSystem/pull/135>

### Development Build: v2.2.0-rc1+dev8

- Replaces old code that caused a cast-align warning when strict. Refactored and removed unnecessary code while also following recommended model for getaddrinfo. Removed old windows support/defines/etc (likely not tested for years, no longer supported).
- Reduce the size of the strncpy so that it ensures there's a null byte at the end of the string buffer.
- See <https://github.com/nasa/cFS-GroundSystem/pull/133>

### Development Build: v2.2.0+dev2

 - Fixes multiple typos
- See <https://github.com/nasa/cFS-GroundSystem/pull/127>

### Development Build: v2.1.0+dev85

- Remove unused code/packages to fix LGTM warnings
- See <https://github.com/nasa/cFS-GroundSystem/pull/120>

### Development Build: v2.1.0+dev76

- Fixes more lgtm warnings
- Allows users to change the byte offsets for sending commands and parsing telemetry, to support different header versions or other implementations of cFS
- Adds a file to store version information and reports version upon ground-system startup.
- See <https://github.com/nasa/cFS-GroundSystem/pull/109>

### Development Build: 2.1.12

- Change all individual UI elements to table widgets. Update backend code accordingly
- Temporary fix for implicit declaration of endian functions on some systems (RH/CentOs). No build errors on CentOS
- See <https://github.com/nasa/cFS-GroundSystem/pull/107>

### Development Build: 2.1.11

- Default behavior is the same except adds checksum and doesn't actually require fields. Adds all the packet fields, overrides, more supported data types, etc.
- See <https://github.com/nasa/cFS-GroundSystem/pull/101>

### Development Build: 2.1.10

- Change documentation for table loading guide to markdown
- See <https://github.com/nasa/cFS-GroundSystem/pull/94>

### Development Build: 2.1.9

- Upgrading PyQt4 to PyQt5 and includes a lot of cleanup/refactoring, and changes to the GUI itself
- See <https://github.com/nasa/cFS-GroundSystem/pull/90>

### Development Build: 2.1.8

- No warnings when building with GCC9
- Event messages now display both Event type and ID.
- See <https://github.com/nasa/cFS-GroundSystem/pull/86>

### Development Build: 2.1.7

- Commands and Telemetry definitions now match code
- See <https://github.com/nasa/cFS-GroundSystem/pull/74>

### Development Build: 2.1.6

- Cmd code (and checksum) are always in the same place
- See <https://github.com/nasa/cFS-GroundSystem/pull/69>

### Development Build: 2.1.5

- Updated build instructions for Python 3
- See <https://github.com/nasa/cFS-GroundSystem/pull/64>

### Development Build: 2.1.4

- Finish conversion to python 3
- cmdutil now accepts --word as alias to --long
- See <https://github.com/nasa/cFS-GroundSystem/pull/54>

### Development Build: 2.1.3

- Minor updates to work with python 3
- No longer compatible with python 2.7
- Note issue #50 is to update the related documentation
- See <https://github.com/nasa/cFS-GroundSystem/pull/47>

### Development Build: 2.1.2

- Minor updates (see <https://github.com/nasa/cFS-GroundSystem/pull/39>)

### Development Build: 2.1.1

- Minor updates (see <https://github.com/nasa/cFS-GroundSystem/pull/36>)

### **_OFFICIAL RELEASE 2.1.0 - Aquila_**

- Minor updates (see <https://github.com/nasa/cFS-GroundSystem/pull/26>)
- Released as part of cFE 6.7.0, Apache 2.0

### **_OFFICIAL RELEASE 2.0.90a_**

- Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

As a lab application, extensive testing is not performed prior to release and only minimal functionality is included.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
68,nasa/prog_algs,Python,"# Prognostics Algorithm Python Package

The Prognostic Algorithm Package is a python framework for model-based prognostics (computation of remaining useful life) of engineering systems, and provides a set of algorithms for state estimation and prediction, including uncertainty propagation. The algorithms take as inputs prognostic models (from NASA's Prognostics Model Package), and perform estimation and prediction functions. The library allows the rapid development of prognostics solutions for given models of components and systems. Different algorithms can be easily swapped to do comparative studies and evaluations of different algorithms to select the best for the application at hand.

## Installation
`pip3 install prog_algs`

## Documentation
See documentation [here](https://nasa.github.io/prog_algs/)

## Repository Directory Structure 

`prog_algs/` - The prognostics algorithm python package<br />
&nbsp;&nbsp; |- `predictors/` - Algorithms for performing the prediction step of model-based prognostics<br />
&nbsp;&nbsp; |- `samplers/` - Standard tools for performing state sampling<br />
&nbsp;&nbsp; |- `state_estimators/` - Algorithms for performing the state estimation step of model-based prognostics<br />
`benchmarking_example` - An example using metrics for benchmarking<br />
`example.py` - An example python script using prog_algs<br />
`README.md` - The readme (this file)<br />
`requirements.txt` - python library dependiencies required to be met to use this package. Install using `pip install -r requirements.txt`<br />
`tutorial.ipynb` - Tutorial (Juypter Notebook)

## Citing this repository
Use the following to cite this repository:

```
@misc{2020_nasa_prog_algs,
    author    = {Christopher Teubert and Matteo Corbetta and Chetan Kulkarni},
    title     = {Prognostics Algorithm Python Package},
    month     = Apr,
    year      = 2021,
    version   = {0.3.0},
    url       = {https://github.com/nasa/prog_algs}
    }
```

The corresponding reference should look like this:

C. Teubert, M. Corbetta, C. Kulkarni, Prognostics Algorithm Python Package, v0.3.0, Apr. 2021. URL https://github.com/nasa/prog_algs.

## Acknowledgements
The structure and algorithms of this package are strongly inspired by the [MATLAB Prognostics Algorithm Library](https://github.com/nasa/PrognosticsAlgorithmLibrary) and the [MATLAB Prognostics Metrics Library](https://github.com/nasa/PrognosticsMetricsLibrary). We would like to recognize Matthew Daigle, Shankar Sankararaman and the rest of the team that contributed to the Prognostics Model Library for the contributions their work on the MATLAB library made to the design of prog_algs

## Notices

Copyright ¬© 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

## Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
69,nasa/exoscene,Jupyter Notebook,"# exoscene

Installation: `pip install exoscene`

**exoscene** is a library of classes and utility functions for simulating
direct images of exoplanetary systems. The package was developed by Neil
Zimmerman (NASA/GSFC), with source code contributions from Maxime Rizzo,
Christopher Stark, and Ell Bogat. This work was funded in part by a WFIRST/Roman Science
Investigation Team contract (PI: Margaret Turnbull). 

**exoscene** makes significant use of the Astropy, NumPy, SciPy, and 
Scikit-image packages.

A jupyter notebook providing usage examples for much of the functionality is included under the docs subdirectory:
[exoscene/docs/notebooks/Roman-CGI_scene_demo.ipynb](exoscene/docs/notebooks/Roman-CGI_scene_demo.ipynb)

The functions are organized in 3 modules: [exoscene/planet.py](exoscene/planet.py), 
[exoscene/star.py](exoscene/star.py), and [exoscene/image.py](exoscene/image.py).

## 1. [exoscene/planet.py](exoscene/planet.py)

* a Planet() class with a data structure for containing the basic physical 
parameters of a planet, its orbit, its host star, and associated methods for
computing its relative astrometry ephemeris, its phase function, and flux ratio.

* A function for modeling the orbital position and the Lambert sphere phase function,
based on the Keplerian orbital elements and date of observation.

* A function for mapping the time-dependent sky-projected position and 
Lambert phase factor.

## 2. [exoscene/star.py](exoscene/star.py)

* Functions for computing the band-integrated irradiance of a star based on its 
apparent magnitude and spectral type, and instrument bandpass, using the built-in 
Bruzual-Persson-Gunn-Stryker (BPGS) Spectral Atlas (under 
[exoscene/data/bpgs/](exoscene/data/bpgs/))

* A function for computing the approximate parallax and proper motion offset for 
a star, based on the celestial coordinates and observing dates.

## 3. [exoscene/image.py](exoscene/image.py)

* A function for accurately resampling an image model array to a detector array.

* Functions for translating a coronagraph PSF model to an arbitrary field point,
taking into account position-dependent properties included in the model.

* Functions for applying a noise model to a detector intensity map, to simulate
an image with photon counting noise, read noise, and dark current, for a given 
integration time.

Copyright ¬© 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Other Rights Reserved.
"
70,nasa/MFISPy,Python,"# MFISPy
Multifidelity Importance Sampling with Python



## License

Notices:
Copyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
71,nasa/TrickHLA,C++,"# TrickHLA is an IEEE 1516 High Level Architecture (HLA) Simulation Interoperability Standard Middleware for the Trick Simulation Environment

## Brief Abstract:

The TrickHLA software supports the IEEE-1516 High Level Architecture (HLA) simulation interoperability standard for the [Trick Simulation Environment](https://github.com/nasa/trick/). The TrickHLA software abstracts away the details of using HLA, allowing the user to concentrate on the simulation and not worry about having to be an HLA distributed simulation expert. The TrickHLA software is data driven and provides a simple Application Programming Interface (API) making it relatively easy to take an existing Trick simulation and make it a HLA distributed simulation.

## Copyright:
Copyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

## Disclaimers:
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

## Responsible Organization:
Simulation and Graphics Branch, Mail Code ER7  
Software, Robotics & Simulation Division  
NASA, Johnson Space Center  
2101 NASA Parkway, Houston, TX  77058  

---

TrickHLA is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/TrickHLA/blob/master/LICENSE.txt).

"
72,nasa/dorado-sensitivity,Jupyter Notebook,"# Dorado sensitivity and exposure time calculator

Dorado is a proposed space mission for ultraviolet follow-up of gravitational
wave events. This repository contains a simple sensitivity and exposure time
calculator for Dorado.

This package can estimate the signal to noise, exposure time, or limiting
magnitude for an astronomical source with a given spectrum using the [CCD
signal to noise equation]. It models the following noise contributions:

*   Zodiacal light
*   Airglow (geocoronal emission)
*   Standard CCD noise (shot noise, read noise, dark current)

## Installation

To install with [Pip]:

    $ pip install dorado-sensitivity

## Examples

For examples, see the [Jupyter notebook].

## Dependencies

*   [Astropy]
*   [Synphot] for combining bandpasses and source spectra
*   [PyYAML] for reading [ECSV] data files

[CCD signal to noise equation]: https://hst-docs.stsci.edu/stisihb/chapter-6-exposure-time-calculations/6-4-computing-exposure-times
[Pip]: https://pip.pypa.io
[Astropy]: https://www.astropy.org
[Synphot]: https://synphot.readthedocs.io/
[PyYAML]: https://pyyaml.org/
[ECSV]: https://github.com/astropy/astropy-APEs/blob/master/APE6.rst
[Jupyter notebook]: https://github.com/nasa/dorado-sensitivity/blob/master/example.ipynb
"
73,nasa/Mixed-Reality-Exploration-Toolkit,C#,"<p align=""center""><img src=https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/blob/master/Unity/Assets/Media/Images/MRET-Banner.png width=250></p>

# Mixed Reality Exploration Toolkit (MRET)

The Mixed Reality Exploration Toolkit (MRET) is an XR toolkit designed for rapidly building environments for NASA domain problems in science, engineering and exploration. It works with CAD-derived models and assemblies, LiDAR data and scientific models.

## Installation (Built Package)

Download the release package from the [release page](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/tag/v21.1.0). Simply extract the zip folder and run MRET.exe.

### Non-HDRP
* [VR](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1VR.zip)
* [Desktop](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1Desktop.zip)

### HDRP (experimental)
* [VR](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1HDRPVR.zip)
* [Desktop](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1HDRPDesktop.zip) (Raytracing)

## Installation (Development Environment)

`git clone https://github.com/nasa/Mixed-Reality-Exploration-Toolkit` into the folder you would like the MRET project to be for your Unity **2019.4.17f1**. Yes, that specific Unity version is important!

In Unity Hub ‚ÄúADD‚Äù the MRET project from the folder where you cloned it to.
Once Unity opens, **DO NOT** change the scene to MRET scene, because you want the necessary assets to be included in the project before. 
Hence import all these assets first into the project under [Unity/Assets/AssetsandLibraries/Non-Distributable](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/tree/master/Unity/Assets/AssetsandLibraries/Non-Distributable):

### Free assets:

* [SteamVR](https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.1) Version 2.6.1
* [VR Capture](https://assetstore.unity.com/packages/tools/video/vr-capture-75654) Version 11.6

### Paid assets:

While MRET is free and [open-source](https://opensource.gsfc.nasa.gov/documents/NASA_Open_Source_Agreement_1.3.pdf), it does rely on third party assets that aren't.

Prices give a ballpark estimate for building MRET as of 2021.03.30. The prices of these assets fluctuate over time and may differ from what is listed.

* [Easy Build System](https://assetstore.unity.com/packages/templates/systems/easy-build-system-v5-2-5-45394) Version 5.2.5 - $20.
* [Embedded Browser](https://assetstore.unity.com/packages/tools/gui/embedded-browser-55459) Version 3.1.0 - $75
* [Final IK](https://assetstore.unity.com/packages/tools/animation/final-ik-14290) Version 2.0 - $90
* [Graph & Chart](https://assetstore.unity.com/packages/tools/gui/graph-and-chart-78488) Version 1.95 - $35
* [Non Convex Mesh Collider](https://assetstore.unity.com/packages/tools/physics/non-convex-mesh-collider-84867) Version 1.0 - $9
* [Universal Media Player](https://assetstore.unity.com/packages/tools/video/ump-win-mac-linux-webgl-49625) Version 2.0.3 - $45
* [Universal Sound FX](https://assetstore.unity.com/packages/audio/sound-fx/universal-sound-fx-17256) Version 1.4 - $40 

### Tricky assets:

* [Pointcloud viewer and tools](https://assetstore.unity.com/packages/tools/utilities/point-cloud-viewer-and-tools-16019) Version 2.40 - $100 When importing this one, uncheck the ‚ÄúEditor‚Äù folder (otherwise MRET will run in Unity but not compile).
* [Runtime OBJ Importer](https://assetstore.unity.com/packages/tools/modeling/runtime-obj-importer-49547) Version 2.02 - Free
After importing this asset, modify the string in ""Shader.Find"" on line 160 of MTLLoader.cs to ""Standard"" (i.e. Shader.Find(""Standard"")).

### Unity Packages
* If the OpenVR Unity XR package doesn't install correctly using the Package Manager, it can also be downloaded from [GitHub](https://github.com/ValveSoftware/unity-xr-plugin). Note, you will need to remove the reference to the OpenVR Unity XR package from the package.manifest if going this route.

Now navigate in Unity to `Project > Assets > Framework > Scenes > MainScene` and open either VR or Desktop scene.

(you will still need the MRET Components to be placed, where they need to be placed to..)

### Additional Configuration
Ensure that Project Settings->Player->Other Settings->Active Input Handling is set to ""Both"".

## Contributing

Pull requests are welcome. For more information on contributing, please see [Contributing.md](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/blob/master/CONTRIBUTING.md).

## License

[NOSA](https://opensource.gsfc.nasa.gov/documents/NASA_Open_Source_Agreement_1.3.pdf)
"
74,nasa/refine,C,"# Description

`refine` is a 3D mesh adaptation tool implemented in the C language.

# Quick Start Compile from Git Repo and Basic Usage

`refine` can function without depencies, but the typical use cases of
parallel execution and geometry evaluation require an MPI implementation
and [Engineering Sketch Pad](https://acdl.mit.edu/ESP/ESPreadme.txt) (ESP).
A native implementaion of a recursive coordinate bisection partition
algorithm is included, but better results are expected with
[ParMETIS](http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview).
Initial mesh generation assumes
[TetGen](http://wias-berlin.de/software/tetgen/) or
[AFLR](http://www.simcenter.msstate.edu/research/cavs_cfd/aflr.php) is in
the shell path.
Configuration and compliation is supported with Autoconf and CMake.

## Automake 1.7 (or later) and Autoconf 2.53 (or later):
```
 ./bootstrap
 mkdir -p build
 cd build
 ../configure --prefix=`pwd` \
   --with-mpi=/mpi/path \
   --with-parmetis=/parmetis/path \
   --with-EGADS=/egads/path \
   --with-OpenCASCADE=/opencascade/path
 make
 make install
```
See the INSTALL file in the top directory or `./configure --help`
for additional build instructions.

## CMake 3.0 (or later):
```
 mkdir -p build
 cd build
 cmake .. -DCMAKE_INSTALL_PREFIX=`pwd` \
   -DCMAKE_PREFIX_PATH=""/mpi/path;/parmetis/path;/egads/path;/opencascade/path""
 make
 make install
```

## Usage

The installed `bin` directory will include the `ref` executable.
Invoking `ref` with no arguments will list available subcommands.
Help on a particular subcommand is available via a `-h`, i.e.,
`ref adapt -h`. If MPI is provided, `refmpi` will allow for parallel
execution. If ESP is provided, `ref` and `refmpifull` includes
EGADS built with OpenCASCADE and `refmpi` includes EGADSlite.

# Examples

The following examples assume that `ref` is in your shell path.
`mpiexec ... refmpi` or `mpiexec ... rempifull` can be substituted for
`ref` in each of these examples if MPI and/or ESP is configured. The
[.meshb and .solb file extensions](https://github.com/LoicMarechal/libMeshb)
are used generically. Other formats are supported, e.g.,
AFLR `*.ugrid`.

## Bootstrapping Mesh Adaptation on an EGADS Model

An `.egads` file can be dumped from OpenCSM in the ESP package.
```
ref bootstrap project.egads
```
or
```
mpiexec ... refmpifull bootstrap project.egads
```
which assume that `tetgen` is in your shell path or
`aflr3` is in your shell path with `--mesher aflr` option.
A `project-vol.meshb` is output that includes the surface mesh,
volume mesh, mesh-to-geometry associtivity, and EGADSlite data.

## Mesh Adaptation

The mesh is adapted with
```
ref adapt input.meshb -x output.meshb [-m metric.solb] [-g geometry.egads]
```
or
```
mpiexec ... refmpi adapt input.meshb -x output.meshb [-m metric.solb]
```
where a surface curvature metric is used if the `-m` argument is not present.

## Multiscale Metric for Control of Interpolation Error in Lp-norm

In conjunction with the
[Unstructured Grid Adaptation Working Group](https://ugawg.github.io/),
an implementation of the multiscale metric is provided.
```
ref multiscale input.meshb scalar.solb complexity output-metric.solb
```
or
```
mpiexec ... refmpi multiscale input.meshb scalar.solb complexity output-metric.solb
```

## Field Interpolation
The fields in a .solb file paired with a donor mesh can be interpolated to
a receptor mesh. This utility can be executed in serial or parallel.

```
ref interp donor-mesh.ext donor-field.solb receptor-mesh.ext receptor-field.solb
```
or 
```
mpiexec ... refmpi interp donor-mesh.ext donor-field.solb receptor-mesh.ext receptor-field.solb
```
where the output is `receptor-field.solb`.

# Description

`refine` is a 2D and 3D mesh adaptation tool implemented in the C
language.  Mesh adaptation mechanics are provided where the primary
target is linear and curved simplex (triangle and tetrahedra)
meshes. A limited capability to store, modify, and insert
mixed-element types is also provided. Typical use is via an executable
that interacts with files, and linking to a library form is also
available. Mesh adaptation metrics can be computed by reconstructing
gradients and Hessians from a field. Visualization files and multiple
mesh formats can be exported. Solutions can be interpolated between
meshes. The distance to lower-dimensional elements can be computed.
Interfaces are available to multiple geometry sources and an internal
surrogate geometry source. Parallel execution is supported with
partitioning and load balancing. Solution fields are provided to
verify the mesh adaptation process.

"
75,nasa/SMCPy,Python,"SMCPy - **S**equential **M**onte **C**arlo with **Py**thon 
==========================================================================
[![Build Status](https://travis-ci.com/nasa/SMCPy.svg?branch=master)](https://travis-ci.com/nasa/SMCPy) &nbsp;[![Coverage Status](https://coveralls.io/repos/github/nasa/SMCPy/badge.svg?branch=master)](https://coveralls.io/github/nasa/SMCPy?branch=master)

Python module for uncertainty quantification using a parallel sequential Monte
Carlo sampler.

To operate the code, the user supplies a computational model built in Python
3.6+, defines prior distributions for each of the model parameters to be
estimated, and provides data to be used for calibration. SMC sampling can then
be conducted with ease through instantiation of the SMCSampler class and a call
to the sample() method. The output of this process is an approximation of the
parameter posterior probability distribution conditional on the data provided.

The primary sampling algorithm implemented in this package is an MPI-enabled
version of that presented in the following IEEE article by Nguyen et al.:

> Nguyen, Thi Le Thu, et al. ""Efficient sequential Monte-Carlo samplers for Bayesian
> inference."" IEEE Transactions on Signal Processing 64.5 (2015): 1305-1319.

[Link to Article](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339702) | [BibTeX Reference](https://scholar.googleusercontent.com/scholar.bib?q=info:L7AZJvppx1MJ:scholar.google.com/&output=citation&scisdr=CgUT24-FENXorVVNYK0:AAGBfm0AAAAAXYJIeK1GJKW947imCXoXAkfc7yZjQ7Oo&scisig=AAGBfm0AAAAAXYJIeNYSGEVCrlauowP6jMwVMHB_blTp&scisf=4&ct=citation&cd=-1&hl=en)

This software was funded by and developed under the High Performance Computing
Incubator (HPCI) at NASA Langley Research Center.

------------------------------------------------------------------------------
## Example Usage

```python
import numpy as np

from scipy.stats import uniform

from spring_mass_model import SpringMassModel
from smcpy.utils.plotter import plot_pairwise
from smcpy import SMCSampler, VectorMCMC, VectorMCMCKernel


# Load data
std_dev = 0.5
displacement_data = np.genfromtxt('noisy_data.txt')

# Define prior distributions & MCMC kernel
priors = [uniform(0, 10), uniform(0, 10)]
vector_mcmc = VectorMCMC(model.evaluate, displacement_data, priors, std_dev)
mcmc_kernel = VectorMCMCKernel(vector_mcmc, param_order=('K', 'g'))

# SMC sampling
smc = SMCSampler(mcmc_kernel)
step_list, mll_list = smc.sample(num_particles=500,
                                 num_mcmc_samples=5,
                                 phi_sequence=np.linspace(0, 1, 20),
                                 ess_threshold=0.8,
                                 progress_bar=True)

# Display results
print(f'parameter means = {step_list[-1].compute_mean()}')

plot_pairwise(step_list[-1].params, step_list[-1].weights, save=True,
              param_labels=['K', 'g'])
```

The above code produces probabilistic estimates of K, the spring stiffness
divided by mass, and g, the gravitational constant on an unknown planet. These
estimates are in the form of weighted particles and can be visualized by
plotting the pairwise weights as shown below. The mean of each parameter is
marked by the dashed red line. The true values for this example were K = 1.67
and g = 4.62. More details can be found in the spring mass example
(smcpy/examples/spring_mass/).

To run this model in parallel using MPI, the MCMC kernel just needs to be built
with the ParallelMCMC class in place of VectorMCMC. More details can be found
in the MPI example (smcpy/examples/mpi_example/).

![Pairwise](https://github.com/nasa/SMCPy/blob/main/examples/spring_mass/pairwise.png)

Tests
-----

The tests can be performed by running ""pytest"" from the tests/unit directory to ensure a proper installation.

Developers
-----------

NASA Langley Research Center <br /> 
Hampton, Virginia <br /> 

This software was funded by and developed under the High Performance Computing Incubator (HPCI) at NASA Langley Research Center. <br /> 

Contributors: Patrick Leser (patrick.e.leser@nasa.gov) and Michael Wang

------------------------------------------------------------------------------

License
-----------
Notices:
Copyright 2018 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration. No copyright is claimed in
the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S
USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR
ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS
AGREEMENT.

"
76,nasa/Kamodo,Python,"# Kamodo

[![codecov](https://codecov.io/gh/asherp/Kamodo/branch/master/graph/badge.svg?token=W1B3L19REF)](https://codecov.io/gh/asherp/Kamodo)

Kamodo is a CCMC tool for access, interpolation, and visualization of space weather models and data in python. Kamodo allows model developers to represent simulation results as mathematical functions which may be manipulated directly by end users. Kamodo handles unit conversion transparently and supports interactive science discovery through jupyter notebooks with minimal coding and is accessible through python.


The project page is located at the Community Coordinated Modeling Center, located at NASA Goddard Space Flight Center.

* Official site page [https://ccmc.gsfc.nasa.gov/Kamodo/](https://ccmc.gsfc.nasa.gov/Kamodo/)

Kamodo's official source code is hosted on github under a permissive NASA open source license:

* [https://github.com/nasa/Kamodo](https://github.com/nasa/Kamodo)

Periodic contributions to Kamodo are made from the unofficial repo located here

* [https://github.com/asherp/Kamodo](https://github.com/asherp/Kamodo)


## Usage
Suppose we have a vector field defined by a function of positions in the x-y plane:

```python
from kamodo import kamodofy
import numpy as np

x = np.linspace(-np.pi, np.pi, 25)
y = np.linspace(-np.pi, np.pi, 30)
xx, yy = np.meshgrid(x,y)
points = np.array(zip(xx.ravel(), yy.ravel()))

@kamodofy(units = 'km/s')
def fvec(rvec = points):
    ux = np.sin(rvec[:,0])
    uy = np.cos(rvec[:,1])
    return np.vstack((ux,uy)).T
```

The @kamodofy decorator lets us register this field with units to enable unit-conversion downstream:
```python
from kamodo import Kamodo

kamodo = Kamodo(fvec = fvec)
kamodo
```
When run in a jupyter notebook, the above kamodo object will render as a set of equations:

$$\vec{f}{\left (\vec{r} \right )} [km/s] = \lambda{\left (\vec{r} \right )}$$

We can now evaluate our function using dot notation:

```python
kamodo.fvec(np.array([[-1,1]]))
```
```console
array([[-0.84147098,  0.54030231]])
```
We can perform unit conversion by function composition:
```python
kamodo['gvec[m/s]'] = 'fvec'
```
kamodo automatically generates the appropriate multiplicative factors:
$$\vec{g}{\left (\vec{r} \right )} [m/s] = 1000 \vec{f}{\left (\vec{r} \right )}$$
we can verify these results through evaluation

```python
kamodo.gvec(np.array([[-1,1]]))
```
```console
array([[-841.47098481,  540.30230587]])
```
Kamodo also generates quick-look graphics via function inspection.
```python
import plotly.io as pio

fig = kamodo.plot('fvec')
pio.write_image(fig, 'images/fig2d-usage.svg')
```
![usage](notebooks/images/fig2d-usage.svg)

Head over to the [Introduction](notebooks/Kamodo.ipynb) page for more details.


## Getting started

Kamodo may be installed from pip

```console
pip install kamodo
```

To get the latest version, install from Asher's fork:

```console
pip install git+https://github.com/asherp/Kamodo.git
```

!!! note
    Asher's fork is periodically merged into the CCMC's official NASA version.

### Kamodo Environment 

We strongly recommend using the conda environment system to avoid library conflicts with your host machine's python.

Download and install miniconda from [here](https://conda.io/miniconda.html). The advantage to using miniconda is that each new environment includes the bare-minimum for a project. This allows you to keep many different projects on a single work station.

#### Create Kamodo environment

Create a new environment for kamodo

```console
conda create -n kamodo python==3.7
conda activate kamodo
(kamodo) pip install kamodo
```
!!! note
    The leading (kamodo) in your prompt indicates that you have activated the `kamodo` environment.
    From here on, anything you install will be isolated to the `kamodo` environment.

#### Loading example notebooks

If you want to run any of the notebooks in docs, you will need to install `jupyter`:

```console
(kamodo) conda install jupyter
```

Navigate to the top-level of the kamodo repo, then point jupyter to `docs/notebooks`:

    (kamodo) jupyter notebook docs/notebooks

This should open a browser window that will allow you to load any of the example notebooks.

#### Requirements

The following requirements are obtained by running `pip install kamodo`

* numpy
* scipy
* sympy
* pandas
* plotly==3.3 
* pytest
* psutil
* conda install antlr-python-runtime (rendering latex)
* conda install -c plotly plotly-orca (for writing images)

!!! note
    plotly version in flux


## Generating Docs

Kamodo's documentation site is a good example of how to embed your own plots in your own website.
The documentation site is generated by the `mkdocs` package with some addons

* mkdocs - handles site generation and deployment (configured by top-level `mkdocs.yaml`)
* markdown-include - allows for embedding of markdown files (and graph divs) outside the docs folder
* python-markdown-math - enables LaTeX rendering
* mknotebooks - allows for the embedding of jupyter notebooks

All of the above requirements can be installed with this line:

```console
pip install mkdocs python-markdown-math markdown-include mknotebooks
```

You can then generate the docs and serve locally with

`mkdocs serve`

To deploy your own documentation on github-pages:

`mkdocs gh-deploy`

This generates a gh-pages branch with the static site files and pushes it to github. Github automatically creates a website url based on that branch.

"
77,nasa/CCDD,Java,"# CCDD
Core Flight System (CFS) Command and Data Dictionary (CCDD) utility

*** CCDD Version 1.5.37 ***

*** CCDD works with JAVA 7-13 ***

CCDD is a software tool for managing the command and telemetry data for CFS and CFS applications.  CCDD is written in Java‚Ñ¢ and interacts with a PostgreSQL database, so it can be used on any operating system that supports the Java Runtime Environment (JRE) and PostgreSQL.  CCDD is released as open source software under the NASA Open Source Software Agreement, version 1.3, and is hosted on GitHub.

The CCDD application uses tables, similar to a spreadsheet, to display and allow manipulation of telemetry data structures, command information, and other data pertinent to a CFS project.  The data is stored in a PostgreSQL database for manipulation and data security.  The PostgreSQL database server can be run locally or centralized on a remote host for easier access by multiple users.  Data can be imported into the application from files in comma-separated values (CSV), JavaScript Object Notation (JSON), electronic data sheet (EDS), and extensible markup language (XML) telemetric and command exchange (XTCE) formats.  Data can be exported from the application to files in CSV, JSON, EDS, and XTCE formats.  The CCDD tables also allow simple cut and paste operations from the host operating system‚Äôs clipboard.  To make use of the project‚Äôs data, CCDD can interact with Java Virtual Machine (JVM)-based scripting languages via a set of supplied data access methods.  Using scripts, the user can translate the data stored in the CCDD‚Äôs database into output files.  Example scripts for creating common CFS related output files are provided in four of these scripting languages.  An embedded web server can be activated, allowing web-based application access to the data.

See the CCDD user's guide for details on set up and use.

## CCDD version 2 

*** Version 2.0.16 is now released (see below for details) ***

*** CCDD version 2 works with JAVA 7-13 ***

*** CCDD version 2 has changed the way that the json import/export works. You can now import and export entire databases. Check CCDDv2 users guide for more details ***

Version 2 redefines the behavior of command tables.  Command arguments are no longer defined as columns within a command table.  Instead, the command table has a column that is a reference to a structure table; this structure defines the command argument(s).  The version 2 user's guide is updated to provide further details.

When version 2 attempts to open a version 1.x.x version project database then a dialog appears asking to convert the project.  Unlike previous patches, this patch alters user-defined tables and table definitions, and creates new ones.  The argument columns in any command tables are replaced with the argument structure reference column, and the argument structure is created and populated using the original argument information.  Many of the command table script data access methods no longer exist, so existing scripts may need to be updated. Before this patch is applied to the version 1.x.x database a backup will be performed to ensure no data loss on the chance that something does not work as anticipated. 
"
78,nasa/SROMPy,Python,"SROMPy - **S**tochastic **R**educed **O**rder **M**odels with **Py**thon 
==========================================================================

<a href='https://travis-ci.com/nasa/SROMPy'><img src='https://travis-ci.com/nasa/SROMPy.svg?branch=master' alt='Coverage Status' /></a> <a href='https://coveralls.io/github/lukemorrill/SROMPy?branch=master'><img src='https://coveralls.io/repos/github/lukemorrill/SROMPy/badge.svg?branch=master' alt='Coverage Status' /></a>

General
--------

Python module for generating Stochastic Reduced Order Models (SROMs) and applying them for uncertainty quantification problems. See documentation in `docs/` directory for details. 

Dependencies
-------------
SROMPy is intended for use with Python 2.7 and relies on the following packages:
* numpy
* scipy
* matplotlib
* mpi4py (optional for running in parallel)
* pytest (optional if the testing suite is to be run)

A requirements.txt file is included for easy installation of dependecies with pip:

```
pip install -r requirements.txt
```

Example Usage
--------------

```python
from SROMPy.postprocess import Postprocessor
from SROMPy.srom import SROM
from SROMPy.target import NormalRandomVariable

#Initialize Normal random variable object to be modeled by SROM:
normal = NormalRandomVariable(mean=3., std_dev=1.5)

#Initialize SROM & optimize to model the normal random variable:
srom = SROM(size=10, dim=1)
srom.optimize(normal)

#Compare the CDF of the SROM & target normal variable:
post_processor = Postprocessor(srom, normal)
post_processor.compare_CDFs()
```
  
The above code snippet produces the following CDF comparison plot: 
  
![CDF comparison](https://github.com/nasa/SROMPy/blob/master/examples/basic_tests/normal_rv_srom.png)

Getting Started
----------------
SROMPy can be installed via pip from [PyPI](https://pypi.org/project/SROMPy/):

```
pip install srompy
```

SROMPy can also be installed using the `git clone` command:

```
git clone https://github.com/nasa/SROMPy.git
```

The best way to get started with SROMPy is to take a look at the scripts in the examples/ directory. A simple example of propagating uncertainty through a spring mass system can be found in the examples/spring_mass/, while the examples/phm18/ directory contains scripts necessary to reproduce the results in the following conference paper on probabilistic prognostics: https://www.phmpapers.org/index.php/phmconf/article/view/551. For more information, see the source code documentation in docs/SROMPy_doc.pdf (a work in progress) or the technical report below that accompanied the release of SROMPy.

Tests
------
The tests can be performed by running ""py.test"" from the tests/ directory to ensure a proper installation.

Reference
-------------
If you use SROMPy for your research, please cite the technical report:

Warner, J. E. (2018). Stochastic reduced order models with Python (SROMPy). NASA/TM-2018-219824. 

The report can be found in the `docs/references` directory. Thanks!

Developers
-----------

UQ Center of Excellence <br />
NASA Langley Research Center <br /> 
Hampton, Virginia <br /> 

This software was funded by and developed under the High Performance Computing Incubator (HPCI) at NASA Langley Research Center. <br /> 

Contributors: James Warner (james.e.warner@nasa.gov), Luke Morrill, Juan Barrientos

License
---------

Copyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
The Stochastic Reduced Order Models with Python (SROMPy) platform is licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. 
 
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.



"
79,nasa/IDF,C++,"# Input Device Framework

IDF is a software library that provides an infrastructure for interfacing
software with physical input devices. Examples of common devices include hand
controllers, joysticks, foot pedals, computer mice, game controllers, etc.
Conceptually, the framework can be extended to support any device that produces
digital output. IDF additionally presents, and is itself an implementation of, a
design methodology that encourages application developers to program against
domain-specific interfaces rather than particular hardware devices. This
abstraction frees the application from the details of communicating with the
underlying devices, resulting in robust and flexible code that is
device-agnostic. IDF ensures that devices meet application interface
requirements, supports many-to-many relationships between application interfaces
and devices, allows for flexible and dynamic interpretation of device inputs,
and provides methods for transforming and combining inputs.

# The Problem

Software often requires input from external physical devices. The range of
devices, data formats, and mediums of communication are virtually limitless.
There thus exists a need for a method by which to deliver data to applications
in a consistent, well-defined manner, and in an application-defined format. The
vast array of ways that devices output data necessitates a separation between
the application, which is concerned only with what the data represents, and the
devices themselves, which have no notion of how their data is to be used. There
must then be a way to bridge this disconnect without burdening either side with
details of the other. Barring such a tool, applications may be written against
specific devices, but this pollutes the program's primary functionality with a
secondary task and forces the application interface to conform to the device's
output format. As the consumer of the data, it is preferable for the
application to define its own context-specific interface through which the data
is received. Changes in the device, data format, or communication medium should
not alter the application interface.

# A Solution

IDF serves to completely separate the ""business"" logic of a program from the
logic responsible for reading raw input from physical devices. Beyond simply
isolating the code, it further aims to abstract each side from the other, so
that both pieces can be developed fully independently. Its goal is to enable
application developers to write robust, flexible code with clearly delineated
interface points which are defined in a domain and context most meaningful to
the application.

# Architecture

### Simulation Interface Layer

This layer defines the point at which external inputs enter the application's
primary functional logic. It is here that a developer decides what kind of
inputs his system will accept, and in what format it will accept them. Once this
interface is established, the developer encodes it as a ""controller"" by
extending IDF and specializing it for this particular application. Devices that
wish to service this interface must meet this controller's contract. This layer
is the only one with which the primary functional logic should interact.

### Hardware Interface Layer

This layer represents the physical input devices and is responsible for all
aspects of communication with the hardware, primarily reading and decoding raw
data. Devices are classified by their communication medium, and a representation
exists for each specific device supported.

### Input Abstraction Layer

This layer provides a means by which to tie the above two layers together. It
represents device input layouts without regard to their particular medium of
communication. A device's available data, represented in this layer, is
populated by the Hardware Interface Layer as it is received from the physical
device. This data can then be presented to a controller from the Simulation
Interface Layer, provided that it can be made to meet that controller's
contract.

# Wiki

See the [wiki](https://github.com/nasa/IDF/wiki) for installation instructions
and lots of additional information.

# License
IDF is released under the [NASA Open Source Agreement, Version 1.3](LICENSE).
"
80,nasa/PSP,C,"![Static Analysis](https://github.com/nasa/psp/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/psp/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : Platform Support Package

This repository contains NASA's Platform Support Package (PSP), which is a framework component of the Core Flight System.

This is a collection of APIs abstracting platform specific functionality to be located in the `psp` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes build and execution instructions.

## Version History


### Development Build: v1.5.0-rc1+dev112

- Cleans up stale code from the previous methods of generating 1Hz. Adds a new PSP module that instantiates an OSAL abstract timebase for use with cFE services. This single module is used across all psp implementations (mcp750, pc-linux, pc-rtems). Results in 1Hz timing tick on MCP750 will be more accurate. No changes to Linux or RTEMS
- Fixes segfaults when `CFE_PSP_Port` routines are invoked on Linux.
- Converts `cfe_psp_ram.c` and `cfe_psp_port.c` into modular components and removes from the ""shared"" directory. The existing implementations become the corresponding ""direct"" module, and are enabled based on the psp module selection. Adds a ""notimpl"" variant where all the functions return `CFE_PSP_ERR_NOT_IMPLEMENTED`. This is used on Linux
or any other system where direct access is not possible.  Renames the existing `eeprom_stub` module to be `eeprom_notimpl` for consistency and to avoid confusion with the unit test stubs.
- Implements two PSP modules to provide `CFE_PSP_GetTime` and `CFE_PSP_GetTimeBase`, one for POSIX-compliant RTOS using `clock_gettime()` and the other specifically for PowerPC processors on VxWorks that have the `vxTimeBaseGet()` routine. Clarifies and documents the difference and use cases for `CFE_PSP_GetTime` and `CFE_PSP_GetTimeBase`. No impact to behavior.
- Adds a coverage test for the VxWorks PSP timebase module and provides an example of how this can be implemented for other modules.
- See <https://github.com/nasa/PSP/pull/289> and <https://github.com/nasa/cFS/pull/238>

### Development Build: v1.5.0-rc1+dev101

- Removes unnecessary global config structure `Target_PspConfigData` and associated elements infavor of the new version API.
- The mem pool stats TLM command now works on 64-bit Linux and sends out the expected telemetry packet.
Converts `CFE_PSP_MemoryTable` to internal object (instead of external) that should only be accessed via the PSP API. Replace `uint32`s with `size_t`. Use full range (SIZE_MAX) in the Linux/RTEMS implementation.
- See <https://github.com/nasa/PSP/pull/288> and <https://github.com/nasa/cFS/pull/233>

### Development Build: v1.5.0-rc1+dev95

- Includes `cfe_psp_version.c` in the cmake source list, which was mistakenly omitted previously.
- Applied the patch and confirmed that CS Cmake unit tests build. Unit tests relying on `CFE_PSP_GetKernelTextSegmentInfo` will build.
- See <https://github.com/nasa/PSP/pull/279>

### Development Build: v1.5.0-rc1+dev90

- Addresses the issue of incompatible/non-portable code blobs in the ""shared"" directory. It uses the same modular init pattern as is used elsewhere in cFE: CMake generates a list of ""base"" modules correlating with the selected PSP (i.e. pc-linux, mcp750-vxworks, etc) and these modules are then initialized (in order) before the rest of PSP runs. The ""direct write"" EEPROM is not used unconditionally. Instead the proper eeprom implementation module is selected based on which PSP is selected. MCP750 uses direct write, pc-linux uses an mmap file, and pc-rtems uses a stub (not implemented).
- Replaces "" used on non-system header #includes with <>
- Adds a contributing guide that links to the main cFS contributing guide.
- See <https://github.com/nasa/PSP/pull/273>


### Development Build: v1.5.0-rc1+dev82

- HOTFIX 20210312, updates to work with older CMake
- See <https://github.com/nasa/PSP/pull/268>

### Development Build: v1.5.0-rc1+dev76

- Fix #246, remove unused code.
- Fix #254, use CMake to publish interface details
- Fix #256, add PSP version API
- Fix #258, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/PSP/pull/260>

### Development Build: 1.5.0-rc1+dev68

- Updates continuous integration workfow by adding static analysis with timeout and code format check. Adds status badges to ReadMe and removes old TravisCI artifacts.
- Adds CodeQL analysis to continuous integration
- Apply standard formatting to psp codebase. Only changes whitespace.
- Adds missing ""+dev"" to development version output
- See <https://github.com/nasa/PSP/pull/250>

### Development Build: 1.5.0-rc1+dev58

- Add `Security.md` with instructions on reporting vulnerabilities.
- Rename `UT_ClearForceFail` as `UT_ClearDefaultValue` to reflect <https://github.com/nasa/osal/issues/724>
- Remove unused elements in `CFE_PSP_ModuleType_t` enum to avoids irregular enum warning
- See <https://github.com/nasa/PSP/pull/243>

### Development Build: 1.5.0-rc1+dev50

-  Instead of accessing `OS_time_t` member values directly, use the OSAL-provided conversion and access methods. This provides independence and abstraction from the specific `OS_time_t` definition and allows OSAL to transition to a 64 bit value.
- See <https://github.com/nasa/psp/pull/240>

### Development Build: 1.5.0-rc1+dev46

- Add cppcheck GitHub Actions workflow file
- See <https://github.com/nasa/PSP/pull/234>


### Development Build: 1.5.0-rc1+dev42

- Updates the Readme for RTEMS and adds `README_RTEMS_5.txt`. The changes include removing references to the CEXP module loader, and describing the development environment setup for RTEMS 5.  
- Remove obsolete OS_TaskRegister comment.  
- See <https://github.com/nasa/PSP/pull/226>


### Development Build: 1.5.0-rc1+dev36

- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing.
- Use of the size_t type instead of uint32 in unit-tests to avoid a compiler type mismatch error on some platforms.
- See <https://github.com/nasa/PSP/pull/221>

### Development Build: 1.5.0-rc1+dev30

- - Use event callback mechanism to invoke pthread_setname_np() such that the OS kernel is informed of the OSAL task name. `/proc` filesystem on Linux now has actual task name, instead of all being core-cpu1. The `pthread_setname_np` API requires `_GNU_SOURCE` to be defined when compiling - this can be local to PSP.
- Set REVISION to ""99"" to indicate development version
- See <https://github.com/nasa/PSP/pull/213>

### Development Build: 1.5.0-rc1+dev24

- Improves the module ID lookup when getting the CFE core text segment info. VxWorks PSP should use the real module name, not assume cfe-core.o when getting text segment info
- See <https://github.com/nasa/PSP/pull/209>

### Development Build: 1.5.0-rc1+dev19

- Use the osal_id_t typedef whenever dealing with an OSAL ID value.
- Resolves build error regarding redefinition of PPC macros in the coverage test, when building on the actual ppc/vxworks target.
- See <https://github.com/nasa/PSP/pull/206>

### Development Build: 1.5.0-rc1+dev14

- Sets the stub config data spacecraft id to historical value 0x42, was 42.
- Installs unit test to target directories.
- See <https://github.com/nasa/PSP/pull/196>

### Development Build: 1.5.0-rc1+dev6

- Adds CFE_PSP_GetProcessorName
- Removes classic make artifacts
- See <https://github.com/nasa/PSP/pull/190>

### Development Build: 1.4.0+dev76

- Provide a new framework and perform PSP coverage tests. New coverage test executable is built and several files within PSP are targeted.
- See <https://github.com/nasa/PSP/pull/184>

### Development Build: 1.4.0+dev71

- Restructure code to make more amicable for rebuilding in a unit test environment. No major changes, primarily just shifting code between locations/headers to support unit testing.
-  Adds a char element `Version` to `CFE_PSP_VersionInfo_t` containing the version number expressed as a string. Defines new macros for the Build Number and the Build Baseline.
- See <https://github.com/nasa/PSP/pull/176>

### Development Build: 1.4.14

- Changes the PSP reference to be compatible with the change in nasa/osal#449 making the BSP modules more generic and changes the name.
- See <https://github.com/nasa/PSP/pull/175>

### Development Build: 1.4.13

- Changes the PSP reference to be compatible with the change in nasa/osal#449 making the BSP modules more generic and changes the name.
- See <https://github.com/nasa/PSP/pull/167>

### Development Build: 1.4.12

- Replace 'OS_VolumeTable' with OS_FileSysAddFixedMap() in all PSPs.
- See <https://github.com/nasa/PSP/pull/166>

### Development Build: 1.4.11

- Removes non-termination string warnings when building with GCC9.
- Exception handling is now implemented on POSIX. There is no longer a separate handler for SIGINT - it is now treated as an exception and goes through the normal process which ends up ""restarting"" CFE. On pc-linux causes the process to exit normally. There is now a mechanism to capture the CTRL+C exception code and use it during normal test cycles.
- See <https://github.com/nasa/PSP/pull/160>

### Development Build: 1.4.10

- Implements full-precision microsecond conversion
- See <https://github.com/nasa/PSP/pull/155>

### Development Build: 1.4.9

- RTEMS builds successfully without errors
- Build script uses a proper CMakeLists.txt instead of the aux_source directory
- Minor updates (see <https://github.com/nasa/PSP/pull/153>)

### Development Build: 1.4.8

- Minor updates (see <https://github.com/nasa/PSP/pull/151>)

### Development Build: 1.4.7

- Fixed some build warnings for MCP750
- Minor updates (see <https://github.com/nasa/PSP/pull/142>)

### Development Build: 1.4.6

- Minor updates (see <https://github.com/nasa/PSP/pull/141>)

### Development Build: 1.4.5

- Simplifies array handling in VxWorks
- Minor updates (see <https://github.com/nasa/PSP/pull/138>)

### Development Build: 1.4.4

- Minor updates (see <https://github.com/nasa/PSP/pull/132>)

### Development Build: 1.4.3

- Minor updates (see <https://github.com/nasa/PSP/pull/130>)

### Development Build: 1.4.2

- Minor updates (see <https://github.com/nasa/PSP/pull/127>)

### Development Build: 1.4.1

- Minor updates (see <https://github.com/nasa/PSP/pull/115>)

### **_1.4.0 OFFICIAL RELEASE - Aquila_**

- This is a point release from an internal repository
- Changes are detailed in [cFS repo](https://github.com/nasa/cFS) release documentation
- Released as part of cFE 6.7.0, Apache 2.0

### **_1.3.0a OFFICIAL RELEASE_**

- This is a point release from an internal repository
- See [version description document](https://github.com/nasa/PSP/blob/v1.3.0a/doc/PSP%201.3.0.0%20Version%20Description%20Document.pdf)
- Released as part of cFE 6.6.0a, Apache 2.0

The open source release does not include all PSPs that have been developed. Only the three PSPs included are managed by the community CCB. PSPs developed by other organizations can be requested through the mechanisms listed below. Note the framework PSPs delivered may change in the future as platforms become obsolete.

## Known issues

See all open issues and closed to milestones later than this version.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
81,nasa/radbelt,Python,"# radbelt: An Astropy-friendly wrapper for the AE-8/AP-8 Van Allen belt model

This is small Python library to model the fluxes of charged particles trapped
in the Van Allen belt. It provides a fast, simple, and convenient Python
interface to the [International Geomagnetic Reference Field (IGRF)] model and
NASA's AE-8/AP-8 models of electron and proton fluxes, which are both
implemented in Fortran. The package is integrated with the [Astropy] ecosystem
for easy conversion of coordinate systems, time scales, and units. With this
package, it is easy and fast to determine the flux of particles above any given
energy, at any position, at any time.

## Acknowledging radbelt

This package is wraps the following Fortran codes, which have been retrieved
from NASA Goddard Space Flight Center's (GSFC) [Community Coordinated Modeling
Center (CCMC)]:

- https://ccmc.gsfc.nasa.gov/models/modelinfo.php?model=IGRF
- https://ccmc.gsfc.nasa.gov/models/modelinfo.php?model=AE-8/AP-8%20RADBELT

When publishing results derived from this Python package, please cite the
following articles:

- [Vette, J.I., Lucero, A.B., Wright, J.A., et al. 1966, ""Models of the Trapped Radiation Environment."" NASA SP-3024.](https://ui.adsabs.harvard.edu/abs/1966NASSP3024.....V)
- [Sawyer, D.M. & Vette, J.I. 1976, ""AP-8 trapped proton environment for solar maximum and solar minimum."" NASA WDC-A-R&S 76-06, NASA-TM-X-72605.](https://ui.adsabs.harvard.edu/abs/1976STIN...7718983S)
- [Vette, J.I. 1991, ""The AE-8 trapped electron model environment."" NSSDC/WDC-A-R&S 91-24.](https://ui.adsabs.harvard.edu/abs/1991STIN...9224228V)
- [Th√©bault, E., Finlay, C.C., Beggan, C.D., et al. 2015, ""International Geomagnetic Reference Field: the 12th generation."" Earth, Planets, and Space, 67, 79.](https://ui.adsabs.harvard.edu/abs/2015EP&S...67...79T)

## To install

    $ pip install .

## Example

```pycon
>>> from radbelt import get_flux
>>> from astropy import units as u
>>> from astropy.coordinates import EarthLocation
>>> from astropy.time import Time
>>> coords = EarthLocation(-45 * u.deg, -30 * u.deg, 500 * u.km)
>>> time = Time('2021-03-01')
>>> energy = 20 * u.MeV
>>> get_flux(coords, time, energy, 'p', 'max')
<Quantity 2642.50268555 1 / (cm2 s)>
```

## Known issues

The CCMC IGRF code has spatially varying errors of a few percent, which will
result in a striped pattern in the resulting particle flux.

[International Geomagnetic Reference Field (IGRF)]: https://www.ngdc.noaa.gov/IAGA/vmod/igrf.html
[Astropy]: https://www.astropy.org
[Community Coordinated Modeling Center (CCMC)]: https://ccmc.gsfc.nasa.gov/
"
82,nasa/GMSEC_API,C++,"ÔªøThis GMSEC API README file contains technical notes and the change summary
for the current release and historical releases.

See the associated Version Description Document (VDD) for further details
of each release.

For technical support regarding this release, or any previous version, please
contact the API Team at gmsec-support@lists.nasa.gov.


==============================================================================
= GMSEC API 4.8 release notes (October 2020)
==============================================================================
o A Ruby binding for the GMSEC API is now offered

o Support for Apache Artemis is now offered

o Python3 binding is built using Python 3.8.5


--- Change summary -----------------------------------------------------------
* API-5275 : C# binding GmsecException missing constructor that accepts Status object
* API-5276 : C# binding Status missing constructor that accepts GmsecException object
* API-5277 : Java binding GMSEC_Exception missing constructor that accepts Status object
* API-5278 : Java binding Status missing constructor that accepts a GMSEC_Exception object
* API-5289 : Provide support for Apache Artemis
* API-5304 : Resource Info Generator is not adding certain fields related to network interfaces
* API-5321 : Add getConnectionEndpoint() method to Connection and ConnectionManager
* API-5323 : ConfigFile does not reset doc-loaded flag on error
* API-5324 : InternalMistMessage should not add fields while iterating over existing fields
* API-5325 : InternalMessage should avoid unnecessary copying of fields
* API-5326 : InternalMistMessage setValue() cannot convert value to BinaryField
* API-5327 : gmsub subscribing to C2MS.TERMINATE when given a custom subject
* API-5329 : Java binding's JNIFieldConverter.cloneField() does not copy header attribute
* API-5330 : Improve efficiency by capturing host name, user name, and process ID only once
* API-5331 : Need to check values/parameters in U16 and U32 classes of Java binding
* API-5334 : Improve efficiency by not forcing Field names to uppercase
* API-5344 : Memory leak in Java binding setStandardFields()
* API-5346 : API should be able to parse XML configuration file that contains lowercase Subcription, Config, and Message definitions
* API-5347 : Augment documentation for shutdownAllMiddlewares() and shutdownMiddleware()
* API-5351 : Create a Ruby binding for the GMSEC API
* API-5355 : StringUtil getValue() can return NaN on macOS when failing to parse a floating point number
* API-5356 : Config getValue() methods that accept default value should not generate error when name is NULL or empty string
* API-5358 : Provide ability for API utility apps to accept configuration file and connection config name
* API-5360 : Flatten NDM message templates
* API-5362 : Message Bus client that has lost its connection is not provided error when attempting to publish
* API-5363 : Improve efficiency of the Message addField() methods in the Java binding
* API-5364 : OpenDDS m/w wrapper is returning incorrect library root name
* API-5365 : ProductFile can throw bad_alloc when it is unable to reserve space for binary file contents
* API-5366 : Internal Heartbeat Generator should use thread-safe objects
* API-5368 : Java JNIHeartbeatGenerator constructor should not be setting MistMessage standard fields
* API-5370 : Config getDoubleValue() in Java binding is calling wrong method to acquire the value
* API-5371 : Config getNext() seg-faults when called before getFirst() when there are no entries to fetch
* API-5372 : Build the Python3 binding of the GMSEC API using Python 3.8.5 or later
* API-5373 : Fix documentation for ConnectionManager's createHeartbeatMessage and createLogMessage
* API-5374 : ConnectionManager setStandardFields() should not throw exception if list of fields is empty
* API-5396 : ZeroMQ m/w wrapper does not remove ZMQ-REPLY-ADDRESS field from request message
* API-5397 : Resource Info Generator should handle improper division when generating metrics
* API-5404 : Update Environment Validator script for Apache Artemis
* API-5406 : MistMessage setValue() should not throw exception when converting GMSEC_I64 to GMSEC_U64
* API-5409 : Scan GMSEC API 4.8 using Fortify and address any issues
* API-5411 : Create middleware wrapper for Apache Artemis
* API-5412 : Allow for authentication credentials to be used with Generic JMS
* API-5416 : API needs to handle exception that may be thrown by message decoder
* API-5417 : IBM MQ m/w wrapper should not attempt to reconnect if user indicates no retries
* API-5418 : The GMSEC API maximum encoded message size limit cannot be represented using native int type
* API-5422 : Incorrect function being referenced to get NSS error string
* API-5425 : Floating point dependency check is calling incorrect Message method to acquire value
* API-5428 : Javadoc generated for Java binding fails on class search



==============================================================================
= GMSEC API 4.7 release notes (May 2020)
==============================================================================
o Support for 32-bit distributions of the GMSEC API has been dropped.

o Java-related facets of the GMSEC API are built using OpenJDK 11. Users can continue to use Java 8 through 11 to run Java applications.

o Support for Python (2.7) has been dropped; users should use the Python3 binding instead.

o Perl binding under Windows is no longer built with Active State Perl; Strawberry Perl is being used instead.


--- Change summary -----------------------------------------------------------
* API-4766 : The API should be able to validate dependent fields as defined by C2MS and GMSEC Addendum
* API-5242 : API should follow soft-link when attempting to determine install directory
* API-5245 : Under Windows, build the Perl binding of the GMSEC API using Strawberry Perl
* API-5248 : OpenDDS m/w occassionally crashes on shutdown
* API-5249 : Retrieval of binary data is not thread-safe in C# binding
* API-5250 : Add example programs that demonstrate basic usage of C2/C2-NSS capabilities of the GMSEC API
* API-5251 : Message validation can fail when comparing supposedly equal F32 values
* API-5252 : Tracking field insertion for C2MS (2019) Heartbeat message
* API-5253 : API should use the 2019 C2MS as the default for XSD files
* API-5255 : API is not referencing C++ header files on MacOSX
* API-5256 : Optimize InternalSpecification to skip loading unused message template files
* API-5258 : Avoid the unnecessary throwing of exception within the GMSEC API
* API-5259 : Condition class reports conflicting status
* API-5260 : By default, gmreq should not be republishing request messages
* API-5261 : Typo in error message when attempting to publish message with a non-PUBLISH message kind
* API-5266 : AMQP m/w wrapper provides vague error message when it fails to initialize
* API-5268 : Validation produces malformed error message when array fields are involved
* API-5269 : Add support for GMSEC-style wildcard characters in OpenDDS m/w wrapper
* API-5270 : Allow users to register custom message validation function
* API-5271 : Functions to create Connection Manager in C binding are not handling exceptions
* API-5272 : Perl and Python3 bindings are not properly handling GMSEC API exceptions
* API-5274 : Add a hasField() method to Message class
* API-5284 : Develop string utils that can be used to validate header string, timestamp, and IP address values that go into message fields
* API-5285 : Algorithm to determine leap-year reports incorrect results
* API-5288 : The gmreq and gmrpl utilities do not create valid C2MS messages
* API-5290 : ResourceInfoGenerator should not give up if it cannot query stats on a disk
* API-5291 : Tracking fields are not cleared from message upon receiving error from m/w wrapper
* API-5292 : ConnectionManager requestDirective() methods do not add REQUEST-ID or MSG-ID
* API-5295 : Provide the means for users to use an alias in lieu of a schema ID when building a MistMessage
* API-5296 : Provide utility app to display all available message schema IDs and associated alias (if any)
* API-5297 : Fix documentation for publish() method that accepts a Config object
* API-5299 : ConnectionManager publishLog() does not include miscellaneous elements in message subject
* API-5301 : Validate Header String, IP Address, and Timestamp contents in Fields
* API-5302 : Add getSchemaLevel() method to Specification
* API-5306 : NODE and USER-NAME fields are not C2MS compliant
* API-5307 : Add C2MS-Pipeline message templates
* API-5308 : Add middleware wrapper for ActiveMQ CMS 3.9.5
* API-5309 : API does not create message templates for all schema levels
* API-5312 : Field getIntegerValue() and getUnsignedIntegerValue() report incorrect error when a value cannot be produced
* API-5313 : MistMessage does not properly convert legacy message into newer format
* API-5315 : Duplicate message in error string when validating fields in an array
* API-5316 : Validation incorrectly identifies name of field when comparing to field template in array
* API-5317 : API's JNI MistMessage should not set standard fields when field count is zero
* API-5318 : Bolt wrapper is not reporting connection endpoint when multiple connections are used
* API-5319 : Fix Python3 and C# documentation regarding ConnectionManager's setStandardFields()
* API-5332 : Message Bus includes homemade field in request message
* API-5333 : IBM MQ m/w wrapper includes homemade field in request messages
* API-5342 : API fails to handle subscription with callback with certain topics ending with '+'
* API-5345 : Do not associate a Specification object with an InternalMistMessage


==============================================================================
= GMSEC API 4.6 release notes (June 2019)
==============================================================================

--- Change summary -----------------------------------------------------------
* API-5067 : Allow for user to set custom value for COUNTER field that is used by Heartbeat Service
* API-5102 : Concatenate Specification's parse errors into string summary to be thrown
* API-5107 : Update InternalDeviceMessage::init to leverage message templates
* API-5109 : Update InternalMnemonicMessage::init to leverage message templates
* API-5129 : Simplify Message Validation's exception string
* API-5130 : MistMessage needs data constructor that takes in data + specification
* API-5139 : Define a public-facing HeartbeatGenerator class
* API-5178 : API should check tracking fields when performing message validation
* API-5181 : Replace usages of std::auto_ptr with StdUniquePtr
* API-5183 : Update API example Heartbeat Service and Resource Service programs
* API-5185 : Add logging arguments to Bolt
* API-5186 : Fix Perl 4.x documentation
* API-5187 : Add Config constructor to the C# binding that accepts a string array
* API-5189 : MistMessage setValue() methods do not indicate if Field is a header field
* API-5190 : Internal Product File Message consructors are not initializing class member data
* API-5193 : Fix documentation issues within the SWIG-generated C# binding.
* API-5196 : INSTALL.txt does not include instructions for installing libxml2 and libxslt under Ubuntu and macOS
* API-5197 : Include JavaDoc for Java binding within gmsecapi.jar file
* API-5198 : The JAVA_LIBS definition in GMSEC API Platform configuration files is incorrect for OpenJDK use
* API-5199 : Message Bus client closes stdin file descriptor when connection to server fails
* API-5201 : Add setName() and clone() methods to Field class
* API-5203 : Log messages are always displayed when using SWIG-generated C# binding, regardless of the logging level
* API-5204 : Fix documentation issues within the SWIG-generated Python3 binding
* API-5205 : Add Config() constructor to Python3 binding that accepts a string list
* API-5206 : TimeUtil.format_time() returns SwigPyObject instead of string in Python3 binding
* API-5208 : Update GMSEC API utility apps to use ConnectionManager
* API-5209 : Trim the directory path from filename used in log messages
* API-5210 : Display file name and line number within Python3 log messages
* API-5211 : Remove AutoMutex from Perl and Python bindings
* API-5212 : Use of the Python3 binding shutdown_all_middlewares() results in dead-lock
* API-5213 : Boolean Field not created correctly when XML content uses ""1"" for field value
* API-5214 : Python3 binding does not register custom log handler with Core API
* API-5215 : Message addField(String, String) in Java binding does not allow for empty string value
* API-5216 : C# binding does not register custom log handler with Core API
* API-5217 : Identify Config.Initialize() in C# binding as being obsolete
* API-5218 : ProductFile GetContents() can throw unmanaged exception if object has no data contents
* API-5220 : Resource Service should not increment COUNTER field with each sample taken
* API-5221 : Resource Message published by Resource Service should include PUB-RATE field
* API-5223 : Scan GMSEC API 4.6 using Fortify and correct/disposition any deficiencies found
* API-5224 : Message Bus Reader Thread does not properly destroy data buffer
* API-5225 : MSG-ID field not included within C2CX HB messages generated by the API when operating with the 2014 GMSEC ISD
* API-5226 : C++ log macros cannot be compiled in extended namespace
* API-5227 : Internal ValueMap does not handle GMSEC_U32 value type
* API-5228 : Java applications cannot run on macOS when SIP is enabled
* API-5230 : Use python-config and python3-config to get compiler and linker flags
* API-5232 : GMSEC API Project File improperly references certain source modules
* API-5233 : AMQP m/w wrapper fails to free resource
* API-5234 : Generic JMS m/w wrapper failes to free resources
* API-5235 : IBM MQ (formerly WebSphere) m/w wrapper does not always retry connecting to broker
* API-5236 : Certain example programs for the SWIG-generated language bindings do not demonstrate usage of release() for disposing of received messages
* API-5237 : MistMessage needs to identify header fields when applying standard fields to message
* API-5238 : Environment Validator does not fully check if Java is installed on macOS


==============================================================================
= GMSEC API 4.5.1 release notes (February 26 2019)
==============================================================================
--- Change summary -----------------------------------------------------------
* API-5192 : API 3.x Request Message Kind should not be altered when message is published
* API-5202 : TYPE attribute should be ignored if KIND attribute is provided with Message definition


==============================================================================
= GMSEC API 4.5 release notes (February 12 2019)
==============================================================================
o Support for Microsoft Visual Studio 2008, 2010, 2012, and 2013 has been dropped. Currently the API is built only with Visual Studio 2017.

o Support for Solaris 10 SPARC has been dropped.

o Support for Ubuntu 18.04 LTS is being provided.

o Java 8 (or later version) must be used to run Bolt and/or to use the Java binding of the GMSEC API.

o This release contains an additional interface for the C# language that can be
    used under Linux (using Mono) or under Windows.

    Please refer to the example programs in GMSEC_API/examples/csharp and the
    code documentation in GMSEC_API_DOCS/manual/csharp_4x to learn more about
    the use of the new binding.

    Note: In future releases, the separate and distinct C# .NET/CLI language
    binding will no longer be provided with the GMSEC API.

o This release contains an additional interface for the Python 3 language.
    This interface is based on the Python 3.6.5 interface.

    Please refer to the example programs in GMSEC_API/examples/python3 and the
    code documentation in GMSEC_API_DOCS/manual/python3_4x to learn more about
    the use of the new binding.

o An OpenDDS middleware wrapper is now available for Windows and RHEL7.

o An IBM MQ 9.0 middleware wrapper is now available.


--- Change summary -----------------------------------------------------------
* API-4730 : The API should be able to validate the values of fields as defined by the GMSEC ISD
* API-4977 : The C# LogEntry does not contain file name or line number information for the C# source file
* API-5036 : Provide support for IBM MQ (formerly WebSphere) 9.0
* API-5043 : Build and test the GMSEC API under Ubuntu 18.04 LTS
* API-5052 : Support message validation logic in the GMSEC API using XSD files
* API-5075 : Add SWIG-generated C# binding for Linux-based Mono projects
* API-5076 : Generate Javadoc JAR file for the Java binding of the GMSEC API
* API-5080 : Update API User's Guide to remove certain unsupported features
* API-5085 : Update API User's Guide to include all tracking field enable/disable options
* API-5088 : Tracking fields are not removed from C2CX Heartbeat messages
* API-5089 : The PROCESS-ID tracking field should not always be set to a U32 field type
* API-5091 : Upgrade Visual Studio 2008 Project/Solution files to the Visual Studio 2017 format
* API-5092 : Messages with excluded subject patterns should not be aggregated when Message Binning is enabled
* API-5094 : The API does not support multi-layered schema addendums
* API-5095 : Newly defined C2CX HB message tracking fields should not be added if 2014 or 2016 ISD is specified
* API-5096 : Allow for customizable time out period for receiving WebSphere messages
* API-5097 : API should not remove user-supplied tracking field(s) from messages
* API-5098 : StringUtil's getValue<T>() method does not always report errors when given bad input
* API-5099 : InternalMistMessage not converting data values correctly with setValue()
* API-5100 : Field GetType() in C#/CLI (dotnet) binding should be renamed to GetFieldType()
* API-5101 : MistMessage needs a constructor that can convert a Message object into a MistMessage object
* API-5103 : Specification (class) is doing unnecessary work when validate() is called with NO_ENFORCEMENT configured
* API-5105 : Removed ConnectionManager's setStandardFields() exception specification
* API-5110 : Clarify API documentation to mention user needs to first add COUNTER field (to the Heartbeat Service) before the API will increment the field
* API-5111 : Bolt server displays incorrect port
* API-5112 : Remove REPLY-UNIQUE-ID from request and response messages
* API-5113 : Offer ability for users to acquire Message Specifications
* API-5114 : StringUtil::toJSON isn't escaping newline characters
* API-5116 : Fix Open-Response example programs
* API-5118 : Build the SWIG-generated C# binding of the API under Windows
* API-5119 : Refactor Specification copy-constructor
* API-5120 : Generate a 32-bit distro of the API using Visual Studio 2017
* API-5121 : Create a Python 3.x binding for the GMSEC API
* API-5122 : Rename SWIG-generated C# libraries to match GMSEC API convention
* API-5123 : Build the GMSEC API using JDK 8
* API-5124 : Setting JVM stack size proves fatal with use of Java 8
* API-5125 : Add support for IBM MQ m/w wrapper to Environment Validator and GMSEC Help utilities
* API-5126 : InternalConnectionManager does not need to check for viable Connection object
* API-5127 : The GMSEC API crashes when handling validation of a MSG.AMSG message
* API-5133 : Custom validation example programs for the GMSEC API need updating
* API-5134 : Scan GMSEC API software using HP Fortify
* API-5135 : Embellish error message when error occurs processing an XML or JSON field definition
* API-5136 : Add API method so a user can add a list of Field objects to a Message
* API-5137 : Implement middleware wrapper that supports OpenDDS
* API-5138 : Update API build framework to check if 3rd-party tools are installed
* API-5140 : Fix comments in Java 4.x binding to follow JavaDoc conventions
* API-5142 : InternalConnectionManager should create MistMessage messages
* API-5143 : Fix package names for JNI related classes
* API-5144 : Generate Doxygen style documentation for the SWIG-generated C# binding
* API-5147 : Connection unsubscribe() documentation should indicate that an exception can be thrown if error occurs
* API-5148 : Create Project and Solution files for C# example programs
* API-5149 : Update C# (dotnet) example programs to build using .NET Framework 4
* API-5150 : ActiveMQ middleware wrapper calls on unsafe ActiveMQ C++ library method
* API-5151 : Enhance gmreq utility program to allow for configurable request republish timeout period
* API-5153 : Exception is not handled by setHeartbeatServiceField()
* API-5154 : Update Environment Validator script for OpenDDS
* API-5155 : Update GMSEC Help utility application for OpenDDS
* API-5156 : Heartbeat Service should publish only one message when PUB-RATE is 0 (zero)
* API-5158 : API should use CONNECTION-ENDPOINT tracking field for C2MS compliancy
* API-5163 : Fix example programs so that they compile and that they produce legal messages
* API-5164 : Late destruction of ConnectionManager affects Java applications relying on MistMessage setStandardFields()
* API-5165 : Memory leak occurs when a received message fails validation
* API-5166 : PUB-RATE cannot be changed when restarting Heartbeat Service
* API-5167 : Unable to run GMSEC API based Java application using Java 10
* API-5168 : ConfigFile should report more concise error messages
* API-5169 : A Field definition in XML representation that spans multiple lines cannot be parsed
* API-5170 : Drop support for IBM WebSphere 7.5
* API-5171 : Sorting fields in a Message leads to a core dump
* API-5173 : API is not identifying tracking fields as header fields
* API-5174 : Python bindings do not allow for bytearray to be used for representing raw binary data
* API-5175 : Provide Python applications with the means to convert a Field object into a more specialized field object
* API-5179 : Replace usage of javah with javac to build JNI header file
* API-5182 : Depending on middleware, API publish operations may not be thread-safe
* API-5184 : ActiveMQ m/w wrapper does not log error messages from client library


==============================================================================
= GMSEC API 4.4.2 release notes (2018 March 14)
==============================================================================
--- Change summary -----------------------------------------------------------
* API-4933 : Provide IPv6 support for Message Bus
* API-5044 : Address the sections of the API which are using the deprecated Mac system-level function OSAtomicAdd32()
* API-5057 : Add option to gmpub to allow it to publish forever
* API-5070 : GMSEC_TimeSpec overloaded operators should not be defined in a namespace
* API-5073 : Example programs should run when configured with the strictest level of message validation
* API-5074 : API Makefile should not attempt to build Perl and Python bindings if swig is unavailable
* API-5077 : The XML templates used by the API for message validation do not include tracking fields
* API-5078 : Product File Message class is not inserting properly named field
* API-5081 : Build 32-bit version of GMSEC API under RHEL (CentOS) 7
* API-5082 : Config object is not passed to internal API by MistMessage constructor
* API-5083 : JMSConnection constructor is not throwing an exception on error
* API-5087 : CONNECTION-ID tracking field should be a U32 field


==============================================================================
= GMSEC API 4.4.1 release notes (2018 February 6)
==============================================================================
--- Change summary -----------------------------------------------------------
* API-4731 : Add message validation to receiving operations and expand validation configuration options
* API-4760 : Support builds on Windows 10
* API-4787 : Provide utility app for publishing a canned GMSEC message supplied by the user
* API-4798 : Add support for certain Connection class methods within Connection Manager
* API-4963 : Add cautionary statements to API User's Guide and code documentation regarding Callbacks
* API-5033 : PROCESS-ID in information bus header needs to be U32 rather than I16
* API-5039 : MnemonicMessage should readily support MVAL request-type messages
* API-5045 : Report the key value in the error when attempting to retrieve a non-existing value from a Config object
* API-5046 : Connection state should not be CONNECTED if a connection authentication failure occurs
* API-5047 : Fix logic error with Bolt's TypeValidater
* API-5048 : Perl and Python bindings should not be built if SWIG_HOME is not defined
* API-5049 : Define PERL5_LIB and PYTHON_INC within API Linux configuration files
* API-5051 : Resource Info Generator should not be adding certain fields to Resource Message
* API-5053 : Use major.minor name convention for ActiveMQ CMS library wrapper file
* API-5056 : Provide an interface to set standard fields in MistMessage
* API-5058 : Further simplify API utility programs
* API-5059 : Exceptions should not be thrown from class destructors
* API-5060 : Allow for registration of Event Callback before initializing Connection Manager
* API-5062 : Certain string characters need to be escaped or converted when generating JSON and XML data
* API-5063 : Python example programs not handling error when user passes no args
* API-5064 : Define macro that contains hex representation of API version number
* API-5065 : Avoid assuming a synchronous request is done when a response has not been received following a timeout
* API-5066 : The default republish period when issuing request messages should be GMSEC_REQUEST_REPUBLISH_NEVER (-1)
* API-5069 : Define RPATH within all GMSEC libraries built for Linux systems
* API-5072 : ConfigFileIterator of the Java binding returns non-persistent references to objects


==============================================================================
= GMSEC API 4.4 release notes (2017 Dec 15)
==============================================================================
o This release contains an additional interface for the Python language.
    This interface is based on the Python 2.x interface and is built and tested
    with CPython. Please refer to the example programs in
    GMSEC_API/examples/python and the code documentation in
    GMSEC_API_DOCS/manual/python_4x to learn more about the use of the new
    binding.

--- Change summary -----------------------------------------------------------
* API-4292 : Add subscription exclusion filters to subscription definitions in ConfigFile
* API-4446 : Consolidate API string constants, especially config keys, into a single header file
* API-4671 : Execute middleware performance testing and generate a performance test report for GMSEC API 4.2
* API-4740 : Add a tracking field which contains a list of unique subscription subjects in a process
* API-4741 : Add a tracking field which shows the server to which a GMSEC Connection is currently connected
* API-4758 : Simplify the example programs
* API-4791 : Fix inconsistencies in the values of the FIELD_CLASS attributes in XML template files used for message validation
* API-4792 : Change the UNSET type attribute to VARIABLE in the XML template files used for message validation
* API-4796 : Fix various minor issues with the XML template files used for message validation
* API-4837 : Fix Magic Bus so that it handles requests for resource information
* API-4869 : Provide support for ActiveMQ CMS 3.9.4 and OpenSSL 1.0.2
* API-4881 : Update MistMessage constructors so that the Schema ID does not require the version number of the ISD
* API-4898 : Allow applications to register an EventCallback before connecting to the bus
* API-4899 : Setting the field storage container type after a message has been created can truncate the message
* API-4900 : API Environment Validator script does not always report the version number of the API
* API-4901 : Allow users to set configuration options for messages that are received from the GMSEC Bus
* API-4921 : Field::getStringValue does not properly convert Binary, I8, or U8 Field values to strings
* API-4934 : Fix the message encoding and decoding logic which prevents Fields from being identified as a header
* API-4938 : Augment Field class to allow type of ""BINARY"" in addition to ""BLOB"" and ""BIN""
* API-4953 : Rename Doxygen documentation directories for API 3.x
* API-4955 : Users shall be able to change the rate of publication of the Heartbeat Service
* API-4964 : JNI objects created within Callbacks need to be destroyed after use
* API-4969 : The JVM stack size should be specified by the Generic JMS wrapper
* API-4973 : The Java API's registerHandler should reset to the default Java handler when invoked with null
* API-4997 : Fix the memory leak in the Java Message.getXXField() functions
* API-4999 : Add a getStandardFields function to the ConnectionManager
* API-5003 : ConnectionManager::request(no callback) and reply() should be able to validate messages
* API-5015 : MessagePopulator class doesn't add MSG-ID field
* API-5027 : Fix the memory leak in the Java ConnectionManager requestSimpleService()
* API-5028 : Fix output from Message toXML() so that the message configuration, if any, is properly indented
* API-5032 : Fix the connectionManagerCreate() function in the C binding so that it does not enable message validation
* API-5034 : MIST ignoring lower level message templates when overriding addendum is applied
* API-5035 : Infinite loop occurring when addendum is added with no header listed in directory
* API-5050 : Mark all deprecated classes and functions of the API


==============================================================================
= GMSEC API 4.3 release notes (2017 May 1)
==============================================================================
o For Mac OS X users, this release may break binary compatibility with previous
    API 4.x releases.  This is due to a change in how the API is built on OS X.
    Previously, libstdc++ was being used; now libc++ is used.  (See item 4462
    below).  This change was made to prevent a sporadic crash from occurring
    when using various API functions.

o Beginning with API 4.3, strict validation of subjects used within Messages
    and for subscriptions will be performed to ensure that they are GMSEC-
    compliant, as defined by the GMSEC Interface Specification Document.
    Users wishing to continue using non-strict validation may employ the use
    of the ""SEC-POLICY"" configuration option and set it equal to the value
    ""API3"".

o Beginning with API 4.3, support for Solaris 10 x86 and HP-UX has been dropped.

o Users of the Java language binding should be aware that the
    Connection.disconnect() and ConnectionManager.cleanup() functions now
    throw a GMSEC_Exception.  This means that any code which calls these
    functions must be encapsulated by a try..catch statement which handles
    a GMSEC_Exception, or else the application will not be able to compile.
    Previously compiled software will still be able to run, just as it did
    prior to this change.  This change was made in response to DR #4788 
    (See below) so that exceptional states can be handled more gracefully.
    
o The Perl language binding of the API is being pared down to only include
    support for synchronous operations.  Features involving asynchronous
    operations (e.g. registration of callbacks, auto-dispatcher, etc.) have
    been removed from the binding.  Additionally, LogHandler subclassing has
    been removed.  The removal of these features is due to an instability
    discovered when executing subclass implementations in a multithreaded
    process, such as when using the AutoDispatcher or invoking a custom
    LogHandler from a non-main thread.
	
o Configurable message bins (aggregated messages) now support subscription-type wildcards (e.g. '>', '*', and '+')

o The ConnectionManager, MISTMessage, DeviceMessage, ProductFileMessage,
    MnemonicMessage, and various iterator classes (Collectively referred to
    as the Messaging Interface Standardization Toolkit (MIST)) have been
    added to the 4.X Perl interface.

o The Config class can now ingest and output JSON data, as well as ingest
    strings of whitespace-delimited ""key=value"" pairs, much like command-line
    arguments used when running example programs such as gmpub and gmsub.

--- Change summary -----------------------------------------------------------
* API-4147 : Refactor implementation of the ActiveMQ Transport Listener
* API-4462 : Build API on Mac OS X using libc++ (not libstdc++)
* API-4512 : Improve efficiency of InternalSpecification
* API-4513 : Move InternalField::getValue() to StringUtil
* API-4514 : Add setSubject function to Message class
* API-4515 : Add support for custom Message validation
* API-4516 : Expand 4.X Perl Unit Testing Coverage
* API-4517 : Add support for MIST into the 4.X Perl interface
* API-4518 : Add Device, Mnemonic and Product File Iterator support in C binding
* API-4559 : Config class should be able to ingest alternative types of String data
* API-4561 : Update gmhelp to provide bind/connect connection examples for ZeroMQ
* API-4567 : Expand OS support for 4.X Perl interface
* API-4572 : Embellish error information included with exceptions thrown from Bolt
* API-4596 : Prevent MagicBus middleware wrapper from attempting to communicate with MB server once it is known that the connection has been lost
* API-4597 : Cannot create a BinaryField object with the Perl interface
* API-4598 : Comparison made against configuration string value should be case insensitive
* API-4600 : In the C++ and C example programs, unsubscribe should only be called on active subscriptions
* API-4601 : Error checking should by default be performed on subscription subjects with wildcard operators
* API-4610 : Create a Python language binding
* API-4615 : Create example programs and unit tests for Python language binding
* API-4616 : Incorrect cast types are used in Java binding when constructing C++ fields
* API-4618 : Users should be able to add or change Heartbeat Service fields
* API-4619 : Message template files do not use correct XML encoding syntax
* API-4657 : Field should be able to ingest JSON with numeric type syntax
* API-4662 : Implement a public toJSON() method for Config
* API-4669 : Config functions getFirst and getNext in Perl interface do not work
* API-4694 : Republish period for asynchronous request is not being properly handled
* API-4699 : Configurable Message Bin fails to handle message subject using wildcard element
* API-4707 : Replace use of deprecated OSAtomicAdd with alternative function for Mac OS X
* API-4717 : Remove references of API 3.x header files from API 4.x modules
* API-4721 : Address issues reported by HP Fortify
* API-4729 : Field.getString() should be Field.getStringValue() in the Java 4.X API
* API-4743 : Re-order field type and name when displaying Field XML information
* API-4744 : Issue Request-kind message when Connection Manager requestDirective() is called
* API-4751 : Make the decoding of a message packet more efficient
* API-4768 : Remove Field descriptions from validation error messages
* API-4788 : Update documentation and example programs to indicate that Connection::disconnect() can throw an exception in rare cases
* API-4803 : Non-ASCII Unicode characters cause String truncation when retrieving data from the Java language binding
* API-4804 : Improve error messages related to MnemonicMessage's dependent field RAW-VALUE
* API-4807 : The getField functions should report the name of the requested field when unable to retrieve it from a message
* API-4809 : GMSEC_Exception does not initialize base class (Exception)
* API-4840 : Fix MistMessage to allow a value of 0 (zero) for F32 fields
* API-4853 : Client applications crash when using a Magic Bus fail-over configuration and yet only one broker is running



==============================================================================
= GMSEC API 4.2.2 release notes (2016 December 16)
==============================================================================
o This is a patch release that includes a couple of bug fixes.

o (Fastpath) Improvements to Configurable Message Binning that allow users to
    configure which messages, if any, should be excluded from being binned.
    Users can also employ the use of wild-cards when indicating which messages
    to bin or exclude from binning.

--- Change summary -----------------------------------------------------------
* API-4509 : Add ability to exclude messages, using subject, from being added to Configurable Message Bins
* API-4655 : Fixed bug related to how message tracking field options are processed by the Connection class
* API-4685 : Resolve dead-lock issue when call to publish() is made within a callback that was summoned by dispatch()



==============================================================================
= GMSEC API 4.2.1 release notes (2016 November 9)
==============================================================================
o This is a patch release to correct a bug in a method that is used by the Time Utility functions.

--- Change summary -----------------------------------------------------------
* API-4609 : Fix StringUtil::getTimeFromString() so that it can process time stamps that do not include fractional time



==============================================================================
= GMSEC API 4.2 release notes (2016 October 31)
==============================================================================
o This release contains an additional interface for the Perl language.
	This new interface exists in parallel with the existing one, so all
	existing software that utilizes the GMSEC API will continue to function
	without interruption. However, new software development should utilize
	this new interface, as it is safer and leaner than the existing interface.
	
o User-defined message validation has been added to the Messaging Interface
	Standardization Toolkit (MIST) portion of the API.  The ConnectionManager
	and Specification classes can now be configured to validate user-defined
	messages, as would be defined in an addendum to the GMSEC ISD.  More
	information on this functionality is outlined in the GMSEC API User's Guide
	in the section named, ""Addendums to the ISD.""
	
o (Fastpath) The API's internal classes have been refactored to use Exceptions
	for error handling rather than checking on a Status object returned from
	each step of an operation.  This has in turn improved the efficiency of
	the API by reducing the number of instruction cycles the CPU must execute
	to perform operations such as publishing and receiving messages.
	
--- Change summary -----------------------------------------------------------
* API-3844 : Implement MIST Build 3
* API-3963 : Add source file name and line number to the Java LogEntry
* API-4122 : Add Perl interface for core (non-MIST) API 4.x
* API-4156 : Refactor Core API C++ Framework
* API-4334 : Fastpath: Streamline Connection publish() and receive() with seamless enhancements
* API-4392 : Add methods that allow users to easily get native value from a Field object
* API-4396 : Protect against concurrent access to the list of pending requests in RequestThread
* API-4399 : Prevent overwriting of EVENT-TIME field in MIST message field populator
* API-4400 : Java binding of API fails to catch exceptions thrown by the C++ layer
* API-4435 : Fastpath: Configurable Message Bins
* API-4444 : Catch exceptions in StdThread and remove include statements for API 3.x header files
* API-4481 : Stop Resource Message Service when ConnectionManager is destroyed
* API-4490 : Allow for XML or JSON data containing fields with an empty value to be parsed without error
* API-4501 : Add missing Time Utility functions to C binding of API
* API-4506 : Implement Mnemonic addSample() within JNI-layer of Java binding
* API-4519 : Increment COUNTER field in C2CX Heartbeat Messages
* API-4537 : Fix Java binding to return correct StatusClassification and StatusCode
* API-4570 : Prevent memory leak when Message class copy-constructor is called
* API-4571 : Bolt Server should report version information that correlates with the version of the API
* API-4582 : Fix Bolt middleware wrapper so that it does not throw an exception when a timeout occurs while attempting to receive a message
* API-4591 : Fix Message class to support U64 fields
* API-4595 : Make InternalConnection's insertTrackingFields() method thread-safe
* API-4606 : Report correct message kind after ingesting XML or JSON data
* API-4607 : Mitigate thread-safe issue within AMQP middleware wrapper.



==============================================================================
= GMSEC API 4.1 release notes (2016 July 25)
==============================================================================

o IMPORTANT: This version corrects a mild memory leak in the 4.X Java Callback
	handling system. All 4.X interface callbacks are now abstract classes,
	instead of interfaces. This requires all code using these classes to 
	change their reference to Callback, ReplyCallback, and EventCallback to
	use the keyword ""extends"" instead of ""implements"".
	This change was necessary to ensure that callback memory is properly cleaned
	up.
	Client programs previously built with version 4.0 or 4.0.1 of the API should
	be recompiled if the plan is to use API 4.1; otherwise undefined runtime
	behavior can be expected.

o Corrected several coding defects identified by static code analysis

--- Change summary -----------------------------------------------------------
* API-4250 : Address Fortify issues within 4.0.1 (see important note above)
* API-4295 : Incorporate memory management corrections in C# binding
* API-4332 : Config class allows merging of config objects
* API-4353 : Correct logic in autodispatcher shutdown timing
* API-4357 : API will now warn users if log file cannot be opened	
* API-4358 : C2 NSS wrapper now frees resources (Compat C2 version only)
* API-4375 : ZeroMQ wrapper now employes the zero copy concept
* API-4385 : ConnectionManager requestDirective() method was not setting appropriate value for RESPONSE field
* API-4390 : WebSphere middleware wrapper asynchronous status check fails to dispatch event
* API-4391 : MagicBus fails to deliver messages to subscribers when two brokers are being used



==============================================================================
= GMSEC API 4.0.1 release notes (2016 May 16)
==============================================================================

o Added new Connection and Publish-level Config options which expose the
	WebSphere MQ Asynchronous Put Response client option.  This new option
	allows users to toggle this functionality on or off and potentially
	achieve a higher throughput rate of message traffic when using
	the WebSphere MQ middleware.  For more information on this topic,
	please see the Installation and Configuration Guide Section 7.7
	as well as the current and legacy gmpub_wsmq_async example programs.

o Added the PublishWithConfig function to the 3.X C.v1 and C.v2 interfaces.

--- Change summary -----------------------------------------------------------
* API-4274 : Exposure of WebSphere MQ Asynchronous Put Response Functionality



==============================================================================
= GMSEC API 4.0 release notes (2016 April 04)
==============================================================================

o This release contains an additional interface for the Java, C, C++, and C#
 	languages. This new interface exists in parallel with the existing one, 
 	so all existing software that utilizes the GMSEC API will continue to 
 	function without interruption. However, new software development should 
 	utilize this new interface, as it is safer and leaner than the existing
 	interface.
 	
o This release also contains an expanded set of functionality for the 
	Messaging Interface Standardion Toolkit (MIST). Available for the new 
	interface only, this functionality includes validation of all GMSEC ISD
	described messages, automatic production of resource messages, and 
	simplified production of Mnemonic, Device, and Product messages. For more
	information, please see the GMSEC API User's Guide or the doxygen 
	for the ConnectionManager (available in GMSEC_API\docs\cpp_4x)

o New middleware supported with this release are Rabbit MQ via AMQP 1.0 and ZeroMQ. 
	
--- Change summary -----------------------------------------------------------
* API-4171 : ZeroMQ wrapper drops messages due to interrupted function calls
* API-4146 : ActiveMQ Durable Subscription functionality has been corrected to work with separate Connection sessions.
* API-4136 : Added ability for users to provide a Config object to a Java Subscribe call
* API-4110 : Add support for anonymous classes in new Java interface
* API-4109 : Add automatic flushing of cached WebSphereMQ topic handles
* API-4069 : Add ability to disable request/reply operations for a Connection object, to simplify topic management
* API-4066 : Correct memory leak in open response reply delivery
* API-4047 : Conform new C# interface functions to industry standard
* API-4009 : Add support for U64 field to new interface
* API-4004 : Add support for ZeroMQ middleware, see API User's Guide for details
* API-3971 : EventCallback reports the ConnectionEvent which resulted in the Callback invocation
* API-3959 : Repair validation error in InteralFieldDescriptor
* API-3924 : Add ability for JSON formatted GMSEC messages to be used as the middleware payload
* API-3905 : Add asynchronous message publishing
* API-3836 : Add JSON support to the Message and Field interfaces
* API-3811 : Add configurable TCP buffer to Bolt middleware client
* API-3777 : Add ""+"" operator to allowed subscription wildcard set
* API-3747 : Allow configuration of backlog queue size for the listen-socket used by Bolt server
* API-3737 : Add support for ActiveMQ CMS 3.8.4
* API-3639 : Add ability for API to report asynchronous middleware events
* API-3622 : Add support for U8
* API-3621 : Correct issue with Message Bus error class reporting
* API-3605 : Normalize middleware disconnect behavior
* API-3600 : Add support for current Mac versions
* API-3576 : Add support for AMQP 1.0, tested with Rabbit MQ
* API-3570 : Correct bug in Connection::Unsubscribe(), where subscription count was not correctly updated
* API-3550 : Correct issue in CMSConnection::GetMWINFO() that could product a segfault.
* API-3548 : Implement connection global unique identifier
* API-3451 : Correct issue with MIST loading templates on Linux
* API-3443 : Create new interface for Java, C, C++, C#
* API-3421 : Add new functional block for MIST
* API-3411 : Correct ConfigFile error handling
* API-3377 : Add exclusion filters to connection to allow simpler message filtering
* API-2604 : Add support for WebSphere MQ 8
* API-2291 : Add support for Component style configuration to ease XML configuration loading


[For change descriptions from prior releases of the GMSEC API, please send your request to gmsec-support@lists.nasa.gov]
"
83,nasa/bingo,Python,"![Bingo Logo](media/logo.png)

master: [![Build Status](https://travis-ci.com/nasa/bingo.svg?branch=master)](https://travis-ci.com/nasa/bingo) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingo/badge.svg?branch=master)](https://coveralls.io/github/nasa/bingo?branch=master)

develop: [![Build Status](https://travis-ci.com/nasa/bingo.svg?branch=develop)](https://travis-ci.com/nasa/bingo) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingo/badge.svg?branch=develop)](https://coveralls.io/github/nasa/bingo?branch=develop) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/9fe09cffafe64032962a82f7f1588e9f)](https://www.codacy.com/app/bingo_developers/bingo?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=nasa/bingo&amp;utm_campaign=Badge_Grade)

## General
Bingo is an open source package for performing symbolic regression, Though it 
can be used as a general purpose evolutionary optimization package.  

### Key Features
  * Integrated local optimization strategies
  * Parallel island evolution strategy implemented with mpi4py
  * Coevolution of fitness predictors
  
### Note
At this point, the API is still in a state of flux. The current release has a 
much more stable API but still lacks some of the features of older releases.

## Getting Started

### Cloning with git
The Bingo repository uses git submodules so make sure to clone all the
submodules when cloning.  Git has an easy way to do this with:
```shell
git clone --recurse-submodules ...
```

### Dependencies
Bingo is intended for use with Python 3.x.  Bingo requires installation of a 
few dependencies which are relatively common for data science work in python:
  - numpy
  - scipy
  - matplotlib
  - mpi4py (if parallel implementations are to be run)
  - pytest, pytest-mock (if the testing suite is to be run)
  
A `requirements.txt` file is included for easy installation of dependencies with 
`pip` or `conda`.

Installation with pip:
```shell
pip install -r requirements.txt
```

Installation with conda:
```shell
conda install --yes --file requirements.txt
```

### BingoCpp
A section of bingo is written in c++ for increased performance.  In order to 
take advantage of this capability, the code must be compiled.  See the 
documentation in the bingocpp submodule for more information.

Note that bingo can be run without the bingocpp portion, it will just have lower 
performance.

If bingocpp has been properly installed, the following command should run 
without error.
```shell
python -c ""from bingocpp""
```

A common error in the installation of bingocpp is that it must be built with 
the same version of python that will run your bingo scripts.  The easiest way 
to ensure consistent python versioning is to build and run in a Python 3 
virtual environment.

### Documentation
Sphynx is used for automatically generating API documentation for bingo. The 
most recent build of the documentation can be found in the repository at: 
`doc/_build/html/index.html`


## Running Tests
An extensive unit test suite is included with bingo to help ensure proper 
installation. The tests can be run using pytest on the tests directory, e.g., 
by running:
```shell
python -m pytest tests 
```
from the root directory of the repository.

## Usage Examples
The best place to get started in bingo is by going through the jupyter notebook
tutorials in the [examples directory](examples/). They step through several of
the most important aspects of running bingo with detailed explanations at each
step.  The [examples directory](examples/) also contains several python scripts
which may act as a good base when starting to write your own custom bingo
scripts.


## Contributing
1. Fork it (<https://github.com/nasa/bingo/fork>)
2. Create your feature branch (`git checkout -b feature/fooBar`)
3. Commit your changes (`git commit -am 'Add some fooBar'`)
4. Push to the branch (`git push origin feature/fooBar`)
5. Create a new Pull Request


## Versioning
We use [SemVer](http://semver.org/) for versioning. For the versions available, 
see the [tags on this repository](https://github.com/nasa/bingo/tags). 

## Authors
  * Geoffrey Bomarito
  * Tyler Townsend
  * Jacob Hochhalter
  * Ethan Adams
  * Kathryn Esham
  * Diana Vera
  
## License 
Copyright 2018 United States Government as represented by the Administrator of 
the National Aeronautics and Space Administration. No copyright is claimed in 
the United States under Title 17, U.S. Code. All Other Rights Reserved.

The Bingo Mini-app framework is licensed under the Apache License, Version 2.0 
(the ""License""); you may not use this application except in compliance with the 
License. You may obtain a copy of the License at 
http://www.apache.org/licenses/LICENSE-2.0 .

Unless required by applicable law or agreed to in writing, software distributed 
under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR 
CONDITIONS OF ANY KIND, either express or implied. See the License for the 
specific language governing permissions and limitations under the License.
"
84,nasa/uam-apis,Shell,
85,nasa/swSim,C++,"# swSim

Project Name: Solid Wave Sim (swSim)
Version 1.0
Authors: Erik Frankforter, Elizabeth Gregory, Bill Schneck
Description:
Solid Wave Simulation (swSim) is software that solves heterogeneous, anisotropic elastodynamic equations for ultrasonic NDE simulation. A time-explicit staggered grid finite difference (FD) scheme is employed and solved on graphics processing units (GPUs). Parallelism via the Message Passing Interface (MPI) allows for deployment on a cluster, or on a single computer with one or more GPUs. Wavefield data is output using Visualization Toolkit (VTK) file formats for 3D rendering capabilities via open source tools, and a kernel composition module allows high-level registration of sequences of matrix operations, providing extensibility in equations and physics solved.

# License: 
Copyright 2020 United States Government as represented by the Administrator of the National 
Aeronautics and Space Administration. No copyright is claimed in the United States under 
Title 17, U.S. Code. All Other Rights Reserved. See Appendix A for 3rd party licenses.

The Solid-Wave Sim (swSIM) platform is licensed under the Apache License, Version 2.0 (the 
""License""); you may not use this file except in compliance with the License. You may obtain 
a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. 

Unless required by applicable law or agreed to in writing, software distributed under the 
License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, 
either express or implied. See the License for the specific language governing permissions 
and limitations under the License.

# Notices:

Googletest is a product of Google Inc. and is subject to the following:
 
Copyright 2008, Google Inc. All rights reserved.
           
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
* Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.

# Documentation

Available at https://nasa.github.io/swSim/

# Building swSim
1. Download latest release
2. Ensure correct dependencies are installed, and are in the search path
  - MPI
  - VTK >= 9.0 (built and linked with the same MPI being used for PanNDE)
  - gcc >= 7.3
  - cmake >= 3.13.5
  - libxml2 >-2.9.10
  - cuda >= 10.1
3. Ensure Python3 is installed for some included post processing utilities
  - this is optional, if the utilities are not desired
4. cd to `swSimroot/directory`
5. generate compilation instructions using `cmake -B build -S .`
6. compile with `make -C build`Create build files using CMAKE 3.13 or higher


# Creating a Case

The testing directory contains four test cases:
* 30x30ElongatedQausiIsoCFRPTestPlate <- necessary for Code Testing
* 120x60CrossPlyCFRPTestPlate
* 120x60CrossPlyCFRPTestPlate_course 
* 180x180ElongatedQausiIsoCFRPTestPlate

Each of these subdirectories contain a .py file that can be ran to generate the required input files. We recommend creating scripts to generate the input files. 


# Running a Case

mpiexec -np <number_of_processes> ./bin/rsg_sim Path/to/Inputs.xml

# Input File Requirements
 * Envelope size in voxels (x, y, & z).
 * Size of voxels in meters (x, y, & z).
 * Root name of the geometry file (example: 'Geometry' for Geometry.raw and Geometry_planCounts.raw)
 * Name of the rotations file (example: 'rotations.bin')
 * Total number of materials 
 * Materials with number corresponding to the values in the geometry file:
    - density
    - 21 elements or the stiffness Matrix
 * Total number of time steps to be simulated.
 * Size of the time step in seconds
 * Total number of excitations
 * Excitations with unique ID number:
    - radius of the excitation
    - location of the excitation in meters
    - initial time step the excitation is active
    - the duration of the excitation in time steps 
    - the orientation of the excitation (x, y, & z)
    - a scaling vector of the excitation (x, y, & z)





"
86,nasa/api-docs,SCSS,"# api-docs: gh-pages

### nasa/api-docs repository contains the front-end for http://api.nasa.gov/. This site is currently CNAME mapped to NASA domain, but in the event the page is down, you can access it through Github Pages here: http://nasa.github.io/api-docs/ .

This is the current iteration of api.nasa.gov.

This project contains API listings of available NASA public APIs and well as a GSA API key form to allow NASA branded API pages to be integrated with the GSA api.data.gov system. Our listing is currently **incomplete** as we are currently displaying the more simple API's in our .json storage system.

For other US government agency API pages, see https://api.data.gov/

## Features

* Obtain an API key by filling out the form in the **Generate API Key** Section
* Information about the usage capabilities of your API in the **Authentication** Section
* A list of a few NASA publics API's and how to use them in the **Browse APIs** Section


## Libraries and Software

api.nasa.gov utilizes the following libraries and resourses:

* NASA Web Design Service(CSS and JS): A style system created in order to help NASA websites have a more unified look. You can find their info here https://nasa.github.io/nasawds-site/
* JQuery (JS): A library that simplifies the javascript coding process

The API information that our site hosts is currently archived in our *apis.json* folder which is then read and generated into the page dynamically.

## NOTICE: NASA does not host/develop these APIs
We only map the orginal developer's endpoint to one of our api.nasa.gov endpoints. Basically, api.nasa.gov acts as a passthrough service. Please do not contact us with technical questions about a particular API as <b> we do not have access to most API production environments</b>. You can follow links on our site to get more information about each API and use the contact information on those pages to reach the people who control each API.

### If you are a NASA civil servant or contractor and wish to add an API to api.nasa.gov, please contact <a href=""mailto.nasa-data@lists.arc.nasa.gov"">nasa-data@lists.arc.nasa.gov</a>.

**Site Developer**: Jenna Horn

**NASA Official**: <a href=""mailto.Brian.a.Thomas@nasa.gov"">Brian Thomas</a>
"
87,nasa/harmony-regression-tests,Python,"# Harmony Regression Tests

# Running the Tests

Each test suite is run in a separate Docker container using a temporary image built at test time.
`conda` is used for dependency management. The two steps for each test suite are building and
running the associated image.

## Install Prerequisites

* [Docker](https://www.docker.com/get-started)

## Set Up Authentication
Create a .netrc file in the `test` directory of this repository. It must contain
credentials for on logging into [Earthdata Login production](https://urs.earthdata.nasa.gov)
for tests run against Harmony production or
[Earthdata Login UAT](https://uat.urs.earthdata.nasa.gov) for Harmony SIT and
UAT environments.

Example `test/.netrc` that can log into both environments:

    machine urs.earthdata.nasa.gov login SOME_USER password SOME_PASSWORD
    machine uat.urs.earthdata.nasa.gov login SOME_UAT_USER password SOME_UAT_PASSWORD

This file will be copied to into the docker images and used when running the
notebooks. The `.gitignore` file will prevent this from being committed to the
git project, but we recommend providing a user account with minimal privileges
for test purposes.

## Build the Images

    $ cd test
    $ make images

`make -j images` can be used to make the images in parallel (faster), although this may lead to
Docker Desktop instabilities

## Run the Notebooks

    $ cd test
    $ export HARMONY_HOST_URL=<url of Harmony in the target environment>
    $ ./run_notebooks.sh

Outputs will be in the `output` directory.
`HARMONY_HOST_URL` for SIT would be `https://harmony.sit.earthdata.nasa.gov`

# Running the Tests in an AWS for Same-Region Data Access

Harmony runs in the AWS us-west-2 region and offers additional access methods for
clients running in the same region.  We have provided a Terraform deployment to
ease test execution in this region.

## Create Terraform Autovars File
In the `terraform` directory create a file called `key.auto.tfvars` and
add a single line indicating the name of the ssh public key file that
should be used for the EC2 instance that runs the notebooks.

This file name is the name of the S3 file created in the Harmony ssh key bucket as described in the Harmony project README.md.

Example:
```
key_name = ""harmony-sit-my-key-name""
```

## Execute the Tests

**Important**: The following steps allocate resources in AWS. To ease repeated
tests and troubleshooting, they also don't automatically clean up the instance
they create.  See ""Clean Up Test Resources"" to ensure you are minimizing costs
by cleaning up resources.

First create a `.env` file in the top level directory by copying in the `dot_env` file and filling
in the proper values. Then execute the following.

    $ cd script
    $ export HARMONY_ENVIRONMENT=<uat|sit|sandbox|prod>
    $ ./test.sh

Output will be in the bucket specified with the `REGRESSION_TEST_OUTPUT_BUCKET`
environment variable with a folder for each notebook.

## Clean Up Test Resources

The prior scripts do not clean up allocated resources.  To remove the resources
used to run the test, run.

    $ terraform destroy

Tests outputs are not automatically deleted.

# Notebook Development

Notebooks and support files should be placed in a subdirectory of the `test` directory.

For example, in the `harmony` directory we have

```
‚îú‚îÄ‚îÄ Harmony.ipynb
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ environment.yaml
‚îî‚îÄ‚îÄ util.py
```

 Notebook dependencies should be listed in file named `environment.yaml` at the top level of the
 subdirectory. The `name` field in the file should be `papermill`. For example:

 ```yaml
 name: papermill
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - jupyter
  - requests
  - netcdf4
  - matplotlib
  - papermill
  - pytest
  - ipytest
```

## Generating a Dependency Lockfile
To increase runtime efficiency, the build relies on [conda-lock](https://pypi.org/project/conda-lock/). This is used to create a dependency lockfile that can be used
by conda to more efficiently load dependencies. The Docker build expects a lockfile
named `conda-linux-64.lock` to exist at the top level of a notebook directory (next to
the `environment.yaml` file).

To build the lockflie install `conda-lock` by following the directions provided on its website. Then generate the lockfile for your notebook by running the following:
```
conda-lock -f environment.yaml -p linux-64
```

Test notebooks should not rely on other forms of dependency management or expect user input.
They _should_ utilize the `harmony_host_url` global variable to communicate with Harmony
or to determine the Harmony environment. This variable is set by `papermill` - see the
`Harmony.ipynb` for how to make use of this variable. More information can be found
in the [papermill](https://papermill.readthedocs.io/en/latest/usage-parameterize.html)
documentation on setting parameters.

New test suites must be added to the `Makefile`. A new `name-image` target (where name is the name of
the test suite) should be added (see the `harmony-image` example), and the new image target
should be added as a dependency of the `images` target. The docker image should have a name like
`harmony/regression-tests-<base_name>`, where `base_name` is the name of the test suite.

Finally, add the image base name to the `images` array on line 6 of the `run_notebooks.sh` file.
For instance, if the image is named `harmony/regression-tests-foo`, then we would add `foo` to the
array.

The `run_notebooks.sh` file can be used as described above to run the test suite. Notebooks are
expected to exit with a non-zero exit code on failure when run from `papermill`.
"
88,nasa/sch_lab,C,"![Static Analysis](https://github.com/nasa/sch_lab/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/sch_lab/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : App : Scheduler Lab

This repository contains NASA's Scheduler Lab (sch_lab), which is a framework component of the Core Flight System.

This lab application is a non-flight packet scheduler application for the cFS Bundle. It is intended to be located in the `apps/sch_lab` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes sch_lab as a submodule), which includes build and execution instructions.

sch_lab is a simple packet scheduler application with a one second resoluton.

To change the list of packets that sch_lab sends out, edit the schedule table located in the platform include file: fsw/platform_inc/sch_lab_sched_tab.h

## Version History

### Development Build: v2.4.0-rc1+dev32

- Removes app registration call, `CFE_ES_RegisterApp()` since applications do not need to register themselves.
- See <https://github.com/nasa/sch_lab/pull/76>

### Development Build: v2.4.0-rc1+dev26

- Fix #67, Update sequence count in transmitted messages
- Fix #69, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/sch_lab/pull/71>

### Development Build: 2.4.0-rc1+dev12

- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the ""raw"" message cmd/tlm types in definition
- See <https://github.com/nasa/sch_lab/pull/59>

### Development Build: 2.4.0-rc1+dev9

- Update to use MSG module. Replaces deprecated SB APIs with MSG
- See <https://github.com/nasa/sch_lab/pull/58>

### Development Build: 2.4.0-rc1+dev6

- Adds header guard (the other warning on the ticket was already resolved)
- See <https://github.com/nasa/sch_lab/pull/53>

### Development Build: 2.4.0-rc1+dev2

- Reorganize the sch_lab table so it matches the sample_app usage and format.
- See <https://github.com/nasa/sch_lab/pull/52>

### Development Build: 2.3.0+dev37

- Fixes schedule table documentation
- Add baseline and build number to version reporting
- See <https://github.com/nasa/sch_lab/pull/48>

### Development Build: 2.3.7

- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.
- See <https://github.com/nasa/sch_lab/pull/41>

### Development Build: 2.3.6

- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.
- See <https://github.com/nasa/sch_lab/pull/39>

### Development Build: 2.3.5

- Improved table handling
- sch_lab now builds on Raspbian OS
- Minor updates (see <https://github.com/nasa/sch_lab/pull/36>)

### Development Build: 2.3.4

- Fix for clean build with OMIT_DEPRECATED
- Minor updates (see <https://github.com/nasa/sch_lab/pull/35>)

  ### Development Build: 2.3.3

- Minor updates (see <https://github.com/nasa/sch_lab/pull/28>)

  ### Development Build: 2.3.2

- Table definition include update (see <https://github.com/nasa/sch_lab/pull/18>)

  ### Development Build: 2.3.1

- Minor updates (see <https://github.com/nasa/sch_lab/pull/16>)

### _**OFFICIAL RELEASE: 2.3.0 - Aquila**_

- Minor updates (see <https://github.com/nasa/sch_lab/pull/13>)
- Not backwards compatible with OSAL 4.2.1
- Released as part of cFE 6.7.0, Apache 2.0

### _**OFFICIAL RELEASE: 2.2.0a**_

- Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

As a lab application, extensive testing is not performed prior to release and only minimal functionality is included.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
89,nasa/ci_lab,C,"![Static Analysis](https://github.com/nasa/ci_lab/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/ci_lab/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : App : Command Ingest Lab

This repository contains NASA's Command Ingest Lab (ci_lab), which is a framework component of the Core Flight System.

This lab application is a non-flight utility to send commands to the cFS Bundle. It is intended to be located in the `apps/ci_lab` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes ci_lab as a submodule), which includes build and execution instructions.

ci_lab is a simple command uplink application that accepts CCSDS telecommand packets over a UDP/IP port. It does not provide a full CCSDS Telecommand stack implementation.

## Version History

### Development Build: v2.4.0-rc1+dev39

- Removes unnecessary call to `CFE_ES_RegisterApp()` since app registration is done automatically.
- Demonstrates the use of the Zero Copy API. Adds a step that obtains a buffer prior to calling `OS_SocketRecvFrom` then transmits that same buffer directly rather than copying it.
- See <https://github.com/nasa/ci_lab/pull/85>

### Development Build: v2.4.0-rc1+dev30

- Use `cfe.h` instead of individual headers
- See <https://github.com/nasa/ci_lab/pull/78>

### Development Build: v2.4.0-rc1+dev25

- Fix #74, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/ci_lab/pull/76>

### Development Build: v2.4.0-rc1+dev14

- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the ""raw"" message cmd/tlm types in definition
- See <https://github.com/nasa/ci_lab/pull/65>

### Development Build: v2.4.0-rc1+dev8

- Replaces deprecated SB API's with MSG
- No behavior impact, removes undesirable pattern use of `OS_PACK`
- See <https://github.com/nasa/ci_lab/pull/60>

### Development Build: v2.4.0-rc1+dev2

- Update the SocketID field to be `osal_id_t` instead of uint32
- Set Revision number to 99 for development version identifier in telemetry
- See <https://github.com/nasa/ci_lab/pull/56>


### Development Build: v2.3.0+dev36

- Add build name and build number to version reporting
- See <https://github.com/nasa/ci_lab/pull/53>

### Development Build: v2.3.5

- Replace references to `ccsds.h` types with the `cfe_sb.h`-provided type.
- See <https://github.com/nasa/ci_lab/pull/50>

### Development Build: v2.3.4

- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.
- Minor changes, see <https://github.com/nasa/ci_lab/pull/47>

### Development Build: v2.3.3

- Offset UDP base port in multi-cpu configurations
- Minor changes, see <https://github.com/nasa/ci_lab/pull/44>

### Development Build: v2.3.2

- Use OSAL socket API instead of BSD sockets
- Remove PDU introspection code
- Update command and telemetry logic
- Collect globals as a single top-level global structure
- Minor changes, see <https://github.com/nasa/ci_lab/pull/38>

### Development Build: v2.3.1

- Code style and enforcement (see <https://github.com/nasa/ci_lab/pull/31>)

### _**OFFICIAL RELEASE: v2.3.0 - Aquila**_

- Minor updates (see <https://github.com/nasa/ci_lab/pull/12>)
- Not backwards compatible with OSAL 4.2.1
- Released as part of cFE 6.7.0, Apache 2.0

### _**OFFICIAL RELEASE: v2.2.0a**_

- Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

Dependence on cfe platform config header is undesirable, and the check is not endian safe. As a lab application, extensive testing is not performed prior to release and only minimal functionality is included.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
90,nasa/to_lab,C,"![Static Analysis](https://github.com/nasa/to_lab/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/to_lab/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : App : Telemetry Output Lab

This repository contains NASA's Telemetry Output Lab (to_lab), which is a framework component of the Core Flight System.

This lab application is a non-flight utility to downlink telemetry from the cFS Bundle. It is intended to be located in the `apps/to_lab` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes to_lab as a submodule), which includes build and execution instructions.

to_lab is a simple telemetry downlink application that sends CCSDS telecommand packets over a UDP/IP port. The UDP port and IP address are specified in the ""Enable Telemetry"" command. It does not provide a full CCSDS Telecommand stack implementation.

To send telemtry to the ""ground"" or UDP/IP port, edit the subscription table in the platform include file: fsw/platform_inc/to_lab_sub_table.h. to_lab will subscribe to the packet IDs that are listed in this table and send the telemetry packets it receives to the UDP/IP port.

## Version History

### Development Build: v2.4.0-rc1+dev47

- Removes app registration call, `CFE_ES_RegisterApp()` since applications do not need to register themselves.
- See <https://github.com/nasa/to_lab/pull/97>

### Development Build: v2.4.0-rc1+dev41

-  Use `cfe.h` header file
- See <https://github.com/nasa/to_lab/pull/91>

### Development Build: v2.4.0-rc1+dev38

- Fix #85, Remove numeric pipe ID from event printf
- Fix #87, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/to_lab/pull/89>

### Development Build: 2.4.0-rc1+dev32

- Removes end-of-function comments in `to_lab_app.c`
- Adds static analysis and code format check to continuous integration workflow. Updates workflow status badges in ReadMe
- Adds CodeQL analysis to continuous integration workflow
- See <https://github.com/nasa/to_lab/pull/84>

### Development Build: 2.4.0-rc1+dev21

- TO remains command-able after a ""remove all subscriptions"" command; the command now only removes all subscriptions to the Tlm_pipe
- See <https://github.com/nasa/to_lab/pull/75>

### Development Build: 2.4.0-rc1+dev17

- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the ""raw"" message cmd/tlm types in definition
- See <https://github.com/nasa/to_lab/pull/70>

### Development Build: 2.4.0-rc1+dev13

- Replaces deprecated SB API's with MSG
- See <https://github.com/nasa/to_lab/pull/65>

### Development Build: 2.4.0-rc1+dev9

- Update the TLMsockid field to be `osal_id_t` instead of uint32
- Set revision number to 99 to indicate development status in telemetry
- See <https://github.com/nasa/to_lab/pull/59>

### Development Build: 2.4.0-rc1+dev6

- Adds header guard to `to_lab_sub_table.h`
- See <https://github.com/nasa/to_lab/pull/59>

### Development Build: 2.4.0-rc1+dev3

- Remove reference to deprecated `CFE_ES_SHELL_TLM_MID`.
- See <https://github.com/nasa/to_lab/pull/58>

### Development Build: 2.3.0+dev45

- Fixes bug where an unset address values caused subscriptions to MsgId 0 over 200 times. Added a `TO_UNUSED` entry at the end of the subscription list and a break in the subscription loop when `TO_UNUSED` found. No more subscriptions on the unused table slots (no MsgId 0 subscriptions).
- Corrects return value of `TO_LAB_init()` to be `int32` instead of `int`. Declaration now matches definition, and app builds without errors.
- Add build number and baseline to version reporting.
- See <https://github.com/nasa/to_lab/pull/53>

### Development Build: 2.3.7

- Makes the `TO_LAB_Subs` table into a CFE_TBL-managed table.
- See <https://github.com/nasa/to_lab/pull/46>


### Development Build: 2.3.6

- Replace references to `ccsds.h` types with the `cfe_sb.h`-provided type.
- See <https://github.com/nasa/to_lab/pull/44>

### Development Build: 2.3.5

- Apply code style
- See <https://github.com/nasa/to_lab/pull/43>

### Development Build: 2.3.4

- Configure the maximum depth supported by OSAL, rather than a hard coded 64.
- See <https://github.com/nasa/to_lab/pull/39>

### Development Build: 2.3.3

- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.
- Deprecates shell tlm subscription
- Changes to documentation
- See <https://github.com/nasa/to_lab/pull/38>

### Development Build: 2.3.2

- Use OSAL socket API instead of BSD Sockets

- Use global namespace to isolate variables

- Minor updates (see <https://github.com/nasa/to_lab/pull/27>)

### Development Build: 2.3.1

- Fix for a clean build with OMIT_DEPRECATED
- Minor updates (see <https://github.com/nasa/to_lab/pull/26>)

### _**OFFICIAL RELEASE: 2.3.0 - Aquila**_

- Minor updates (see <https://github.com/nasa/to_lab/pull/13>)

- Not backwards compatible with OSAL 4.2.1

- Released as part of cFE 6.7.0, Apache 2.0

### _**OFFICIAL RELEASE: 2.2.0a**_

- Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

As a lab application, extensive testing is not performed prior to release and only minimal functionality is included.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
91,nasa/sample_app,C,"![Static Analysis](https://github.com/nasa/sample_app/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/sample_app/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : App : Sample

This repository contains a sample application (sample_app), which is a framework component of the Core Flight System.

This sample application is a non-flight example application implementation for the cFS Bundle. It is intended to be located in the `apps/sample_app` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes sample_app as a submodule), which includes build and execution instructions.

sample_app is an example for how to build and link an application in cFS. See also the skeleton_app (<https://github.com/nasa/skeleton_app>) if you are looking for a bare-bones application to which to add your business logic.

## Version History

### Development Build: 1.2.0-rc1+dev62

- Removes app registration call, `CFE_ES_RegisterApp()` since applications do not need to register themselves.
- Apply standard header guard on all header files by removing leading underscore. Convert file-scope block comments to doxygen format.
- See <https://github.com/nasa/sample_app/pull/145>


### Development Build: 1.2.0-rc1+dev56

- Replaces <> with "" in local includes
- Adds CONTRIBUTIING.md that links to the main cFS contributing guide.
- Adds a description for the requirements of command and telemetry Message IDs to explain why the Msg IDs have those requirements in documentation.
- See <https://github.com/nasa/sample_app/pull/137>

### Development Build: 1.2.0-rc1+dev48

- Fix #126, simplify build to use wrappers and interface libs
- Fix #128, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/sample_app/pull/130>

### Development Build: 1.2.0-rc1+dev37

- Documentation: Add `Security.md` with instructions on reporting vulnerabilities
- Resolves bug where success code was reported as an error for `CFE_TBL_GetAddress`.
- Rename `UT_ClearForceFail` as `UT_ClearDefaultValue` given change from <https://github.com/nasa/osal/issues/724>
- See <https://github.com/nasa/sample_app/pull/121>

### Development Build: 1.2.0-rc1+dev29

- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the ""raw"" message cmd/tlm types in definition
- See <https://github.com/nasa/sample_app/pull/114>


### Development Build: 1.2.0-rc1+dev25

- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing
- See <https://github.com/nasa/sample_app/pull/113>


### Development Build: 1.2.0-rc1+dev22

- Replaces deprecated SB API's with MSG
- No impact, removes undesirable pattern use of `OS_PACK`
- See <https://github.com/nasa/sample_app/pull/108>

### Development Build: 1.2.0-rc1+dev18

- No behavior changes. All identifiers now use the prefix `SAMPLE_APP_`. Changes the name of the main function from SAMPLE_AppMain to SAMPLE_APP_Main which affects the CFE startup script.
- Set REVISION to ""99"" to indicate development version status
- See <https://github.com/nasa/sample_app/pull/102>

### Development Build: 1.2.0-rc1+dev13

- Unit test MID string format now 32bit
- Installs unit test to target directory
- Checks only format string in UT event test
- See <https://github.com/nasa/sample_app/pull/98>

### Development Build: 1.2.0-rc1+dev5

- Applies standard coding style.
- Removes test code call of CFE_SB_InitMsg and sets the API/stub buffers directly.
- See <https://github.com/nasa/sample_app/pull/93>

### Development Build: 1.1.0+dev65

- Add build number and baseline to version report
- Install unit test as part of cmake recipe. Sample app test runner now shows up in expected install directory
- See <https://github.com/nasa/sample_app/pull/86>

### Development Build: 1.1.11

- Move the table to fsw/tables and renames ""sample_table"" to ""sample_app_table
- See <https://github.com/nasa/sample_app/pull/76>

### Development Build: 1.1.10

- Test cases now compare an expected event string with a string derived from the spec string and arguments that were output by the unit under test.
- Replace references to `ccsds.h` types with the `cfe_sb.h`-provided type.
- See <https://github.com/nasa/sample_app/pull/71>

### Development Build: 1.1.9

- Applies the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.
- No more format conversion error in RTEMS build
- See <https://github.com/nasa/sample_app/pull/63>

### Development Build: 1.1.8

- Coverage data from make lcov includes the sample_app code
- See <https://github.com/nasa/sample_app/pull/62>

### Development Build: 1.1.7

- Fix bug where table is not released after being used
- Minor updates (see <https://github.com/nasa/sample_app/pull/52>)

### Development Build: 1.1.6

- Minor updates (see <https://github.com/nasa/sample_app/pull/49>)

### Development Build: 1.1.5

- Fix to build on RASPBIAN OS
- Minor updates (see <https://github.com/nasa/sample_app/pull/47>)

### Development Build: 1.1.4

- Fix for a clean build with OMIT_DEPRECATED
- Minor updates (see <https://github.com/nasa/sample_app/pull/44>)

### Development Build: 1.1.3

- Minor updates (see <https://github.com/nasa/sample_app/pull/34>)

### Development Build: 1.1.2

- Minor updates (see <https://github.com/nasa/sample_app/pull/20>)

### Development Build: 1.1.1

- Minor updates (see <https://github.com/nasa/sample_app/pull/15>)

### _**OFFICIAL RELEASE: 1.1.0 - Aquila**_

- Minor updates (see <https://github.com/nasa/sample_app/pull/11>)
- Not backwards compatible with OSAL 4.2.1
- Released as part of cFE 6.7.0, Apache 2.0

### _**OFFICIAL RELEASE: 1.0.0a**_

- Released as part of cFE 6.6.0a, Apache 2.0

## Known issues

As a sample application, extensive testing is not performed prior to release and only minimal functionality is included. Note discrepancies likely exist between this application and the example detailed in the application developer guide.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
92,nasa/svs,,"# svs
NASA's Scientific Visualization Studio

[NASA's Scientific Visualization Studio](http://svs.gsfc.nasa.gov) The Scientific Visualization Studio works closely with scientists 
in the creation of visualizations, animations,
and images in order to promote a greater understanding of Earth and Space Science research activities at NASA and within the 
academic research community supported by NASA.
"
93,nasa/Lightkurve,Python,"Lightkurve
==========

**A friendly package for Kepler & TESS time series analysis in Python.**

**Documentation: https://docs.lightkurve.org**

|pypi-badge| |conda-badge| |azure-badge| |cov-badge| |doi-badge| |astropy-badge|

.. |pypi-badge| image:: https://img.shields.io/pypi/v/lightkurve.svg
                :target: https://pypi.python.org/pypi/lightkurve
.. |conda-badge| image:: https://img.shields.io/conda/vn/conda-forge/lightkurve.svg
                 :target: https://anaconda.org/conda-forge/lightkurve
.. |azure-badge| image:: https://dev.azure.com/KeplerGO/Lightkurve/_apis/build/status/Lightkurve-PyTest?branchName=master
                 :target: https://dev.azure.com/KeplerGO/Lightkurve/_build/latest?definitionId=1&branchName=master
.. |cov-badge| image:: https://codecov.io/gh/KeplerGO/lightkurve/branch/master/graph/badge.svg
              :target: https://codecov.io/gh/KeplerGO/lightkurve
.. |astropy-badge| image:: https://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat
                   :target: http://www.astropy.org
.. |doi-badge| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1181928.svg
              :target: https://doi.org/10.5281/zenodo.1181928


**Lightkurve** is a community-developed, open-source Python package which offers a beautiful and user-friendly way
to analyze astronomical flux time series data,
in particular the pixels and lightcurves obtained by
**NASA's Kepler and TESS exoplanet missions**.

.. image:: https://raw.githubusercontent.com/KeplerGO/lightkurve/master/docs/source/_static/images/lightkurve-teaser.gif

This package aims to lower the barrier for students, astronomers,
and citizen scientists interested in analyzing Kepler and TESS space telescope data.
It does this by providing **high-quality building blocks and tutorials**
which enable both hand-tailored data analyses and advanced automated pipelines.


Documentation
-------------

Read the documentation at `https://docs.lightkurve.org <https://docs.lightkurve.org>`_.


Quickstart
----------

Please visit our quickstart guide at `https://docs.lightkurve.org/quickstart.html <https://docs.lightkurve.org/quickstart.html>`_.


Contributing
------------

We welcome community contributions!
Please read the  guidelines at `https://docs.lightkurve.org/about/contributing.html <https://docs.lightkurve.org/about/contributing.html>`_.


Citing
------

If you find Lightkurve useful in your research, please cite it and give us a GitHub star!
Please read the citation instructions at `https://docs.lightkurve.org/about/citing.html <https://docs.lightkurve.org/about/citing.html>`_.


Contact
-------
Lightkurve is an open source community project created by `the authors <AUTHORS.rst>`_.
You can contact several of its main contributors via keplergo@mail.arc.nasa.gov.
"
94,nasa/DTNME,C++,"DTN Marshal Enterprise Implementation
============================

This is the Delay Tolerant Networking reference implementation managed 
by Marshal Space Flight Center. Please bear with us as we get this repo
up and running. We have worked hard to clean up the code to remove 
unused or half finsihed classes from the original DTN2 implementation
in an attempt to stabilize the code. It is expected that with v0.1
there will be some things that we missed, but should be caught by the
time we reach v1.0. See the file STATUS for an overview of the state
of the code, particularly in reference to the various internet drafts
and (someday) RFCs that describe the bundling protocol. Also, the file
RELEASE-NOTES describes major changes in each release. This implementation
is based on DTN2, but also includes all the changes that Marshal Space
Flight Center has made over the years. As such intermediary version DTNME
v0.1 will include some things that will not be in the final version DTNME v1.0

Compilation / Installation Instructions
---------------------------------------

Dependencies
libdb-dev g++ automake autotools-dev tk tk-dev tcl tcl-dev libssl-dev

<core> can be replaced with the number of cores you would like to use to in 
conjunction with the make -j option to try to speed up the build process.

First Time Building
./init_dtnme.sh <cores>

Subsequent Times Building
./make_dtnme.sh <cores>

Installing
./install_dtnme.sh <Desired Installation Directory>

Note that by default, the configure script will also run configure
inside the oasys/ subdirectory.

Note that if you need to make changes to the configure.ac script or
one of the helper scripts in aclocal/*.ac, run make_dtnme.sh to
recreate configure and then check if both your changes as well as the
newly generated configure script are ready.

Reporting Bugs / Other Help
---------------------------
A bug tracking system is in place. Please direct bug reports to 
https://github.com/nasa/DTNME.

DTNME Directory Structure
------------------------

applib/			application interface library and ipc layer
apps/			example dtn applications
daemon/			dtn router daemon sources
servlib/		dtn router internals
|-- bundling		bundle management and forwarding logic
|-- cmd			tcl based command interface
|-- contacts
|-- conv_layers		convergence layers
|-- discovery
|-- gcm
|-- naming		endpoint identifier schemes
|-- reg			local registrations
|-- routing		bundle routing logic
|-- security		bundle security protocol
|-- session
`-- storage		persistent storage management
sim/			simulation framework
test/			unit tests and other test files
test-utils/		test scripts and utilities

External Requirements
---------------------
oasys-1.3.0+ (see Note - Oasys)
gcc/g++

Optional External Packages
--------------------------
bonjour

Optional Internal Packages
--------------------------
NORM convergence layer support
Bundle Security Protocol support (see Note BSP)
LTP convergence layer support via TCD's LTPlib
Internet Draft Compliant IP Neighbor Discovery (see README.IPND)


Note - Oasys
--------------------------
Before compiling DTNME please compile Oasys-1.3.0+ (must be 
installed or located in DTNME or ../). Support for the
following DTNME options should be specified when configuring
oasys

specify location / support of:
    
    Python 
    tcl                  
    google perftools
    expat
    xerces-c
    xsd tool
    Berkeley DB                  
    mysql 
    postgres             

compile with or without support for:

    bluetooth 
    zlib 
    tclreadline
    profiling
    google profiling
    assembly-based atomic functions

enable or disable:

    oasys debugging
    oasys memory debugging
    oasys lock debugging
    oasys optimization


Note - BSP
--------------------------
The standard ciphersuites require, amongst other things, 
an implementation of sha-256 message digest algorithm.

The DTN reference code uses OpenSSL for cryptographic
and related functions. Unfortunately, some versions of
OpenSSL do not include sha-256.

The ""configure"" process checks for the availability of
sha-256 and provides an error if it is not found.

If your system's OpenSSL does not have sha-256 then you 
can either upgrade it or build and use a local  version 
of OpenSSL. OpenSSL can be obtained from
http://www.openssl.org

OpenSSL 0.9.8 version include sha-256 by default. If your
system uses version 0.9.7 and you do not wish to upgrade
then you can enable sha-256 in later versions of 0.9.7,
such as 0.9.7l and 0.9.7m. To enable sha-256, specify ""fips""
when running ""Configure"".

If you wish to leave you system installation untouched and
build against a local version, then configure dtn using
./configure --with-bsp --with-openssl=/path/to/openssl

Mac OS X note: for Mac OS X users ONLY. If you build dtn
against a local OpenSSL using ""--with-openssl=/path/to/openssl""
you MUST also specify with it LDFLAGS=""-Wl,-search_paths_first"". 
The configuration for OS X users would then be 
./configure --with-bsp --with-openssl=/path/to/openssl LDFLAGS=""-Wl,-search_paths_first""
Note that the quotes are required for the LDFLAGS argument.

"
95,nasa/eefs,C,"eefs
====

EEPROM File System

This is the EEPROM File System Project (EEFS). It is a simple file system for memory devices such as EEPROM, RAM, ROM, etc. Currently it is not intended for block oriented devices such as disks and flash devices.

It can be used as a simple file system to boot an embedded system running vxWorks, RTEMS, or even no operating system. An EEFS image can be created on the development host, providing a single file to burn into an image that is loaded on a target. The file system is easy to understand, debug, and dump. 

There are drivers for RTEMS, vxWorks, and there is a standalone API for systems that do not have a file system. 

There is even a ""microeefs"" interface that allows the lookup of a file from a single function. This allows the bootloader to locate an image in EEPROM by the file name with a minimal amount of code. 
Future releases will include the ability to allow multiple EEFS volumes ( volumes in RAM and EEPROM at the same time ) 


"
96,nasa/GeneLab_Data_Processing,R,"<img src=""NASA_GeneLab_logo-2019.png"" align=""left"" alt=""""/>


# GeneLab_Data_Processing

## About
The [NASA GeneLab](https://genelab.nasa.gov/) Data Processing team and [Analysis Working Group](https://genelab.nasa.gov/awg/charter) members have created standard pipelines for processing omics data from spaceflight and space-relevant experiments. This repository contains the processing pipelines that have been standardized to date for the assay types indicated below. Each subdirectory in this repository holds current and previous pipeline versions for the respective assay type, including detailed descriptions and processing instructions as well as the exact processing commands used to generate processed data for datasets hosted in the [GeneLab Data Repository](https://genelab-data.ndc.nasa.gov/genelab/projects).

---
## Assay Types
Click on an assay type below for data processing information.
- [Amplicon](Amplicon)  
- [RNAseq](RNAseq)  

---
## Usage
We encourage all investigators working with space-relevant omics data to process their data using the standard pipelines described here when possible. Anyone planning to publish analyses derived from [GeneLab processed data](https://genelab-data.ndc.nasa.gov/genelab/projects) may refer to this repository for data processing methods. If you have omics data from a spaceflight or space-relevant experiment, you can submit your data to GeneLab through our [submission portal](https://genelab-data.ndc.nasa.gov/geode-sso-login/).

---
### Contact
For any questions, comments, and/or issues please [contact GeneLab](https://genelab.nasa.gov/help/contact).
"
97,nasa/cumulus-process-py,Python,"# cumulus-process-py

[![CircleCI](https://circleci.com/gh/nasa/cumulus-process-py.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-process-py)
[![PyPI version](https://badge.fury.io/py/cumulus-process.svg)](https://badge.fury.io/py/cumulus-process)

cumulus-process-py is a collection of python utilities for the NASA Cumulus project.

## The Use Case

This library provides various utilities and helper functions for python tasks developed to work with the Cumulus framework.

The utilities help writing tasks that involve metadata extraction from input files, thumbnail creation, or even more complex data processing such as running custom science code to create higher level products from an input file.

## Installation

    $ pip install cumulus-process

## Development

    $ pip install -r requirements.txt
    $ pip install -r requirements-dev.txt

## Testing

Testing requires [localstack](https://github.com/localstack/localstack). Follow the instruction for localstack and install it on your machine then:

    $ nosetests -v

## Usage

To use the library, subclass `Process` class from `cumulus_process` and implement:
1. the `process` method,
2. a `default_keys` property (needed for functionality such as `self.fetch()` unless you are overriding input_keys in config)

## Example:

See the [example](example) folder."
98,nasa/GSAP,C++,"# GSAP

<font size=""+3""><div align=""center""> **[For complete documentation see the wiki
here](https://github.com/nasa/GSAP/wiki)** </div></font>

## About
The Generic Software Architecture for Prognostics (GSAP) is a framework for
applying prognostics. It makes applying prognostics easier by implementing many
of the common elements across prognostic applications. The standard interface
enables reuse of prognostic algorithms and models across using the GSAP
framework.

There are two operational modes for GSAP: 'async' and 'simple'. These are both described below:
* 'async': Asynchronous GSAP. This takes advantage of parallization, including automatic thread management to perform calculations on multiple threads. 
* 'simple': Simple single-threaded GSAP. Resulting prognostics application is smaller and simplier, but does not take advantage of multi-threading. Ideal for resource constrained hardware. 

![GSAP Layers](images/Layers.png)

The GSAP framework is used through the creation of communicators, prognosers, or
models (the deployment layer). The elements of the deployment layer plugs into
the framework and use the tools of the support layer. These elements are
described further below:

* **Communicators**:

Communicators are used to communicate data with the outside world. These
function as interfaces with various data sources and sinks. Some examples could
be a playback agent that reads from a file, a GUI for displaying prognostic
results, an automated report generator, or a client that connects into a network
messaging system (for example: SCADA). These systems can receive data which will
be used by prognosers or communicate the results with operators.

* **Prognosers**:

  This is the core of the GSAP system. Inside the prognosers is the core logic for performing prognostics. A new prognoser is created to support a new method for performing prognostics. Many prognostics systems follow a common model-based structure. Those systems do not require the creation of a new prognoser, only the creation of a new model that will be used by the modelBasedPrognoser. For more information on this see the section Extending.

* **Models**:

  Models are a method of representing the behavior of a component. A common way of performing prognostics is using a model that describes both the healthy and damaged behavior of the components. The modelBasedPrognoser uses the models to perform prognostics.

Each of these components is configured through the use of configuration files.
This allows for a GSAP deployment to be configured to a new configuration or
system without any software changes.

## Using
[See the Getting Started page for
information](https://github.com/nasa/GSAP/wiki/Getting-Started)

## Extending
GSAP is designed to be easy to extend to fit your use. Extending GSAP is done by
adding Prognosers, Models, Algorithms, or Communicators. When the behavior of the component
being prognosed is represented by a model, users can create a new model and use
the supplied modelBasedPrognoser for prognostics. This is done instead of adding
a new prognoser. [For more information on extending see the wiki
here](https://github.com/nasa/GSAP/wiki)

## Contact
If you have questions, please contact Chris Teubert
(christopher.a.teubert@nasa.gov)

## Contributing
All contributions are welcome! If you are having problems with the plugin, please open an issue on GitHub or email Chris Teubert. If you would like to contribute directly, please feel free to open a pull request against the ""develop"" branch. Pull requests will be evaluated and integrated into the next official release.

## Notices

Copyright ¬©2016, 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
99,nasa/MOSAIC,Python,"# MOSAIC: Mars On Site Shared Analytics, Information, and Computing

![The MOSAIC scheduler in action](images/preview.gif)

This repository contains MOSAIC schedulers and the Pluggable Distributed Resource Allocator (PDRA).

Together, the tools in this repository enable heterogeneous multi-robot systems to share computational tasks with complex dependencies among agents with heterogeneous computation capabilities over time-varying communication links.

Maintainers:

- [Federico Rossi](https://github.com/federico3) `federico.rossi@jpl.nasa.gov`
- [Tiago Stegun Vaquero](https://github.com/tvaquero) `tiago.stegun.vaquero@jpl.nasa.gov`
- [Marc Sanchez Net](https://github.com/msancheznet) `marc.sanchez.net@jpl.nasa.gov`
- [Joshua Vander Hook](https://github.com/jodavaho/) `hook@jpl.nasa.gov`

![MOSAIC live demo](images/MOSAIC_demo.png)

## [MOSAIC schedulers](schedulers)

![MOSAIC schedules](images/examples.png)

We propose scheduling and task-allocation algorithms to share computational tasks among heterogeneous agents over time-varying communication links.

Specifically, we propose:

- A mixed-integer programming algorithm for scheduling tasks in heterogeneous robotic networks with time-varying communication links. The scheduler can accommodate any non-cyclical dependencies between tasks and arbitrary time-varying communication links, handle optional tasks with associated rewards, and optimize cost functions including rewards for optional tasks, makespan, and energy usage. The scheduler is presented in \[1\].

- A mixed-integer programming algorithm for task allocation in  heterogeneous robotic networks with *periodic* communication links. The task allocation algorithm also accommodates any non-cyclical dependencies between tasks and handles optional tasks with associated rewards and maximum latency requirements; it can maximize reward from optional tasks or minimize energy use. The task allocation algorithm is presented in \[2\].

<!-- [![Alt text](https://img.youtube.com/vi/VID/0.jpg)](https://www.youtube.com/watch?v=VID) -->

## [Pluggable Distributed Resource Allocator](distributed_resource_allocator)

![Animation of the Pluggable Distributed Resource Allocator in action](images/pdra.gif)

The Pluggable Distributed Resource Allocator (PDRA) is a middleware for distributed computing in heterogeneous mobile robotic networks. It allows the MOSAIC schedulers to be easily ""plugged"" in existing autonomy executives with minimal software changes. PDRA sits between an existing single-agent planner/executor and existing computational resources (e.g. ROS packages). It intercepts the executor‚Äôs requests and, if needed, transparently routes them to other nodes for execution.
Simulation results show that PDRA can reduce energy and CPU usage by over 50\% in representative multi-robot scenarios compared to a naive scheduler; runs on embedded platforms; and performs well in delay- and disruption-tolerant networks (DTNs). PDRA is available to the community under an open-source license.

<!-- [![Alt text](https://img.youtube.com/vi/VID/0.jpg)](https://www.youtube.com/watch?v=VID) -->

## References

\[1\] Joshua Vander Hook, Tiago Vaquero, Federico Rossi, Martina Troesch, Marc Sanchez Net, Joshua Schoolcraft, Jean-Pierre de la Croix, and Steve Chien, [""Mars On-Site Shared Analytics Information and Computing,""](https://aaai.org/ojs/index.php/ICAPS/article/view/3556) in Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling, vol. 29, no. 1, pp. 707-715, July 2019.

\[2\] Federico Rossi\*, Tiago Stegun Vaquero\*, Marc Sanchez Net, Ma√≠ra Saboia da Silva, and Joshua Vander Hook, [""The Pluggable Distributed Resource Allocator (PDRA):a Middleware for Distributed Computing in Mobile Robotic Networks""](https://arxiv.org/abs/2003.13813), under review.

## Copyright

Copyright 2019 by California Institute of Technology.  ALL RIGHTS RESERVED.
United  States  Government  sponsorship  acknowledged.   Any commercial use
must   be  negotiated  with  the  Office  of  Technology  Transfer  at  the
California Institute of Technology.

This software may be subject to  U.S. export control laws  and regulations.
By accepting this document,  the user agrees to comply  with all applicable
U.S. export laws and regulations.  User  has the responsibility  to  obtain
export  licenses,  or  other  export  authority  as may be required  before
exporting  such  information  to  foreign  countries or providing access to
foreign persons.

This  software  is a copy  and  may not be current.  The latest  version is
maintained by and may be obtained from the Mobility  and  Robotics  Sytstem
Section (347) at the Jet  Propulsion  Laboratory.   Suggestions and patches
are welcome and should be sent to the software's maintainer.
"
100,nasa/concept-tagging-api,Python,"# OCIO STI Concept Tagging Service

An API for exposing models created with [STI concept training](https://github.com/nasa/concept-tagging-training). This project was written about [here](https://strategy.data.gov/proof-points/2019/05/28/improving-data-access-and-data-management-artificial-intelligence-generated-metadata-tags-at-nasa/) for the Federal Data Strategy Incubator Project. A running version of this API may be found [here](http://ec2-100-25-26-114.compute-1.amazonaws.com:5001/), however, this is a temporary instance for demos purposes. It may not be available long-term. Please do not use it in production or at scale.

### What is Concept Tagging
By concept tagging, we mean you can supply text, for example:
`Volcanic activity, or volcanism, has played a significant role in the geologic evolution of Mars.[2] Scientists have known since the Mariner 9 mission in 1972 that volcanic features cover large portions of the Martian surface.` and get back predicted keywords, like `volcanology, mars surface, and structural properties`, as well as topics, like `space sciences, geosciences`, from a standardized list of several thousand NASA concepts with a probability score for each prediction.

## Index
1. [Using Endpoint](#using-endpoint)
    1. [Request](#request)
    2. [Response](#response)
2. [Running Your Own Instance](#running-your-own-instance)
    1. [Installation](#installation)
        1. [Pull Docker Image](#pull-docker-image)
        2. [Build Docker Image](#build-docker-image)
        3. [With Local Python](#with-local-python)
    2. [Download Models](#downloading-models)
    3. [Running Service](#running-service)
        1. [Using Docker](#using-docker)
        2. [Using Local Python](#using-local-python)

## Using Endpoint
### Request
The endpoint accepts a few fields, shown in this example:
```json
{
    ""text"": [
        ""Astronauts go on space walks."",
        ""Basalt rocks and minerals are on earth.""
    ], 
    ""probability_threshold"":""0.5"",
    ""topic_threshold"":""0.9"", 
    ""request_id"":""example_id10""
}
```
- **text** *(string or list of strings)* -- The text(s) to be tagged.
- **probability_threshold** *(float in [0, 1])* -- a threshold under which a concept tag will not be returned by the API. For example, if the threshold is set to 0.8 and a concept only scores 0.5, the concept will be omitted from the response. Setting to 1 will yield no results. Setting to 0 will yield all of the classifiers and their scores, no matter how low.
- **topic_threshold** *(float in [0, 1])* -- A probability threshold for categories. If a category falls under this threshold, its respective suite of models will not be utilized for prediction. If you set this value to 1, only the generalized concept models will be used for tagging, yielding significant speed gains.
- **request_id** *(string)* -- an optional ID for your request.  

You might send this request using curl. In the command below:
1. Substitute `example_payload_multiple.json` with the path to your json request.
2. Substitute `http://0.0.0.0:5000/` with the address of the API instance.
```
curl -X POST -H ""Content-Type: application/json"" -d @example_payload_multiple.json http://0.0.0.0:5000/findterms/
```
### Response
You will then receive a response like that [here](docs/multiple_response.json). In the `payload`, you will see multiple fields, including:
- **features** -- words and phrases directly extracted from the document. 
- **sti_keywords** -- concepts and their prediction scores. 
- **topic_probability** -- model scores for all of the categories.

## Running Your Own Instance
### Installation
For most people, the simplest installation entails [building the docker image](#build-docker-image), [downloading the models](#downloading-models), and [running the docker container](#using-docker).


#### Build Docker Image
First, clone this repository and enter its root.
Now, you can build the image with:
```
docker build -t concept_tagging_api:example .
```
\* Developers should look at the `make build` command in the [Makefile](Makefile). It has an automated process for tagging the image with useful metadata.

#### With Local Python
\* tested with python:3.7  
First, clone this repository and enter its root.  
Now, create a virtual environment. For example, using [venv](https://docs.python.org/3/library/venv.html):
```
python -m venv venv
source venv/bin/activate
```
Now install the requirements with:
```
make requirements
```

### Downloading Models
Then, you need to download the machine learning models upon which the service relies. 

You can find zipped file which contains all of the models [here](https://data.nasa.gov/docs/datasets/public/concept_tagging_models/10_23_2019.zip). Now, to get the models in the right place and unzip:
```bash
mkdir models
mv <YOUR_ZIPPED_MODELS_NAME>.zip models
cd models
unzip <YOUR_ZIPPED_MODELS_NAME>.zip
```
Alternatively, the models can also be downloaded from data.nasa.gov where they are named <a href='https://data.nasa.gov/Software/STI-Tagging-Models/jd6d-mr3p'>STI Tagging Models</a>. However, they download slower from that location.

### Running Service

#### Using Docker
With the docker image and model files in place, you can now run the service with a simple docker command. In the below command be sure to:
 1. Substitute `concept_tagging_api:example` for the name of your image.
 2. Substitute `$(pwd)/models/10_23_2019` to the path to your models directory. 
 3. Substitute `5001` with the port on your local machine from which you wish to access the API.
```
docker run -it \
    -p 5001:5000 \
    -v $(pwd)/models/10_23_2019:/home/service/models/experiment \
    concept_tagging_api:example
```

Note that you you may experience permission errors when you start the container. To resolve this issue, set the user and group of your `models` directory to 999. This is the uid for the user 

**optional**
The entrypoint to the docker image is [gunicorn](https://docs.gunicorn.org/en/stable/index.html), a python WSGI HTTP Server which runs our flask app. You can optionally pass additionally arguments to gunicorn. For example:
```bash
docker run -it \
    -p 5001:5000 \
    -v $(pwd)/models/10_23_2019:/home/service/models/experiment \
    concept_tagging_api:example --timeout 9000 
```
See [here](https://docs.gunicorn.org/en/stable/design.html#async-workers) for more information about design considerations for these gunicorn settings.

#### Pitfalls & Gotchas to Remeber
- If you run this on a cloud service and run an upgrade on everything out of date for security reasons, you may need to run `sudo service docker stop`
and then `sudo service docker start` to get docker going again. You'll also have to find the docker container that you had last running and restart it.
- If you run the docker container as described above, remember to try the URL of your service with the proper port at the end of the URL. 

#### Using Local Python
With the requirements installed and the model files in place, you can now run the service with python locally. 
In the command below, substitute `models/test` with the path to your models directory. For example, if you followed the example from [With Bucket Access](#with-bucket-access), it will be `models/10_23_2019`.
```
export MODELS_DIR=models/test; \
python service/app.py
```
"
101,nasa/CompDam_DGD,Fortran,"# CompDam - Deformation Gradient Decomposition (DGD)
This code is a continuum damage mechanics (CDM) material model intended for use with the Abaqus finite element code. This is a research code which aims to provide an accurate representation of mesoscale damage modes in fiber-reinforced polymer composite materials in finite element models in which each ply is discretely represented.

The CDM material model is implemented as an Abaqus/Explicit user subroutine (VUMAT) for the simulation of matrix cracks formed under tensile, compressive, and shear loading conditions and fiber fracture under tensile and compressive loading. Within CompDam, the emphasis of many recent developments has been on accurately representing the kinematics of composite damage. The kinematics of matrix cracks are represented by treating them as cohesive surfaces embedded in a deformable bulk material in accordance with the Deformation Gradient Decomposition (DGD) approach. Fiber tensile damage is modeled using conventional CDM strain-softening.

This software may be used, reproduced, and provided to others only as permitted under the terms of the agreement under which it was acquired from the U.S. Government. Neither title to, nor ownership of, the software is hereby transferred. This notice shall remain on all copies of the software.

Copyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

Publications that describe the theories used in this code:
- Andrew C. Bergan, [""A Three-Dimensional Mesoscale Model for In-Plane and Out-of-Plane Fiber Kinking""](https://doi.org/10.2514/6.2019-1548) *AIAA SciTech Forum*, San Diego, California, 7-11 January 2019.
- Carlos D√°vila, [""From S-N to the Paris Law with a New Mixed-Mode Cohesive Fatigue Model""](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20180004395.pdf) NASA/TP-2018-219838, June 2018.
- Andrew C. Bergan, et al., [""Development of a Mesoscale Finite Element Constitutive Model for Fiber Kinking""](https://doi.org/10.2514/6.2018-1221) *AIAA SciTech Forum*, Kissimmee, Florida, 8-12 January 2018.
- Frank A. Leone Jr. [""Deformation gradient tensor decomposition for representing matrix cracks in fiber-reinforced materials""](https://dx.doi.org/10.1016/j.compositesa.2015.06.014) *Composites Part A* (2015) **76**:334-341.
- Frank A. Leone Jr. [""Representing matrix cracks through decomposition of the deformation gradient tensor in continuum damage mechanics methods""](https://iccm20.org/fullpapers/file?f=Abk7n4gkWV) *Proceedings of the 20th International Conference on Composite Materials*, Copenhagen, Denmark, 19-24 July 2015.
- Cheryl A. Rose, et al. [""Analysis Methods for Progressive Damage of Composite Structures""](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20140001002.pdf) NASA/TM-2013-218024, July 2013.

Examples of this code being applied can be found in the following publications:
- Andrew C. Bergan and Wade C. Jackson, [""Validation of a Mesoscale Fiber Kinking Model through Test and Analysis of Double Edge Notch Compression Specimens""](https://doi.org/10.12783/asc33/26003) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.
- Imran Hyder, et al., [""Implementation of a Matrix Crack Spacing Parameter in a Continuum Damage Mechanics Finite Element Model""](https://doi.org/10.12783/asc33/26052) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.
- Frank Leone, et al., [""Benchmarking Mixed Mode Matrix Failure in Progressive Damage and Failure Analysis Methods""](https://doi.org/10.12783/asc33/26030) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.
- Brian Justusson, et al., et al., [""Quantification of Error Associated with Using Misaligned Meshes in Continuum Damage Mechanics Material Models for Matrix Crack Growth Predictions in Composites""](https://doi.org/10.12783/asc33/26097) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.
- Kyongchan Song, et al. [""Continuum Damage Mechanics Models for the Analysis of Progressive Damage in Cross-Ply and Quasi-Isotropic Panels Subjected to Static Indentation""](https://doi.org/10.2514/6.2018-1466) *AIAA SciTech Forum*, Kissimmee, Florida, 8-12 January 2018.
- Imran Hyder, et al. [""Assessment of Intralaminar Progressive Damage and Failure Analysis Using an Efficient Evaluation Framework""](https://doi.org/10.12783/asc2017/15405) *32nd American Society for Composites (ASC) Annual Technical Conference*, West Lafayette, Indiana, 22-25 October 2017.
- Frank A. Leone, et al. [""Fracture-Based Mesh Size Requirements for Matrix Cracks in Continuum Damage Mechanics Models""](https://doi.org/10.2514/6.2017-0198) *AIAA SciTech Forum*, Grapevine, Texas, 9-13 January 2017.
- Mark McElroy, et al. [""Simulation of delamination-migration and core crushing in a CFRP sandwich structure""](https://doi.org/10.1016/j.compositesa.2015.08.026) *Composites Part A* (2015) **79**:192-202.

For any questions, please contact the developers:
- Frank Leone   | [frank.a.leone@nasa.gov](mailto:frank.a.leone@nasa.gov)     | (W) 757-864-3050
- Andrew Bergan | [andrew.c.bergan@nasa.gov](mailto:andrew.c.bergan@nasa.gov) | (W) 757-864-3744
- Carlos D√°vila | [carlos.g.davila@nasa.gov](mailto:carlos.g.davila@nasa.gov) | (W) 757-864-9130


## Table of contents
- [Getting started](#getting-started)
- [Model features](#model-features)
- [Elements](#elements)
- [Material properties](#material-properties)
- [State variables](#state-variables)
- [Fatigue analyses](#fatigue-analyses)
- [Implicit solver compatibility](#implicit-solver-compatibility)
- [Example problems](#example-problems)
- [Advanced debugging](#advanced-debugging)
- [Python extension module](#python-extension-module)
- [Summary of tests classes](#summary-of-tests-classes)
- [Contributing](#contributing)
- [Citing CompDam](#citing-compdam)


## Getting started

### Source code
The user subroutine source code is located in the `for` directory. The main entry point is `CompDam_DGD.for`.

### Prerequisites
[Intel Fortran Compiler](https://software.intel.com/en-us/fortran-compilers) version 11.1 or newer is required to compile the code ([more information about compiler versions](usersubroutine-prerequisites.md)). MPI must be installed and configured properly so that the MPI libraries can be linked by CompDam. It is recommended that Abaqus 2016 or newer is used with this code. Current developments and testing are conducted with Abaqus 2019. Python supporting files require Python 2.7.

### Initial setup
After cloning the CompDam_DGD git repository, it is necessary to run the setup script file `setup.py` located in the repository root directory:
```
$ python setup.py
```

The main purpose of the setup.py script is to 1) set the `for/version.for` file and 2) add git-hooks that automatically update the `for/version.for`.

In the event that you do not have access to python, rename `for/version.for.nogit` to `for/version.for` manually. The additional configuration done by `setup.py` is not strictly required.

### Abaqus environment file settings
The `abaqus_v6.env` file must have [`/fpp`](https://software.intel.com/en-us/fortran-compiler-developer-guide-and-reference-fpp), [`/Qmkl:sequential`](https://software.intel.com/en-us/fortran-compiler-developer-guide-and-reference-mkl-qmkl), and [`/free`](https://software.intel.com/en-us/fortran-compiler-developer-guide-and-reference-free) in the `ifort` command where the format for Windows is used. The corresponding Linux format is: `-fpp`, `-free`, and `-mkl=sequential`. The `/fpp` option enables the Fortran preprocessor, which is required for the code to compile correctly. The `/free` option sets the compiler to free-formatting for the source code files. The `/Qmkl:sequential` enables the [Intel Math Kernel Library (MKL)](https://software.intel.com/en-us/mkl), which provides optimized and verified functions for many mathematical operations. The MKL is used in this code for calculating eigenvalues and eigenvectors.

A sample environment file is provided in the `tests` directory for Windows and Linux systems.

### Submitting a job
This code is an Abaqus/Explicit VUMAT. Please refer to the Abaqus documentation for the general instructions on how to submit finite element analyses using user subroutines. Please see the [example input file statements](#example-input-file-statements) for details on how to interface with this particular VUMAT subroutine.

Analyses with this code **must** be run in double precision. Some of the code has double precision statements and variables hard-coded, so if Abaqus/Explicit is run in single precision, compile-time errors will arise. When submitting an Abaqus/Explicit job from the command line, double precision is specified by including the command line argument `double=both`.

For example, run the test model `test_C3D8R_elastic_fiberTension` in the `tests` directory with the following command:
```
$ abaqus job=test_C3D8R_elastic_fiberTension user=../for/CompDam_DGD.for double=both
```

### Example input file statements
Example 1, using an [external material properties file](#defining-the-material-properties-in-a-props-file):

    *Section controls, name=control_name, distortion control=YES
    **
    *Material, name=IM7-8552
    *Density
     1.57e-09,
    *Depvar, delete=11
    ** the above delete statement is optional
      19,
      1, CDM_d2
      2, CDM_Fb1
      3, CDM_Fb2
      4, CDM_Fb3
      5, CDM_B
      6, CDM_Lc1
      7, CDM_Lc2
      8, CDM_Lc3
      9, CDM_FIm
     10, CDM_alpha
     11, CDM_STATUS
     12, CDM_Plas12
     13, CDM_Inel12
     14, CDM_FIfT
     15, CDM_slide1
     16, CDM_slide2
     17, CDM_FIfC
     18, CDM_d1T
     19, CDM_d1C
    *Characteristic Length, definition=USER, components=3
    *User material, constants=3
    ** 1              2  3
    ** feature flags,  , thickness
              100001,  ,       0.1
    **
    *Initial Conditions, type=SOLUTION
     elset_name,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,
     0.d0,  0.d0,  -999,     1,  0.d0,  0.d0,  0.d0, 0.d0,
     0.d0,  0.d0,  0.d0,  0.d0
    ** In each step, NLGEOM=YES must be used. This is the default setting.

Example 2, using an [input deck command](#defining-the-material-properties-in-the-input-deck):

    *Section controls, name=control_name, distortion control=YES
    **
    *Material, name=IM7-8552
    *Density
     1.57e-09,
    *Depvar, delete=11
    ** the above delete statement is optional
      19,
      1, CDM_d2
      2, CDM_Fb1
      3, CDM_Fb2
      4, CDM_Fb3
      5, CDM_B
      6, CDM_Lc1
      7, CDM_Lc2
      8, CDM_Lc3
      9, CDM_FIm
     10, CDM_alpha
     11, CDM_STATUS
     12, CDM_Plas12
     13, CDM_Inel12
     14, CDM_FIfT
     15, CDM_slide1
     16, CDM_slide2
     17, CDM_FIfC
     18, CDM_d1T
     19, CDM_d1C
    *Characteristic Length, definition=USER, components=3
    *User material, constants=40
    ** 1              2  3          4  5  6  7  8
    ** feature flags,  , thickness, 4, 5, 6, 7, 8
              100001,  ,       0.1,  ,  ,  ,  ,  ,
    **
    **  9         10        11        12        13        14        15        16
    **  E1,       E2,       G12,      nu12,     nu23,     YT,       SL        GYT,
        171420.0, 9080.0,   5290.0,   0.32,     0.52,     62.3,     92.30,    0.277,
    **
    **  17        18        19        20        21        22        23        24
    **  GSL,      eta_BK,   YC,       alpha0    E3,       G13,      G23,      nu13,
        0.788,    1.634,    199.8,    0.925,      ,          ,         ,          ,
    **
    **  25        26        27        28        29        30        31        32
    **  alpha11,  alpha22,  alpha_PL, n_PL,     XT,       fXT,      GXT,      fGXT,
        -5.5d-6,  2.58d-5,          ,     ,     2326.2,   0.2,      133.3,    0.5,
    **
    **  33        34        35        36        37        38        39        40
    **  XC,       fXC,      GXC,      fGXC,       cl,     w_kb,     None,     mu
        1200.1,      ,         ,          ,         ,     0.1,          ,     0.3
    ** For spacing below a6=schaefer_a6, b2=schaefer_b2, n=schaefer_n and A=schaefer_A
    **  41        42        43        44
    **  a6,       b2,       n,        A
    **
    *Initial Conditions, type=SOLUTION
     elset_name,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,
     0.d0,  0.d0,  -999,     1,  0.d0,  0.d0,  0.d0, 0.d0,
     0.d0,  0.d0,  0.d0,  0.d0
    ** In each step, NLGEOM=YES must be used. This is the default setting.

### Running tests
Test cases are available in the `tests` directory. The tests are useful for demonstrating the capabilities of the VUMAT as well as to verify that the code performs as intended. Try running some of the test cases to see how the code works. The test cases can be submitted as a typical Abaqus job using the Abaqus command line arguments.

### Building a shared library
CompDam_DGD can be built into a shared library file. Follow these steps:
1. Place a copy of the Abaqus environment file (with the compiler flags specified) in the `for` directory
2. In Linux, and when using Abaqus versions prior to 2017, rename `CompDam_DGD.for` to `CompDam_DGD.f`
3. From the `for` directory, run:
```
$ abaqus make library=CompDam_DGD
```
This command will create shared libraries for the operating system it is executed on (`.dll` for Windows and `.so` for Linux).

When using a pre-compiled shared library, it is only necessary to specify the location of the shared library files in the environment file (the compiler options are not required). To run an analysis using a shared library, add `usub_lib_dir = <full path to shared library file>` to the Abaqus environment file in the Abaqus working directory.

## Model features
The CompDam_DGD material model implements a variety of features that can be enabled or disabled by the user. An overview of these features is provided in this section. The material properties required for each feature are listed. References are provided to more detailed discussions of the theoretical framework for each feature.

### Fully orthotropic elasticity
The composite materials modeled with CompDam_DGD can be defined assuming either [transverse isotropy](https://www.efunda.com/formulae/solid_mechanics/mat_mechanics/hooke_iso_transverse.cfm) or [orthotropy](https://www.efunda.com/formulae/solid_mechanics/mat_mechanics/hooke_orthotropic.cfm). For a transversely isotropic material definition, the following properties must be defined: E1, E2, G12, v12, and v23. For an orthotropic material definition, the following additional properties must be defined: E2, G13, G23, and nu13.

### Matrix damage
Tensile and compressive matrix damage is modeled by embedding cohesive laws to represent cracks in the material according to the deformation gradient decomposition method of [Leone (2015)](https://doi.org/10.1016/j.compositesa.2015.06.014). The matrix crack normals can have any orientation in the 2-3 plane, defined by the angle `CDM_alpha`. The mixed-mode behavior of matrix damage initiation and evolution is defined according to the Benzeggagh-Kenane law. The initiation of compressive matrix cracks accounts for friction on the potential crack surface according to the LaRC04 failure criteria. In the notation of the [paper](https://doi.org/10.1016/j.compositesa.2015.06.014), `Q` defines the material direction that is severed by the crack. In this implementation `Q=2` except when `CDM_alpha = 90`.

The following material properties are required for the prediction of matrix damage: YT, SL, GYT, GSL, eta_BK, YC, and alpha0. The state variables related to matrix damage are `CDM_d2`, `CDM_FIm`, `CDM_B`, `CDM_alpha`, `CDM_Fb1`, `CDM_Fb2`, and `CDM_Fb3`.

### Thermal strains
The thermal strains are calculated by multiplying the 1-, 2-, and 3-direction coefficients of thermal expansion by the current &Delta;T, as provided by the Abaqus solver. The thermal strains are subtracted from the current total strain.

The required material properties are the coefficients of thermal expansion in the 1 and 2 directions. It is assumed that the 2- and 3-direction coefficients of thermal expansion are equal.

Hygroscopic strains are not accounted for. If the effects of hygroscopic expansion are to be modeled, it is recommended to smear the hygroscopic and thermal expansion coefficients to approximate the response using the solver-provided &Delta;T.

### Shear nonlinearity
Three approaches to modeling the matrix nonlinearity are available: Ramberg-Osgood plasticity, Schapery theory, and Schaefer plasticity. These three methods are mutually exclusive and optional.

#### Ramberg-Osgood plasticity
Shear nonlinearity in the 1-2 and/or the 1-3 plane can be modeled using the [Ramberg-Osgood equation](https://en.wikipedia.org/wiki/Ramberg%E2%80%93Osgood_relationship), with its parameters selected to fit experimental data. As applied herein, the Ramberg-Ogsood equation is written in the following form for the 1-2 plane:

*&gamma;*<sub>12</sub> = [*&tau;*<sub>12</sub> + *&alpha;*<sub>PL</sub>sign(*&tau;*<sub>12</sub>)|*&tau;*<sub>12</sub>|<sup>*n*<sub>PL</sub></sup>]/*G*<sub>12</sub>

where *&gamma;*<sub>12</sub> is the shear strain and *&tau;*<sub>12</sub> is the shear stress. Likewise, the expression for the 1-3 plane is

*&gamma;*<sub>13</sub> = [*&tau;*<sub>13</sub> + *&alpha;*<sub>PL</sub>sign(*&tau;*<sub>13</sub>)|*&tau;*<sub>13</sub>|<sup>*n*<sub>PL</sub></sup>]/*G*<sub>13</sub>

Prior to the initiation of matrix damage (i.e., `CDM_d2 = 0`), the nonlinear shear response due to the above equation is plastic, and the unloading/reloading slope is unchanged. No pre-peak nonlinearity is applied to the matrix tensile or compressive responses (i.e., *&sigma;<sub>22</sub>*).

The required material inputs are the two parameters in the above equation: *&alpha;*<sub>PL</sub> and *n*<sub>PL</sub>. Note that the same constants are used for the 1-2 and 1-3 planes under the assumption of transverse isotropy (see [Seon et al. (2017)](https://doi.org/10.12783/asc2017/15267)). For the 1-2 plane, the state variables `CDM_Plas12` and `CDM_Inel12` are used to track the current plastic shear strain and the total amount of inelastic plastic shear strain that has occurred through the local deformation history, respectively. For cases of monotonic loading, `CDM_Plas12` and `CDM_Inel12` should have the same magnitude. Likewise, the state variables `CDM_Plas13` and `CDM_Inel13` are utilized for the 1-3 plane. The [feature flags](#controlling-which-features-are-enabled) can be used to enable this Ramberg-Osgood model in the 1-2 plane, 1-3 plane, or both planes.

#### Schapery micro-damage
Matrix nonlinearity in the 1-2 plane can also be modeled using Schapery theory, in which all pre-peak matrix nonlinearity is attributed to the initiation and development of micro-scale matrix damage. With this approach, the local stress/strain curves will unload to the origin, and not develop plastic strain. A simplified version of the approach of [Pineda and Waas](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20120000914.pdf) is here applied. The micro-damage functions *e<sub>s</sub>* and *g<sub>s</sub>* are limited to third degree polynomials for ease of implementation. As such, four fitting parameters are required for each of *e<sub>s</sub>* and *g<sub>s</sub>* to define the softening of the matrix normal and shear responses to micro-damage development.

*e<sub>s</sub>*(*S<sub>r</sub>*) = *e<sub>s0</sub>* + *e<sub>s1</sub>S<sub>r</sub>* + *e<sub>s2</sub>S<sub>r</sub>*<sup>2</sup> + *e<sub>s3</sub>S<sub>r</sub>*<sup>3</sup>

*g<sub>s</sub>*(*S<sub>r</sub>*) = *g<sub>s0</sub>* + *g<sub>s1</sub>S<sub>r</sub>* + *g<sub>s2</sub>S<sub>r</sub>*<sup>2</sup> + *g<sub>s3</sub>S<sub>r</sub>*<sup>3</sup>

where *S<sub>r</sub>* is the micro-damage reduced internal state variable. These eight material properties must be defined in an [external material properties file](#defining-the-material-properties-in-a-props-file). *S<sub>r</sub>* is stored in the 12<sup>th</sup> state variable slot, replacing `CDM_Plas12`, when Schapery theory is used in a model. The 13<sup>th</sup> state variable slot is not used when Schapery micro-damage is used.

#### Schaefer
Shear nonlinearity in the 1-2 plane can be modeled using the Schaefer prepeak model. In this model, effective plastic strain is related to effective plastic stress through the power law:

*&epsilon;*<sub>plastic</sub> = *A &sigma;*<sup>*n*</sup>

Additionally, a yield criterion function (effective stress) is defined as:

*f* = *&sigma;* = (*S*<sub>22</sub> + *a*<sub>6</sub> *S*<sub>12</sub><sup>2</sup>)<sup>1/2</sup> + *b*<sub>2</sub> *S*<sub>22</sub>

where *a*<sub>6</sub>, *b*<sub>2</sub>, *A* and *n* are material constants needed for the Schaefer prepeak material model. These four material properties must be defined in an [external material properties file](#defining-the-material-properties-in-a-props-file). Additionally, when defining  *a*<sub>6</sub>, *b*<sub>2</sub>, *A* and *n* in the external material properties file the variables are prefixed with schaefer_ (to disambiguate the otherwise nondescript material property names and symbols).

The above two equations are used in concert to determine plastic strain through the relationship:

*&epsilon;* <sub>plastic</sub> = *n A f* <sup>*n* - 1</sup> &part;*f* / &part;*S*<sub>*i*</sub> &part;*f* / &part;*S*<sub>j</sub>

*f* (i.e., schaefer_f) and the tensorial plastic strain determined by the nonlinearity model are stored as state variables (27 through 32 for plastic strain and 33 for *f*)

### Fiber tensile damage
A continuum damage mechanics model similar to the work of [Maim√≠ et al. (2007)](https://doi.org/10.1016/j.mechmat.2007.03.005) is used to model tensile fiber damage evolution. The model utilizes a non-interacting maximum strain failure criterion, and bilinear softening after the initiation of failure. The area under the stress-strain curve is equal to the fracture toughness divided by the element length normal to fracture, i.e., `CDM_Lc1`. The required material properties are: XT, fXT, GXT, and fGXT, where fXT and fGXT are ratios of strength and fracture toughness for bilinear softening, defined as the *n* and *m* terms in equations (25) and (26) of [D√°vila et al. (2009)](https://doi.org/10.1007/s10704-009-9366-z). To model a linear softening response, both fXT and fGXT should be set equal to 0.5.

### Fiber compression damage

#### Model 1: Max strain, bilinear softening (BL)
Same model as in tension, but for compression. Assumes maximum strain failure criterion and bilinear softening. The required material properties are: XC, fXC, GXC, and fGXC.

Load reversal assumptions from [Maim√≠ et al. (2007)](https://doi.org/10.1016/j.mechmat.2007.03.006).

#### Model 2: Placeholder

#### Model 3, 4, 5: Fiber kinking theory (FKT)
A model based on Budiansky's fiber kinking theory from [Budiansky (1983)](https://doi.org/10.1016/0045-7949(83)90141-4), [Budiansky and Fleck (1993)](https://doi.org/10.1016/0022-5096(93)90068-Q), and [Budiansky et al. (1998)](https://doi.org/10.1016/S0022-5096(97)00042-2) implemented using the DGD framework to model in-plane (1-2) and/or out-of-plane (1-3) fiber kinking. The model is described in detail in [Bergan et al. (2018)](https://doi.org/10.2514/6.2018-1221), [Bergan and Jackson (2018)](https://doi.org/10.12783/asc33/26003), and [Bergan (2019)](https://doi.org/10.2514/6.2019-1548). The model accounts for fiber kinking due to shear instability by considering an initial fiber misalignment, nonlinear shear stress-strain behavior via Ramberg-Osgood, and geometric nonlinearity. Fiber failure can be introduced by specifying a critical fiber rotation angle.

The required material properties are: XC, YC, wkb, alpha0, alpha_PL, and n_PL. The [feature flag](#controlling-which-features-are-enabled) for fiber compression should be set to '3', '4', or '5' to activate this model feature. Model '3' enables in-plane (1-2 plane) fiber kinking. Model '4' enables out-of-plane (1-3 plane) fiber kinking. Model '5' enables fiber kinking in both planes (uncoupled). This feature requires 25 state variables to be defined and initialized. The relevant state variables are:
- `CDM_phi0_12`: initial fiber misalignment (radians) in the 1-2 plane.
- `CDM_phi0_13`: initial fiber misalignment (radians) in the 1-3 plane.
- `CDM_gamma_12`: rotation of the fibers due to loading (radians) in the 1-2 plane.
- `CDM_gamma_13`: rotation of the fibers due to loading (radians) in the 1-3 plane.
- `CDM_Fbi`: the components of the first column of `Fm` used for decomposing the element where `i=1,2,3`.

The current fiber misalignment is `CDM_phi0_1i + CDM_gamma_1i` where `i=2 or 3`.

The initial conditions for the state variable `CDM_phi0_12` and `CDM_phi0_13` determine the initial fiber misalignments as described in [initial conditions](#initial-conditions).

A fiber failure criterion described in [Bergan and Jackson (2018)](https://doi.org/10.12783/asc33/26003) is implemented to represent the material behavior in confined conditions under large strains (post failure). The fiber failure criterion is satisfied when

*&phi;* &ge; *&phi;*<sub>ff,c</sub>

where *&phi;* is the current fiber rotation. Once the fiber failure criterion is satisfied, the plastic shear strain is held constant. The value for *&phi;*<sub>ff,c</sub> is defined as the parameter `fkt_fiber_failure_angle` since it is not a well-defined material property. The fiber failure criterion is disabled when *&phi;*<sub>ff,c</sub> < 0. The same angle is used for in-plane and out-of-plane kinking.

The fiber kinking theory model implemented here is preliminary and has some known shortcomings and caveats:
- The model has only been tested for C3D8R. Limited application with C3D6 demonstrated issues. No testing has been completed for other element types.
- The interaction of this model with matrix cracking has not been fully tested and verified.
- No effort has been made to model progressive crushing.
- Other mechanisms of fiber compressive failure (e.g., shear driven fiber breaks) are not accounted for. An outcome of this is that the model predicts the material does not fail if shear deformation is fully constrained.
- No special consideration for load reversal has been included.

Relevant single element tests are named starting with `test_C3D8R_fiberCompression_FKT`.

### Friction
Friction is modeled on the damaged fraction of the cross-sectional area of DGD cracks using the approach of [Alfano and Sacco (2006)](https://doi.org/10.1002/nme.1728). The coefficient of friction *&mu;* must be defined to account for friction on the failed crack surface.

The amount of sliding which has taken place in the longitudinal and transverse directions are stored in state variables `CDM_slide1` and `CDM_slide2`, respectively.

### Strain definition
The strain is calculated using the deformation gradient tensor provided by the Abaqus solver. The default strain definition used is the Green-Lagrange strain:

*E* = (*F*<sup>T</sup>*F* - *I*)/2

Hooke's law is applied using the Green-Lagrange strain to calculate the 2<sup>nd</sup> Piola-Kirchhoff stress *S*.

### Fiber nonlinearity
Nonlinear elastic behavior in the fiber direction can be introduced with the material property c<sub>*l*</sub>. The expression used follows [Kowalski (1988)](https://doi.org/10.1520/STP26136S):

*E<sub>1</sub>* = *E<sub>1</sub>*(1 + c<sub>*l*</sub>*&epsilon;*<sub>11</sub>)

By default, fiber nonlinearity is disabled by setting c<sub>*l*</sub> = 0.


## Elements
CompDam_DGD has been developed and tested using the Abaqus three-dimensional, reduced-integration `C3D8R` solid elements. Limited testing has been performed using the `CPS4R` plane stress element, the `SC8R` continuum shell element, the fully-integrated `C3D8` solid element, and the `COH3D8` cohesive element.

Because CompDam_DGD is a material model, it is expected to be compatible with structural elements generally. However, users are advised to perform tests with any previously untested element types before proceeding to use CompDam_DGD in larger structural models.


## Material properties
A set of material properties must be defined for the material of interest. This section describes how to specify the material properties.

### Defining material properties
Material properties can be defined in the input deck or in a separate `.props` file. Definition of the material properties in a `.props` file is more convenient and generally preferred since it isolates the material properties from the structural model definition.

#### Defining the material properties in a `.props` file
Using a `.props` file is a versatile means of defining material properties. The subroutine looks for a file named as `jobName_materialName` where the job name is the name of the Abaqus job (default is input deck name) and the material is name assigned as `*Material, name=materialName` in the input deck. If no file is found, then the subroutine looks for `materialName.props`. The `.props` file must be located in the Abaqus working directory.

The `.props` should contain one property per line with the format `[NAME] = [VALUE]` where the name is symbolic name for the property and the value is assigned value for the property. Blank lines or commented text (denoted by `//`) is ignored. See the [table of material properties](#table-of-material-properties) for a complete list of material property symbolic names, acceptable values, and recommended test methods for characterizing the properties.

When a `.props` is used to define the material properties, the feature flags and thickness still must be defined in the input deck.

#### Defining the material properties in the input deck
Material properties can be defined in the input deck. Any optional material property can be left blank and the corresponding feature(s) will be disabled. The ordering of the material properties for the input deck definition is given in the first (#) column of the [table of material properties](#table-of-material-properties).

#### Table of material properties
| #  |         Symbol         |   Name   |                  Description                  |                       Units                    |            Admissible values             | Recommended Test |
|----|------------------------|----------|-----------------------------------------------|------------------------------------------------|------------------------------------------|-----------------|
|  9 | *E<sub>1</sub>*        | E1       | Longitudinal Young's modulus                  | F/L<sup>2</sup>                                | 0 < *E<sub>1</sub>* < &infin;            | ASTM D3039      |
| 10 | *E<sub>2</sub>*        | E2       | Transverse Young's modulus                    | F/L<sup>2</sup>                                | 0 < *E<sub>2</sub>* < &infin;            | ASTM D3039      |
| 11 | *G<sub>12</sub>*       | G12      | In-plane Shear modulus                        | F/L<sup>2</sup>                                | 0 < *G<sub>12</sub>* < &infin;           | ASTM D3039      |
| 12 | *&nu;<sub>12</sub>*    | nu12     | Major Poisson's ratio                         | -                                              | 0 &le; *&nu;<sub>12</sub>* &le; 1        | ASTM D3039      |
| 13 | *&nu;<sub>23</sub>*    | nu23     | Minor Poisson's ratio                         | -                                              | 0 &le; *&nu;<sub>23</sub>* &le; 1        |                 |
|    | ===                    |          |                                               |                                                |                                          |                 |
| 14 | *Y<sub>T</sub>*        | YT       | Transverse tensile strength                   | F/L<sup>2</sup>                                | 0 < *Y<sub>T</sub>* < &infin;            | ASTM D3039      |
| 15 | *S<sub>L</sub>*        | SL       | Shear strength                                | F/L<sup>2</sup>                                | 0 < *S<sub>L</sub>* < &infin;            |                 |
| 16 | *G<sub>Ic</sub>*       | GYT      | Mode I fracture toughness                     | F/L                                            | 0 < *G<sub>Ic</sub>* < &infin;           | ASTM D5528      |
| 17 | *G<sub>IIc</sub>*      | GSL      | Mode II fracture toughness                    | F/L                                            | 0 < *G<sub>IIc</sub>* < &infin;          | ASTM D7905      |
| 18 | *&eta;*                | eta_BK   | BK exponent for mode-mixity                   | -                                              | 0 < *&eta;* < &infin;                 |                 |
| 19 | *Y<sub>C</sub>*        | YC       | Transverse compressive strength               | F/L<sup>2</sup>                                | 0 < *Y<sub>C</sub>* < &infin;            | ASTM D3410      |
| 20 | *&alpha;<sub>0</sub>*  | alpha0   | Fracture plane angle for pure trans. comp.    | Radians                                        | 0 &le; *&alpha;<sub>0</sub>* &le; &pi;/2 |                 |
|    | ------                 |          |                                               |                                                |                                          |                 |
| 21 | *E<sub>3</sub>*        | E3       | 3-direction Young's modulus                   | F/L<sup>2</sup>                                | 0 < *E<sub>3</sub>* < &infin;            |                 |
| 22 | *G<sub>13</sub>*       | G13      | Shear modulus in 1-3 plane                    | F/L<sup>2</sup>                                | 0 < *G<sub>13</sub>* < &infin;           |                 |
| 23 | *G<sub>23</sub>*       | G23      | Shear modulus in 1-2 plane                    | F/L<sup>2</sup>                                | 0 < *G<sub>23</sub>* < &infin;           |                 |
| 24 | *&nu;<sub>13</sub>*    | nu13     | Poisson's ratio in 2-3 plane                  | -                                              | 0 &le; *&nu;<sub>13</sub>* &le; 1        |                 |
|    | ------                 |          |                                               |                                                |                                          |                 |
| 25 | *&alpha;<sub>11</sub>* | alpha11  | Coefficient of long. thermal expansion        | /&deg;                                         | -1 &le; *&alpha;<sub>11</sub>* &le; 1    |                 |
| 26 | *&alpha;<sub>22</sub>* | alpha22  | Coefficient of tran. thermal expansion        | /&deg;                                         | -1 &le; *&alpha;<sub>22</sub>* &le; 1    |                 |
|    | ------                 |          |                                               |                                                |                                          |                 |
| 27 | *&alpha;<sub>PL</sub>* | alpha_PL | Nonlinear shear parameter                     | (F/L<sup>2</sup>)<sup>1-*n*<sub>PL</sub></sup> | 0 &le; *&alpha;<sub>PL</sub>* < &infin;  |                 |
| 28 | *n<sub>PL</sub>*       | n_PL     | Nonlinear shear parameter                     | -                                              | 0 &le; *n<sub>PL</sub>* < &infin;        |                 |
|    | ------                 |          |                                               |                                                |                                          |                 |
| 29 | *X<sub>T</sub>*        | XT       | Long. tensile strength                        | F/L<sup>2</sup>                                | 0 < *X<sub>T</sub>* < &infin;            | ASTM D3039      |
| 30 | *f<sub>XT</sub>*       | fXT      | Long. tensile strength ratio                  | -                                              | 0 &le; *f<sub>XT</sub>* &le; 1           |                 |
| 31 | *G<sub>XT</sub>*       | GXT      | Long. tensile fracture toughness              | F/L                                            | 0 < *G<sub>XT</sub>* < &infin;           |                 |
| 32 | *f<sub>GXT</sub>*      | fGXT     | Long. tensile fracture toughness ratio        | -                                              | 0 &le; *f<sub>GXT</sub>* &le; 1          |                 |
|    | ------                 |          |                                               |                                                |                                          |                 |
| 33 | *X<sub>C</sub>*        | XC       | Long. compressive strength                    | F/L<sup>2</sup>                                | 0 < *X<sub>C</sub>* < &infin;            | ASTM D3410      |
| 34 | *f<sub>XC</sub>*       | fXC      | Long. compression strength ratio              | -                                              | 0 &le; *f<sub>XC</sub>* &le; 1           |                 |
| 35 | *G<sub>XC</sub>*       | GXC      | Long. compression fracture toughness          | F/L                                            | 0 < *G<sub>XC</sub>* < &infin;           |                 |
| 36 | *f<sub>GXC</sub>*      | fGXC     | Long. compression fracture toughness ratio    | -                                              | 0 &le; *f<sub>GXC</sub>* &le; 1          |                 |
|    | ------                 |          |                                               |                                                |                                          |                 |
| 37 | *c<sub>l</sub>*        | cl       | Fiber nonlinearity coefficient                | -                                              | 0 &le; *c<sub>l</sub>* 33                |                 |
| 38 | *w<sub>kb</sub>*       | w_kb     | Width of the kink band                        | L                                              | 0 &le; *w<sub>kb</sub>* &infin;          |                 |
| 40 | *&mu;*                 | mu       | Coefficient of friction                       | -                                              | 0 &le; *&mu;* &le; 1                     |                 ||

Notes:
- The first five inputs (above the ===) are required
- Properties for each feature are grouped and separated by ------
- The number in the first column corresponds to the property order when defined in the input deck
- Properties not listed in the table above (see next section):
  1. [feature flags](#controlling-which-features-are-enabled)
  2. *reserved*
  3. [thickness](#definition-of-thickness)
  4. *reserved*
  5. *reserved*
  6. *reserved*
  7. *reserved*
  8. *reserved*
- &infin; is calculated with the Fortran intrinsic `Huge` for double precision
- In the event that both a `.props` file is found and material properties are specified in the input deck (`nprops > 8`), then the material properties from the input deck are used and a warning is used.

### Required inputs for the `*Material` data lines in the input deck
The feature flags and thickness are defined in the input deck on the material property data lines. These properties must be defined in the input deck whether the other material properties are defined via the .props file or via the input deck. While feature flags and thickness are not material properties per se, they are used in controlling the behavior of the material model.

#### Controlling which features are enabled
Model features can be enabled or disabled by two methods. The first method is specifying only the material properties required for the features you would like to enable. CompDam_DGD disables any feature for which all of the required material properties have not been assigned. If an incomplete set of material properties are defined for a feature, a warning is issued.

The second method is by specifying the status of each feature directly as a material property in the input deck. Each feature of the subroutine is controlled by a position in an integer, where 0 is disabled and 1 is enabled. In cases where mutually exclusive options are available, numbers greater than 1 are used to specify the particular option to use.

The positions correspond to the features as follows:
- Position 1: Matrix damage (1=intra-laminar cracking in solid elements, 2=interlaminar cracking in cohesive elements)
- Position 2: Shear nonlinearity (1=Ramberg-Osgood 1-2 plane, 2=Schapery, 3=Ramberg-Osgood 3-D, 4=Ramberg-Osgood 1-3 plane, 5=Schaefer || more information [here](#shear-nonlinearity))
- Position 3: Fiber tensile damage
- Position 4: Fiber compression damage (1=max strain, 2=N/A, 3=FKT-12, 4=FKT-13, 5=FKT-3D || more information [here](#fiber-compression-damage))
- Position 5: Energy output contribution (0=all mechanisms, 1=only fracture energy, 2=only plastic energy)
- Position 6: Friction

For example, `101000` indicates that the model will run with matrix damage and fiber tension damage enabled; `120001` indicates that the model will run with matrix damage, in-plane shear nonlinearity using Schapery theory, and friction; and `200000` indicates that the model is being applied to cohesive elements.

#### Definition of thickness
Length along the thickness-direction associated with the current integration point.

## State variables
The table below lists all of the state variables in the model. The model requires a minimum of 18 state variables. Additional state variables are defined depending on which (if any) shear nonlinearity and fiber compression features are enabled. For fiber compression model 1: nstatev = 19 and for model 3: nstatev = 25. For shear nonlinearity models 3 or 4: nstatev = 21.

| # | Name             | Description                                                           |
|---|------------------|-----------------------------------------------------------------------|
|  1| `CDM_d2`         | d2, Matrix cohesive damage variable                                   |
|  2| `CDM_Fb1`        | Bulk material deformation gradient tensor component 12                |
|  3| `CDM_Fb2`        | Bulk material deformation gradient tensor component 22                |
|  4| `CDM_Fb3`        | Bulk material deformation gradient tensor component 32                |
|  5| `CDM_B`          | Mode Mixity (*G*<sub>II</sub> / (*G*<sub>I</sub> + *G*<sub>II</sub>)) |
|  6| `CDM_Lc1`        | Characteristic element length along 1-direction                       |
|  7| `CDM_Lc2`        | Characteristic element length along 2-direction                       |
|  8| `CDM_Lc3`        | Characteristic element length along 3-direction                       |
|  9| `CDM_FIm`        | Matrix cohesive failure criterion (del/del_0)                         |
| 10| `CDM_alpha`      | alpha, the cohesive surface normal [degrees, integer]                 |
| 11| `CDM_STATUS`     | STATUS (for element deletion)                                         |
| 12| `CDM_Plas12`     | 1-2 plastic strain                                                    |
| 13| `CDM_Inel12`     | 1-2 inelastic strain                                                  |
| 14| `CDM_FIfT`       | Failure index for fiber tension                                       |
| 15| `CDM_slide1`     | Cohesive sliding displacement, fiber direction                        |
| 16| `CDM_slide2`     | Cohesive sliding displacement, transverse direction                   |
| 17| `CDM_FIfC`       | Failure index for fiber compression                                   |
| 18| `CDM_d1T`        | Fiber tension damage variable                                         |
|---|------------------|-----------------------------------------------------------------------|
| 19| `CDM_d1C`        | Fiber compression damage variable                                     |
|---|------------------|-----------------------------------------------------------------------|
| 20| `CDM_Plas13`     | 1-3 plastic strain                                                    |
| 21| `CDM_Inel13`     | 1-3 inelastic strain                                                  |
|---|------------------|-----------------------------------------------------------------------|
| 22| `CDM_phi0_12`    | Initial fiber misalignment, 1-2 plane (radians)                       |
| 23| `CDM_gamma_12`   | Current rotation of the fibers due to loading, 1-2 plane (radians)    |
| 24| `CDM_phi0_13`    | Initial fiber misalignment, 1-3 plane (radians)                       |
| 25| `CDM_gamma_13`   | Current rotation of the fibers due to loading, 1-3 plane (radians)    |
| 26| `CDM_reserve`    | Reserved                                                              |
|---|------------------|-----------------------------------------------------------------------|
| 27| `CDM_Ep1`        | Plastic strain in 11 direction calculated using Schaefer Theory       |
| 28| `CDM_Ep2`        | Plastic strain in 22 direction calculated using Schaefer Theory       |
| 29| `CDM_Ep3`        | Plastic strain in 33 direction calculated using Schaefer Theory       |
| 30| `CDM_Ep4`        | Plastic strain in 12 direction calculated using Schaefer Theory       |
| 31| `CDM_Ep5`        | Plastic strain in 23 direction calculated using Schaefer Theory       |
| 32| `CDM_Ep6`        | Plastic strain in 31 direction calculated using Schaefer Theory       |
| 33| `CDM_fp1`        | Yield criterion (effective stress) calculated using Schaefer Theory   |

When using the material model with cohesive elements, a different set of state variables are used. Cohesive state variables with a similar continuum damage mechanics counterpart utilize the same state variable number. The cohesive element material model requires fewer state variables, however, resulting in ""gaps"" in the cohesive state variable numbering.

| # | Name             | Description                                                           |
|---|------------------|-----------------------------------------------------------------------|
|  1| `COH_dmg`        | Cohesive damage variable                                              |
|  2| `COH_delta_s1`   | Displacement-jump in the first shear direction                        |
|  3| `COH_delta_n`    | Displacement-jump in the normal direction                             |
|  4| `COH_delta_s2`   | Displacement-jump in the second shear direction                       |
|  5| `COH_B`          | Mode Mixity (*G*<sub>II</sub> / (*G*<sub>I</sub> + *G*<sub>II</sub>)) |
|  9| `COH_FI`         | Cohesive failure criterion (del/del_0)                                |
| 15| `COH_slide1`     | Cohesive sliding displacement, fiber direction                        |
| 16| `COH_slide2`     | Cohesive sliding displacement, transverse direction                   |

### Initial conditions
All state variables should be initialized using the `*Initial conditions` command. As a default, all state variables should be initialized as zero, except `CDM_alpha`, `CDM_STATUS`, `CDM_phi0_12`, and `CDM_phi0_13`.

The initial condition for `CDM_alpha` can be used to specify a predefined angle for the cohesive surface normal. To specify a predefined `CDM_alpha`, set the initial condition for `CDM_alpha` to an integer (degrees). The range of valid values for `CDM_alpha` depends on the aspect ratio of the element, but values in the range of 0 to 90 degrees are always valid. Setting `CDM_alpha` to -999 will make the subroutine evaluate cracks every 10 degrees in the 2-3 plane to find the correct crack initiation angle. Note that `CDM_alpha` is measured from the 2-axis rotating about the 1-direction. The amount by which alpha is incremented when evaluating matrix crack initiation can be changed from the default of 10 degrees by modifying `alpha_inc` in the `CompDam.parameters` file. Note that `CDM_alpha = 90` only occurs when `CDM_alpha` is initialized as 90; when `CDM_alpha` is initialized to -999, the value of 90 is ignored in the search to find the correct initiation angle since it is assumed that delaminations are handled elsewhere in the finite element model (e.g., using cohesive interface elements).

Since `CDM_STATUS` is used for element deletion, always initialize `CDM_STATUS` to 1.

The initial condition for `CDM_phi0_12` and `CDM_phi0_13` are used to specify the initial fiber misalignment. One of the followings options is used depending on the initial condition specified for `CDM_phi0_12` and `CDM_phi0_13` as follows:
- *&phi;<sub>0</sub>* = 0 :: The value for *&phi;<sub>0</sub>* is calculated for shear instability. For 3-D kinking, *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>* is required.
- *&phi;<sub>0</sub>* &le; 0.5 :: The value provided in the initial condition is used as the initial fiber misalignment.
- *&phi;<sub>0</sub>* = 1 :: A pseudo random uniform distribution varying spatially in the 1-direction is used. The spatial distribution algorithm relies on an uniform element size and fiber aligned mesh. The random number generator can be set to generate the same realizations or different realizations on multiple nominally identical analyses using the Boolean parameter `fkt_random_seed`. When using the random distribution for *&phi;<sub>0</sub>*, the characteristic length must be set to include 6 components: `*Characteristic Length, definition=USER, components=6`. For 3-D kinking, *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>* is required.
- *&phi;<sub>0</sub>* = 2 :: Identical to *&phi;<sub>0</sub>* = 1, with the exception that a different realization is calculated for each ply. For 3-D kinking, *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>* is required.
- *&phi;<sub>0</sub>* = 3 :: (Intended for use with 3-D FKT only) A pseudo random distribution varying spatially in the 1-direction is used with a 2-parameter lognormal distribution for the polar angle and a normal distribution for the azimuthal angle. The parameters starting with `fkt_init_misalignment` are used to control the polar and azimuthal distributions. Requires *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>*.

Pre-existing damage can be modeled by creating an element set for the damaged region and specifying different initial conditions for this element set. For example, to create an intraply matrix crack with no out-of-plane orientation, the following initial conditions could be specified for the cracked elements:

    *Initial Conditions, type=SOLUTION
     damaged_elset,  1.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,
              0.d0,  0.d0,     0,     1,  0.d0,  0.d0,  0.d0,  0.d0,
              0.d0,  0.d0,  0.d0,  0.d0


## Fatigue analyses
The cohesive fatigue constitutive model in CompDam can predict the initiation and the propagation of matrix cracks and delaminations as a function of fatigue cycles. The analyses are conducted such that the applied load (or displacement) corresponds to the maximum load of a fatigue cycle. The intended use is that the maximum load (or displacement) is held constant while fatigue damage develops with increasing step time. The constitutive model uses a specified load ratio *R*<sub>min</sub>/*R*<sub>max</sub>, the solution increment, and an automatically-calculated cycles-per-increment ratio to accumulate the damage due to fatigue loading. The cohesive fatigue model response is based on engineering approximations of the endurance limit as well as the Goodman diagram. No additional material inputs must be defined or state variables requested beyond those required for a quasi-static analysis step. This approach can predict the stress-life diagrams for crack initiation, the Paris law regime, as well as the transient effects of crack initiation and stable tearing.

A detailed description of the cohesive fatigue implemented herein is available in a [2018 NASA technical paper by Carlos D√°vila](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20180004395.pdf).

### Usage
The fatigue capability of CompDam is disabled by default. To run a fatigue analysis, one of the analysis steps must be identified as a fatigue step. A step is identified as a fatigue step by setting the `fatigue_step` parameter to the target step number, e.g., `fatigue_step = 2` for the second analysis step to be a fatigue step. The first analysis step cannot be a fatigue step, as the model is assumed to be unloaded at that point.

The load ratio *R*<sub>min</sub>/*R*<sub>max</sub> has a default value of 0.1, and can be changed using the parameter `fatigue_R_ratio`.

An example of a double cantilever beam subjected to fatigue under displacement-control is included in the `examples/` directory. The geometry and conditions of this example problem correspond to the results presented in Figure 20 of [D√°vila (2018)](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20180004395.pdf).

### Interpreting the results of a fatigue analysis
Within a fatigue step, each solution increment represents either a number of fatigue cycles or a fractional part of a single fatigue cycle. During the solution, the number of fatigue cycles per solution increment changes based on the maximum amount of energy dissipation in any single element. If the rate of energy dissipation is too high (as defined by the parameter `fatigue_damage_max_threshold`), the increments-to-cycles ratio is decreased. If the rate of energy dissipation is too low (as defined by the parameter `fatigue_damage_min_threshold`), the increments-to-cycles ratio is increased. The parameter `cycles_per_increment_init` defines the initial ratio of fatigue cycles per solution increment. Any changes to increments-to-cycles ratio are logged in an additional output file ending in `_inc2cycles.log`, with columns for the fatigue step solution increment, the updated increments-to-cycles ratio, and the accumulated fatigue cycles.


## Implicit solver compatibility
The repository includes a developmental capability to run the CompDam VUMAT in an Abaqus/Standard analysis using a wrapper, `for/vumatWrapper.for`, which translates between the UMAT and VUMAT user subroutine interfaces. The intended usage is for Abaqus/Standard runs with little or no damage.

### Usage
To run an analysis with CompDam in Abaqus/Standard, the following input deck template is provided. Note that 9 additional state variables are required.

    *Section controls, name=control_name, hourglass=ENHANCED
    **
    *Material, name=IM7-8552
    *Density
     1.57e-09,
    *User material, constants=40
    ** 1              2  3          4  5  6  7  8
    ** feature flags,  , thickness, 4, 5, 6, 7, 8
              100001,  ,       0.1,  ,  ,  ,  ,  ,
    **
    **  9         10        11        12        13        14        15        16
    **  E1,       E2,       G12,      nu12,     nu23,     YT,       SL        GYT,
        171420.0, 9080.0,   5290.0,   0.32,     0.52,     62.3,     92.30,    0.277,
    **
    **  17        18        19        20        21        22        23        24
    **  GSL,      eta_BK,   YC,       alpha0    E3,       G13,      G23,      nu13,
        0.788,    1.634,    199.8,    0.925,      ,          ,         ,          ,
    **
    **  25        26        27        28        29        30        31        32
    **  alpha11,  alpha22,  alpha_PL, n_PL,     XT,       fXT,      GXT,      fGXT,
        -5.5d-6,  2.58d-5,          ,     ,     2326.2,   0.2,      133.3,    0.5,
    **
    **  33        34        35        36        37        38        39        40
    **  XC,       fXC,      GXC,      fGXC,       cl,     w_kb,     None,     mu
        1200.1,      ,         ,          ,         ,     0.1,          ,     0.3
    **
    *Depvar, delete=11
      28,
      1, CDM_d2
      2, CDM_Fb1
      3, CDM_Fb2
      4, CDM_Fb3
      5, CDM_B
      6, CDM_Lc1
      7, CDM_Lc2
      8, CDM_Lc3
      9, CDM_FIm
     10, CDM_alpha
     11, CDM_STATUS
     12, CDM_Plas12
     13, CDM_Inel12
     14, CDM_FIfT
     15, CDM_slide1
     16, CDM_slide2
     17, CDM_FIfC
     18, CDM_d1T
     19, CDM_d1C
     20, CDM_DIRECT11
     21, CDM_DIRECT21
     22, CDM_DIRECT31
     23, CDM_DIRECT12
     24, CDM_DIRECT22
     25, CDM_DIRECT32
     26, CDM_DIRECT13
     27, CDM_DIRECT23
     28, CDM_DIRECT33
    *User defined field
    **
    ** INITIAL CONDITIONS
    **
    *Initial Conditions, Type=Solution
    ALL_ELEMS,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,
    0.d0,  0.d0,  -999,     1,  0.d0,  0.d0,  0.d0,  0.d0,
    0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,
    0.d0,  0.d0,  0.d0,  0.d0,  0.d0
    *Initial Conditions, Type=Field, Variable=1
    GLOBAL,  0.d0
    ** GLOBAL is an nset with all nodes attached to CompDam-enabled elements
    ** In each step, NLGEOM=YES must be used. This is NOT the default setting.

### Current limitations
As the `vumatWrapper` is a developmental capability, several important limitations exist at present:
1. The material Jacobian tensor is hard-coded in `for/vumatWrapper.for` for IM7/8552 elastic stiffnesses. A more general Jacobian is needed.
2. The material response can become inaccurate for large increments in rotations. If large rotations occur, small increments must be used. A cut-back scheme based on rotation increment size is needed.
3. Testing has been conducted on the C3D8R element type only.


## Example problems
The directory `examples/` includes example models that use CompDam along with corresponding files that defined the expected results (for use with Abaverify, following the same pattern as the test models in the `tests/` directory. The file `example_runner.py` can be used to automate submission of several models and/or for automatically post-processing the model results to verify that they match the expected results.


## Advanced debugging
Using an interactive debugger helps to identify issues in the Fortran code. Abaqus knowledge base article QA00000007986 describes the details involved. The following is a quick-start guide for direct application to CompDam.

Several statements for debugging need to be uncommented in the environment file. Follow these steps:
1. Copy your system environment file to your local working directory. For the example below, copy the environment file to the `tests` directory.
2. Edit the local environment file: uncomment lines that end with `# <-- Debugging`, `# <-- Debug symbols`, and `# <-- Optimization Debugging`

Run the job with the `-debug` and `-explicit` arguments. For example:
```
$ abaqus -j test_C3D8R_fiberTension -user ../for/CompDam_DGD.for -double both -debug -explicit
```

This command should open the [Visual Studio debugging software](https://msdn.microsoft.com/en-us/library/sc65sadd.aspx) automatically. Open the source file(s) to debug. At a minimum, open the file with the subroutine entry point `for/CompDam_DGD.for`. Set a break point by clicking in the shaded column on the left edge of the viewer. The break point will halt execution. Press <kbd>F5</kbd> to start the solver. When the break point is reached, a yellow arrow will appear and code execution will pause. Press <kbd>F5</kbd> to continue to the next break point, press <kbd>F11</kbd> to execute the next line of code following execution into function calls (Step Into), or press <kbd>F10</kbd> to execute the next line of code but not follow execution into function calls (Step Over).

To stop execution, close the Visual Studio window. Choose stop debugging and do not save your changes.

[More tips on debugging Fortran programs from Intel](https://software.intel.com/en-us/articles/tips-for-debugging-run-time-failures-in-intel-fortran-applications).

## Python extension module
CompDam can be compiled into a [Python extension module](https://docs.python.org/2/extending/extending.html), which allows many of the Fortran subroutines and functions in the `for` directory to be called from Python. The Python package [`f90wrap`](https://github.com/jameskermode/f90wrap) is used to automatically generate the Python extension modules that interface with the Fortran code. This Python extension module functionality is useful for development and debugging.

### Dependencies and setup
The python extension module requires some additional dependencies. First, the procedure only works on Linux using the bash shell. Windows users can use the [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/about). In addition, `gfortran` 4.6+ is required. Type `gfortran --version` to check if you have this available. The remaining dependencies are python packages and can be installed as follows. The Python extension module works with Python 2 and 3; Python 2.7 is used for consistency with Abaqus in the following description.

Using [Conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) significantly simplifies the setup process, so it is assumed that you have a recent version of Conda available (see the [Conda installation guide](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)). Further, the bash scripts described below include calls to Conda, so they will not work correctly without installing and configuring Conda as follows. Add the Conda-Forge channel by typing:
```
$ conda config --add channels conda-forge
```

Conda stores Python packages in containers called environments. Create a new environment:
```
$ conda create --name compdam python=2.7
```
and switch to your new environment:
```
$ source activate compdam
```
which will add `(compdam)` to the prompt. Install `numpy`, `matplotlib`, and `f90wrap` by typing:
```
(compdam) $ conda install numpy matplotlib f90wrap
```
After typing 'y' in response to the prompt asking if you would like to proceed, Conda will install `f90wrap` and all of its dependencies. This completes the setup process. These steps only need to be executed once.

Note, you can exit the Conda environment by typing `source deactivate`. When you open a new session, you will need to activate the Conda environment by typing `source activate compdam`.

### Example usage
This section describes how to compile CompDam into a Python extension module and run a simple example.

The relevant files are in the `pyextmod` directory, so set `pyextmod` as your current working directory.

The bash shell is required. Type `bash` to open a bash shell session if you are using a different shell. Activate the environment in which you have installed the dependencies listed above, e.g. `source activate compdam`.

Next compile the python extension module by executing `make` in the `pyextmod` directory as follows:
```
(compdam) CompDam_DGD/pyextmod> make
```

When you execute `make`, the Fortran modules in the `for` directory are compiled to object files, the shared library `_CompDam_DGD.so` is built, and the Python extension module interface `CompDam_DGD.py` is created. A large amount of output is given in the terminal. After the module is created, most of the functionality of CompDam is available in python with `import CompDam_DGD`.

The file `test_pyextmod_dgdevolve.py` shows an example of how the python extension module can be used. Just as when CompDam_DGD is used in Abaqus, `CompDam_DGD.py` expects `CompDam.parameters` and `props` files (if provided) are located in the working directory. Note the `test_pyextmod_dgdevolve.py` loads a visualization tool that shows how the element deforms as equilibrium is sought by the DGD algorithm.

It is necessary to recompile the CompDam after making changes to the Fortran code. Recompile with the `make` command. It is a good idea to run `make clean` before rerunning `make` to remove old build files.

Note that portions of CompDam that are specific to Abaqus are hidden from `f90wrap` using the preprocessor directive `#ifndef PYEXT`.

### Associated scripts
In the `tests` directory the shell scripts `pyextmod_compile.sh` and `pyextmod_run.sh` are available to help streamline execution of the python extension module. These two scripts assume that Conda environment called `compdam` is available with `abaverify` and `f90wrap`. Both must be executed with the `-i` option. The script `pyextmod_run.sh` loads a debug file and executes the specified DGD routine. The DGD routine and the debug file are specified as arguments as follows:
```
$ bash -i pyextmod_run.sh dgdevolve <job-name>
```
The `<job-name>` is the Abaqus job name where it is assumed that the debug file resides in the testOutput folder with the name `job-name-1-debug-0.py`

In the `pyextmod` directory, the `helpers.py` file includes logic to load debug.py files.


## Summary of tests classes
This section includes a brief summary of each test implemented in the `tests` folder. The input deck file names briefly describe the test. All of the input decks start with `test_<elementType>_` and end with a few words describing the test. A more detailed description for each is given below:
- *elastic_fiberTension*: Demonstrates the elastic response in the 1-direction under prescribed extension. The 1-direction stress-strain curve has the modulus E1.
- *elastic_matrixTension*: Demonstrates the elastic response in the 2-direction under prescribed extension. The 2-direction stress-strain curve has the modulus E2.
- *elastic_simpeShear12*: Demonstrates the elastic response in the 1-2 plane. The 1-2 plane stress-strain curve has the module G12.
- *elementSize*: Verifies that the characteristic element lengths Lc1, Lc2, and Lc3 are being properly calculated.
- *error*: Verifies that analyses can cleanly terminate upon encountering an error within the user subroutine.
- *failureEnvelope_sig11sig22*: A parametric model in which *&sigma;<sub>11</sub>* is swept from *-X<sub>C</sub>* to *X<sub>T</sub>* and *&sigma;<sub>22</sub>* is swept from *-Y<sub>C</sub>* to *Y<sub>T</sub>* in order to re-create the corresponding failure envelope.
- *failureEnvelope_sig12sig22*: A parametric model in which *&tau;<sub>12</sub>* is swept from *0* to *S<sub>L</sub>* and *&sigma;<sub>22</sub>* is swept from *-Y<sub>C</sub>* to *Y<sub>T</sub>* in order to re-create the corresponding failure envelope.
- *failureEnvelope_sig12sig23*: A parametric model in which *&tau;<sub>12</sub>* is swept from *0* to *S<sub>L</sub>* and *&tau;<sub>23</sub>* is swept from *0* to *S<sub>T</sub>* in order to re-create the corresponding failure envelope.
- *fatigue_normal*: Demonstrates the traction-displacement curve of a cohesive law subjected to mode I fatigue loading.
- *fatigue_shear13*: Demonstrates the traction-displacement curve of a cohesive law subjected to shear fatigue loading in the 1-3 plane.
- *fiberCompression_BL*: Demonstrates the constitutive response in the 1-direction under prescribed shortening. The 1-direction stress-strain curve has a bilinear softening law. A conventional CDM approach to material degradation is used.
- *fiberCompression_FKT*: Demonstrates the constitutive response in the 1-direction under prescribed shortening. The fiber kink band model is used. `FF` indicates that the fiber failure criterion is enabled. `FN` indicates that fiber nonlinearity is enabled.
- *fiberLoadReversal*: Demonstrates the constitutive response in the 1-direction under prescribed extension and shortening reversals. The 1-direction stress-strain curve shows the intended behavior under load reversal.
- *fiberTension*: Demonstrates the constitutive response in the 1-direction under prescribed extension. The 1-direction stress-strain curve is trilinear.
- *matrixCompression*: Demonstrates the constitutive response in the 2-direction under prescribed compression displacement.
- *matrixTension*: Demonstrates the constitutive response in the 2-direction under prescribed extension. The 2-direction stress-strain curve is bilinear.
- *mixedModeMatrix*: A parametric model used to stress the internal DGD convergence loop. The crack angle *&alpha;* and the direction of loading are varied. Large tensile and compressive displacements are prescribed to ensure the DGD method is able to find converged solutions under a wide variety of deformations.
- *nonlinearShear12*: Demonstrates the nonlinear Ramberg-Osgood constitutive response under prescribed simple shear deformation in the 1-2 plane with matrix damage enabled. Several cycles of loading and unloading are applied, with increasing peak displacements in each cycle.
- *nonlinearShear12_loadReversal*: Demonstrates the response of the Ramberg-Osgood model under load reversal in the 1-2 plane. Several cycles of loading and unloading are applied, with inelastic strain accumulated throughout the load history. Damage is disabled.
- *nonlinearShear13*: Demonstrates the nonlinear Ramberg-Osgood constitutive response under prescribed simple shear deformation in the 1-3 plane with matrix damage enabled. Several cycles of loading and unloading are applied, with increasing peak displacements in each cycle.
- *nonlinearShear13_loadReversal*: Demonstrates the response of the Ramberg-Osgood model under load reversal in the 1-3 plane. Several cycles of loading and unloading are applied, with inelastic strain accumulated throughout the load history. Damage is disabled.
- *schapery12*: Demonstrates the in-plane response to prescribed simple shear deformation for the Schapery micro-damage model. Several cycles of loading and unloading are applied, with increasing peak shear displacements in each cycle.
- *simpleShear12*: Demonstrates the constitutive response under prescribed simple shear. The shear stress-strain curve is bilinear.
- *simpleShear12friction*: Demonstrates the constitutive response under prescribed simple shear with friction enabled. The element is loaded under transverse compression and then sheared. Shows the friction-induced stresses.

## Contributing
We invite your contributions to CompDam_DGD! Please submit contributions (including a test case) with pull requests so that we can reproduce the behavior of interest. Commits history should be clean. Please contact the developers if you would like to make a major contribution to this repository. Here is a [checklist](contributing-checklist.md) that we use for contributions.

## Citing CompDam
If you use CompDam, please cite using the following BibTex entry:

    @misc{CompDam,
    title={Comp{D}am - {D}eformation {G}radient {D}ecomposition ({DGD}), v2.5.0},
    author={Frank A. Leone and Andrew C. Bergan and Carlos G. D\'{a}vila},
    note={https://github.com/nasa/CompDam\_DGD},
    year={2019}
    }
"
102,nasa/astrobot,JavaScript,"# astrobot
A Slack/Discord bot integration with NASA data.

This is a Slack/Discord bot that is designed to use the NASA APOD API to allow users to query the API through Slack or Discord. 

## Contributing
We do accept pull requests from the public. Please note that we can be slow to respond. Please be patient. Pull requests should not impact previous functionality. 

## Feedback
Star this repo if you found it useful. Use the github issue tracker to give feedback on this repo.
"
103,nasa/irg_open,C++,"Notices:
--------
Copyright ¬© 2020 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers
-----------
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.


irg_open
==============================
This is a repository of tools to be shared across projects. It is open source,
so do not add anything private or EAR/ITAR restricted. It originated in the
Intelligent Robotics Group (IRG) at NASA Ames Research Center. Hence the name.

"
104,nasa/tblCRCTool,C,"![Static Analysis](https://github.com/nasa/tblCRCTool/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/tblCRCTool/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : Tool : Table CRC Generator

This repository contains NASA's Table CRC Generator Tool (tblCRCTool), which is a framework component of the Core Flight System.

This lab application is a ground utility to generate binary table CRCs for cFS. It is intended to be located in the `tools/tblCRCTool` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.

## Version Notes

### Development Build: 1.2.0-rc1+dev25

- Fix #43, Add Testing Tools to the Security Policy
- Fix #36 #38 #40 #41, Check lseek return and exit/error processing updates
- See <https://github.com/nasa/tblCRCTool/pull/89>

### Development Build: 1.2.0-rc1+dev19

- Changes CLI ""help"" option to use two dashes: `--help`
- Adds static analysis and format check to continuous integration workflow. Adds workflow status badges to ReadMe.
- Adds CodeQL Analysis to continuous integration workflow.
- See <https://github.com/nasa/tblCRCTool/pull/35>

### Development Build: 1.2.0-rc1+dev12

- Documentation: Add `Security.md` with instructions on reporting vulnerabilities
- Removes unimplemented CRC cases to eliminate static analysis warnings
- See <https://github.com/nasa/tblCRCTool/pull/29>

### Development Build: 1.2.0-rc1+dev3

- Use `sizeof()` instead of a hard coded value for the table file header size to keep this tool in sync if the size of the cFE file or table header should ever change.
- Update version baseline to v1.2.0-rc1
- Set REVISION number to 99 to indicate development version
- See <https://github.com/nasa/tblCRCTool/pull/25>

### Development Build: 1.1.0+dev7

- Create a version header file
- Report version when responding to `-help` command
- See <https://github.com/nasa/tblCRCTool/pull/22>

### Development Build: 1.1.1

- Apply Code Style
- See <https://github.com/nasa/tblCRCTool/pull/18>

### **_OFFICIAL RELEASE: 1.1.0 - Aquila_**

- Minor updates (see https://github.com/nasa/tblCRCTool/pull/12)
- Released as part of cFE 6.7.0, Apache 2.0

### **_OFFICIAL RELEASE: 1.0.0a_**

- Released as part of cFE 6.6.0a, Apache 2.0

NOTE - there are other parameter set management schemes used with the cFS (JSON, csv, etc) which may be more applicable for modern missions.  Contact the community as detailed below for more information.

## Known issues

This ground utility was developed for a specific mission/configuration, and may not be applicable for general use.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.

Official cFS page: http://cfs.gsfc.nasa.gov
"
105,nasa/elf2cfetbl,C,"![Static Analysis](https://github.com/nasa/elf2cfetbl/workflows/Static%20Analysis/badge.svg)
![Format Check](https://github.com/nasa/elf2cfetbl/workflows/Format%20Check/badge.svg)

# Core Flight System : Framework : Tool : ELF to cFE Table Converter

This repository contains NASA's ELF to cFE Table Converter Tool (elf2cfetbl), which is a framework component of the Core Flight System.

This lab application is a ground utility to convert ELF to cFE binary tables for cFS. It is intended to be located in the `tools/elf2cfetbl` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes this tool as a submodule, and includes build and execution instructions.

See README.txt for more information.

## Version History

### Development Build: v3.2.0-rc1+dev24
- Fix #73, Add Testing Tools to the Security Policy
- See <https://github.com/nasa/elf2cfetbl/pull/75>

### Development Build: v3.2.0+dev20
- Changes cpp-styled comments to c-style to meet coding standard
- See <https://github.com/nasa/elf2cfetbl/pull/72>

### Development Build: v3.2.0+dev9
- Restricts destination file permissions
- Squash int comparison warning
- Replace ctime (which generates LGTM warning) with ctime_r
- Deconflicts global/local parameters
- See <https://github.com/nasa/elf2cfetbl/pull/62>

### Development Build: v3.1.0+dev39

- Adds a null to the end of SrcFilename and DstFilename when using strncpy.
- Support ELF files that have all strings, including ELF section names, in one single "".strtab"" section in the ELF file.
- Version reporting now uses the version numbers defined in elf_version.h and reports build number.
- See  <https://github.com/nasa/elf2cfetbl/pull/47>

### Development Build: 3.1.5

- Apply code style
- See <https://github.com/nasa/elf2cfetbl/pull/44>

### Development Build: 3.1.4

- Fix string termination warnings in GCC9
- See <https://github.com/nasa/elf2cfetbl/pull/41>

### Development Build: 3.1.3

- Builds for vxworks w/ 32-bit host
- See <https://github.com/nasa/elf2cfetbl/pull/40>

### Development Build: 3.1.2

- Minor bug fixes and documentation (see <https://github.com/nasa/elf2cfetbl/pull/25>)

### Development Build: 3.1.1

- Minor updates (see <https://github.com/nasa/elf2cfetbl/pull/19>)

### **_OFFICIAL RELEASE: 3.1.0 - Aquila_**

- Minor updates (see <https://github.com/nasa/elf2cfetbl/pull/13>)
- Not backwards compatible with OSAL 4.2.1
- Released as part of cFE 6.7.0, Apache 2.0

### **_OFFICIAL RELEASE: 3.0a_**

- Released as part of cFE 6.6.0a, Apache 2.0

NOTE - there are other parameter set management schemes used with the cFS (JSON, csv, etc) which may be more applicable for modern missions. Contact the community as detailed below for more information.

## Known issues

This ground utility was developed for a specific mission/configuration, and may not be applicable for general use. The Makefile and for_build/Makefile are no longer supported or tested.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.

Official cFS page: <http://cfs.gsfc.nasa.gov>
"
106,nasa/nasa,,"[<img align=""left"" alt=""https://nasa.gov/"" width=""22px"" src=""https://raw.githubusercontent.com/iconic/open-iconic/master/svg/globe.svg"" />](http://justingosses.com/)
[<img align=""left"" alt=""nasa | Twitter"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/twitter.svg"" />](https://twitter.com/nasa?lang=en)
[<img align=""left"" alt=""nasa | LinkedIn"" width=""22px"" src=""https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg"" />](https://www.linkedin.com/in/nasa/)


<br />
<br />

Hi :wave:, welcome to the NASA org on github.com! 

Github.com/nasa has one of the largest collections of NASA open-source code repositories. Members of the NASA org can find instructions for github.com/nasa in <a href=""http://nasa.github.io/"">http://nasa.github.io/</a>. 

üî≠ Additional open-source code repositories resides in a variety of locations other than github.com/nasa. To discover code across all of these locations, we suggest you use code.nasa.gov & software.nasa.gov. These are two different sites holding metadata that describe code projects. Any code released through the NASA Software Release Authority process should be cataloged on those sites.

### [`Code.nasa.gov`](https://code.nasa.gov) &mdash; 
Is a page with short descriptions of all of NASA's open-source code. Code.nasa.gov feeds into code.gov, which covers open-source and government-source code from many different U.S. governmental agencies. To assist in discovery, code projects described on code.nasa.gov have both human and A.I.-generated tags. These can be useful for finding related code projects.
### [`software.nasa.gov`](https://software.nasa.gov) &mdash; 
Contains metadata descriptions for all code projects in code.nasa.gov as well as government-source code projects only sharable with other government agencies. It is part of the large [`https://technology.nasa.gov/`](https://technology.nasa.gov/) that also includes patents and spinoffs. To help discoverability, software.nasa.gov puts each code project into one fo the following categories: <small>Business Systems and Project Management, System Testing, Operations, Design and Integration Tools, Vehicle Management (Space/Air/Ground), Data Servers Processing and Handling, Propulsion, Structures and Mechanisms, Crew and Life Support, Data and Image Processing, Materials and Processes, Electronics and Electrical Power, Environmental Science (Earth, Air, Space, Exoplanet), Autonomous Systems, and Aeronautics</small>.

<br />
<br />

# NOTE - PROFILE READMES CURRENTLY DON'T WORK FOR ORG PROFILES ONLY USER PROFILES :( 
https://github.community/t/readme-for-organization-front-page/2920
 
"
107,nasa/digital-strategy,HTML,"NASA Digital Government Strategy
===========================

This repository serves as a source for NASA's response to the president's digital strategy. Citizen developers are encouraged to use this information to build applications and tools.

API
---

The files contained in this repository are available as a psuedo-service using the following syntax:

`https://raw.github.com/NASA/digital-strategy/master/{file}.{format}`

Examples:

[`https://raw.github.com/NASA/digital-strategy/master/digitalstrategy.xml`](https://raw.github.com/NASA/digital-strategy/master/digitalstrategy.xml)

Files
-----

* `digitalstrategy.json` and `digitalstrategy.xml` - machine-readable representation of the action items from the digital strategy
* `digitalstrategy.html` - embeddable HTML version of the action items from the digital strategy

Questions
----

Implementation of the Digital Strategy at NASA is coordinated by the NASA's Open Government Team, located in the Office of the Chief Information Officer. For general information, contact [Beth Beck](mailto:beth.beck@nasa.gov).  
Questions about the website, APIs, data or code can be directed to [Jason Duley](jason.duley@nasa.gov).
"
108,nasa/GeneLab-sampleProcessing,," <img src=""NASA_GeneLab_logo-2019.png"" align=""middle"" alt=""""/>

# GeneLab-sampleProcessing

### About
The NASA GeneLab Sample Processing Laboratory generates open science data from spaceflight missions. Over the years, it has developed a set of standard operating procedures (SOPs) that it utilizes to generate high-quality standardized data. This repository houses these SOPs.


## Tissue Storage and Cutting ##
#### [1.1 Sample aliquoting, labeling and storage](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/1.1_sample_archiving_v1.0.md) ####
This SOP describes in detail how GeneLab SPL handles sample storage, aliquoting, labeling and the consensus acronyms we use.

#### [1.2 Frozen tissue cutting](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/1.2_tissue_cutting_v1.0.md) ####
This SOP describes the steps required to safely section a tissue prior the extraction of nucleic acids. The procedure, if followed correctly will allow portioning a piece of tissue without thawing and compromising the original biological sample.

## Homogenization ##
#### [2.1 Tissue homogenization using Bullet Blender Gold Bead Beater](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/2.1_homogenization_bead_beater_v1.0.md) ####
This SOP describes the steps required to lyse and homogenize biological material using a Bullet Blender 24 Gold bead beater. This procedure was validated for downstream RNA/DNA extraction using the Qiagen AllPrep kits (SOP#3.1), it is also possible to use the procedure with downstream RNA extraction using Trizol (SOP#3.2).

#### [2.2 Tissue homogenization using Polytron Rotor Stator Homogenizer](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/2.2_homogenization_polytron_v1.0.md) ####
This SOP describes the steps required to homogenize biological samples using the handheld rotor stator homogenizer Polytron. This type of homogenator allow for a larger lysis buffer volume and is used mainly for samples that require larger yield and/or not yet optimized for bead homogenization. This procedure is currently routinely used for mouse skin RNA extraction that requires a downstream Trizol extraction (SOP#3.2).

## Extraction ##
#### [3.1 QIAGEN AllPrep DNA/RNA Mini (Cat#80204) with QIAGEN RNase-Free DNase Set. (Cat#79254)](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/3.1_qiagen_allprep_rna_dna_v1.0.md) ####
This SOP describes the steps required to extract both RNA and DNA from mammalian tissues. The extraction kit used in this procedure is Qiagen AllPrep DNA/RNA Mini. In addition, in this procedure we describe steps for depleting the isolated RNA from DNA using QIAgen RNase-Free DNase set. This step is required for RNA that will be used for sequencing. It is strongly advised to read the AllPrep DNA/RNA Mini-Handbook in full.

#### [3.2 TRIzol RNA Extraction with QIAGEN RNase-Free DNase Set and QIAGEN Allprep](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/3.2_trizol_rna_with_qiagen_cleanup_v1.0.md) ####
This SOP describes the steps required to extract RNA from mammalian tissue using the TRIzol reagent for isolation and Qiagen Allprep mini kit for clean-up. In addition, in this procedure we describe steps for depleting the isolated RNA from DNA using QIAgen RNase-Free DNase set. This step is required for RNA that will be used for sequencing. It is strongly advised to read the AllPrep DNA/RNA Mini-Handbook in full.

#### [3.3 Feces DNA Extraction using Maxwell RSC instrument with Purefood GMO kit](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/3.3_feces_dna_extraction_v1.0.md) ####
This SOP describes the steps required to isolate DNA from mouse fecal pellets using Maxwell RSC instrument with Purefood GMO kit.

## DNA/RNA QC and Troubleshooting ##
#### [4.1 RNA/DNA/miRNA/cDNA quantification using Qubit Fluorimeter](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/4.1_dna_rna_quant_qubit_v1.0.md) ####
This SOP describes the steps required to perform quantification of RNA/DNA or cDNA using the Invitrogen (Thermo Fisher Scientific) fluorimeter ‚Äì Qubit and the Qubit kits. Flourimetric methods are advantageous over spectrophotometric methods since they are more precise and specific to the molecule being measured.

#### [4.2 QC genomic DNA](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/4.2_genomic_dna_tapestation_v1.0.md) ####
This SOP describes the steps required to perform automated electrophoresis of genomic DNA to assess DNA quality using the Agilent 4200 TapeStation System and Agilent Genomic DNA TapeStation reagents. Any number of samples can be analyzed between 1 and 96.

#### [4.3 Quality analysis of RNA using Agilent Bioanalyzer 2100 System](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/4.3_rna_bioanalyzer_v1.0.md) ####
This SOP describes the steps required to perform automated electrophoresis of RNA samples. This procedure is using the Agilent 2100 Bioanalyzer System and Agilent RNA 6000 Nano and Pico kits. This procedure will generate a gel image of the RNA as well as RIN and DV200 values, those are recorded and used to track sample quality.

## Library Preparation ##
##### [5.1 Illumina TruSeq Stranded Total RNA Library Prep using EpMotion](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/5.1_truseq_stranded_total_rna_epmotion_v1.0.md) ####
This SOP describes the steps used to automate library preparation using the Illumina TruSeq Stranded Total RNA Gold kit on an EpMotion 5073/5075.

#### [5.2 Use of ERCC spike in mixes and UMRR/UHRR controls for Total RNA-Sequencing](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/5.2_controls_and_spike_ins_v1.0.md) ####
As described in detail on GeneLab.nasa.gov webpage titled ‚ÄúGeneLab Sequencing Standards and Services‚Äù we encourage the researchers to use two levels of control in the Total RNA sequencing workflow. This SOP describes the suggested protocol.

#### [5.3 Illumina Nextera DNA Flex Manual Library Preparation](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/5.3_nextera_flex_manual_v1.0.md) ####
This SOP follows the Illumina Nextera DNA Flex Library Prep Guide, Doc# 1000000025416 v07. It is strongly advised to read the guide in full before using this SOP.

## Library QC and Troubleshooting ##
#### [6.1 qPCR quantification of Illumina sequencing libraries](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.1_qiagility_lib_quant_v1.0.md) ####
This SOP describes the steps for qPCR quantification of Illumina Sequencing libraries using a QIAgility.

#### [6.2 Quant-iT PicoGreen dsDNA quantification of Illumina sequencing libraries](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.2_picogreen_v1.0.md) ####
This SOP lists the steps for dsDNA quantification of sequencing libraries using the Quant-iT‚Ñ¢ PicoGreen‚Ñ¢ dsDNA Assay Kit.

#### [6.3 QC cDNA](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.3_d1000_dna_tapestation_v1.0.md) ####
This SOP lists the steps for DNA quantification of sequencing libraries using an Agilent D1000 TapeStation.

#### [6.4 Normalizing TruSeq Stranded Total RNA Library](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.4_normalize_truseq_lib_v1.0.md) ####
This SOP describes the steps to normalize TruSeq Stranded Total RNA libraries.

#### [6.5 TruSeq Total RNA library pooling, normalization and QC](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.5_truseq_total_rna_library_pooling_normalization_qc_v1.0.md) ####
This SOP describes the steps to pool, normalize, and check the quality of TruSeq Total RNA libraries using an Agilent Tapestation and an Illumina iSeq.

#### [6.6 Manual Illumina TruSeq total RNA (Ribo Gold) library clean-up from adapter dimers](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.6_library_manual_cleanup.md) ####
This SOP describes the steps for manually cleaning Illumina TruSeq Total RNA libraries of adapter dimers.

## Sequencing Parameters ##
#### [7.1 Setting up NovaSeq 6000 and iSeq 100 sequencers](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/7.1_sequencer_setup_v1.0.md) ####
This SOP describes the setup of Illumina NovaSeq 6000 and iSeq 100 sequences used by NASA GeneLab.

## Sequencing QC and Troubleshooting ##
#### [8.1 GeneLab SOP for Generating iSeq QC complete report from HTStream on MMOC](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/8.1_generate_htstream_iseq_qc_report_on_mmoc_v1.0.md) ####
This SOP describes the steps for generating an iSeq quality control report using the high throughput genomic data pre-processing tool HTStream.

#### [8.2 GeneLab SOP for Generating iSeq HTStream output to calculate library pooling values](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/8.1_generate_htstream_iseq_qc_report_on_mmoc_v1.0.md) ####
This SOP describes the steps required to calculate library pooling volumes after the first iSeq (pool by volume) run and instructions on how to pool libraries when there is not enough volume.

**_If you have any questions, please contact us at ARC-DL-GeneLab-sequencing-group@mail.nasa.gov._**
"
109,nasa/RtRetrievalFramework,C++,"======================
RT Retrieval Framework
======================

Jet Propulsion Laboratory, California Institute of Technology. 
Copyright 2017 California Institute of Technology. 
U.S. Government sponsorship acknowledged.

This software (NASA NTR-49044) retrieves a set of atmospheric/surface/instrument
parameters from a simultaneous fit to spectra from multiple absorption bands.
The software uses an iterative, non-linear retrieval technique (optimal
estimation). After the retrieval process has converged, the software performs an
error analysis. The products of the software include all quantities needed to
understand the information content of the measurement, its uncertainty, and its
dependence on interfering atmospheric properties.

The software provides a flexible, efficient, and accurate tool to retrieve the
atmospheric composition from near-infrared spectra. Its unique features are:

* Spectra from ground-based or space-based measurement with arbitrary
observation geometry can be analyzed.
* The retrieved parameters can be chosen from a large set of atmospheric (e.g.,
volume mixing ratio of gases or aerosol optical depth), surface (e.g.,
Lambertian reflection), and instrument (e.g., spectral shift or instrument line
shape parameters) parameters.
* The software uses an accurate, state-of-the-art, multiple-scattering radiative
transfer code combined with an efficient polarization approximation to simulate
measured spectra.
* The software enables fast and highly accurate simulations of broad spectral
ranges by an optional parallelization of the frequency processing in the
radiative transfer model.

The software was originally created for JPL's OCO, ACOS and OCO-2 projects for
Level-2 processing.

Documentation
-------------

Documentation on setup and compiling of the software can be found in the
doc/users_guide.pdf PDF file, or online in HTML form at:
http://nasa.github.io/RtRetrievalFrameworkDoc/ 

For information on the algorithms used by the software the OCO2 L2 Algorithm
Theoretical Basis Document can be found at the Goddard DAAC: 
http://disc.sci.gsfc.nasa.gov/OCO-2/documentation/oco-2-v6/OCO2_L2_ATBD.V6.pdf

For further information on the methods used by OCO, see the OCO-2 project
publications page: 
http://oco.jpl.nasa.gov/science/publications/
"
110,nasa/PanNDE,C++,"# Welcome to PanNDE!

PanNDE is an attempt at building a modular, developable infrastructure for doing field simulations (initially elastodynamic) for Non-Destructive Evaluation applications. Performance as well as low-level access is a key concern to enable rapid, adaptive modelling for model inversion or for MAPOD like applications. As with all research, work begets work, and development will hopefully continue. For now, if you find use for this, great, and if not, hopefully future development will bring features that are useful, or that the code base provides a jumping-off point for other development by other researchers. For now, good luck, and have fun! Science is a grand adventure!

# Notices:

Copyright 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Googletest is a product of Google Inc. and is subject to the following:
 
Copyright 2008, Google Inc. All rights reserved.
           
Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
* Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
 
# Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

# Building PanNDE

1. Download latest release
2. Ensure correct dependencies are installed, and are in the search path
  - MPI
  - VTK >= 9.0 (built and linked with the same MPI being used for PanNDE)
  - Metis
  - gcc >= 7.3
  - cmake >= 3.13.5
3. Ensure Python3 is installed for some included post processing utilities
  - this is optional, if the utilities are not desired
4. cd to `PanNDE/root/directory`
5. generate compilation instructions using `cmake -B build -S .`
6. compile with `make -C build`

# Running the Test Suites

1. Ensure PanNDE is built on the system
2. Run serial tests:
  - `./bin/HostDataTests > HostDataTestResults.log`
3. Run parallel tests:
  - `mpirun -n 4 -outfile-pattern=NetMPITestResult_%r.log ./bin/NetMPITests`
  - `mpirun -n 4 -outfile-pattern=VTKIOTestResult_%r.log ./bin/VTKIOTests`
  - `mpirun -n 4 -outfile-pattern=HostSolverTestResult_%r.log ./bin/HostSolverTests`


# Creating a Demonstration Case

1. Ensure PanNDE is built on the system
2. There are three demonstration case builders provided:
  - Aluminum angle stock with single transducer:
    1. Run `./bin/DemoCaseBuilder`
    2. The resulting case file will be `./data/demo_case_0.vtu`
  - Quasi-isotropic 8-ply IM7 CFRP layup with single transducer:
    1. Run `./bin/PlateCaseBuilder -o $JOB_FILENAME_NO_EXTENSION`
    2. The resulting case file will be named with the filename provided
  - Parametric plate with two transducers:
    1. Run `./bin/ParameterizedDemoPlateCase -p $PARAMETER_FILE_YAML`
    2. The resulting case file will be generated based on the parameters contained in the provided parameter file
    3. A sample parameter file that produces the previous quasi-isotropic 8-ply CFRP case is provided in `demo_quasi_iso_cfrp.yaml`. The case can be changed to suit user's use cases, however stability criteria are the user's responsibility

# Running a Case

1. Ensure PanNDE is built on the system
2. Produce a `*.vtu` file that provides all required input data
  - For required data, see Job File Requirements, below
3. Run `mpirun -n $NPROCS ./bin/DemoModelNxd -f $JOB_FILE -o $RESULT_FILE_NAME_ROOT`
  - results will be written to a `*.pvtu`/`*.vtu` set with appropriate file numbering schema for reconstruction in scientific visualization software

# Job File Requirements

1. Elastodynamic parameter fields:
  - cell centered stiffness coefficients named:
    - `C11 C12 C13 C14 C15 C16 C22 C23 C24 C25 C26 C33 C34 C35 C36 C44 C45 C46 C55 C56 C66`
  - cell centered density values named: 
    - `density`
2. Time step and file write frequency information:
  - simulation time step named:
    - `dt`
  - simulation write times array named:
    - `write_times`
3. Hanning windowed Ncycle sine transducer excitation information:
  - Number of transducers named:
    - `NTransducers`
  - Transducer parameters:
    - `XD<transducer index>/XCenter`
    - `XD<transducer index>/YCenter`
    - `XD<transducer index>/ZCenter`
    - `XD<transducer index>/Frequency`
    - `XD<transducer index>/NCycle`
    - `XD<transducer index>/Phase`
    - `XD<transducer index>/Radius`

# Future Work/Features

1. Generic transducers, or more transducer type options
2. PZT model
3. Thermal conduction model
4. GPU data allocator and solver

"
111,nasa/harmony-qgis,Python,"# Harmony QGIS

This plugin allows the user to select areas on the map and upload them as queries (along with other parameters) to the [Harmony](https://harmony.earthdata.nasa.gov) Earth science data/service broker, showing the results on the map.

## Requirements
QGIS version 3.12 or higher

## Installation
1. Clone this repository under the plugins directory for QGIS (/Users/USER_NAME/Library/Application Support/QGIS/QGIS3/profiles/default/python/plugins on a Mac)
```
$ git clone https://github.com/nasa/harmony-qgis.git
```
3. Restart QGIS
4. Select Plugins->Manage and Install Plugins from the menu
5. Click the check box next to the Harmony plugin

## Usage

The basic workflow begins as labeled in red numbers on figure 1:

1. (optional) select a layer in the layers panel. This will preselect the layer in the main dialog, but is not strictly necessary.
2. Click the Harmony plugin icon in the toolbar.

<figure>
<img src=""qgis1_labeled.png"">
<figcaption>Figure 1 - Using the plugin</figcaption>
</figure>

This will open the main dialog shown in figure 2.
Here you can configure a Harmony query by entering parameters in the relevant fields. At a minimum you must fill in the `Collection`, `Version`, and `Variable` fields in order to execute a query.

As labeled in figure 2, these fields are

1. `collection` (required) Used to indicate the Common Metadata Repository (CMR) collection id to use
2. `version` (required) Used to indicate which version of the Harmony API to use
3. `variable` (required) Used to indicate which variable to use
4. `download directory` Where to store the images downloaded from Harmony
5. `harmony url` The URL of the Harmony service to use (for accessing various environments such as UAT or Prod)
6. `shape layer` A selection box to choose a layer to be sent as a shapefile to indicate a sub-setting boundary with the Harmony query. This will be preselected if the layer was selected when the plugin icon was selected. **Note** it is not necessary to provide a layer to execute a query.
7. `additional parameters` This table can be used to add additional query parameters as key/value pairs. Consult the Harmony documentation for available parameters.
8. `session selector` This selector can be used to fill in the other fields from a previous session (see the 'Sessions' section below).
9. `session management` This button will bring up the sessions management dialog (Figure 3) to allow the user to delete, import, and export sessions.

<figure>
<img src=""dialog.png"" height=735 width=728/>
<figcaption>Figure 2 - The main dialog</figcaption>
</figure>

After filling in the query parameter fields and clicking `OK`, the plugin will send the query to Harmony and begin downloading any results (or present error messages in the case of failure). Image results are downloaded in the background to preserve responsiveness in the main UI. Results are downloaded as they become available (rather than all at once) and are rendered as raster layers. A progress bar is rendered in the status window at the bottom and updated as new results are downloaded.

Figure 3 shows the results of a query that generated 20 new layers. The layers are shown in the main map (1) with the new layers added to the layer panel (2). A text message indicating the number of new layers is shown in the status bar (3).

<figure>
<img src=""qgis2_labeled.png"">
<figcaption>Figure 3 - Displaying Harmony results</figcaption>
</figure>

### Sessions

Sessions are a way to save settings from the main dialog for later reuse. This is particularly useful when showing demos.

When the main dialog first opens, the `Session` selector is set to `<NEW SESSION>`. If the user fills in the dialog, doesn't change the `Session` selector, and then selects `OK`, a pop-up dialog will appear asking to save the settings as a session. If the user creates the new session, the name will appear in the `Session` selector in subsequent invocations. Choosing a session name from the selector populates the other fields in the dialog with the last saved settings.

The user can click on the session management button, (9) in figure 2, to bring up the sessions management dialog shown in figure 5.

<figure>
<img src=""sessions_dialog.png"" height=480 width=569/>
<figcaption>Figure 5 - The sessions management dialog</figcaption>
</figure>

All the saved sessions are shown in the list widget (1). Selecting one ore more (shift select) saved sessions will enable the `Delete` button (2). As the name suggests, this will delete the selected sessions. Selecting sessions will also enable the `Export` button (4)
which can be used to write out the data for one or more sessions as a JSON file. This can be shared with other users to allow them to reproduce the Harmony queries defined by sessions. Conversely, the `Import` button (3) can be used to load sessions from an exported file.

**Note** the selected layer (if any) is saved as part of the session data. If an imported session contains a layer name that does not exist in the current project, this information is discarded.
"
112,nasa/T-MATS,HTML,"T-MATS
==========

Toolbox for the Modeling and Analysis of Thermodynamic Systems <br>
<meta name=""keywords"" content=""T-MATS, TMATS, Control System, Numerical Methods, Newton-Raphson, Jacobian Calculation, Propulsion, Aircraft Engine, Jet, Turbofan, Turbojet, Compressor, Turbine, Nozzle, Inlet, open source, simulation, modeling, NASA, thermodynamics, turbomachinery, MATLAB, Simulink, jet, engine,  etc."">
<meta name=""google-site-verification"" content=""nqMigEmX-6lqKvj4sewxDamtZEXHEvE1VfzjVRZoJ40"" />
<b> <a href= ""https://github.com/nasa/T-MATS/releases"" >Click Here</a> for stable release download</b> <br>
For questions, comments, and general support see the <b> <a href= ""https://groups.google.com/forum/#!forum/t-mats-user-group"" >T-MATS user's forum</a></b>. <br>
Please take a moment and fill out our <b> <a href= ""https://docs.google.com/forms/d/1cjcCyOKZpV49-gsdGGaKHUsZ2c2fKX5mwkKrntt60Eo/viewform?usp=send_form"" >Survey</a></b>, user interest and feedback allows us to continue working on this package.<br><br>

<b>Introduction</b> <br>
The Toolbox for the Modeling and Analysis of Thermodynamic Systems (T-MATS) 
is a Simulink toolbox intended for use in the modeling and simulation of thermodynamic 
systems and their controls. T-MATS contains generic thermodynamic and controls 
components that may be combined with a variable input iterative solver and optimization 
algorithm to create complex systems to meet the needs of a developer. Development of this tool
was initiated on behalf of the NASA Aviation Safety Program's Vehicle Systems Safety Technologies
(VSST) project.
<br><br>
<b>Description</b> <br>
The T-MATS software provides a toolbox for the development of thermodynamic 
system models; it contains a simulation framework, multi-loop solver techniques, and modular 
thermodynamic simulation blocks. While much of the capability in T-MATS is in transient 
thermodynamic simulation, the developers' main interests are in aero-thermal applications; 
as such, one highlight of the T-MATS software package is the turbomachinery block set. This 
set of Simulink blocks gives a developer the tools required to create virtually any steady 
state or dynamic turbomachinery simulation, e.g., a gas turbine simulation. In systems where 
the control or other related systems are modeled in MATLAB/Simulink, the T-MATS developer has 
the ability to create the complete system in a single tool.
<br><br>
T-MATS is written in MATLAB/Simulink (The Mathworks, Inc.), is open source, 
and is intended for use by industry, government, and academia. All T-MATS equations 
were developed from public sources and all default maps and constants provided in the 
T-MATS software package are nonproprietary and available to the public. The software 
is released under the Apache V2.0 license agreement. 
<br><br>
<b>Getting Started</b> <br>
Stable releases of T-MATS are located under the <a href= ""https://github.com/nasa/T-MATS/releases"" >releases tab</a>. It is encouraged that a user
download the most up to date version using the appropriate software download button (green button). 
Installation instructions are detailed in the user's manual which is included within the package. 
<br><br>
T-MATS encourages open collaboration and if a user wishes to become a developer the software 
may be forked at any time via the main page link.
"
113,nasa/bplib,C,"# bplib

[1. Overview](#1-overview)  
[2. Build with Make](#2-build-with-make)  
[3. Application Design](#3-application-design)  
[4. Application Programming Interface](#4-application-programming-interface)  
[5. Storage Service](#5-storage-service)  

[Note #1 - Bundle Protocol Version 6](doc/bpv6_notes.md)  
[Note #2 - Library Development Guidelines](doc/dev_notes.md)  
[Note #3 - Configuration Parameter Trades](doc/parm_notes.md)  
[Note #4 - Bundle Flow Analysis for Intermittent Communication](doc/perf_analysis_ic.md)  

----------------------------------------------------------------------
## 1. Overview
----------------------------------------------------------------------

The Bundle Protocol library (bplib) implements a subset of the RFC5050 Bundle Protocol and targets embedded space flight applications. The library uses the concept of a bundle channel to manage the process of encapsulating application data in bundles, and extracting application data out of bundles.  A channel specifies how the bundles are created (e.g. primary header block fields), and how bundles are processed (e.g. payloads extracted from payload block). Bplib contains no threads and relies entirely on the calling application for its execution context and implements a thread-safe blocking I/O model where requested operations will either block according to the provided timeout, or return an error code immediately if the operation cannot be performed.

Bplib assumes the availability of a persistent queued storage system for managing the rate buffering that must occur between data and bundle processing. This storage system is provided at run-time by the application, which can either use its own or can use one of the included storage services. In addition to the storage service, bplib needs an operating system interface provided at compile-time. By default a POSIX compliant operating systems interface is built with the included makefile - see below for further instructions on changing the operating system interface.

----------------------------------------------------------------------
## 2. Build with Make
----------------------------------------------------------------------

#### Prerequisites

1. To build the static and shared libraries, the only prerequisites are the __make__ build system and a compiler toolchain (by default __gcc__).

2. Tailoring the build to provide unique system prerequisites is accomplished by providing a custom configuration makefile.  See `posix.mk` as an example. If your custom file is called `{my_config_makefile}` then the following commands would be used:
   * `make CONFIG={my_config_makefile}`
   * `sudo make CONFIG={my_config_makefile} install`

3. To build the optional Lua extension used for unit testing (and useful for any user implemented Lua applications), then you need to have Lua installed on your system.  Given the various versions and configurations Lua can be found in for different systems, the default behavior of the makefile is to look for Lua 5.3 in `/opt/lua5.3` which assumes you've downloaded and installed Lua yourself.  This can be accomplished via the followings steps:
   * Download Lua 5.3 from `www.lua.org`
   * `tar -xvzf lua-5.3.X.tar.gz` where X is whatever the latest stable version of Lau 5.3 is.
   * `cd lua-5.3.X`
   * `make MYCFLAGS=""-fpic"" linux`
   * `sudo make install INSTALL_TOP=/opt/lua5.3`
   * `alias bplua=""LUA_CPATH=/opt/lua5.3/lib/lua/5.3/?.so LUA_PATH=/opt/lua5.3/lib/lua/5.3/?.lua /opt/lua5.3/bin/lua""`

4. If you want to use a different Lua installation, you must run the Makefile found in `binding/lua` directly.  From the command line: the `PREFIX` variable can be set to the installed version of Lua you want to use (e.g. __/usr__), and the `LIBDIR` variable can be set to where the bplib.so extension module should be installed (e.g. __/usr/lib64/lua/5.1__).  Managing multiple Lua distributions on a single system can be a little tricky - the biggest problem being that as of 5.3 there is still a global search path for shared objects and Lua files that is independent of the location of the binary Lua interpreter.  To get around this problem you can either set the `package.cpath` and `package.path` variables within your Lua scripts or you can alias the call to start the Lua interpreter with the paths called out in the command (as shown above).

#### Building

To build only the static and shared libraries (which is recommended), use the following commands:
* `make`
* `sudo make install`

To build everything, including the language bindings and example application, go to repository root directory and execute the following commands:
* `make dev`
* `sudo make install-dev`

The dev target of the makefile produces the following binaries:
* `build/libbp.so.<version>` - shared library
* `build/libbp.a` - static library
* `bindings/lua/build/bplib.so` - lua extension module
* `app/build/bpsend` - example application that sends bundles
* `app/build/bprecv` - example program that receives bundles

And performs the following installations:
* `/usr/local/lib`: bplib libraries
* `/usr/local/inc`: bplib includes
* `/usr/local/lib/lua/5.3`: lua extensions and helper scripts
* `/opt/bpio/bin`: example application binaries

Additional make commands are as follows:
* `make clean` will remove all generated files and directories
* `make testmem` will call valgrind for detecting memory leaks
* `make testcpu` will call valgrind/callgrind for detecting cpu bottlenecks
* `make testheap` will call valgrind/massif for detecting sources of memory bloat
* `make testcov` will generate a line coverage report (if built and run with gcov, which is enabled by default)

On CentOS you may need to create a file with the conf extension in /etc/ld.so.conf.d that contains the line '/usr/local/lib'.
* `sudo echo ""/usr/local/lib"" > /etc/ld.so.conf.d/local.conf`
* `sudo ldconfig`

#### Example Application

For those that learn better through examples, an example application is provided in the `apps` directory.  This example program is not intended to be complete, but provides a quick way to see how to use the library.  After building and installing bplib on your system, you can do a simple test to see if the application will run by doing the following:

* `cd apps`
* `make`
* `./test_run.sh`

This will create two windows, the first executing the **bprecv** program, and the second executing the **bpsend** program.  Any line you type in the **bpsend** window is bundled and sent over UDP to the **bprecv** program.  Custody transfer is employed and the **bpsend** program will keep track of the number of messages it has sent vs. the number of messages that have been acknowledged.

#### Unit Tests

To manually run the unit test suite:
* `lua5.3 binding/lua/test/test_runner.lua`

To run a specific unit test using one of the test targets provided in the makefile:
* make test{mem|cpu|heap|cov} testcase=binding/lua/test/ut_{test}.lua

Note that getting the lua extension to compile for your specific linux distribution can be difficult as they often come with different versions and named in different ways.  Please see the prerequisites section above and the makefile in `bindline/lua` for hints to how to get your version of Lua working with this library.

#### Releases

The default `posix.mk` configuration makefile is for development and builds additional C unit tests, code coverage profiling, stack protector, and uses minimum compiler optimizations. When releasing the code, the library should be built with `release.mk` as follows:
* `make CONFIG=release.mk`

----------------------------------------------------------------------
## 3. Application Design
----------------------------------------------------------------------

![Figure 1](doc/bp_api_architecture.png ""BP Library API (Architecture)"")

Bplib is written in ""vanilla C"" and is intended to be linked in as either a shared or static library into an application with an API for reading/writing application data and reading/writing bundles.

Conceptually, the library is meant to exist inside a board support package for an operating system and be presented to the application as a service.  In such a design only the interface for reading/writing data would be provided to the application, and the interface for reading/writing bundles would be kept inside the board support package.  This use case would look a lot like a typical socket application where a bundle channel (socket) is opened, data is read/written, and then at some later time the channel is closed.  Underneath, the operating system would take care of sending and receiving bundles.

In order to support bplib being used directly by the application, both the data and the bundle interfaces are provided in the API. In these cases, the application is also responsible for sending and receiving the bundles.

An example application design that manages both the data and bundle interfaces could look as follows:
1. A __bundle reader__ thread that receives bundles from a convergence layer and calls bplib to _process_ them
2. A __data writer__ thread that _accepts_ application data from bplib
3. A __bundle writer__ thread that _loads_ bundles from bplib and sends bundles over a convergence layer
4. A __data reader__ thread that _stores_ application data to bplib

The stream of bundles received by the application is handled by the bundle reader and data writer threads. The __bundle reader__ uses the `bplib_process` function to pass bundles read from the convergence layer into the library.  If those bundles contain payload data bound for the application, that data is pulled out of the bundles and queued in storage until the __data writer__ thread calls the `bplib_accept` function to dequeue the data out of storage and write it to the application.

Conversely, the stream of bundles sent by the application is handled by the data reader and bundler writer threads. The __data reader__ thread calls `bplib_store` to pass data from the application into the library to be bundled.  Those bundles are queued in storage until the __bundle writer__ threads calls the `bplib_load` function to dequeue them out of storage and write them to the convergence layer.

----------------------------------------------------------------------
## 4. Application Programming Interface
----------------------------------------------------------------------

#### 4.1 Functions

| Function        | Purpose |
| --------------- | ------- |
| [bplib_init](#initialize)                | Initialize the BP library - called once at program start |
| [bplib_open](#open-channel)              | Open a channel - provides handle to channel for future channel operations |
| [bplib_close](#close-channel)            | Close a channel |
| [bplib_flush](#flush-channel)            | Flush active bundles on a channel |
| [bplib_config](#config-channel)          | Change and retrieve channel settings |
| [bplib_latchstats](#latch-statistics)    | Read out bundle statistics for a channel |
| [bplib_store](#store-payload)            | Create a bundle from application data and queue in storage for transmission |
| [bplib_load](#load-bundle)               | Retrieve the next available bundle from storage to transmit |
| [bplib_process](#process-bundle)         | Process a bundle for data extraction, custody acceptance, and/or forwarding |
| [bplib_accept](#accept-payload)          | Retrieve the next available data payload from a received bundle |
| [bplib_ackbundle](#acknowledge-bundle)   | Release bundle memory pointer for reuse (needed after bplib_load) |
| [bplib_ackpayload](#acknowledge-payload) | Release payload memory pointer for reuse (needed after bplib_accept) |
| [bplib_routeinfo](#route-information)    | Parse bundle and return routing information |
| [bplib_display](#display-bundle)         | Parse bundle and log a break-down of the bundle elements |
| [bplib_eid2ipn](#eid-to-ipn)             | Utility function to translate an EID string into node and service numbers |
| [bplib_ipn2eid](#ipn-to-eid)             | Utility function to translate node and service numbers into an EID string |
| [bplib_attrinit](#attr-init)             | Utility to initialize a channel attribute structure with default values.  Useful if the calling application only wants to change a few attributes without setting them all. |

__Note__: _functions that operate on a channel are thread-safe with other functions that operate on channels, but they are not thread-safe with the open and close functions.  A channel can only be closed when no other operations are being performed on it._


----------------------------------------------------------------------
##### Initialize

`void bplib_init (void)`

Initializes the BP library.  This must be called before any other call to the library is made.  It calls the operating system layer initialization routine.

----------------------------------------------------------------------
##### Open Channel

`bp_desc_t* bplib_open (bp_route_t route, bp_store_t store, bp_attr_t* attributes)`

Opens a bundle channel that uses the provided endpoint IDs, storage service, and attributes.

This function returns a channel handle that is used for all future operations on the channel.  The open and close calls are mutex'ed against other open and close calls, but once a channel is created, operations on that channel are only mutex'ed against other operations on the same channel.  A channel persists until it is closed.

`route` - a set of endpoing IDs defining the source, destination, and report to endpoints

* __local node__: The {node} number of the ipn:{node}.{service} endpoint ID used for the source and custody endpoints of bundles generated on the channel.

* __local service__: The {service} number of the ipn:{node}.{service} endpoint ID used for the source and custody enpoints of bundles generated on the channel.

* __destination node__: The {node} number of the ipn:{node}.{service} endpoint ID used for the destination enpoint of bundles generated on the channel.

* __destination service__: The {service} number of the ipn:{node}.{service} endpoint ID used for the destination enpoint of bundles generated on the channel.

* __report to node__: The {node} number of the ipn:{node}.{service} endpoint ID used for the report to enpoint of bundles generated on the channel.

* __report to service__: The {service} number of the ipn:{node}.{service} endpoint ID used for the report to enpoint of bundles generated on the channel.

`store` - a set of callbacks that provide access to the desired storage service.  See [Storage Service](#storage-service) section for more details.

`attributes` - set of characteristics and settings for the channel that trade memory usage and performance

* __lifetime__: Bundle generation parameter - the number of seconds since its creation that the bundle is valid.  Once the lifetime of a bundle expires, the bundle can be deleted by the bundle agent.

* __request_custody__: Bundle generation parameter - if set then the bundle request custody transfer and includes a CTEB extension block.

* __admin_record__: Bundle generation parameter - if set then the bundle is set as an adminstrative record.  The library handles this setting automatically for Aggregate Custody Signals that it generates; but if the user wants to create their own adminstrative record, then this attribute provides that option.

* __integrity_check__: Bundle generation parameter - if set then the bundle includes a BIB extension block.

* __allow_fragmentation__: Bundle generation parameter - if set then any generated or forwarded bundles on the channel will be fragmented if the size of the bundle exceeds the __max_length__ attribute of the channel; if not set, then any bundle generated or forwarded that exceeds the __max_length__ will be dropped.

* __cipher_suite__: Bundle generation parameter - provides the CRC type used inside the BIB extension block.  If the __integrity_check__ attribute is not set, then this setting is ignored.  If the __integrity_check__ attribute is set and this attribute is set to BP_BIB_NONE, then a BIB is included but the cipher result length is zero (this provide unambigous indication that no integrity check is included). Currently supported cipher suites are: BP_BIB_CRC16_X25, and BP_BIB_CRC32_CASTAGNOLI.

* __timeout__: The number of seconds the library waits before re-loading an unacknowledged bundle.

* __max_length__: The maximum size in bytes that a bundle can be, both on receipt and on transmission.

* __cid_reuse__: The library's behavior when a bundle times-out - if set, bundles that are retransmitted use the original Custody ID of the bundle when it was originally sent; if not set, then a new Custody ID is used when the bundle is retransmitted.  Re-using the Custody ID bounds the size of the Aggregrate Custody Signal coming back (worse-case gaps).  Using a new Custody ID makes the average size of the Aggregate Custody Signal smaller.

* __dacs_rate__: The maximum number of seconds to wait before an Aggregate Custody Signal which has accumulated acknowledgments is sent.  Every time a call to `bplib_load` is made, the code checks to see if there is an Aggregate Custody Signal which exists in memory but has not been sent for at least __dacs_rate__ seconds.

* __protocol_version__: Which version of the bundle protocol to use; currently the library only supports version 6.

* __retransmit_order__: The order in which bundles that have timed-out are retransmitted. There are currently two retransmission orders supported: BP_RETX_OLDEST_BUNDLE, and BP_RETX_SMALLEST_CID.

* __active_table_size__:  The number of unacknowledged bundles to keep track of. The larger this number, the more bundles can be sent before a ""wrap"" occurs (see BP_OPT_WRAP_RESPONSE).  But every unacknowledged bundle consumes 8 bytes of CPU memory making this attribute the primary driver for a channel's memory usage.

* __max_fills_per_dacs__: The maximum number of fills in the Aggregate Custody Signal.  An Aggregate Custody Signal is sent when the maximum fills are reached or the __dacs_rate__ period has expired (see BP_OPT_DACS_RATE).

* __max_gaps_per_dacs__: The maximum number of Custody ID gaps a channel can keep track up when receiving bundles requesting custody transfer.  If this gap limit is reached, the Aggregate Custody Signal is sent and a new one immediately begins to accumulate acknowledgments.

* recover_storage: Instructs the storage service to attempt to recover the bundles and payloads assocaited with a previous channel with the same local node and service.

* __storage_service_parm__: A pass through to the storage service `create` function.

`returns` - pointer to a channel descriptor.  On error, NULL is returned.

----------------------------------------------------------------------
##### Close Channel

`void bplib_close (bp_desc_t* desc)`

Closes the specified bundle channel and releases all run-time resources associated with it; this does not include the bundles stored in the storage service; nor does it include bundles that have been transmitted but not yet acknowledged (active bundles).  The close call is not mutex'ed against other channel operations - it is the caller's responsibility that the close call is made non-concurrently with any other library function call on that channel.

`desc` - a descriptor for which channel to close

----------------------------------------------------------------------
##### Flush Channel

`int bplib_flush (bp_desc_t* desc)`

Flushes all active bundles on a channel; this treats each bundle that has been transmitted but not yet acknowledged as if it was immediately acknowledged.  This function is separate from the bplib_close function because it is possible that a storage service supports resuming where it left off after a channel is closed.  In such a case, closing the channel would occur without flushing the data since the next time the channel was opened, the data that had not yet been relinquished would resume being sent.

`channel` - a descriptor for which channel to flush

----------------------------------------------------------------------
##### Config Channel

`int bplib_config (bp_desc_t* desc, int mode, int opt, void* val, int len)`

Configures or retrieves an attribute on a channel.

`desc` - a descriptor for which channel to configure or retrieve attribute

`mode` - whether to read or write the attribute

* BP_OPT_MODE_READ: the attribute is read and placed into the memory localtion pointed to by _val_

* BP_OPT_MODE_WRITE: the attribute is written with the value stored at the memory location pointed to by _val_

`opt` - the attribute to perform the operation on, as described in the table below.  The different attributes that can be changed or read are further described in the [Open Channel](#open-channel) section.

| Option                 | Units    | Default | Description |
| ---------------------- | -------- | ------- | ----------- |
| BP_OPT_LIFETIME        | int      | 0 | Amount of time in seconds added to creation time specifying duration of time bundle is considered valid, 0: infinite |
| BP_OPT_REQUEST_CUSTODY | int      | 1 | Sets whether transmitted bundles request custody transfer, 0: false, 1: true |
| BP_OPT_ADMIN_RECORD    | int      | 0 | Sets whether generated bundles are administrative records, 0: false, 1: true |
| BP_OPT_INTEGRITY_CHECK | int      | 1 | Sets whether transmitted bundles include a BIB extension block, 0: false, 1: true |
| BP_OPT_ALLOW_FRAGMENTATION | int  | 1 | Sets whether transmitted bundles are allowed to be fragmented, 0: false, 1: true |
| BP_OPT_CIPHER_SUITE    | int      | BP_BIB_CRC16_X25 | The type of Cyclic Redundancy Check used in the BIB extension block - BP_BIB_NONE, BP_BIB_CRC16_X25, BP_BIB_CRC32_CASTAGNOLI |
| BP_OPT_TIMEOUT         | int      | 10 | Amount of time in seconds to wait for positive acknowledgment of transmitted bundles before retransmitting, 0: infinite |
| BP_OPT_MAX_LENGTH      | int      | 4096 | Maximum length of the transmitetd bundles |
BP_WRAP_BLOCK, BP_WRAP_DROP |
| BP_OPT_CID_REUSE       | int      | 0 | Sets whether retransmitted bundles reuse their original custody ID, 0: false, 1: true |
| BP_OPT_DACS_RATE       | int      | 5 | Sets minimum rate of ACS generation |

__NOTE__: _transmitted_ bundles include both bundles generated on the channel from local data that is stored, as well as bundles that are received and forwarded by the channel.

`val` - the value set or returned

`len` - the length in bytes of the memory pointed to by _val_

`returns` - [return code](#4-2-return-codes).

----------------------------------------------------------------------
##### Latch Statistics

`int bplib_latchstats (bp_desc_t* desc, bp_stats_t* stats)`

Retrieve channel statistics populated in the structure pointed to by _stats_.

`desc` - a descriptor for channel to retrieve statistics on

`stats` - pointer to the statistics structure to be populated

* __lost__: number of deleted bundles due to: storage failure, and memory copy failure

* __expired__: number of deleted bundles due to their lifetime expiring

* __unrecognized__: number of bundles that were attempted to be processed but either could not be parsed or were of an unsupported type

* __transmitted_bundles__: number of bundles returned by the `bplib_load` function for the first time (does not include retransmissions)

* __transmitted_dacs__: number of dacs returned by the `bplib_load` function

* __retransmitted_bundles__: number of bundles returned by the `bplib_load` function because the bundle timed-out and is being resent

* __delivered_payloads__: number of bundle payloads delivered to the application via the `bplib_accept` function

* __received_bundles__: number of bundles destined for the local node that were successfully processed by the `bplib_process` function; the payload was successfully stored by the storage service and is awaiting acceptance

* __forwarded_bundles__: number of bundles destined for the another node that were successfully processed by the `bplib_process` function; this does not indicated that the forwarded bundle was transmitted, only that it was successfully stored by the storage service and is awaiting transmission.

* __received_dacs__: number of DACS destined for the local node that were successfully processed by the `bplib_process` function; this only counts the DACS bundles received by the local node, not the bundles acknowledged by the DACS - that is represented in the acknowledged_bundles statistic.

* __stored_bundles__: number of data bundles currently in storage

* __stored_payloads__: number of payloads currently in storage

* __stored_dacs__: number of aggregate custody signal bundles currently in storage

* __acknowledged_bundles__: number of locally stored bundles positively acknowleged and deleted due to a custody signal acknowledgment

* __active_bundles__: number of bundles that have been loaded for which no acknowledgment has been received

----------------------------------------------------------------------
##### Store Payload

`int bplib_store (bp_desc_t* desc, void* payload, int size, int timeout, uint32_t* flags)`

Initiates sending the data pointed to by _payload_ as a bundle. The data will be encapsulated in a bundle (or many bundles if the channel allows fragmentation and the payload exceeds the maximum bundle length) and queued in storage for later retrieval and transmission.

`desc` - a descriptor for channel to create bundle on

`payload` - pointer to data to be bundled

`size` - size of payload in bytes

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`flags` - flags that provide additional information on the result of the store operation (see [flags](#6-3-flag-definitions)).  The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.

`returns` - size of bundle created in bytes, or [return code](#4-2-return-codes) on error.

----------------------------------------------------------------------
##### Load Bundle

`int bplib_load (bp_desc_t* desc, void** bundle,  int* size, int timeout, uint32_t* flags)`

Reads the next bundle from storage to be sent by the application over the convergence layer.  From the perspective of the library, once a bundle is loaded to the application, it is as good as sent.  Any failure of the application to send the bundle is treated no differently that a failure downstream in the bundle reaching its destination.  On the other hand, the memory containing the bundle returned by the library is kept valid until the `bplib_ackbundle` function is called, which must be called once for every returned bundle.  So while subsequent calls to `bplib_load` will continue to provide the next bundle the library determines should be sent, the application is free to hold onto the bundle buffer and keep trying to send it until it acknowledges the bundle to the library.

`desc` - a descriptor for channel to retrieve bundle from

`bundle` - pointer to a bundle buffer pointer; on success, the library will populate this pointer with the address of a buffer containing the bundle that is loaded.

`size` - pointer to a variable holding the size in bytes of the bundle buffer being returned, populated on success.

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`flags` - flags that provide additional information on the result of the load operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.

`returns` - the bundle reference, the size of the bundle, and [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Process Bundle

`int bplib_process (bp_desc_t* desc, void* bundle,  int size, int timeout, uint32_t* flags)`

Processes the provided bundle.

There are three types of bundles processed by this function:
(1) If the bundle is an aggregate custody signal, then any acknowledged bundles will be freed from storage.
(2) If the bundle is destined for the local node, then the payload data will be extracted and queued for retrieval by the application; and if custody is requested, then the current aggregate custody signal will be updated and queued for transmission if necessary.
(3) If the bundle is not destined for the local node, then the bundle will be queued for transmission as a forwarded bundle; and if custody is requested, then the current aggregate custody signal will be updated and queued for transmission if necessary.

`desc` - a descriptor for channel to process bundle on

`bundle` - pointer to a bundle

`size` - size of the bundle in bytes

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`flags` - flags that provide additional information on the result of the process operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.

`returns` - [return code](#4-2-return-codes).

----------------------------------------------------------------------
##### Accept Payload

`int bplib_accept (bp_desc_t* desc, void** payload, int* size, int timeout, uint32_t* flags)`

Returns the next available bundle payload (from bundles that have been received and processed via the `bplib_process` function) to the application. The memory containing the payload returned by the library is kept valid until the `bplib_ackpayload` function is called, which must be called once for every returned payload.  So while subsequent calls to `bplib_accept` will continue to provide the next payload the library determines should be accepted, the payload will not be deleted from the library's storage service until it is acknowledged by the application.

`desc` - a descriptor for channel to accept payload from

`payload` - pointer to a payload buffer pointer; on success, the library will populate this pointer with the address of a buffer containing the payload that is accepted.

`size` - pointer to a variable holding the size in bytes of the payload buffer being returned, populated on success.

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`flags` - flags that provide additional information on the result of the accept operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.

`returns` - the payload reference, the size of the payload, and [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Acknowledge Bundle

`int bplib_ackbundle (bp_desc_t* desc, void* bundle)`

Informs the library that the memory and storage used for the payload can be freed.  The memory will be immediately freed, the storage will be freed immediately only if the bundle is not requesting custody transfer (otherwise, if the bundle is requesting custody transfer, then the ACS acknowledgment frees the storage).  This must be called at some point after every bundle that is loaded.

`desc` - a descriptor for channel to acknowlwedge bundle

`bundle` - pointer to the bundle buffer to be acknowledged

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Acknowledge Payload

`int bplib_ackpayload (bp_desc_t* desc, void* payload)`

Informs the library that the memory and storage used for the payload can be freed.  This must be called at some point after every payload that is accepted.

`desc` - a descriptor for channel to acknowlwedge payload

`payload` - pointer to the payload buffer to be acknowledged

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Route Information

`int bplib_routeinfo (void* bundle, int size, bp_route_t* route)`

Parses the provided bundle and supplies its endpoint ID node and service numbers.  Used to route a received bundle to the appropriate channel by looking up its destination endpoint ID prior to making other library calls that require a channel identifier.

`bundle` - pointer to a buffer of memory containing a properly formatted bundle

`size` - size of the bundle

`route` - pointer to a route structure that is populated by the function (see [Open Channel](#open-channel) for more details on the structure contents).

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Display Bundle

`int bplib_display (void* bundle, int size, uint32_t* flags)`

Parses the provided bundle (transversing the primary block, extension blocks, and payload block), and logs debug information about the bundle.

`bundle` - pointer to a buffer of memory containing a properly formatted bundle

`size` - size of the bundle

`flags` - flags that provide additional information on the result of the accept operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### EID to IPN

`int bplib_eid2ipn (const char* eid, int len, bp_ipn_t* node, bp_ipn_t* service)`

Convert a enpoint ID string into the IPN node and service numbers

`eid` - string containing the endpoint ID

`len` - length of the __eid__ string

`node` - pointer to variable that will be populated with the node number

`service` - pointer to the variable that will be populated with the service number

----------------------------------------------------------------------
##### IPN to EID

`int bplib_ipn2eid (char* eid, int len, bp_ipn_t node, bp_ipn_t service)`

Convert an IPN node and service number to an enpoint ID string

`eid` - pointer to a buffer that will be populated with the endpoint ID string

`len` - length of the __eid__ buffer

`node` - node number used to populate the endpoint ID string

`service` - service number used to populate the endpoint ID string

----------------------------------------------------------------------
##### Initialize Attributes

`int bplib_attrinit (bp_attr_t* attr)`

Initialize an attribute structure with the library default values.  This is useful when creating a channel where only a few attributes need to be changed.

`attr` - pointer to attributes structure populated by the function (see [Open Channel](#open-channel) for more details on the attributes structure contents).

----------------------------------------------------------------------
#### 4.2 Return Codes

| Code                    | Value | Description |
| ----------------------- | ----- | ----------- |
| BP_SUCCESS              | 0     | Operation successfully performed |
| BP_ERROR                | -1    | Generic error occurred; further information provided in flags to determine root cause |
| BP_TIMEOUT              | -2    | A timeout occurred when a blocking operation was performed |

----------------------------------------------------------------------
#### 4.3 Flag Definitions

| Flag                           | Value | Description |
| ------------------------------ | ----- | ----------- |
| BP_FLAG_DIAGNOSTIC             | 0x00000000 | No event issued - diagnostic message only |
| BP_FLAG_NONCOMPLIANT           | 0x00000001 | Valid bundle but the library was not able to comply with the standard |
| BP_FLAG_INCOMPLETE             | 0x00000002 | At least one block in bundle was not recognized |
| BP_FLAG_UNRELIABLE_TIME        | 0x00000004 | The time returned by the O.S. preceded the January 2000 epoch, or went backwards |
| BP_FLAG_DROPPED                | 0x00000008 | A bundle was dropped because a required extension block could not be processed |
| BP_FLAG_FAILED_INTEGRITY_CHECK | 0x00000010 | A bundle with a BIB failed the integrity check on the payload |
| BP_FLAG_BUNDLE_TOO_LARGE       | 0x00000020 | The size of a bundle exceeds the capacity allowed by library |
| BP_FLAG_ROUTE_NEEDED           | 0x00000040 | A bundle needs to be routed before transmission |
| BP_FLAG_STORE_FAILURE          | 0x00000080 | Storage service failed to deliver data |
| BP_FLAG_UNKNOWN_CID            | 0x00000100 | An ACS bundle acknowledged a CID for which no bundle was found |
| BP_FLAG_SDNV_OVERFLOW          | 0x00000200 | The local variable used to read/write and the value was of insufficient width |
| BP_FLAG_SDNV_INCOMPLETE        | 0x00000400 | There was insufficient room in block to read/write value |
| BP_FLAG_ACTIVE_TABLE_WRAP      | 0x00000800 | The active table wrapped; see BP_OPT_WRAP_RESPONSE |
| BP_FLAG_DUPLICATES             | 0x00001000 | The custody ID was already acknowledged |
| BP_FLAG_CUSTODY_FULL           | 0x00002000 | An aggregate custody signal was generated due the number of custody ID gaps exceeded the maximum allowed |
| BP_FLAG_UNKNOWNREC             | 0x00004000 | A bundle contained unknown adminstrative record |
| BP_FLAG_INVALID_CIPHER_SUITEID | 0x00008000 | An invalid cipher suite ID was found in a BIB |
| BP_FLAG_INVALID_BIB_RESULT_TYPE| 0x00010000 | An invalid result type was found in a BIB |
| BP_FLAG_INVALID_BIB_TARGET_TYPE| 0x00020000 | An invalid target type was found in a BIB |
| BP_FLAG_FAILED_TO_PARSE        | 0x00040000 | Unable to parse a bundle due to internal inconsistencies in bundle |
| BP_FLAG_API_ERROR              | 0x00080000 | Calling program incorrectly used a library function, e.g. passing in invalid parameter |

----------------------------------------------------------------------
## 5. Storage Service
----------------------------------------------------------------------

The application is responsible for providing the storage service to the library at run-time through call-backs passed to the `bplib_open` function.

----------------------------------------------------------------------
##### Create Storage Service

`int create (int type, bp_ipn_t node, bp_ipn_t service, bool recover, void* parm)`

Creates a storage service.

`type` - the type of bundle being stored, will be one of the following (defined in bplib.h): BP_STORE_DATA_TYPE, BP_STORE_DACS_TYPE, BP_STORE_PAYLOAD_TYPE

`node` - the {node} number of the ipn:{node}.{service} endpoint ID used for the source of bundles generated on the channel

`service` - the {service} number of the ipn:{node}.{service} endpoint ID used for the source of bundles generated on the channel

`recover` - _true_: attempt to recover bundles from existing storage that matches the type, node, and service; _false_: do not recover any bundles. Note that a storage service does not need to provide a recovery capability, in which case this parameter is ignored and bundles are never recovered.

`parm` - service specific parameters pass through library to this function.  See the storage_service_parm of the attributes structure passed to the `bplib_open` function.

`returns` - handle for storage service used in subsequence calls.

----------------------------------------------------------------------
##### Destroy Storage Service

`int destroy (int handle)`

Destroys a storage service.  This does not mean that the data stored in the service is freed - that is service specific.

`handle` - handle to the storage service

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Enqueue Storage Service

`int enqueue (int handle, void* data1, int data1_size, void* data2, int data2_size, int timeout)`

Stores the pointed to data into the storage service.

`handle` - handle to the storage service

`data1` - pointer to first block of memory to store.  This must be concatenated with __data2__ by the function into one continuous block of data.

`data1_size` - size of first block of memory to store.

`data2` - pointer to second block of memory to store.  This must be concatenated with __data1__ by the function into one continuous block of data.

`data2_size` - size of the second block of memory to store.

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Dequeue Storage Service

`int dequeue (int handle, void** data, int* size, bp_sid_t* sid, int timeout)`

Retrieves the oldest data block stored in the storage service that has not yet been dequeued, and returns a _Storage ID_ that can be used to retrieve the data block in the future.

`handle` - handle to the storage service

`data` - the pointer that will be updated to point to the retrieved block of memory.  This function returns the data block via a pointer and performs no copy.  The data is still owned by the storage service and is only valid until the next dequeue or relinquish call.

`size` - size of data block being retrieved.

`sid` - pointer to a _Storage ID_ variable populated by the function.  The sid variable is used in future storage service functions to identify the retrieved data block.

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Retrieve Storage Service

`int retrieve (int handle, void** data, int* size, bp_sid_t sid, int timeout)`

Retrieves the data block stored in the storage service identified by the _Storage ID_ sid parameter.

`handle` - handle to the storage service

`data` - the pointer that will be updated to point to the retrieved block of memory.  This function returns the data block via a pointer and performs no copy.  The data is still owned by the storage service and is only valid until the next dequeue or relinquish call.

`size` - size of data block being retrieved.

`sid` - the _Storage ID_ that identifies which data block to retrieve from the storage service

`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Release Storage Service

`int release (int handle, bp_sid_t sid)`

Releases any in-memory resources associated with the dequeueing or retrieval of a bundle.

`handle` - handle to the storage service

`sid` - the _Storage ID_ that identifies the data block for which memory resources are released.

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Relinquish Storage Service

`int relinquish (int handle, bp_sid_t sid)`

Deletes the stored data block identified by the _Storage ID_ sid parameter.

`handle` - handle to the storage service

`sid` - the _Storage ID_ that identifies which data block to delete from storage

`returns` - [return code](#4-2-return-codes)

----------------------------------------------------------------------
##### Get Count Storage Service

`int getcount (int handle)`

Returns the number of data blocks currently stored in the storage service.

`handle` - handle to the storage service

`returns` - number of data blocks

----------------------------------------------------------------------
The storage service call-backs must have the following characteristics:
* `enqueue`, `dequeue`, `retrieve`, and `relinquish` are expected to be thread safe against each other.
* `create` and `destroy` do not need to be thread safe against each other or any other function call - the application is responsible for calling them when it can complete atomically with respect to any other storage service call
* The memory returned by the dequeue and retrieve function is valid until the release function call.  Every dequeue and retrieve issued by the library will be followed by a release.
* The _Storage ID (SID)_ returned by the storage service cannot be zero since that is marked as a _VACANT_ SID


"
114,nasa/simupy-flight,Python,"SimuPy Flight Vehicle Toolkit
=============================

Vehicle flight simulation is an important part of the innovation of aerospace vehicle technology. The NASA Engineering Safety Center (NESC) has identified and addressed the need to verify flight vehicle simulations through their work on the `‚ÄúSix degree-of-freedom (6-DOF) Flight Simulation Test Cases.‚Äù <https://nescacademy.nasa.gov/flightsim/>`_ The author was prompted to develop tools that would allow for the rapid implementation of simulations for novel vehicle concepts including hypersonic re-entry vehicles and urban air mobility vehicles.

This software library leverages open source scientific computing tools to implement an efficient simulation framework for flight vehicles in Python. Equations of motion are composed in blocks using the SimuPy library, an open source Python alternative to Simulink, and integrated using SciPy‚Äôs wrappers for standard Fortran implementations of ordinary differential equation solvers. Dynamics equations of the inertial state variables for the position, orientation, and their corresponding rates for integration are developed using the SymPy symbolic library and implemented using code generation. Kinematics equations are implemented through symbolic definition and code generation as well as leveraging other open source software that implements useful functions, such as the solutions to the inverse geodesy problem.


NESC Test Cases
---------------

A number of the NESC Atmospheric test cases have been implemented to verify the implementation and derivation of the equations of motion. These are located in the `nesc_test_cases` directory. To run, simply execute any of `nesc_case##.py` files or the `run_nesc_cases.py` which will iterate through test cases that have been implemented. These scripts will attempt to load the NESC reference results from the parent directory and plot the results along with the results from the SimuPy implemntation. To include the NESC results in the comparison plots, download the `Atmospheric trajectory data <https://nescacademy.nasa.gov/src/flightsim/Datasets/Atmospheric_checkcases.zip>`_ and unzip the `Atmospheric_checkcases` directory to the root `simupy_flight` directory. You can place the `Atmospheric_checkcases` directory in different location by changing the `data_relative_path` variable in the `nesc_testcase_helper.py` script.

License
-------

This software is released under the `NASA Open Source Agreement Version 1.3 <https://github.com/nasa/simupy-flight/raw/master/license.pdf>`_.


Notices
-------

Copyright ¬© 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers
-----------

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT."
115,nasa/europa,C++,"# EUROPA 

[![Build Status](https://travis-ci.org/nasa/europa.svg?branch=master)](https://travis-ci.org/nasa/europa) <a href=""https://scan.coverity.com/projects/3615"">
  <img alt=""Coverity Scan Build Status""
       src=""https://scan.coverity.com/projects/3615/badge.svg""/>
</a>
[![Coverage Status](https://coveralls.io/repos/nasa/europa/badge.png)](https://coveralls.io/r/nasa/europa)

**EUROPA is available under [NASA's Open Source Agreement (NOSA)](https://ti.arc.nasa.gov/opensource/nosa)**

Welcome!  EUROPA is a framework to model and tackle problems in Planning, Scheduling and Constraint Programming. EUROPA is typically embedded in a host application. It is designed to be expressive, efficient, extendable and configurable. It includes:
 
- **A Plan Database:** The technology cornerstone of EUROPA for storage and manipulation of plans as they are initialized and refined. The EUROPA Plan Database integrates a rich representation for actions, states, objects and constraints with powerful algorithms for automated reasoning, propagation, querying and manipulation.
- **A Problem Solver:** A core solver to automatically find and fix flaws in the plan database. It can be configured to plan, schedule or both. It can be easily customized to integrate specialized heuristics and resolution operations.
- **A Tool Box:** Europa includes a debugger for instrumentation and visualization of applications. It also includes a very high-level, declarative modeling language for describing problem domains and partial-plans.

[Learn more...](//github.com/nasa/europa/wiki/What-Is-Europa)

EUROPA was developed at NASA's Ames Research Center and is available under NASA's open source agreement [(NOSA)](https://ti.arc.nasa.gov/opensource/nosa) 

|**Getting Started**|**Documentation**|**Development**|
|-------------------|-----------------|---------------|
|[Download](https://github.com/nasa/europa/wiki/Europa-Download)|[Background/Overview](https://github.com/nasa/europa/wiki/Europa-Background)|[Building EUROPA](https://github.com/nasa/europa/wiki/Building-Europa)|
|[Installation](https://github.com/nasa/europa/wiki/Europa-Installation)|[User Documentation](https://github.com/nasa/europa/wiki/Europa-Docs)|[Automated Builds](https://github.com/nasa/europa/wiki/Nightly-Builds)|
|[Quick Start](https://github.com/nasa/europa/wiki/Quick-Start)|[Examples](https://github.com/nasa/europa/wiki/Europa-Examples)|[Developer Notes](https://github.com/nasa/europa/wiki/Misc-Development)|
||[Publications](https://github.com/nasa/europa/wiki/Europa-Publications)|[Product Roadmap](https://github.com/nasa/europa/wiki/Europa-Roadmap)|
|||[People](https://github.com/nasa/europa/wiki/Europa-Team)|

For questions, please use the [europa-users](http://groups.google.com/group/europa-users) google group.
"
116,nasa/SBN,C,"# Software Bus Network
NASA Core Flight System (cFS) Software Bus Network (SBN) Application

## Description
The SBN is a cFS application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

The SBN application connects the cFE Software Bus (SB) to other buses, bridging the publish/subscribe messaging service to separate cFS instances in separate partitions, processes, processors, and/or networks.

## License
This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa
"
117,nasa/atd2-fuser,Java,"# Overview
---

The Fuser aggregates flight data from multiple FAA sources, Airline data, and 
3rd party data into a unified source. Flight information is organized by 
individual flights (one take-off and one landing) using the Globally Unique 
Flight Identifier (GUFI). As new messages are received, the flight file is 
updated. Clean and accurate data is assured through the use of transformation 
and mediation processes which enforce business rules on the received data. For 
additional details see the [NASA ATD-2 Industry Workshop Fuser Architecture Overview](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Architecture-Overview_84377881.html) 
  
>**Fuser Archicture Diagram**  
>![Fuser Architecture Diagram](images/Fuser_Architecture_Overview.png)


## Data Mappings
---

Data mapping representing the set of available flight fields produced by the 
Fuser and how those fields map to SWIM data can be found at 
[NASA ATD-2 Industry Workshop Data Dictionary](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Data-Dictionary_92471950.html)
and
[NASA ATD-2 Industry Workshop Fuser Database Input Mapping Table](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Database-Input-Mapping-Table_85328219.html)


## Configuration
---

The Fuser is highly configurable allowing for customization of attribute
filters, mediation rules, updaters, and message endpoints.  All property files 
are located under *src/main/resources/config/fuser* directory.

  
| Configuration | Location | Description |
|---|---|---|
| Default | properties.props | Default properties file containing definitions which affect the Fuser behavior. Definitions are store in *key=value* pairings. These definitions include but are not limited to: broker URL's,  logging behavior, active updaters, which data sources are active, etc. |
| Attribute Filters | /attributes/attributes.xml | XML file containing sets of *attributeFilter* elements grouped by arrival/departure category. Each *attributeFilter* element contains attributes defining the source and type (exclusion/inclusion) of filter applied to the nested set of flight elements. Additional information about attribute filters can be found at [NASA ATD-2 Industry Workshop Fuser Filtering Overview](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Filtering-Overview_85328206.html) |
| Mediation Rules | rules.properties.props beans.rules.xml | The *beans.rules.xml* Spring context defines the set of mediation rules which may be applied to  messages received by the Fuser.  Each bean definition represents a mediation rule determining how  specific flight fields are handled given a list of allowable sources.  The *rules.properties.props* file defines *key=value* pairings for controlling which mediation rules  are active and the priority for which mediation rules should be interpreted.  Additional information about mediation rules can be found at [NASA ATD-2 Industry Workshop Data Mediation Overview](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Data-Mediation-Overview_85328193.html) |
| Messaging | topics.properties.props | Collection of *key=value* pairings defining the JMS routing of data through the Fuser.  Notable  properties in this file are:   - fuser.endpoint.fromFuser.fused  - fuser.endpoint.fromFuser.envelope.fused  - fuser.endpoint.fromFuser.remove  The above listed properties control the JMS topics used for distributed Fused data to downstream  applications. |
| Environment | src/main/resources/env | Environment property files provide support for easily running the Fuser in different environments  (e.g. production, development, test) without having to repeatedly reconfigure several different files.  An environment file may contain a subset of properties defined in *key=value* pairings which will  override other properties contained in: properties.props, rules.properties.props, and topics.properties.props. |

## Installation
---

The Fuser depends on Apache Maven 3.5.4 or higher and requires Java 8 to compile
all sources. As part of the Maven build process a deploy ready zip artifact will 
be generated.
  
From the root directory of the fuser-nasa-atd2-opensource project execute: *mvn clean install*

This step is necessary to install all Mosaic ATM libraries.
  
The following artifacts should be produced in the fuser-nasa-atd2-opensource/FuserCoreParent/Fuser/target/ folder:
  
 - Fuser.jar: JAR file containing the compiled binary representation of the Fuser
 - Fuser-bin.zip: Compressed artifact containing all of the dependencies, configurations, and
scripts required to run the Fuser as a stand-alone application
 - Fuser-resources.zip: Compressed artifact containing only the Fuser configuration files


## Execution
---

Complete the [Installation](#Installation) steps as execution requires the 
Fuser-bin.zip artifact generated as part of the installation.  Execution
requires that a Java 8 JRE be installed on the target system.  Steps to execute 
the Fuser as a stand-alone application are:

1.  Unzip the Fuser-bin.zip artifact.  This should result in a directory structure
similar to:
  
>```
>Fuser
>   |--/config        : contains all property and configuration files
>   |--/env           : contains environment property overrride files
>   |--/lib           : contains all required dependencies
>   |--/plugins       : contains plugins for consuming data sources
>   |--redis-utils
>   |--runFuser
>   |--runFuser.bat
>```
  
2. Make any modifications to the Fuser configuration as outlined in the 
[Configuration](#Configuration) section.  Note that all configuration files
are available under the */config* directory.
  
3. Execute the *./runFuser [environment property name]* where 
*environment property name* refers to the name of a specific property file 
located in the */env* directory which will be used to override any default
configurations.  If no environment property name is
provided the default configurations will be used.


## License
---
This software is released under the [NASA Open Source Agreement Version 1.3](license.pdf)

## Notices
---
Copyright ÔøΩ 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

  
## Disclaimers
---
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""
  
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.


"
118,nasa/utm-apis,,"# UTM APIs

This repository contains the collection of OpenAPI specification APIs within the NASA's research version of the UTM System.  Many of UTM's data models are common and these are maintained using Swagger's ""Domain"" references.

## References

- [UTM Home Page](https://utm.arc.nasa.gov/)
- [UTM Publications](https://utm.arc.nasa.gov/documents.shtml)
- [UTM Swaggerhub](https://app.swaggerhub.com/organizations/utm)


## Sandbox and RELEASE branch

utm-apis commits pinned to our Sandbox are represented in the RELEASE branch, and the master branch is our development branch.  You will generally find the master branch to be ahead of the Sandbox.  The tags in RELEASE correspond with our sandbox releases.

For codegen you generally will point to RELEASE.  


## Viewing Local Swagger Spec files

You have choices to edit/view local swagger files. IMO option 1 is better for viewing all files.  A local swagger editor is good if you want rendering.

1. Use a text editor that supports YAML:
  - Sublime Text 2 highlights YAML well
  - [Atom](https://atom.io/) works well and has a [Swagger Lint Plugin](https://atom.io/packages/linter-swagger) that provides a good first cut at valid OpenAPI 2.0 correctness

2. Install local [Swagger Editor](https://swagger.io/swagger-editor/)

3. Bring up an [online swagger editor](https://editor.swagger.io/) and copy or import the source swagger.


## Codegen

You can generate code from OpenAPI Specifications (swagger).  

Swagger Hub has a feature where the site will generate code into a zip file which is downloaded.  This is a great first checkout, however, it uses the default language-specific configurations.

One option (which we use) is to generate all the data models into a library.
Note that codegen parses only from API Specifications, not directly from Swagger Domains.  
An approach to creating a model-only library is to codegen against all the APIs whereby the generation
output is filtered for model-only.
The argument to the input spec can be a local file or internet

````````
GENERATE=""java  -Dmodels -DmodelDocs=false -DapiDocs=false -jar $CODEGEN generate  -l spring --config config.json""

$GENERATE -i ${SWREPO}/fims-uss-api/swagger.yaml   #localfile input
or
$GENERATE -i https://raw.githubusercontent.com/nasa/utm-apis/master/uss-api/swagger.yaml

````````

Your language-specific configurations can also generate model-specific data validations.
For example using swagger-codegen's 'Spring server' language you can codegen bean validations
and java8's OffsetDateTime class for date-time using this language-specific config.

````````

{
  ""library"": ""spring-boot"",
  ""java8"": ""true"",
  ""dateLibrary"":""java8"",
  ""modelPackage"": ""gov.nasa.utm.utmcommons.model"",
  ""useBeanValidation"":""true""
}

````````
"
119,nasa/aladynpi,,"-----------------------------------------------------------------------
04-26-2019

ALADYN_PI mini-app is a simple molecular dynamics (MD) code for performing 
constant energy atomistic simulations using either a straight 
Artificial Neural Network (ANN) interatomic potential, or
Physically Informed Neural Network (PINN) interatomic potential. 

References:
 1. Vesselin I. Yamakov, Edward H. Glaessgen, NASA/TM-2019-220431
    (https://ntrs.nasa.gov/citations/20200000069)
 2. G.P.Purja Pun, V.Yamakov, J.Hickman, E.H.Glaessgen, Y.Mishin, 
    Physical Review Materials, 4, 113807 (2020).
    (https://link.aps.org/doi/10.1103/PhysRevMaterials.4.113807)

The trained ANN was produced and offered by Ganga P. Pun and Yuri Mishin 
from George Mason University.

=======================================================================

 COMPILATION

-----------------------------------------------------------------------
* FORTRAN compiler (Intel 2003 or newer, or PGI FORTRAN with OpenACC)
-----------------------------------------------------------------------
Source directory: ALADYN_PI.source
-----------------------------------------------------------------------
Compilation: use the provided makefiles with the following options:

Intel compiler:
> make -f makefile.intel            ! compiles with -O3 optimization on 
> make -f makefile.intel DEBUG=TRUE ! check and warning flags on
> make -f makefile.intel OMP=TRUE   ! compile with OpenMP direectives

PGI compiler:
> make -f makefile.pgi              ! compiles with -O3 optimization on 
> make -f makefile.pgi DEBUG=TRUE   ! check and warning flags on
> make -f makefile.pgi OMP=TRUE     ! compile with OpenMP direectives
> make -f makefile.pgi ACC=TRUE  ! compile with OpenMP+OpenACC direectives

Edit the provided makefiles for specific compiler option of your choice
=======================================================================

 EXECUTION

-----------------------------------------------------------------------
Run from the example (test) directory: ALADYN_PI.test

Running a tets case:
> aladyn_pi              ! executes 10 MD steps report at each step
> aladyn_pi -n 100 -m 10 ! executes 100 MD steps, report at each 10-th step

Available PBS scripts used for NASA/LaRC K3 cluster:
ALADYN_PI_intel.job      - run the intel version
ALADYN_PI_pgi_V100.job   - run the pgi version with OpneACC for V100 gpu.

Screen output is riderected to aladyn_pi.out

Example outputs are saved in: aladyn_pi_intel.out and aladyn_pi_pgi.out
=======================================================================

INPUT FILES:

-----------------------------------------------------------------------
PINN.dat - Neural network potential file
structure.plt - input atomic structure file

-----------------------------------------------------------------------
--- Available potential files in directory POT:
(to be linked or copied as PINN.dat in the working directory of aladyn_pi)

ANN_Si.dat - Straight Artificial Neural Network potential for Si
PINN_Si.dat - Physically Informed Neural Network potential for Si
PINN_Al.dat - Physically Informed Neural Network potential for Al (ref.2)

-----------------------------------------------------------------------
--- Available Si single crystal test structures in directory STR_Si:

Si_N4000.plt      -   4000 atoms Si crystal
Si_N8000.plt      -   8000 atoms Si crystal
Si_N16000.plt     -  16000 atoms Si crystal
Si_N32000.plt     -  32000 atoms Si crystal
Si_N64000.plt     -  64000 atoms Si crystal
Si_N128000.plt    - 128000 atoms Si crystal
Si_N192000.plt    - 192000 atoms Si crystal
Si_N256000.plt    - 256000 atoms Si crystal
Si_N512000.plt    - 512000 atoms Si crystal

-----------------------------------------------------------------------
--- Available Al single crystal test structures in directory STR_Al:

Al_N4000.plt      -   4000 atoms Al crystal
Al_N8000.plt      -   8000 atoms Al crystal
Al_N16000.plt     -  16000 atoms Al crystal
Al_N32000.plt     -  32000 atoms Al crystal
Al_N64000.plt     -  64000 atoms Al crystal
Al_N128000.plt    - 128000 atoms Al crystal
Al_N192000.plt    - 192000 atoms Al crystal
Al_N256000.plt    - 256000 atoms Al crystal

Use any of the above structures by linking them to structure.plt, e.g.,
> ln -s STR/Al_N4000.plt structure.plt

=======================================================================

SOURCE FILES: 

File: ALADYN_PI.source.zip contains Full Double PRECISION ANN calculation and BOP calculations.

File: ALADYN_PIs.source.zip contains SINGLE PRECISION ( rela(kind=4) ) ANN calculation and Double PRECISION BOP calculations.

-----------------------

Files in ALADYN_PI.source.zip

    aladyn_pi.f       - Main program
    aladyn_pi_sys.f   - system modul
    aladyn_pi_sys_OMP.f    - system modul for OpneMP compilation
    aladyn_pi_sys_NO_OMP.f - system modul without OpneMP compilation
    aladyn_pi_sys_ACC.f    - system modul for OpneACC compilation
    aladyn_pi_mods.f  - contains general purpose modules
    aladyn_pi_IO.f    - I/O operations

    aladyn_pi_ANN_OMP.f  - Artificial Neural Network OpneMP code
    contains:
     subroutine Frc_ANN_OMP  ! ANN force & energy (OpenMP version)

    aladyn_pi_ANN_ACC.f  - Artificial Neural Network OpneACC code
     subroutine Frc_ANN_ACC  ! ANN force & energy (OpenACC version)

    aladyn_pi_PINN_OMP.f - Physically Informed NN OpneMP code
     subroutine Frc_PINN_OMP ! PINN force & energy (OpenMP version)

    aladyn_pi_PINN_ACC.f - Physically Informed NN OpneACC code
     subroutine Frc_PINN_ACC ! PINN force & energy (OpenACC version)

    aladyn_pi_MD.f    - molecular dynamics module
     contains:
      subroutine get_T     ! Calculates current system temperature
      subroutine predict_atoms ! Gear predictor call !
      subroutine correct_atoms ! Gear corrector call !
      
-----------------------

Files in ALADYN_PIs.source.zip

    aladyn_pi.f       - Main program
    aladyn_pi_sys.f   - system modul
    aladyn_pi_sys_OMP.f    - system modul for OpneMP compilation
    aladyn_pi_sys_NO_OMP.f - system modul without OpneMP compilation
    aladyn_pi_sys_ACC.f    - system modul for OpneACC compilation
    aladyn_pi_mods.f  - contains general purpose modules
    aladyn_pi_IO.f    - I/O operations

    aladyn_pi_ANN_OMPs.f  - Single Precision Artificial Neural Network OpneMP code
    contains:
     subroutine Frc_ANN_OMP  ! ANN force & energy (OpenMP version)

    aladyn_pi_ANN_ACCs.f  - Single Precision Artificial Neural Network OpneACC code
     subroutine Frc_ANN_ACC  ! ANN force & energy (OpenACC version)

    aladyn_pi_PINN_OMP.f - Physically Informed NN OpneMP code
     subroutine Frc_PINN_OMP ! PINN force & energy (OpenMP version)

    aladyn_pi_PINN_ACC.f - Physically Informed NN OpneACC code
     subroutine Frc_PINN_ACC ! PINN force & energy (OpenACC version)

    aladyn_pi_MD.f    - molecular dynamics module
     contains:
      subroutine get_T     ! Calculates current system temperature
      subroutine predict_atoms ! Gear predictor call !
      subroutine correct_atoms ! Gear corrector call !


-----------------------------------------------------------------------
Suggested subroutines for optimization: 
Frc_ANN_OMP and Frc_ANN_ACC (Optimized versions from ALADYN miniapp)

Frc_PINN_OMP and Frc_PINN_ACC
(Optimized versions from ALADYN miniapp)

-----------------------------------------------------------------------
 For further information contact:

 Vesselin Yamakov
 National Institute of Aerospace
 100 Exploration Way,
 Hampton, VA 23666
 phone: (757)-864-2850
 fax:   (757)-864-8911
 e-mail: yamakov@nianet.org

=======================================================================
 Notices:
 Copyright 2020 United States Government as represented by the 
 Administrator of the National Aeronautics and Space Administration. 
 All Rights Reserved.
 
 Disclaimers:
 No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY 
 WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, 
 INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE 
 WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF 
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM 
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR 
 FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM 
 TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, 
 CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT 
 OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY 
 OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  
 FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES 
 REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, 
 AND DISTRIBUTES IT ""AS IS.""‚Ä®

 Waiver and Indemnity:  
 RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES 
 GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR 
 RECIPIENT. IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY 
 LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH 
 USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, 
 RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND 
 HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND 
 SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED 
 BY LAW. RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE 
 IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
=======================================================================

"
120,nasa/astrobee_media,CMake,"# Astrobee Robot Software - Multimedia

## About

Astrobee is a free-flying robot that is designed to operate as a payload inside
the International Space Station (ISS). The Astrobee Robot Software consists of
embedded (on-board) software, supporting tools and a simulator. The Astrobee
Robot Software operates on Astrobee's three internal single board computers and
uses the open-source Robot Operating System (ROS) framework as message-passing
middleware. The Astrobee Robot Software performs vision-based localization,
provides autonomous navigation, docking and perching, manages various sensors
and actuators, and supports user interaction via screen-based displays, light
signaling, and sound. The Astrobee Robot Software enables Astrobee to be
operated in multiple modes: plan-based task execution (command sequencing),
teleoperation, or autonomously through execution of hosted code uploaded by
project partners (guest science). The software simulator enables Astrobee Robot
Software to be evaluated without the need for robot hardware.

This repository provides multimedia used by Astrobee Robot Software, for both
visualizing experiments and simulating free-flyers. It includes meshes and
textures from the following two external sources:

* [IGOAL](https://nasa3d.arc.nasa.gov/detail/iss-internal) - ISS Interior.
* [RoboNaut2](https://gitlab.com/nasa-jsc-robotics/r2_gazebo) - ISS Handrails.

## Change Log

v0.0.4 : Improved handrail meshes and perching arm textures.
v0.0.3 : Restructured media to allow spawning certain models from world file.
v0.0.2 : Added granite lab mesh.
v0.0.1 : Added dock, space station, free-flyer and perching arm meshes.

## Usage Guidelines

The repository contains original multimedia created in the Astrobee project and
redistributed multimedia from other NASA projects. All multimedia is covered by
NASAs [Media Usage Guidelines](https://www.nasa.gov/multimedia/guidelines/index.html).

## Instructions

Setup instructions for the Astrobee Robot Software may be found on the
[official respository](https://github.com/nasa/astrobee).
"
121,nasa/nasa-latex-docs,TeX,"NASA-LaTeX-Docs
================

Please refer to the official documentation: [NASA-LaTeX-Docs Documentation](https://nasa.github.io/nasa-latex-docs/html)

Usage:
-------

With this LaTeX package, the formatting is all handled internally so that report developers only need to create their own content following a provided template (or even create their own). If not using a template (example: AIAA or IEEE submission), this package also comes with an incredibly versatile, OS independent, Python build script and Latexmk configuration file for easy document type-setting and warning/error detection. 

The build script is located in the following directory and can be used to build **any LaTeX document**:

    nasa-latex-docs/buildPDF.py

Please see [NASA-LaTeX-Docs Documentation](https://nasa.github.io/nasa-latex-docs/html) for more details, examples, and sample outputs!

System Requirements:
-------

- TeX Live Distribution 2015+ (with latest updates)
- Python 2.6+, Python 3+"
122,nasa/cumulus-message-adapter-java,Java,"# cumulus-message-adapter-java

[![CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter-java.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-message-adapter-java)

This repository contains a client library for integrating Lambda task code written in Java with the core [Cumulus Message Adapter](#about-the-cumulus-message-adapter-cma).

## About Cumulus

Cumulus is a cloud-based data ingest, archive, distribution and management
prototype for NASA's future Earth science data streams.

Read the [Cumulus Documentation](https://cumulus-nasa.github.io/)

## About the Cumulus Message Adapter (CMA)

The [Cumulus Message Adapter (CMA)](https://github.com/cumulus-nasa/cumulus-message-adapter) is a library that adapts incoming messages in the
Cumulus protocol to a format more easily consumable by Cumulus tasks, invokes
the tasks, and then adapts their response back to the Cumulus message protocol
to be sent to the next task.

## Installation

Add the CMA Java client library as a dependency in your project. The current version and dependency installation code can be found [here](https://clojars.org/gov.nasa.earthdata/cumulus-message-adapter).

## Task definition

In order to use the CMA Java client library, you will need to create two
methods in your task module: a handler function and a business logic function.

The handler function is a standard Lambda handler function.

The business logic function is where the actual work of your task occurs. The class containing this work should implement the `ITask` interface and the `String PerformFunction(String input, Context context);` function. `input` is the simplified JSON from the message adapter and `context` is the AWS Lambda Context.

## Cumulus Message Adapter interface

Create an instance of `MessageParser` and call

```java
RunCumulusTask(String input, Context context, ITask task)
```

or

```java
RunCumulusTask(String input, Context context, ITask task, String inputSchemaLocation, String outputSchemaLocation, String configSchemaLocation)
```

with the following parameters:

* `input` - the input to the Lambda function
* `context` - the Lambda context
* `task` - an instance of the class that implements `ITask`

And optionally:

* `inputSchemaLocation` - file location of the input JSON schema, can be null
* `outputSchemaLocation` - file location of the output JSON schema, can be null
* `configSchemaLocation` - file location of the config JSON schema, can be null

If the schema locations are not specified, the message adapter will look for schemas in a schemas directory at the root level for the files: input.json, output.json, or config.json. If the schema is not specified or missing, schema validation will not be peformed.

 `RunCumulusTask` throws a `MessageAdapterException` when there is an error.

## Example Cumulus task

For a full example see the [task folder](./task).

## Creating a deployment package

The compiled task code, the message parser uber-jar, the cumulus message adapter zip, and any other dependencies should all be included in a zip file and uploaded to lambda. Information on the zip file folder structure is located [here](https://docs.aws.amazon.com/lambda/latest/dg/create-deployment-pkg-zip-java.html).

## Usage in Cumulus Deployment

For documenation on how to utilize this package in a Cumulus Deployment, please view the [Cumulus Workflow Lambda Documentation](https://nasa.github.io/cumulus/docs/workflows/lambda#deploy-a-lambda) and the [Cumulus Workflow Input/Output Documentation](https://nasa.github.io/cumulus/docs/workflows/input_output).

## Logging

The message adapter library contains a logging class `AdapterLogger` that standardizes the log format for Cumulus. Static functions are provided to log error, fatal, warning, debug, info, and trace.

For example, to log an error, call:

```java
AdapterLogger.LogError(""Error message"");
```

## Development

### Prerequisites

* [Apache Maven](https://maven.apache.org/install.html)

### Building

To build a new version of the CMA Java client library:

```shell
cd message_parser
mvn -B package
```

#### Using locally built package with task code

These instructions assume that your task is using Maven for dependency management.

1. Get the current version number of the CMA Java client library from [`message_parser/pom.xml`](./message_parser/pom.xml):

    ```xml
    <version>1.2.12</version>
    ```

2. Make sure the `pom.xml` for your task includes a `dependency` referencing the correct version:

    ```xml
    <dependency>
      <groupId>gov.nasa.earthdata</groupId>
      <artifactId>cumulus-message-adapter</artifactId>
      <version>1.2.12</version>
    </dependency>
    ```

3. From the task project root, install the locally built package as a dependency and copy it into place (making sure that the version number is correct in `-Dfile` and `-Dversion`):

    ```shell
    mvn install:install-file \
      -Dfile=/path/to/cumulus-message-adapter-java/message_parser/target/cumulus-message-adapter-1.2.12.jar \
      -DgroupId=gov.nasa.earthdata \
      -DartifactId=cumulus-message-adapter \
      -Dversion=1.2.12 \
      -Dpackaging=jar \
      -DgeneratePom=true
    mvn clean dependency:copy-dependencies
    ```

Running your task code (locally or when packaged and deployed) should now use the locally built CMA Java client package.

### Integration Tests

Integration tests are located in the test folder in `MessageParserTest.java`. By default, the latest [Cumulus Message Adapter](https://github.com/cumulus-nasa/cumulus-message-adapter) will be downloaded and used for the tests.

To build and run the tests using the latest version of Cumulus Message Adapter, run:

```shell
cd message_parser
mvn -B test
```

To build and run the tests using a different version of the Cumulus Message Adapter, run

```shell
MESSAGE_ADAPTER_VERSION=vx.x.xx mvn -B test
```

### Running the example task

Follow the installation instructions above for the example task.

If updating the version of the message parser, make sure to update the `pom.xml` in the task code. To build the task with this dependency, run:

```shell
cd task/
mvn clean install -U
mvn -B package
```

## Benefits of the Cumulus Message Adapter

This approach has a few major advantages:

1. It explicitly prevents tasks from making assumptions about data structures
   like `meta` and `cumulus_meta` that are owned internally and may therefore
   be broken in future updates. To gain access to fields in these structures,
   tasks must be passed the data explicitly in the workflow configuration.
2. It provides clearer ownership of the various data structures. Operators own
   `meta`. Cumulus owns `cumulus_meta`. Tasks define their own `config`,
   `input`, and `output` formats.
3. The Cumulus Message Adapter greatly simplifies running Lambda functions not
   explicitly created for Cumulus.
4. The approach greatly simplifies testing for tasks, as tasks don't need to
   set up cumbersome structures to emulate the message protocol and can just
   test their business function.
"
123,nasa/webgs,JavaScript,"# WebGS

WebGS is a web-based ground control station that is compatible with [ICAROUS](https://github.com/nasa/icarous) (versions greater than 2.1.19) and capable of multi-aircraft simulations.

<a href=""screenshots/screenshot1_webgs.png""><img src=""screenshots/screenshot1_webgs_small.png""><a href=""screenshots/screenshot2_webgs.png""><img src=""screenshots/screenshot2_webgs_small.png"">

<a href=""screenshots/screenshot4_webgs.png""><img src=""screenshots/screenshot4_webgs_small.png""><a href=""screenshots/screenshot3_webgs.png""><img src=""screenshots/screenshot3_webgs_small.png"">

## Installation

Clone the repository.
Make sure you have installed node.js and [npm](https://www.npmjs.com/get-npm).
[Python3](https://www.python.org/downloads/) is also required.

The install script will download the required python and node packages, update the submodules to the latest versions, and build daa-displays.
Move into the webgs directory and run:

    ./install.sh

If updating WebGS and having trouble with submodules not loading. You may need to delete WebGS and start from scratch.

Webgs is setup by default to connect to Open Street Maps. Webgs is also configured to use mapbox for the background display. To get an authorization key go to <https://www.mapbox.com/> and create an account. After receiving an authorization token open `/webgs/MainJS/settings/MapSettings.js` in a text editor, and follow the instructions to update.

[DAA-Displays](https://github.com/nasa/daa-displays) is installed as a submodule in the applications folder `/DAA/daa-displays` by default. The installation script will build the daa-displays automatically. This behavior can be skipped by running `install.sh -D`. [DAA-Displays](https://github.com/nasa/daa-displays) can be launched from WebGS after connecting to an aircraft (no need to follow the instructions on <https://github.com/nasa/daa-displays>).  

WINDOWS USERS: SITL is disabled since ICAROUS cannot be built and run on Windows. Use 'Connect to Hardware' instead. DAA-Displays are also not able to be built on Windows. Run the install script with the `-D` option. Efforts have been made to make WebGS usable on Windows, but there may still be issues. Please notify the developers of any issues. We will happily fix what we can.  

## Startup
By default WebGS uses https. This assumes the proper ssl certs have been generated and loaded into the `/certs` directory. Webgs can also be run in developer mode which uses http and does not require certs. Detailed instructions on creating self signed certificates are located in `/certs.README.md`.

To start Webgs:

    python3 start_webgs.py -HOST {name or localhost} -CERT {filename}.crt -KEY {filename}.key

Update `-HOST`, `-CERT` and `-KEY` as needed, `-CERT` and `-KEY` default to `localhost.crt` and `localhost.key` respectively, so they are not needed if that is the names you chose.

    python3 start_webgs.py -HOST {name or localhost}

Or, start with http:

    python3 start_webgs.py -DEV True

This script starts a local http server, starts the webgs socket server, and opens a web browser (chrome if it can be found otherwise the default browser) to `{hostname}:8082`.

There are potentially some compatibility issues with browsers other than Chrome and Firefox. These issues are mainly just styling. There may be some weird colors, or things may be slightly out of place.

### To connect to the server from another device (only if on the same local network)

If the web server and socket server are on another device on your local network. The server is not public facing, and will not be seen by anyone outside of the local network. Enter the ip address of the machine running the web server into the browser address bar in format `- {https or http}://<hostname>:8082`

### Connect WebGS over UDP

Assuming [ICAROUS](https://github.com/nasa/icarous) is configured properly, ensure you are on the same network as the device running [ICAROUS](https://github.com/nasa/icarous). Typically this will involve changing the IP address of your machine. Start the web server and the socket server. Ensure the Web page is connected to the socket server. In the settings panel, set:

    GCS Mode -> 'Connect to Hardware'
    Select Input Type -> IP
    IP Address -> {the same IP address ICAROUS is configured to output to}
    Component Id -> 5 (Default is 5. This is the standard ICAROUS Config. 0 will connect to Autopilot in most configurations.)

Ensure the Port and Baud Rate are correct. Press connect to aircraft.

### Connect WebGS via Serial USB Device

Assuming [ICAROUS](https://github.com/nasa/icarous) is configured properly, ensure you are on the same network as the device running [ICAROUS](https://github.com/nasa/icarous). Typically this will involve changing the IP address of your machine. Start the web server and the socket server. Ensure the Web page is connected to the socket server. In the settings panel, set:

    GCS Mode -> 'Connect to Hardware'
    Select Input Type -> USB
    IP Address -> {the same IP address ICAROUS is configured to output to}
    Component Id -> 5 (Default is 5. This is the standard ICAROUS Config. 0 will connect to Autopilot in most configurations.)

Ensure the Port and Baud Rate are correct. Press connect to aircraft.

### To run simulations

[ICAROUS](https://github.com/nasa/icarous) must be installed and properly built. On the settings page ensure

    GCS Mode is set to 'SITL'
    Path to ICAROUS is set correctly
    Path to Ardupilot is set correctly (if needed)
    SIM TYPE -> ArduCopter (Spelling and Capitalization Matter)

Then either right click on the map or click on the Aircraft button and select 'New Aircraft'. The parameters for [ICAROUS](https://github.com/nasa/icarous) in version 2 are auto loaded and they may need to be changed. This can be done once the aircraft is started.

### To view own-ship perspective flight instruments

After the aircraft has started, click Open DAA Display. This will open the display in a new tab. Currently this display only works on port 8082. If the server was launched on another port the map will not be displayed.

### Playback

Webgs uses the MAVProxy format for creating `.tlog` files for each flight. These files along with the Server logs, [ICAROUS](https://github.com/nasa/icarous) outputs, ardupilot outputs, and a text file containing all of the received mavlink messages are stored in the LogFiles directory. To playback a file, change the GCS Mode to Playback. Enter the file name in the text box. (It assumes files will be located in the LogFiles directory.) Click Start Playback. It may take a few seconds to load the file.

Note: I would not recommend fast forwarding at the beginning of the file. If you miss the flight plan messages, a flight plan will not show up on the map.

### Merging .tlog files for multi-aircraft playback

A Python3 script has been included for creating a `.mlog` file that webgs is capable of playing. It is located in webgs/utils/

    python3 mergeTlogs.py -h or --help for instructions on how to use it.

### Fly By File

Webgs is capable of flying scripted scenarios that are repeatable and adjustable. Functionality is still limited but it has been tested with four simulated aircraft flying simultaneously, each with multiple intruders and a geofence, repeated 50 times, adjusting parameters, flight plans, and intruders after 25 flights. Examples and instructions on building a script are located in `/webgs/Examples/TestScripts`.

## Current version

WebGS v1.0.10

## Notices

Copyright 2019 United States Government as represented by the Administrator of the National Aeronautics
and Space Administration. All Rights Reserved.

Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED¬†""AS IS"" WITHOUT ANY WARRANTY OF ANY
KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY
WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED
WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY
WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE.
THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT
AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE,
SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT
SOFTWARE.¬† FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND
DISTRIBUTES IT¬†""AS IS.""

Waiver and Indemnity:
RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT.¬† IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES,
DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY
DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT
SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES
GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT,
TO THE EXTENT PERMITTED BY LAW.¬† RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL
BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

### Contact

Andrew Peters andrew.peters@nianet.org  
Cesar Munoz cesar.a.munoz@nasa.gov
"
124,nasa/NASAaccess,R,"NASAaccess
=====================
Ibrahim N. Mohammed (Ibrahim.mohammed@nasa.gov)

What is NASAaccess?
NASAaccess is a software application in the form of a R package (https://www.r-project.org/about.html) and a web application.  NASAaccess software can generate gridded ascii tables of climate (CIMP5) and weather data (GPM, TRMM, GLDAS) needed to drive various hydrological models (e.g., SWAT, VIC, RHESSys, ...etc.).

How to cite the NASAaccess software?
Mohammed, I. N., 2019, NASAaccess: Downloading and reformatting tool for NASA earth observation data products [software]. National Aeronautics and Space Administration, Goddard Space Flight Center, Greenbelt, Maryland. https://github.com/nasa/NASAaccess

Where to find the NASAaccess software?
The NASAaccess R package is an open source software package under NASA Open Source Agreement v1.3 (https://opensource.org/licenses/NASA-1.3) and can be downloaded from Github at https://github.com/nasa/NASAaccess.

NASAaccess is also available as a web-based application that can be installed on servers for multiple usage suited for agencies and centers.  Full details on installing the web-based version of NASAaccess at local servers can be found at https://github.com/BYU-Hydroinformatics/SWATOnline/blob/master/Documentation/nasaaccess%20-%20Installation%20Guide.pdf

What is needed to run the NASAaccess software on my local machine?
On a local machine the user should have installed the following programs as well as setting up a user account.  The list below gives a summary of what is needed to be done prior to work with NASAaccess software on any local machine:
	‚Ä¢	Installing R software ‚Äî https://cloud.r-project.org/
	‚Ä¢	Installing Rstudio software ‚Äî https://rstudio.com/ (OPTIONAL)
	‚Ä¢	NASAaccess R package needs a user registration access with Earthdata (https://earthdata.nasa.gov/).  Users should set up a registration account(s) with Earthdata login as well as well as authorizing NASA GESDISC data access.  Please refer to https://disc.gsfc.nasa.gov/data-access for further details.
	‚Ä¢	After registration with Earthdata the NASAaccess software package users should create a reference file (‚Äònetrc‚Äô) with Earthdata credentials stored in it to streamline the retrieval access to NASA servers.
			o	Creating the ‚Äò.netrc‚Äô file at the user machine 'Home' directory and storing the user NASA GESDISC logging information in it is needed to execute the package commands is explained at https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+cURL+And+Wget
			o	For Windows users the NASA GESDISC logging information should be saved in a file ‚Äò_netrc‚Äô beside the ‚Äò.netrc‚Äô file explained above.
	‚Ä¢	Installing ‚Äòcurl‚Äô software (https://curl.haxx.se/).  Since Mac users have ‚Äòcurl‚Äô as part of macOS build, Windows users should make sure that their local machines build have 'curl' installed properly.
	‚Ä¢	Checking if you can run curl from your command prompt.  Type curl --help and you should see the help pages for the curl program once everything is defined correctly.
	‚Ä¢	Within Rstudio or R terminal run the following commands to install NASAaccess:
			o	library(devtools)
			o	install_github(""nasa/NASAaccess"")
			o	library(NASAaccess)

Within the Rstudio help tab the user can verify that the package has been installed and browse the help pages of the various functions of NASAaccess.
"
125,nasa/daidalus,HTML,"![](docs/DAIDALUS.jpeg """")

Detect and AvoID Alerting Logic for Unmanned Systems
========

Detect and Avoid Alerting Logic for Unmanned Systems (DAIDALUS) is a
software library that implements a configurable detect and avoid (DAA)
concept intended to support the integration of Unmanned Aircraft
Systems into civil airspace.  DAIDALUS serves as a reference
implementation of the functional requirements specified in DO-365, the
Minimum Operational Performance Standards (MOPS) for Unmanned Aircraft
Systems (UAS) developed by RTCA Special Committee 228 (SC-228).

This repository includes a prototype
implementations written in Java and C++.  The repository also includes
the formal specification of DAIDALUS core algorithms witten the Prototype
Verification System ([PVS](http://pvs.csl.sri.com)).

The core functionalities implemented by DAIDALUS include

*  detection logic,
*  alerting logic based on hazard and non-hazard volumes, and
*  instantaneous and kinematic maneuver guidance in the form of
suggestive guidance (i.e., bands) and directive guidance (i.e.,
preferred direction and velocity vector).

### Documentation

A draft of user guide is available at https://nasa.github.io/daidalus. 

### Software Library
The library can be compiled using the Unix utility `make` with the
provided `Makefile` in both the [Java](Java/Makefile) and
[C++](C++/Makefile) directories. From the directory Java,
the `make` command produces a jar file in the directory `Java/lib`.
From the directory C++, the `make` command will generate a static library
in `C++/lib`.

The sample application `DaidalusExample`, which is available in
[Java](Java/src/DaidalusExample.java) and
[C++](C++/src/DaidalusExample.cpp), illustrates the main
functionalities provided by DAIDALUS including reading/writing
configuration files, detection logic, alerting logic, maneuver
guidance logic, and computation of loss of well-clear contours.  This
application can be compiled using the provided `Makefile`.
To run the example application in a Unix environment, type from the
directory Java (or C++):

```
$ ./DaidalusExample
```

Other sample applications that illustrate DAIDALUS functionalities on
encounter files are provided in [Java](Java/README.md) and
[C++](C++/README.md).

### More information about DAIDALUS
For technical information about the definitions and algorithms in this
repository, visit https://shemesh.larc.nasa.gov/fm/DAIDALUS.

### Current Release

v2.0.2, December-11-2020

### License

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. 

### Contact

[Cesar A. Munoz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.

### Logo

The DAIDALUS logo was designed by 
[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC), NASA Langley Research Center.

### Copyright Notice

Copyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
126,nasa/EMTG,C++,"Welcome to EMTGv9 Open Source Release
README.opensource file compiled by Jacob Englander 4-20-2020

This package contains all of the code that NASA GSFC is releasing for the Evolutionary Mission Trajectory Generator, version 9. No third-party code is included in this release. You will need to download the third-party components yourself. This information is detailed in documents that may be found in the docs/build_system folder.

In particular, you will need a license for SNOPT. This is not free. Many of you probably already have SNOPT, and the rest may have to purchase it. EMTGv9 is known to work with SNOPT 7.5 and 7.6. It probably also works with 7.7 but I can't verify this because I don't have a license for 7.7. Older versions *might* work.

You will want to download SPICE ephemeris files for the bodies that you are visiting. I recommend downloading the full 3+ GB set from http://naif.jpl.nasa.gov/naif/ and placing it in your Universe/ephemeris_files folder

Note that regardless of whether you download all of the various SPICE ephemerides for the solar system, you will need to download de430.bsp from https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de430.bsp and place it in your Universe/ephemeris_files folder. I could not supply this file because it is too large for Github.

You must edit the EMTG-Config.cmake file as follows:

1) Change the CSPICE_DIR to wherever you have placed your CSPICE package. CSPICE_DIR should point to the root CSPICE directory.

2) Change the SNOPT_ROOT_DIR to be wherever you have placed the unzipped SNOPT folder.  From this directory, it will seek the traditional /lib and /interface folders (as appropriate to your SNOPT version, which should be auto detected).  Report any problems to the EMTG lead developers.
	Example: This is currently usually a directory called ""snopt7""

3) Change the 'BOOST_ROOT' to the root of your boost installer.  This presupposes you have already built boost (run its bootstrapper)
		3a) Optionally, if during the run phase of CMAKE boost is not being found, uncomment the next two lines and specifically specify the BOOST_INCLUDEDIR and BOOST_LIBRARYDIR.  The default options are traditional variants that are sometimes chosen by boost installers, but yours may vary.

Once all the above is in place, open CMake, and point the source directory at the EMTG root directory.  Point the build directory to the directory of your choice (may be the same). Run configure, then Generate, as per the usual CMake Process. This will create either  makefile for Linux or a Visual Studio project for Windows, or an Xcode project for Mac.

EMTGv9 is provided ""as is"" in its imperfect but very capable state. The US Government, NASA, and the developers cannot guarantee that the results you produce with EMTG are correct. EMTG is intended for use as a trade study tool and an initial guess generator for a flight navigation tool. We recommend that you use it in those contexts and only those contexts.
"
127,nasa/HDTN-BPCodec,C++,"## BPCodec ##

BPCodec is a stand-alone library designed to support encoding and decoding the bundle protocol format.  Both version 6 and version 7 of the bundle protocol are supported.  The version 6 encode and decode functions have been tested with JPL's ION [1] and GSFC's bplib [2], [3]. The version 7 decode functions have been tested on bundles from uPCN [4]. The version 7 encode functions will be released in the near term. The version 7 implemention is in early stages (pre-alpha) but is being released for research and collaborative purposes in the overall DTN community.

Note that the use of this library will produce results that are compatible with the Bundle Protocol from a wire perspective.  It does not, however, intend to offer a complete implementation of either version 6 or version 7 of the Bundle Protocol - semantics related to receipt, forwarding, and custody must be independently implemented and observed by a system that wishes to advertise compatibility with such.

## Build Environment ##
* BPCodec build environment requires CMake version 3.13
* Testing has been done with gcc version 8.3.0 (Debian 8.3.0-6) 
* Target: x86_64-linux-gnu 
* Tested on Debian 10
* Known issues:
* Ubuntu distributions may install an older CMake version that is not compatible
* Mac OS may not support recvmmsg and sendmmsg functions, recvmsg and sendmsg could be used

## Overview ##

The core of the library is found in bpv6.h and bpv7.h - each header file is responsible for the corresponding protocol version.  

The library is geared around two primary operations:

* Taking a byte string and properly decoding it into a C structure, and
* Taking a C structure and properly encoding it into a byte string

The library expects some familiarity with the bundle protocol: for example, one should know that the primary block should come first, and that the primary block is followed by additional canonical blocks.  The library also does not presently enforce constraints (e.g. ""the payload block shall always be the last block in a bundle"").

The ""test"" directory includes a small collection of bundles used to validate that encoding / decoding work as expected.  These bundles are utilized by a small collection of benchmark utilities included with the library.  

Note that this library does not yet support the generation of bundle security headers.

## Applications ##
The ""apps"" directory includes several tools built upon the basic bpcodec functions.
* The encode-benchmark simply tests the CBOR (BP v7) and SDNV (BP v6) functions on generated values.
* The bundle-benchmark reads bundles from files in the ""test"" directory. These bundles were generated from ION (JPL), bplib (GSFC) and uPCN in order to test the bundle decoding on a variety of implementations. The ION and bplib bundles are BP v6 and the uPCN bundle is BP v7. In the current release not all bundle extension blocks are supported, although this functionality will be added to the release in the near term.
* The bpgen tool will generate BP v6 bundles of a specified size as well as an approximate data rate. Source and destination node numbers can also be specified. Endpoint IDs currently follow the IPN style of addressing (node_number.service_number). This tool has been used with both ION and bplib in previous testing. The BP v7 functions have not been integrated into this tool yet, although this is planned in the near term. It will generate a csv file of statistics from the bundles sent. 
* The bpsink tool will listen for and receive bundles. It will generate a csv file of statistics from the bundles received. 

## Quick Start ##
* git clone https://github.com/nasa/HDTN-BPCodec.git
* cd ./HDTN-BPCodec
* mkdir ./build
* cd ./build
* cmake ..
* make

The simplest test you can start with is to run bpgen with bpsink. Bpgen will generate bundles and send them out as UDP packets on 127.0.0.1 port 4556.

In one terminal:
* cd ./HDTN-BPCodec/build/apps
* ./bpgen    (there are command line parameters you can set as well but you don't need to)


In another terminal:
* cd ./HDTN-BPCodec/build/apps
* ./bpsink

## Future Work ##
There are several areas currently under development with functionality that will be released in the near term.
* BP v7 primary and canonical block encode functions are being developed.
* BP v6 extension blocks are in the process of being tested with current BP v6 implementations of ION and BPlib.
* CRC validation is planned to be added to the BP v7 functions.

## References ##
1. https://sourceforge.net/projects/ion-dtn/
2. https://software.nasa.gov/software/GSC-18318-1
3. Hylton, A., Raible, D., Clark, G.A., Dudukovich, R., Tomko, B., Burke, L. (2019). Rising Above the Cloud- Toward High-Rate Delay-Tolerant Networking in Low- Earth Orbit.
4. https://upcn.eu/
"
128,nasa/LHASA,R,"# LHASA
Landslide Hazard Analysis for Situational Awareness

LHASA was created at Goddard Space Flight Center to identify the potential for landslides in nearly real time. The model is currently running, and it's output can be seen at https://pmm.nasa.gov/precip-apps. It can also be accessed as an ArcGIS web map at https://landslides.nasa.gov/viewer. For more detail about the model, please see the published article at http://dx.doi.org/10.1002/2017ef000715.

The R scripts here are written to be easily understood, executed, and modified by potential users of this research.

Full operational code is available in python at https://github.com/vightel/ojo-bot. 
"
129,nasa/System_Monitor_for_Radiation_Testing,Python,"# System Monitor for Radiation Testing (SMRT)

SMRT is a Python¬Æ-based software package intended to monitor computer system performance during radiation testing.

----

**Table of Contents**

[[_TOC_]]

----

# Purpose

This tool is intended to provide a comprehensive, cross-platform (Windows¬Æ/Linux¬Æ, Intel¬Æ/ARM¬Æ, etc.) test suite geared towards assessing computer performance as they go through radiation testing.  It's hoped that this will be widely adopted by the aerospace avionics community, creating a common standard for radiation survivability of computers (especially commercial-off-the-shelf single board computers) and that the community will continually improve this tool.

# Questions/Comments/Concerns?

The best way to get these things addressed is to open an issue on GitHub¬Æ and/or email the developer (samuel.m.pedrotty@nasa.gov).  Don't hesitate to reach out-- it's likely others also have your questions/comments/concerns.  We encourage users to add capability and fix bugs and then submit merge/pull requests to have their improvements brought back to the repository for all to benefit from.

# Basic Use

1. move the repository to the target computer
2. install Python 3
3. run the install script in the ""setup"" directory
4. run the start_tests.py script in the py_src directory
    * the test scripts will start assessing the system and logging data to a folder named with the start time in the ""data"" folder
5. stop the script or shutdown the SBC to terminate the software

# Data Analysis
1. after running the tool, run the ""plot_results.py"" script in the ""py_src"" folder to load, concatenate, and plot the data.
    * in some cases, it may be best to transfer the data to a faster computer for analysis and plotting

# Detailed Use

It's assumed that all of the interfaces of the computer under test will be connected appropriately to allow their monitoring (e.g. displays, ethernet, USB devices, etc.).

## Pre-test

1. appropriately install the Operating System on the computer (this software should support Linux and Windows)
2. move the repository to the computer
3. install Python 3
4. run the install script in the ""setup"" directory
5. update the user input section of the ""start_tests.py"" script in the ""py_src"" directory
    * set the *ram_pct_to_use*, the RAM usage parameter with units of percents (< 95% is recommended to provide the OS some headroom)
    * set the *data_save_interval*, the  parameter that determines the time between data file saving with units of seconds (if a high particle flux is to be used, an interval of < 3 seconds is recommended)
    * set the *test_cycle_time*, the parameter that sets the run rate of the test scripts with units of seconds (0.1 is recommended.  Note that some tests will run slower than the rate as their functions take a set time that may be larger than some user-input cycle times)
6. configure the computer as it will be when under test (software and hardware)
7. run the ""start_tests.py"" script in the ""py_src"" directory for a couple minutes to verify functionality
    * it's recommended to run the script as 'sudo' on Linux and as administrator on Windows
8. transfer the data to a faster computer that also has Python 3 installed and run the ""plot_results.py"" script to verify the tool (and computer) functioned as intended
9. delete the pre-test data from the computer to save disk space, keep the set that was moved onto the faster computer as a pre-test artifact

## Test use

10. appropriately deploy the computer in the test environment, connecting all interfaces as briefly mentioned above
11. run the ""start_tests.py"" script in the ""py_src"" directory and wait until you see the terminal print stating RAM allocation is complete, and that the cpu, disk, and network monitoring functions are operating
12. start the radiation test (e.g., start the particle flow and other test support equipment)
13. stop the test and/or make note when any off nominal behavior is observed, including unresponsive interfaces, crashed software, or prints stating off-nominal conditions were detected (including RAM changes, disk disconnections, and network adapter disconnections)
    * noting the flux/fluence and failure type will provide insight into the MTBF of that computer subsystem and will inform fault recovery and mitigation design
14. when the test is complete, assuming the device is still functional, transfer the data off and save it as a test artifact
15. plot the data with ""plot_results.py"" script in the ""py_src"" directory and review the data for other indicators of faults to inform MTBF analysis


# Detailed Function Description

## install_tool.py

This script automatically installs all of the necessary software requirements for Windows and Linux platforms after Python 3 is installed by the user.  The packages required for data plotting are optional and user-prompted as they can be memory-intensive for small, cheap, computers with limited RAM.

## start_tests.py

This script starts all of the test scripts and provides a regular, 1 Hz print to the terminal to provide a ""heartbeat"" to indicate if the computer/interface is still functioning.  This script also contains the aforementioned user inputs.

## test_ram.py

This script starts writing 1s to a struct until a user-defined amount of RAM is consumed.  Each cycle, the script will check the struct to make sure it's consistent.  If it's not, it will log an 'upset' and will print *RAM STATE CHANGE DETECTED*.  The RAM will not be overwritten to reset it.  Sometimes restarting the script can reset upsets, but sometimes the whole device must be restarted.  At other times, especially after the unit under test has been subject to a considerable particle flux, upsets may trigger after restarting even when the particle beam is off.  The script also records the percentage of RAM used.

## test_cpu.py

This script records the percentage of CPU used, current frequency in MHz, and CPU temperature in C (temperature is Linux-only).

## test_disks.py

This script records the number of disks/drives detected and will print *NUMBER OF DISKS HAS CHANGED!* if a change in the number of drives is detected.  This script also records some information regarding the drives.

## test_networks.py

This script records the number of network adapters detected and will print *NUMBER OF NETWORK ADAPTERS HAS CHANGED!* if a change in the number of adapters is detected.  This script also records some information regarding the adapters.

## plot_results.py

This script concatenates and loads all data in the most recently created data folder.  It then plots the data and, depending on a user-set flag, can also save the plots and pngs.

# Known Issues and Forward Work
1. The CPU temperature code only works on Linux operating systems.  Windows CPU temp values are hard-coded to 9999
2. The test_disks and test_networks scripts leave much to the user to parse in post-processing.  This should be intelligently handled by the tool.
3. The CPU frequency code only works on Linux operating systems.  Windows CPU frequency values are fixed to the system's base clock speed.

"
130,nasa/NASA-3D-Resources,Mathematica,"NASA-3D-Resources
=================

Welcome to the 3D Resources github site. Here you'll find a growing collection of 3D models, textures, and images from inside NASA. All of these resources are free to download and use. 

Please read the Usage Guidelines here: 

http://www.nasa.gov/audience/formedia/features/MP_Photo_Guidelines.html

Our team's goal is to provide a one-stop shop for 3D models, images, textures, and visualizations. We offer these assets for your use, free and without copyright. We recognize that the site is only as good as its contributors and thank them kindly.

We welcome feedback and comments. Tell us how you're using our models, and let us know what you think: arc-special-proj@lists.nasa.gov

We acknowledge and thank the many authors who have contributed to the 3D Resources collections.

NASA Jet Propulsion Laboratory - Solar System Simulator	

Michael Carbajal. NASA Headquarters	

NASA Ames Research Center	

Brian Kumanchik, Christian Lopez. NASA/JPL-Caltech	

NASA Johnson Space Center	

DigitalSpace Corporation	

Eyes on the Solar System. NASA/JPL-Caltech	

Doug Ellison / NASA-JPL	

Christopher M. Garcia. NASA/JPL-Caltech	

Christian A. Lopez. NASA/JPL-Caltech	

Kevin Lane. NASA/JPL-Caltech	

NASA Jet Propulsion Laboratory	

NASA Goddard Space Flight Center
	
NASA Johnson Space Center - Space Educators' Handbook	

Chris Meaney. NASA	

Ball Aerospace	

Dana Berry. NASA/Kepler Mission	

NASA/JPL-Caltech	

Christopher M. Garcia, Christian A. Lopez. NASA/JPL-Caltech	

Carlo Conti	



## Other Resources for NASA 3D Models
- https://nasa3d.arc.nasa.gov/
"
131,nasa/CTF,Python,"# Table of Contents
   * [cFS Test Framework](#cfs-test-framework)
      * [Getting Started](#getting-started)
         * [CTF Prerequisites](#ctf-prerequisites)
         * [CTF Directory Structure](#ctf-directory-structure)
         * [Configuration](#configuration)
         * [Sample cFS Workspace](#sample-cfs-workspace)
            * [Running the Sample cFS Workspace Scripts](#running-the-sample-cfs-workspace-scripts)
      * [Using CTF on a New cFS Project](#using-ctf-on-a-new-cfs-project)
         * [CCSDS Message Definitions](#ccsds-message-definitions)
            * [CCDD Auto Export](#ccdd-auto-export)
            * [Other](#other)
         * [New cFS Project Configuration](#new-cfs-project-configuration)
         * [Developing New Test Scripts](#developing-new-test-scripts)
      * [Developing or Extending CTF Plugins](#developing-or-extending-ctf-plugins)
      * [Updating CTF](#updating-ctf)
      * [Release Notes](#release-notes)
      * [License](#license)

# cFS Test Framework

The cFS Test Framework (CTF) provides cFS projects with the capability to develop and run automated test and verification scripts. The CTF tool parses and executes JSON-based test scripts containing test instructions, while logging and reporting the results. CTF utilizes a plugin-based architecture to allow developers to extend CTF with new test instructions, external interfaces, and custom functionality.

CTF is currently in active development, and is available under the NASA Open Source Agreement license (NOSA). See the [license](#license) section for more information.

## Getting Started

To get started, clone the CTF repository using the following:

`git clone https://github.com/nasa/CTF`

### CTF Prerequisites

CTF has been developed and tested on Linux (CentOS 7) and requires Python 3.x. The CTF Editor requires an installation of NodeJS/NPM.

There are two methods to install dependencies. 

1. An Anaconda environment setup script is provided to install all OS/Python dependencies into a self contained Anaconda environment. This method does not require sudo privileges. However, the CTF Python 3 environment must be activated prior to running CTF.

2. a PIP `requirements.txt` file is provided to install all Python 3 CTF dependencies. OS dependencies need to be installed manually and may require sudo privileges. This method provides the most light-weight dependency installation, but is more involved than the Anaconda setup.

#### CTF Prerequisites: Satisfied with Anaconda Environment 

The `setup_ctf_env.sh` script will setup an Anaconda 3 environment, which contains Python3, along with the python components identified in `requirements.txt` along with NodeJS/NPM.

To set up the CTF environment execute `source setup_ctf_env.sh`. Note: this may take several minutes depending on your network connection.

After the initial setup to activate the ctf environment execute `source activate_ctf_env.sh`.

If the Anaconda environment is corrupted, the environment can be reinstalled by executing `source setup_ctf_env.sh -u`.

#### CTF Prerequisites: Satisfied Without Anaconda Environment

Install Python3 (on CentOS run `sudo yum install python3-devel python3 python3-pip`)

Install NodeJS/NPM, visit https://nodejs.org/en/ and install Node version >= 10.12.0 (tested with v12.5.0). NPM will be included in the installation.

With Python 3 installed, PIP dependencies can be installed using `pip install -r requirements.txt`. Note: ensure that dependencies are installed to a PIP venv in order to easily update/reinstall packages, or install using  `--user` in the above pip command.

#### CTF Prerequisites: Other applications needed

`xterm` is required in order to run cFS in a separate terminal. Install `xterm` by using `sudo yum install xterm` or `sudo apt install xterm` if on Debian.

`cmake3` is required in order to build cFS projects. Install cmake3 using `sudo yum install cmake3` if needed.

A working cFS project is also needed. Note that CTF provides the [sample_cfs_workspace](#sample-cfs-workspace) if no existing cFS project is available.

### CTF Directory Structure
```
‚îú‚îÄ‚îÄ activate_ctf_env.sh
‚îú‚îÄ‚îÄ ctf (executable)
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run_editor.sh
‚îú‚îÄ‚îÄ setup_ctf_env.sh
‚îú‚îÄ‚îÄ configs/
‚îú‚îÄ‚îÄ external/
‚îú‚îÄ‚îÄ lib/
‚îú‚îÄ‚îÄ plugins/
‚îú‚îÄ‚îÄ schemas/
‚îî‚îÄ‚îÄ tools/
    ‚îî‚îÄ‚îÄ ctf_ui/
    ‚îî‚îÄ‚îÄ schema_validator/
```

### Configuration

The configuration file contains configuration options for each CTF plugin. To run CTF with a selected configuration, add the `--config_file <path_to_config_file>` command line argument when running CTF. Each plugin can define one (or more) sections with specific configuration values as follows

```
[some_plugin_config]
some_field = true
some_other_field = xyz
```

Please refer to each plugin's `README` document for information on the required configuration fields for each plugin. In addition, example configuration files are provided under `configs/examples`.

###### Note: More variants of the config can be created, and loaded via the ctf command-line argument (`--config_file`). It is recommended to create different configurations for different testing use-cases such as a CI config, a Test Authoring/Debugging config, etc..

###### Note: Environment variables can be used as a value using similar syntax as follows `${env_variable}`

###### Note: Other INI field values can be referenced using the following syntax `field2 = ${section:field1}`. 

### Sample cFS Workspace

By default, The CTF tool contains a minimal cFS workspace (`external/sample_cfs_workspace.tgz`) for CTF testing and evaluation purposes. It also provides a reference cFS/CTF workspace layout that is applicable to other cFS projects. 

The `sample_cfs_workspace` contains a set of cFS apps that receive CCSDS commands and output CCSDS telemetry messages through the CI/TO apps. It also contains a set of sample CTF test scripts that can be executed against the compiled cFS system from `sample_cfs_workspace`.

To get started using the `sample_cfs_workspace`, follow the documentation under [external/README.md](external/README.md). After completing the instructions, CTF will be set up as a cFS tool *within* the `sample_cfs_workspace` and can be used to execute test scripts against that cFS workspace. A similar approach can be followed to add CTF to an existing cFS project.

###### Note: If cFS does not build, you may have missing dependencies. Install of `gcc-multilib` in order to build cFS projects successfully. On CentOS 7, run the following command `sudo yum install libgcc.i686 glibc-devel.i686`

#### Running the Sample cFS Workspace Scripts
First, review the contents of `configs/default_config.ini`. Update any fields as needed. Specifically, the `[cfs]: workspace_dir` may need to be update with the appropriate path according to where `sample_cfs_workspace` is extracted.

To run the provided example scripts, simply run the `ctf` executable (ensure the CTF environment is activated) with the configuration file and the script to run as follows.

```
cd ~/sample_cfs_workspace/ctf_tests
source activate_ctf_env.sh
ctf --config_file configs/default_config.ini --script_dir test_scripts/sample_test_suite
```

###### Note: Ensure `build_cfs` is set to `true` in the `configs/default_config.ini` file, if the cFS executable has not been built manually.

CTF will proceed to build and start the cFS project and execute the test script. An `xterm` window showing the output of the cFS instance will launch, after which the test script begins execution.

###### Note: run `sudo sysctl -w fs.mqueue.msg_max=1024` if you see an error from cFS regarding the msg_max queue size.

Sample CTF output is shown below.

```
[13:00:47.209] ctf                             (42 ) *** INFO: Status_Manager created
[13:00:47.209] ctf                             (48 ) *** INFO: Loading Plugins - Start
[13:00:47.209] plugin_manager                  (202) *** INFO: Looking for plugins under package: plugins
[13:00:47.214] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.ccsds_plugin.ccsds_plugin.CCSDSPlugin
[13:00:47.214] ccsds_plugin                    (35 ) *** WARNING: CFS Plugin not yet loaded... 
[13:00:47.345] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.cfs.cfs_plugin.CfsPlugin
[13:00:47.346] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.example_plugin.example_plugin.ExamplePlugin
[13:00:47.348] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.sp0_plugin.sp0_plugin.sp0Plugin
[13:00:47.348] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.ssh.ssh_plugin.SshPlugin
[13:00:47.349] plugin_manager                  (211) *** INFO: From package: plugins  - Loaded the following plugins: ['CCSDS Plugin', 'CFS Plugin', 'ExamplePlugin', 'sp0_plugin', 'SshPlugin']
[13:00:47.349] ctf                             (50 ) *** INFO: Loading Plugins - End
[13:00:47.349] ctf                             (58 ) *** INFO: Status_Manager created
[13:00:47.349] ctf                             (62 ) *** INFO: Script_Manager created
[13:00:47.349] ctf                             (64 ) *** INFO: Reading Test Scripts
[13:00:47.350] ctf                             (95 ) *** INFO: Loaded Script: <lib.test_script.TestScript object at 0x7f9d731e7bb0>
[13:00:47.350] ctf                             (108) *** INFO: Completed reading in the json script file/files passed in the arguments
[13:00:47.350] ctf                             (111) *** INFO: Script Manager - Start
[13:00:47.350] ccsds_plugin                    (38 ) *** INFO: Initialized CCSDS Plugin
[13:00:47.350] cfs_time_manager                (45 ) *** INFO: CfsTimeManager Initialized. Verification Poll Period = 0.05.
[13:00:47.350] cfs_plugin                      (152) *** INFO: Initialized CfsPlugin
[13:00:47.350] sp0_plugin                      (64 ) *** INFO: Initialized SP0 plugin
[13:00:47.351] test_script                     (92 ) *** INFO: Verification Test Name: CFS CI Functions Test
[13:00:47.351] test_script                     (93 ) *** INFO: Verification Test Number: CFS-CI-Functions-Test
[13:00:47.351] test_script                     (94 ) *** INFO: Test Conductor: ashehata
[13:00:47.351] test_script                     (95 ) *** INFO: Run Date/Time: 10/08/2020 / 13:00:47
[13:00:47.351] test_script                     (96 ) *** INFO: Platform: #1 SMP Tue Mar 31 23:36:51 UTC 2020
[13:00:47.351] test_script                     (97 ) *** INFO: Requirement Verification Targets: MyRequirement
[13:00:47.351] test_script                     (98 ) *** INFO: Test Description : Testing CI Functions
[13:00:47.351] test_script                     (99 ) *** INFO: Input file utilized : CiFunctionTests.json
[13:00:47.352] test                            (199) *** INFO: Test CI-Function-Test-001: Starting
[13:00:47.352] test                            (200) *** INFO: Test CI Enable TO command
[13:00:47.352] test                            (157) *** INFO: Waiting 1 time-units before executing RegisterCfs
[13:00:48.374] cfs_plugin                      (161) *** INFO: RegisterCfs: Name cfs_LX1
[13:00:48.379] cfs_config                      (93 ) *** WARNING: Config Value cfs_LX1:evs_messages_clear_after_time does not exist or is not the right type. Attempting to load from base section [cfs].
[13:00:48.379] cfs_controllers                 (56 ) *** INFO: Creating MID Map from CCDD Data at /home/ashehata/sample_cfs_workspace/ccdd/json
[13:00:48.396] ccsds_packet_interface          (69 ) *** INFO: Importing CCSDS header definitions from /home/ashehata/sample_cfs_workspace/tools/ctf/plugins/ccsds_plugin/cfe/ccsds_v2/ccsds_v2.py
[13:00:48.397] cfs_controllers                 (71 ) *** INFO: Starting Local CFS Interface
[13:00:48.397] cfs_controllers                 (85 ) *** WARNING: Not starting CFS executable... Expecting ""StartCfs"" in test script...
[13:00:48.397] cfs_controllers                 (88 ) *** INFO: CfsController Initialized
[13:00:48.397] cfs_plugin                      (197) *** INFO: Register for cfs_LX1 finished.
[13:00:48.397] test                            (74 ) *** TEST_PASS: Instruction RegisterCfs: {'name': 'cfs_LX1'}
[13:00:48.397] test                            (157) *** INFO: Waiting 1 time-units before executing StartCfs
[13:00:49.412] cfs_plugin                      (254) *** INFO: StartCfs: Name cfs_LX1
[13:00:49.412] cfs_controllers                 (96 ) *** INFO: Starting CFS on cfs_LX1
[13:00:49.413] local_cfs_interface             (100) *** INFO: Starting CFS Executable
xterm: cannot load font '-misc-fixed-medium-r-semicondensed--13-120-75-75-c-60-iso10646-1'
[13:00:52.518] cfs_controllers                 (105) *** INFO: Skipping enable output...
[13:00:52.519] test                            (74 ) *** TEST_PASS: Instruction StartCfs: {'name': 'cfs_LX1'}
[13:00:52.519] test                            (157) *** INFO: Waiting 1 time-units before executing SendCfsCommand
[13:00:53.584] cfs_controllers                 (123) *** INFO: Sending CFS Command to target: cfs_LX1, CI_CMD_MID:CI_ENABLE_TO_CC with Args: {""cDestIp"": ""127.0.0.1"", ""usDestPort"": 5011, ""usRouteMask"": 0, ""iFileDesc"": 0}
[13:00:53.585] test                            (74 ) *** TEST_PASS: Instruction SendCfsCommand: {'name': '', 'mid': 'CI_CMD_MID', 'cc': 'CI_ENABLE_TO_CC', 'args': {'cDestIp': '127.0.0.1', 'usDestPort': 5011, 'usRouteMask': 0, 'iFileDesc': 0}}
[13:00:53.585] test                            (157) *** INFO: Waiting 0 time-units before executing CheckTlmValue
[13:00:53.585] test                            (96 ) *** INFO: Waiting up to 2.0 time-units for verification of CheckTlmValue: {'name': '', 'mid': 'CI_HK_TLM_MID', 'args': [{'variable': 'usCmdCnt', 'value': [1], 'compare': '=='}, {'variable': 'usCmdErrCnt', 'value': [0], 'compare': '=='}]}
[13:00:53.585] cfs_plugin                      (289) *** INFO: CheckTlmValue: CFS Target: , MID CI_HK_TLM_MID, Args [{""variable"": ""usCmdCnt"", ""value"": [1], ""compare"": ""==""}, {""variable"": ""usCmdErrCnt"", ""value"": [0], ""compare"": ""==""}]
[13:00:53.636] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_SB_HousekeepingTlm_t with MID: 0x2008
[13:00:53.636] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_EVS_LongEventTlm_t with MID: 0x2006
[13:00:53.842] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_TIME_HousekeepingTlm_t with MID: 0x200e
[13:00:54.047] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_TBL_HousekeepingTlm_t with MID: 0x200c
[13:00:54.047] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_ES_HousekeepingTlm_t with MID: 0x2001
[13:00:54.253] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: SCH_HkPacket_t with MID: 0x240b
[13:00:54.357] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CI_HkTlm_t with MID: 0x2404
[13:00:54.408] cfs_interface                   (503) *** INFO: PASSED Intermediate Check - usCmdCnt: Actual: 1, Expected: 1, Comparison: ==, Tol: +0, -0
[13:00:54.408] cfs_interface                   (503) *** INFO: PASSED Intermediate Check - usCmdErrCnt: Actual: 0, Expected: 0, Comparison: ==, Tol: +0, -0
[13:00:54.408] cfs_controllers                 (319) *** INFO: PASSED Final Check for MID:{'MID': 9220, 'name': 'CI_HkTlm_t', 'PARAM_CLASS': <class 'plugins.cfs.ccsds.ccdd_export_reader.CI_HkTlm_t'>}, Args:[{'variable': 'usCmdCnt', 'value': [1], 'compare': '=='}, {'variable': 'usCmdErrCnt', 'value': [0], 'compare': '=='}]
[13:00:54.408] test                            (134) *** TEST_PASS: Verification Passed CheckTlmValue: {'name': '', 'mid': 'CI_HK_TLM_MID', 'args': [{'variable': 'usCmdCnt', 'value': [1], 'compare': '=='}, {'variable': 'usCmdErrCnt', 'value': [0], 'compare': '=='}]}
```

## Using CTF on a New cFS Project

### CCSDS Message Definitions

To use CTF for a new cFS project, message definition files must first be generated. The message information files allow CTF to create the command/telemetry C structures in python in order to build CCSDS commands and process CCSDS telemetry.

#### CCDD Auto Export

Currently, CTF obtains the CCSDS message definitions by parsing a set of files that adhere to a CTF-supported JSON schema to define command/telemetry message structures. These files can be created manually for testing, or created automatically using a tool such as CCDD.

For the `sample_cfs_workspace` project, the CCSDS message definition JSONs are automatically generated by the [CCDD](https://github.com/nasa/CCDD) tool.

CTF utilizes a CCDD export reader that parses and stores the message definitions JSON into a format usable by CTF.

###### Note: Ensure that the CCSDS JSON files directory is correctly set in the INI configuration file under `[cfs]: CCSDS_data_dir` in order for CTF to correctly process these files.

#### Other

Projects that do not use CCDD to maintain their command and telemetry dictionary can utilize the same current JSON schema to define message information. This can be done automatically (generated from some external tool), or manually. In addition, more complex projects may implement their own `Custom CCSDS Reader` using the provided CCSDS Interface calls to set up the message information in CTF. An example of that is shown under `plugins/cfs/ccsds/ccdd_export_reader.py`.

Essentially, each reader will need to read the message information files from some source, and relay that information to CTF using the following functions

```python
add_telem_msg(self, mid_name, mid, name, parameters, parameter_enums = None)

add_cmd_msg(self, mid_name, mid, command_code_map, command_enums = None)

add_enumeration(self, key, value)
```

Example usage can be found in the `plugins/cfs/ccsds/ccsds_export_reader.py`

### New cFS Project Configuration

To configure CTF for a new project, simply create a configuration INI file that is specific to the project's testing needs. `config_template.ini` is provided under the `configs/` directory as a template for new projects to use.

It is recommended to start with simple tests that start/stop cFS to verify that the build and run capability is working. Once that is achieved, more complicated test scripts can be created.

### Developing New Test Scripts

The CTF tool provides a [CTF Editor](tools/ctf_ui) to assist in the creation, modification, and running of scripts. Currently, the editor can be obtained from the repository above. Please refer to the [CTF Editor User's Guide](docs/ctf_editor/usage_guide.md) for information on how to run and use the editor.


Refer to the following [guide](docs/ctf/ctf_json_test_script_guide.md) for information on the JSON input script file description. This is useful when scripts are to be written manually without the aid of the editor.

Plugin READMEs include documentation of their own instructions and configuration fields:

* [CCSDS Plugin](plugins/ccsds_plugin/README.md)
* [CFS Plugin](plugins/cfs/README.md)
* [Example Plugin](plugins/example_plugin/README.md)
* [SSH Plugin](plugins/ssh/README.md)

## Developing or Extending CTF Plugins

Refer to the [plugin guide](docs/ctf/ctf_plugin_guide.md) for information on creating custom CTF plugins.

## Updating CTF

CTF versions can be checked out by running `git checkout <version_tag>`. This updates the CTF submodule/repository to the selected version.

In addition, the configuration (and test script) files may need to be updated with new configuration fields, or test script additions/changes.

The sections below describe the changes needed to quickly update to a specific version. Note that the release notes contain a more complete set of changes.

### Updating to v1.X

CTF v1.0 introduces major additions to the configuration file, as well as changes to the test script schema and instructions.

* Existing CTF Test Scripts
    * Using a find & replace tool, update all references within a test script as follows
        * All Instances
            * `""commands""` -> `""instructions""`
            * `""command`   -> `""instruction""`
        * CFS Plugin Instructions
            * `""name""`     -> `""target""` 
            * `""set_length""` to `""payload_length""`
            
* Config changes (reference `configs/default_config.ini` for descriptions and examples)
    * Add fields
        * core:additional_plugin_path
        * core:ignored_instructions
        * core:delay_between_scripts
        * cfs:CCSDS_target
        * cfs:evs_event_mid_name
    * Delete fields
        * cfs:evs_event_msg_mid

* CCSDS Message Definitions
    * **Update to CCSDS Definition Schema (JSON) to support multiple MID values for the same message definitions**
        * For command structure definitions
            * Rename `command_id_name` to `cmd_mid_name`
            * Remove `command_message_id`. Now maintained in a separate MID map file
            * Add `cmd_description`
            * Rename `command_codes` to `cmd_codes`
                * Rename `name` to `cc_name`
                * Rename `code` to `cc_value`
                * Rename `description` to `cc_description`
                * Add `cc_data_type`
                * Rename `args` to `cc_parameters`
                    * No changes needed within `cc_parameters` (previously `args`)
        
        * For telemetry structure definitions
            * Rename `name` to `tlm_mid_name`
            * Remove `mid`. Now maintained in a separate MID map file
            * Rename `mid_name` to `tlm_data_type`
            * Add `tlm_description`
            * Rename `parameters` to `tlm_parameters`
                * No changes to `name`, `array_size`, `description`, `bit_length`, `parameters`

        * For alias and constant definitions
            * Rename `cfs_type_name` to `alias_name`
            * Rename `c_type` to `actual_name`
            * Rename `cfs_macro_name` to `constant_name`
            * Rename `c_macro` to `constant_value`

        * MID Map JSON
            * Move the ""mid"" field from each message definition to a separate MIDs map file, mapping between MID names and raw MID values per cFS target.
            
            * An array of objects mapping target to MID value as follows
                ```
                [
                    {
                        ""target"": ""my_target"",
                        ""mids"": [ 
                                    {""mid_name"": ""CFS_MID_NAME"", ""mid_value"": ""0x1234""}
                                    ...
                                ]
                    }
                ]
                ```
    * Refer to the `sample_cfs_workspace/ccdd/json` for reference (after extracting the workspace from `external/`)

## Release Notes
### v1.1
12/18/2020

* CTF Core Changes
    * Add support for a `disable` field to CTF instructions within a test script to temporarily disable that instruction.
    
    * Add support for a `description` field to CTF instructions within a test script to capture comments.
    
    * Add `end_test_on_fail_commands` attribute to the `Plugin` class, allowing plugins to define a list of critical commands that end the current test script on failure.
    
    * Minor improvements and bug fixes.

* CFS Plugin Changes
    * Add the `RegisterCfs` and `StartCfs` instructions to the `end_test_on_fail_commands` such that test execution is halted if registering or starting cFS fails.
    
    * Add support for Short Event Messages by setting the `cfs:evs_short_event_mid_name` field in the config to match the MID name of the `CFE_EVS_ShortEventTlm_t` within the CCSDS JSON definitions.
    
    * Add the CheckNoEvent instruction to check that an event message was *not* sent during the verification timeout.
    
    * Ensure args do not get malformed while sending a command to multiple cFS targets
    
    * Resolve CheckTlmValue passing if one or more variables fails to be evaluated from the telemetry packet.
    
    * Resolve connection and deployment issues with cFS targets running with the SP0 protocol (WIP)
    
    * Minor improvements and bug fixes.

    
* CTF Editor Changes
    * Add support for disabling/enabling test instructions or test cases
    
    * Add support for viewing/editing descriptions (comments) within the test script
    
    * Add the ability to rename folders or test scripts within the editor
    
    * Minor improvements and bug fixes.
     
### v1.0
10/30/2020

Major Release

This is the first open source release for CTF. Note that CTF remains in active development.

Existing users should review the change logs below and ensure current configurations and scripts are updated.

* Release Additions
    * Add `external/` directory with a self contained example CFS project. This project can be configured to utilize CTF out of the box. Refer to `external/README.md` for more information.

* CTF Core Changes
    
    * **Update test script JSON schema with more accurate field names**
        * Update `commands` array to `instructions`
        * Update `command` object to `instruction`
        * Refer to [Updating to v1.0](updating-to-v10) 
        
    * Add an option to provide an additional plugin path within the configuration file. Custom plugins are loaded from that directory, in addition to CTF default plugins.
        * Add the `additional_plugin_path = <path_to_custom_plugins_dir>` field to the config ini.
        * Note - Give the custom plugin directory a unique name (do not use `plugins`, `lib`, etc...), so as to not shadow any modules within the CTF repo.
    
    * Add an option to ignore specific CTF instructions within the configuration file. This is useful for CI or specific-configurations that may not have the ability to run certain instructions.
        * Add the `ignored_instructions = <instruction_1>, <instruction_2>, ...` field to the config ini. Specify the instructions to ignore (comma-seperated).
    
    * Minor improvements and bug-fixes.
        
* CCSDS Plugin Changes
    * **Update to CCSDS Definition Schema (JSON) to support multiple MID values for the same message definitions**
        * Refer to [Updating to v1.0](updating-to-v10) 
        
    * **Change config field `evs_event_msg_mid` to `evs_event_mid_name` which should now be set to the MID name for the Event Messages (for example `CFE_EVS_LONG_EVENT_MSG_MID`).** 
        * The actual MID value will be retrieved from the CCSDS message definitions.

* CFS Plugin Changes
    * **Update `name` field to `target`**
        * The `name` field is used for *all* cFS Plugin Instructions. This field is now renamed to `target` for clarity, and specifies the cFS target to apply the instruction to.
            * **Please change all instances of `""name"":` to `""target""` within existing test scripts. Example scripts have been updated as part of the release.**
    
    * No longer validate `cfs_run_dir` on registration. Previously, if the cFS instance was not built (i.e the run directory does not exist), validation would fail.
    * Rename `set_length` to `payload_length` for the `SendInvalidLengthCfsCommand` test instruction.
    
* User IO Plugin (New)
    * Add support for `WaitForUserInput` test instruction (with a prompt message shown to the user). This is useful when testing safety-critical functionality, or need to pause the test until the user confirms to proceed.

* Example Plugin Changes
    * Add an example of loading and utilizing a C shared library within a CTF plugin.

* CTF Editor Changes
    * Improvements to the Run Status View
        * Instructions can now be expanded while/after a test run to inspect instruction arguments and data.
    
    * Updates to support CTF and CCSDS definition changes.

### v0.6
2020-09-14

* CCSDS Plugin Changes
    * Configurable CCSDS Headers
        * Allows users to define CCSDS header structures for Command and Telemetry packets
            * Reference CCSDS Plugin README for documentation on creating custom headers and their associated implementations
        * Migrate CCSDS V1 and V2 definitions to utilize new configurable header functionality
        * Added a new config field `CCSDS_header_path` to set the appropriate python header module for CTF to use

* cFS Plugin Changes
    * Allow ""RegisterCfs"" instruction to be used without specifying a name. This uses the default `[cfs]` configuration section. 
    * Add new field to ""StartCfs"" test instruction to support additional command-line arguments when starting the cFS instance.
        * Example
            ```json
                {
                    ""command"": ""StartCfs"",
                    ""data"": {
                        ""name"": """",
                        ""run_args"": ""-R PO""
                    },
                    ""wait"": 1
                }
            ```
        * Note: The StartCfs run_args value will append arguments to the `cfs_run_args` field defined in the config
    * Fix ""SendInvalidLengthCfsCommand"" instruction to send the actual length specified in the instruction as the payload length
        * Note: Total length received by FSW will be equal to the specified `payload_length` + length of the CCSDS header(s)

    * Fix command/telemetry sockets not sending/receiving packets when cFS restarts within a single script
        * CTF to re-initialize command/telemetry sockets after restart
        
    * Write cFS output log files in append mode such that logs are not overwritten when cFS restarts
    
* CTF Editor Changes

    * Fix rendering bug causing last script to be hidden in the file-pane behind sidebar collapse button

### v0.5.1

Disable implicit padding of telemetry and command payload structures (assumes explicit padding in the C data structures, or disabled C compiler implicit padding).

Minor improvements and bug-fixes.

### v0.5
2020-08-06

Major backend updates to improve reliability/maintainability of CTF.

* cFS Plugin Changes
    * Major architecture rework to support multiple and remote cfs instances
    * cFS Plugin
        * Telemetry Reader
            * Add time-tags to telemetry packets for more accurate telemetry verifications.
            * Log begin time of a verification, and clear all packets received for the selected MID prior to verification time.
            * For event messages, the config field `evs_message_clear_after_time` is added to keep events available in CTF telemetry
          up to a specific time. This is needed since EVS messages are only output once.
        * CheckEvent 
            * Now support regular expressions via the `is_regex` parameter.
            * `msg` parameter is now optional. Leaving it blank will only verify the APP and ID fields of EVS messages. 
            * Note: Ensure that the TO subscription table for the FSW project is configured to output a large number of EVS messages in order to
            avoid dropped packets.
    
* Logging
    * Substantially clean up logging when running in INFO or ERROR log level
        * Polling instruction logs should be reduced to only show actual vs. expected and if no packets are received in that poll...
    * cFS plugin will directly write the following to the script log folder
        * cFS Build/Output Logs
        * cFS EVS Logs
        * cFS Telemetry Logs (Experimental, does not output array elements)

* Config INI Changes
    * Reference configs/default_config.ini or config_template.ini for descriptions of fields
    * Core
        * Rename `telem_verify_timeout` to `ctf_verification_timeout`
        * Rename `telem_verify_poll_period` to `ctf_verification_poll_period`
    * CCSDS
        * Remove `CCSDS_reader_script_path`, `CCSDS_reader_class`, `CCSDS_module`
    * cFS
        * Remove `evs_tlm_list_depth`
        * Rename `ip` to `cfs_target_ip`
        * Add `cfs_output_file`, `log_ccsds_imports`, `evs_messages_clear_after_time`

* CTF Editor
    * Update to allow saving commands or function instructions with empty arguments.
        * Empty arguments will be saved to the test script with an empty string value.
        * CTF Backend will zero-out any command argument set to an empty string.
    * Add `is_regex` field to CheckEvent instruction. 
        * Refer to `scripts/example/test_advanced_example.json` for reference.
        
### v0.3.2
2020-08-05

* Log MID values of unknown packets
* Resolve cfs_output file not being written to results when no event messages are received over telemetry
* Add initial support for command array arguments
    
### v0.3.1
2020-07-08

* cFS Plugin Changes
    * Add new instruction ""ArchiveCfsFiles"", which accepts a FSW artifacts and moves any files created during the test run to the test results directory.
        * Note: Currently, this feature is supported for Linux cFS targets only. SSH and SP0 support for archiving FSW artifacts is planned for a later version.
    * Redesign multi-cfs architecture to be purely configuration based (remove script-specific overrides to allow scripts to be platform independent.) 
        * Each cFS Target should have its own configuration section in the config INI file.
        * RegisterCfs receives a name, and attempts to load target configuration (including protocol) from the loaded configuration.
        * A PASS/FAIL for that instruction will be set after validating the necessary config fields for cFS and the specific protocol (Linux, SSH, SP0)
    * Rework macro replacement logic such that 
        * Macro replacement occurs for command argument values, telemetry field values, and array indices.
        * All macros in a test script should start with the # prefix (example: ""#SOME_MACRO_IN_MACRO_MAP"").
    * Resolve CheckEvent is a polling instruction, to allow for a timeout/polling (similar to typical CheckTlmValue).
    * Resolve missing ""build cfs"" implementation for local cfs targets.
    
* General
    * Rework setup_ctf_env.sh and activate_ctf_env.sh such that 
        * Setup only needs to be run once (within the CTF install directory) and will install the anaconda environment to the user's home directory, or a custom path.
        * Activate *can* be placed at the project-specific CTF test directory
        * Multiple projects using CTF on the same platform can share the user-specific anaconda environment.

* CTF Editor
    * Resolve rendering bug when attempting to select an array element within a telemetry field.
    * Ensure arrays are left with ""index-placeholders"" `[]` where the test author can insert either a hard-coded index, or a macro from the macro map.
        * Note: macros should be preceded with the # token to identify that a replacement needs to take place before evaluating.

* CTF Core
    * Update Test Pass/Fail log message with string-based payload to mirror the data within the test script during test run.
    
### v0.3
2020-05-21

* cFS Plugin Changes
    * Serialize Telemetry Receiving Logic (and accompanying lessons learned about the cFS SCH configuration, and how it should be set up‚Ä¶) -> This resolves the timing issues and dropped packets we were experiencing
    * Continuous Telemetry Verification Capability -> CTF now has the ability to ‚Äúcontinually‚Äù verify a piece of telemetry against a condition defined in the test script. If the verification fails at any point during the test run, the failure is logged and the test will fail

* CTF Core
    * Configuration/Path improvements to resolve the relative/absolute path issues we were seeing when CTF is configured outside of its repository

* CTF Editor
    * Update editor to support new instruction (continuous verifications)
    * Fixe bug in editor launching backend in pre-defined working directory (as opposed to dynamically).
    
* Other minor bug fixes and improvements

### v0.2
2020-04-24

* cFS Plugin Changes
    * More Generic CCSDS/CCDD Interfaces
        * Add CCSDS Reader Interface (CFE 6.6/CFE 6.7, CCSDS V1/V2)
        * Add CCDD JSON Export Reader
    * Multi/Remote cFS Support
        * Allow the cFS Plugin to execute/manage multiple instances of cFS running on local or remote targets
        * Remote Targets Support: SSH, SP0, Local (linux). More targets can be added as needed.
    * Change command argument structure in test script JSON. Arguments are now encoded in the test script as a JSON dictionary.   

* Initial Plugin Implementations For
    * SSH Plugin
    * SP0 Plugin
    * CCSDS Plugin
           
* Re-haul CTF Editor to support generic CCSDS JSON Exports, and multi/remote cFS.

* Other bug-fixes and improvements
    
### v0.1
2019-11-22

* Initial CTF Release

* Initial Plugin Implementations For
    * cFS Plugin
    * Example Plugin
    * PLATO Plugin (Deprecated)
    * Remote Execution Plugin (Deprecated)
    * Trick cFS (Removed for later release)

* Initial CTF Editor Release

## License

MSC-26646-1, ""Core Flight System Test Framework (CTF)""

Copyright (c) 2019-2020 United States Government as represented by the
Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

This software is governed by the NASA Open Source Agreement (NOSA) License and may be used,
distributed and modified only pursuant to the terms of that agreement.
See the License for the specific language governing permissions and limitations under the
License at https://software.nasa.gov/.

Unless required by applicable law or agreed to in writing, software distributed under the
License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
either expressed or implied.


"
132,nasa/cumulus-ecs-task,JavaScript,"# cumulus-ecs-task

[![CircleCI](https://circleci.com/gh/nasa/cumulus-ecs-task.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-ecs-task)
[![npm version](https://badge.fury.io/js/%40cumulus%2Fcumulus-ecs-task.svg)](https://badge.fury.io/js/%40cumulus%2Fcumulus-ecs-task)

Use this Docker image to run a Node.js Lambda function in AWS
[ECS](https://aws.amazon.com/ecs/).

## About

cumulus-ecs-task is a Docker image that can run Lambda functions as ECS
services.

When included in a Cumulus workflow and deployed to AWS, it will download a
specified Lambda function, and act as an activity in a Step Functions workflow.

## Compatibility

This only works with Node.js Lambda functions, and requires that the Lambda
function it is running has a dependency of at least v1.0.1 of
[cumulus-message-adapter-js](https://github.com/cumulus-nasa/cumulus-message-adapter-js).

## Usage

Like other Cumulus libraries, cumulus-ecs-task is designed to be deployed to AWS
using [kes](https://github.com/developmentseed/kes) to manage Cloudformation
config. This documentation assumes you're working with a Cumulus deployment and
that you have files and directory structure similar to what's found in the
[cumulus template repository](https://github.com/cumulus-nasa/template-deploy).

### Options

This library has two options:

- `activityArn` **required**
  - The arn of the activity in a step functions workflow. Used to receive
    messages for that activity and send success/failure responses.
- `lambdaArn` **required**
  - The arn of the lambda function you want to run in ECS.

### Workflow config

For examples of how to integrate this image with Cumulus, please see the
[documentation](https://nasa.github.io/cumulus/docs/workflows/developing-workflow-tasks#ecs-activities)
and our
[example workflow](https://github.com/nasa/cumulus/blob/master/example/cumulus-tf/ecs_hello_world_workflow.tf)
in source.

## Development

To run locally:

```bash
npm start -- --activityArn <your-activity-arn> --lambdaArn <your-lambda-arn>
```

To build the docker image:

```bash
npm run build
```

To run in Docker locally:

```bash
docker run -e AWS_ACCESS_KEY_ID='<aws-access-key>' \
  -e AWS_SECRET_ACCESS_KEY='<aws-secret-key>' \
  cumuluss/cumulus-ecs-task \
  --activityArn <your-activity-arn> \
  --lambdaArn <your-lambda-arn>
```

### To test a workflow while developing locally

You can execute workflows on AWS that test the version of cumulus-ecs-task that
you're developing on locally.

First, make sure that the ECS cluster for your deployment has zero tasks running
that might respond to a workflow's requests.

That way only your local version will respond to your workflow.

Next, start ecs-cumulus-task locally.

Either with node:

```bash
npm start -- --activityArn <your-activity-arn> --lambdaArn <your-lambda-arn>
```

Or with docker:

```bash
# build the image
npm run build

# run the image
docker run -e AWS_ACCESS_KEY_ID='<aws-access-key>' \
  -e AWS_SECRET_ACCESS_KEY='<aws-secret-key>' \
  cumuluss/cumulus-ecs-task \
  --activityArn <your-activity-arn> \
  --lambdaArn <your-lambda-arn>
```

Finally, trigger a workflow. You can do this from the Cumulus dashboard, the
Cumulus API, or with the AWS Console.

## Troubleshooting

SSH into the ECS container instance.

Make sure the EC2 instance has internet access and is able to pull the image
from docker hub by doing:

```bash
docker pull cumuluss/cumulus-ecs-task:1.7.0
```

`cat` the ecs config file to make sure credentials are correct:

```bash
cat /etc/ecs/ecs.config
```

Check if there's multiple entries of the config.

If there is, there are two things to try:

- Delete the ec2 instance and redeploy
- Delete the incorrect config and restart the ecs agent (I haven't tested this
  much but I expect it to work. You'll still want to update the docker
  credentials in the deployment's app directory). Restart the agent by doing:

```bash
sudo stop ecs
source /etc/ecs/ecs.config
sudo start ecs
```

## Create a release

To create a release, first make sure the [CHANGELOG.md](CHANGELOG.md) file is
updated with all the changes made.

Next, bump the version and the changes will automatically be released upon merge
to master.

```bash
npm version <major|minor|patch|specific version>
```

Create the build

```bash
npm run build
```

Release to Docker Hub

```bash
npm run release
```

## Contributing

See the [CONTRIBUTING.md](CONTRIBUTING.md) file.

## License

[Apache-2.0](LICENSE)
"
133,nasa/dawn-grand-toolbox,IDL,"# dawn-grand-toolbox

Software to access and process data acquired by the NASA Dawn mission's Gamma-Ray and Neutron Detector (GRaND). The software consists of routines written in the Interactive Data Language (IDL), compatible with IDL version 8.6.1+. The GRaND archive consists entirely of character and binary tables. Two routines are provided to ingest the data for processing. These are found in the grd4-IDL subdirectory:

## grd4_read_character_table
Function that reads character (ASCII) tables, including most raw files and all calibrated and derived data files.

## grd4_read_binary_table
Function that reads binary data found in raw data files that contain event-mode gamma-ray and neutron data (-EMG and -EMN files).

The routines parse the label, extracting metadata useful for analysis, and read the data.¬†For time series data files (raw and calibrated), the metadata includes mission phase and target infomation instrument settings (STATE, TELREADOUT, and TELSOH) and observing conditions.

## sample_labels
This directory contains sample labels for evaluation of the software. The accompanying data files can be downloaded from the Planetary Data System Small Bodies Node: https://sbn.psi.edu/pds/resource/dawn/dawngrandPDS4.html. The GRD_STATE_TABLE.TAB and label are included in the dawn-grand-ancillary/miscellaneous/ directory."
134,nasa/PRECiSA,Haskell,"PRECiSA
=======

PRECiSA (Program Round-off Error Certifier via Static Analysis) is a fully automatic static analyzer for floating-point programs.

The main functionality of PRECiSA is the [round-off error estimation](#floating-point-round-off-error-estimation).
Given a floating-point program, PRECiSA computes a sound overapproximation of the round-off error that may occur together with PVS certificates ensuring its correctness.
[Here](http://precisa.nianet.org/) you can try the PRECiSA web-interface and find more information on the tool.

In addition, PRECiSA implements a [program transformation](#automatic-generation-of-stable-C-code-from-PVS) from a PVS real-valued program to floating-point C code.
This transformation corrects unstable tests by over-approximating the conditional guards in the if-then-else statements.
The resulting transformed program emits a warning when an unstable test is detected (i.e., the floating-point computational flow diverges with respect to the ideal real number one).
The generated C code is annotated with ACSL contracts that relate the floating-point implementation with the real-valued program specification.



# Installation

PRECiSA runs on Linux and Mac OS X operating systems.


## Prerequisites


To build and install PRECiSA you need:

* The [Glorious Haskell Compiler](https://www.haskell.org/ghc/) and its package manager Cabal, both available as part of the [Haskell platform](https://www.haskell.org/platform/),
* [CMake](https://cmake.org/),
* the [NASA Kodiak library](https://github.com/nasa/Kodiak) to compute numerical error bounds.

To verify certificates generated by PRECiSA you need:

* [PVS version 6.0](http://pvs.csl.sri.com/),
* the [NASA PVS library](https://github.com/nasa/pvslib).

If you want to use the SMT optimization you need [FPRock](https://github.com/nasa/FPRoCK).


## Build

1. Install [Kodiak](https://github.com/nasa/Kodiak) producing a dynamic library `libKodiakDynamic.so` (for Linux) or `libKodiakDynamic.dylib` (for MacOS).

2. Make accessible the *Kodiak* dynamic library to your linker.
   Normally, this is done by updating the LD_LIBRARY_PATH environment variable with the directory in which the `libKodiakDynamic.so` (or `libKodiakDynamic.dylib`) file is present.
   Let `<kodiak-directory>` represent this directory.
   In a bourne shell that environment variable can be set like this:
   ```
   $ export LD_LIBRARY_PATH=""<directory-containing-libKodiakDynamic.so>:$LD_LIBRARY_PATH""
   ```
   You can add the previous line to your shell initialization script (for example, `~/.profile` or `~/.bashrc`) to make the `LD_LIBRARY_PATH` change permanent.

3. Go to the `PRECiSA` sub-directory of the repository.
   ```
   $ cd <repository-root>/PRECiSA
   ```

4. Use the following commands to create a `cabal` *sandbox* in this folder and install the executable in `<repository-root>/.cabal-sandbox/bin/precisa`
   ```
   $ cabal v1-sandbox init    # Creates a new sandbox.
                              # Old cabal versions may need the command `cabal sandbox init` instead

   $ cabal v1-install --enable-optimization --extra-lib-dirs=<kodiak-directory>
   ```

5. Now, the `precisa` executable can be run with:
   ```
   $ .cabal-sandbox/bin/precisa
   ```



# Floating-Point Round-Off Error Estimation

The input to the PRECiSA round-off error estimator are the following files:

* A program `example.pvs` composed of floating-point valued functions. In its current version, PRECiSA accepts a subset of the language of the Prototype Verification System (PVS), including LET-IN expressions, IF-THEN-ELSE constructions, function calls, and floating-point values and operations such as: addition, multiplication, division, subtraction, floor, square root, trigonometric functions, logarithm, and exponential. For example:
   ```
   example: THEORY
   BEGIN
   IMPORTING float@aerr754dp

	example (X,Y: unb_double) : unb_double =
		IF (X >= 0 AND Y < 4)
		THEN IF (Y > 0)
			 THEN X+Y
			 ELSE X*Y
			 ENDIF
        ELSE X-Y ENDIF

   END example
   ```

* A file `example.input` containing initial ranges for the input variables of the program. For example:
   ```
   example(X,Y): X in [-10,10], Y in [0, 40]
   ```

* Optionally, a file `example.path` containing a set of decision paths/sub-programs of interests.
   The user can specify these paths by listing the paths/sub-programs of interests as a list of True and False.
   For instance, in the example above, the path [True, True] corresponds to the sub-program Dadd(X,Y), the path [False] to the subprogram Dsub(X,Y), and the path [True] to the subprogram IF (Y > RtoD(0)) THEN Dadd(X,Y) ELSE Dmul(X,Y) ENDIF.
   The analysis is done for all the execution paths in the program, or better for all combination of real/FP execution path in the program).
   For the selected sub-programs, a dedicated error expression representing the round-off error estimation is computed.
   For the others, the tool will generate an overall error which is the maximum of all the round-off error estimations corresponding to these sub-programs.
   If the user does not select any sub-program of interest (None), the tool will produce the overall round-off error for the stable cases (when real and floating-point execution flows coincide) and the one for the unstable cases (when real and floating-point execution flows diverge).
   If the user is interested in a precise analysis of the entire program (All), the analysis will generate a semantic element for each combination of real/FP execution path in the program.
   Examples of possible input for the decision pahts are the following:
   ```
   example(X,Y): None
   ```
   or
   ```
   example(X,Y): All
   ```
   or
   ```
   example(X,Y): [True, True]
   ```

More examples can be found in the [PRECiSA benchmarks folder](PRECiSA/benchmarks).

The analysis performed by PRECiSA results in one upper-bound of the floating-point round-off error for each decision path of interest, an overall upper-bound for the rest of decision paths, and an overall upper-bound for the unstable test cases (when real and floating-point flows diverge).
Additionally, PRECiSA generates two PVS theories:

* a theory containing a collection of lemmas stating symbolic accumulated round-off error estimations for the input program, and
* a theory containing a collection of lemmas stating numerical accumulated round-off error estimations.  The numerical estimations are computed using Kodiak.

All the generated lemmas are equipped with PVS proof scripts that automatically discharge them.


## How to run the PRECiSA round-off error estimator

We assume that `precisa` (the executable of PRECiSA) is in the current directory.

To launch the round-off error analysis of PRECiSA with the default parameters run:
```
$ ./precisa analyze ""example.pvs"" ""example.input""
```
- the first parameter is the path to the PVS program to be analyzed;
- the second one is the path to the file that indicates the initial values for the input variables of the input program;

### Command Line Options

- `--paths example.path` specifies the path to the file that indicates the decision paths of interest for every function in the program. The default is the empty set, i.e., there is no path of interest and the output of PRECiSA consists of two errors: one for the stable cases (when real and floating-point flows agree) and one for the unstable cases (when real and floating-point flows diverge).

- Options for the branch-and-bound search used to compute the numerical estimation:
  - `--max-depth 7` (or `-d 7`) is the maximum depth of the branch-and-bound exploration with a default value of `7`.
  - `--precision 14` (or `-p 14`) is the negative exponent of `10` representing the numerical precision used.
  It has a default value of `14` which stands for a precision of <span style=""white-space: nowrap;""><math>10<sup>-14<sup></math></span>.

- `--max-lemmas 50` (or `-l 50`) sets the maximum number of lemmas allowed to be generated by PRECiSA. This avoids having certificates too big to be treated. If your program generates a huge number of lemmas, this means probably that you have several nested if-then-else. In this case, try to run PRECiSA with the default settings, or try to set some decision paths of interests. Alternatively, you can activate the Stable Test Assumption.

- `--assume stability` (or `-s`) if this option is activated, real and floating-point execution flows are assumed to coincide (Stable Test Assumption). Therefore, the analysis can be unsound since the cases where the execution paths diverge (unstable cases) are not considered.

- `--no-collapsed-unstables` (or `-u`) if this option is activated, the unstable tests are not collapsed in a unique case.

- `--smt-optimization` (or `-u`) if this option is activated, PRECiSA checks the satisfiability of each path condition by calling an external SMT solver through the FPRoCK tool. In this way, it is possible to detect and remove the spurious execution paths, improving the accuracy of the round-off error estimation.

An example of how to execute PRECiSA by manually setting some options is the following:
```
$ ./precisa analyze ""example.pvs"" ""example.input"" --paths ""example.path"" --max-depth 7 --precision 14 --max-lemmas 40
```

## How to verify the PVS certificates

[PVS version 6.0](http://pvs.csl.sri.com) and the development version
of the [NASA PVS Library](https://github.com/nasa/pvslib) are required
to proof-check the symbolic and the numerical certificates generated by PRECiSA in
PVS. Furthermore, the directory
`PVS` has to be added to the Unix environment variable
`PVS_LIBRARY_PATH`.  Depending upon your shell, one of the following lines
has to be added to your startup script.  In C shell (csh or tcsh), put this line in
`~/.cshrc`, where `<precisapvsdir>` is the absolute path to the
directory `PVS`:

~~~
setenv PVS_LIBRARY_PATH ""<precisapvsdir>:$PVS_LIBRARY_PATH""
~~~

In Borne shell (bash or sh), put this line in either `~/.bashrc or ~/.profile`:

~~~
export PVS_LIBRARY_PATH=""<precisapvsdir>:$PVS_LIBRARY_PATH""
~~~

You can use the proveit shell script to automatically check the proofs in the symbolic and numerical certificates generated by PRECiSA. For example, if you analyzed the program `example.pvs`, in the same folder you will find two files: `cert_example.pvs` and `num_cert_example.pvs`.

To check the correctness of the PVS theories in `cert_example.pvs` and `num_cert_example.pvs` you can run:
```
$ proveit -sc cert_example.pvs
$ proveit -sc num_cert_example.pvs
```

### PVS basic troubleshooting ###

If the PVS verification is not behaving as expected, try cleaning the PVS binaries in the NASA PVS library. Simply run cleanbin-all in the NASA PVS library folder of your installation and try again.



# Automatic generation of stable C code from PVS

The input to the PRECiSA C code generator are the following files:

* A program `example.pvs` composed of real-valued functions. In its current version, PRECiSA accepts a subset of the language of the Prototype Verification System (PVS), including LET expressions, IF-THEN-ELSE constructions, function calls, and floating point values and operations such as: addition, multiplication, division, subtraction, floor, square root, trigonometric functions, logarithm, and exponential. For example:
   ```
   example: THEORY
   BEGIN

	example (X,Y: real) : real =
		IF (X >= 0 AND Y < 4)
		THEN IF (Y > 0)
			 THEN X+Y
			 ELSE X*Y
			 ENDIF
        ELSE X-Y ENDIF

   END example
   ```

* A file `example.input` containing initial ranges for the input variables of the program. For example:
   ```
   example(X,Y): X in [-10,10], Y in [0, 40]
   ```

The output is a C floating-point program `example.c` instrumented to detect unstable tests.
This program is annotated with ACSL contracts stating the relation between the floating-point program and the real number specification. This annotated program can be analyzed with the [Frama-C](https://frama-c.com/) static analyzer.

Besides, PVS certificates are provided for ensuring that all the unstable tests are detected.
These certificates can be automatically checked as explained [here](#How-to-verify-the-PVS-certificates).


## How to run the PRECiSA stable C code generator

We assume that `precisa` (the executable of PRECiSA) is in the current directory.

To launch the round-off error analysis of PRECiSA with the default parameters run:
```
$ ./precisa gen-code ""example.pvs"" ""example.input""
```

- the first parameter is the path to the PVS program to be analyzed;
- the second one is the path to the file that indicates the initial values for the input variables of the input program;

### Command Line Options

- `--format FORMAT` where FORMAT can be double or single, indicating the target format of the floating-point C code. The default value is double precision.



# Additional information

## Version

*PRECiSA v-3.0.0* (July 2020)

## Contact information
If you have any question or problem, please contact:

* [Laura Titolo](mailto:laura.titolo@nianet.org) (for PRECiSA)
* [Mariano Moscato](mailto:mariano.moscato@nianet.org) (for PVS)
* [Marco A. Feliu](mailto:marco.feliu@nianet.org) (for Kodiak)
* [C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (PRECiSA @ NASA)

## Related Publications

- Mariano Moscato, Laura Titolo, Marco Antonio Feliu Gabaldon and C√©sar Mu√±oz: A Provably Correct Floating-Point Implementation of a Point-in-Polygon Algorithm. FM 2019

-	Rocco Salvia, Laura Titolo, Marco A. Feli√∫, Mariano M. Moscato, C√©sar A. Mu√±oz, Zvonimir Rakamaric: A Mixed Real and Floating-Point Solver. NFM 2019: 363-370

- 	Laura Titolo, C√©sar A. Mu√±oz, Marco A. Feli√∫, Mariano M. Moscato:
Eliminating Unstable Tests in Floating-Point Programs. LOPSTR 2018: 169-183

- Laura Titolo, Marco A. Feli√∫, Mariano M. Moscato, C√©sar A. Mu√±oz:
An Abstract Interpretation Framework for the Round-Off Error Analysis of Floating-Point Programs. VMCAI 2018: 516-537

- Mariano M. Moscato, Laura Titolo, Aaron Dutle, C√©sar A. Mu√±oz:
Automatic Estimation of Verified Floating-Point Round-Off Errors via Static Analysis. SAFECOMP 2017: 213-229


## License and Copyright Notice

The code in this repository is released under NASA's Open Source Agreement.  See the directory [`LICENSES`](LICENSES).

<pre>

Notices:

Copyright 2020 United States Government as represented by the Administrator of the National Aeronautics
and Space Administration. All Rights Reserved.

Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND,
EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY
THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES
OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT,
ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT
DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT,
IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF
ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS
ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL
SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES
GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. 
IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,
EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED
ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY
AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS,
AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY
FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

</pre>
"
135,nasa/cpr,OCaml,"# CPR*: Formally Verified Compact Position Reporting Algorithm 

The Compact Position Reporting (CPR) algorithm consists of a set of functions defined in the standard RTCA-DO- 260B/Eurocae ED-102A, Minimum Operational Performance Standards for 1090 MHz extended squitter Automatic Dependent Surveillance - Broadcast (ADS- B) and Traffic Information Services - Broadcast (TIS-B). These function encode and decode aircraft positions. CPR* is a formally verified C implementation of CPR's functions that use computer arithmetic in fixed- and floating-point formats.

## The Fixed-Point Implementation

The directory [`C/fixed-point`](C/fixed-point) contains a fixed-point C implementation of the CPR functions.
The [ACSL](https://frama-c.com/acsl.html) specification language was used to state correctness properties for the core components of this implementation.
Such correctness properties assure that each C function behaves accordingly with a logical description of it written also in ACSL.
The [Frama-C](https://frama-c.com/index.html) static analyzer was then used to process these annotated C functions and output verification conditions (VCs) expressed in [PVS](https://pvs.csl.sri.com/) language.
The directory `PVS` contains the resulting VCs and proofs for them.
Additionally, proofs of the correctness of the logical descriptions w.r.t. the aforementioned standard (under the modified set of requirements presented in [1]) can also be found in the same directory.

## The Floating-Point Implementation

The floating-point C implementation can be found in the directory [`C/floating-point`](C/floating-point).
As in the previous case, ACSL annotations along the code are used to relate the result of each core function with the result of its real valued specification.
Since in this case the implementation was developed using floating-point operations, the annotations take into consideration the maximum possible roundoff error occurred in the calculations performed on each operation. 
In order to deal with this kind of VCs, the Frama-C analyzer was used to generate [Gappa](http://gappa.gforge.inria.fr/) representations of them.
These Gappa specifications were manually modified to add hints allowing the Gappa solver to automatically discharge the VCs.
The modified Gappa files can be found in the [`Gappa`](Gappa) directory.
Finally, PVS was used to prove that the real valued specifications of the core functions are correct w.r.t. the standard assuming the set of corrected requirements reported in [1]. 
The [`PVS`](PVS) directory contains these proofs.
For details on the verification process for the floating-point implementation, see [2].

### Version

Current version is 1.0.0

### License

The code in this repository is released under NASA's Open Source Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. 

## Contact

[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.

## Copyright Notice

Copyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

# Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

# Bibliography

[1] A. Dutle, M. Moscato, L. Titolo, and C. Munoz. A formal analysis
of the compact position reporting algorithm. 9th Working Conference on
Verified Software: Theories, Tools, and Experiments, VSTTE 2017,
Revised Selected Papers, 10712:19‚Äì34, 2017.

[2] L. Titolo, M. Moscato, C. A. Munoz, A. Dutle, and F. Bobot. A
formally verified floating-point implementation of the compact
position reporting algorithm. In Klaus Havelund, Jan Peleska, Bill
Roscoe, and Erik de Vink, editors, Formal Methods, pages 364-381,
Cham, 2018. Springer International Publishing.
"
136,nasa/PolyCARP,HTML,"PolyCARP 
========

PolyCARP (Algorithms and Software for Computations with Polygons)
is a package of algorithms, implemented in Java, C++, and Python, 
for computing containment,
collision, resolution, and recovery information for polygons. The
intended applications of PolyCARP are related, but not limited, to
safety critical systems in air traffic management.

This repository also the formal specification
of PolyCARP algorithms in the Prototype Verification System ([PVS](http://pvs.csl.sri.com)).

### Documentation

The API document for PolyCARP is still work in progress. In the meantime,
please refer to the release notes and the examples files
[`PolycarpExample.java`](Java/src/PolycarpExample.java) and [`PolycarpExample.cpp`](C++/src/PolycarpExample.cpp).

For technical information about the definitions and algorithms in this
repository, visit [http://shemesh.larc.nasa.gov/fm/PolyCARP](http://shemesh.larc.nasa.gov/fm/PolyCARP).

### Current Release

PolyCARP@FormalATMv2.6.2 (March-18-2017) 

### License

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. 

### Contact

[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.

### Copyright Notice

Copyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
137,nasa/LC,C,"core Flight System (cFS) Limit Checker (LC)
======================================================

Open Source Release Readme
==========================

LC Release 2.1.2

Date: 10/7/2020

Introduction
-------------
  The Limit Checker (LC) is a core Flight System (cFS) application
  that is a plug in to the Core Flight Executive (cFE) component of the cFS.

  The LC application monitors telemetry data points in a cFS system and compares 
  the values against predefined threshold limits. When a threshold condition is 
  encountered, an event message is issued and a Relative Time Sequence (RTS) 
  command script may be initiated to respond/react to the threshold violation.  

  The LC application is written in C and depends on the cFS Operating System
  Abstraction Layer (OSAL) and cFE components.  There is additional LC application
  specific configuration information contained in the application user's guide
  available in https://github.com/nasa/LC/tree/master/docs/users_guide

  This software is licensed under the NASA Open Source Agreement.
  http://ti.arc.nasa.gov/opensource/nosa


Software Included
------------------

  Limit Checker application (LC) 2.1.2


Software Required
------------------

 Operating System Abstraction Layer 5.0 or higher can be
 obtained at https://github.com/nasa/osal

 core Flight Executive 6.8.0 or higher can be obtained at
 https://github.com/nasa/cfe

 Note: An integrated bundle including the cFE, OSAL, and PSP can
 be obtained at https://github.com/nasa/cfs

About cFS
----------
  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.
  This framework is used as the basis for the flight software for satellite data
  systems and instruments, but can be used on other embedded systems.  More
  information on the cFS can be found at http://cfs.gsfc.nasa.gov

EOF
"
138,nasa/CS,C,"core Flight System (cFS) Checksum Application (CS) 
==================================================

Open Source Release Readme
==========================

CS Release 2.4.2

Date: 
10/07/2020

Introduction
-------------
  The Checksum application (CS) is a core Flight System (cFS) application that 
  is a plug in to the Core Flight Executive (cFE) component of the cFS.  
  
  The CS application is used for for ensuring the integrity of onboard memory.  
  CS calculates Cyclic Redundancy Checks (CRCs) on the different memory regions 
  and compares the CRC values with a baseline value calculated at system startup. 
  CS has the ability to ensure the integrity of cFE applications, cFE tables, the 
  cFE core, the onboard operating system (OS), onboard EEPROM, as well as, any 
  memory regions (""Memory"") specified by the users.

  The CS application is written in C and depends on the cFS Operating System 
  Abstraction Layer (OSAL) and cFE components. There is additional CS application 
  specific configuration information contained in the application user's guide
  available in https://github.com/nasa/CS/tree/master/docs/users_guide

  This software is licensed under the NASA Open Source Agreement. 
  http://ti.arc.nasa.gov/opensource/nosa
 
 
Software Included
------------------
  Checksum application (CS) 2.4.2
  
 
Software Required
------------------

 Operating System Abstraction Layer 5.0 or higher can be 
 obtained at https://github.com/nasa/osal
 
 core Flight Executive 6.8.0 or higher can be obtained at
 https://github.com/nasa/cfe

 Note: An integrated bundle including the cFE, OSAL, and PSP can 
 be obtained at https://github.com/nasa/cfs
 
About cFS
----------
  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.
  This framework is used as the basis for the flight software for satellite data
  systems and instruments, but can be used on other embedded systems.  More
  information on the cFS can be found at http://cfs.gsfc.nasa.gov

EOF
"
139,nasa/MD,C,"core Flight System (cFS) Memory Dwell Application (MD)
======================================================

Open Source Release Readme
==========================

MD Release 2.3.3

Date:
10/7/2020

Introduction
-------------
  The Memory Dwell application (MD) is a core Flight System (cFS) application
  that is a plug in to the Core Flight Executive (cFE) component of the cFS.

  The MD application monitors memory addresses accessed by the CPU. This task
  is used for both debugging and monitoring unanticipated telemetry that had
  not been previously defined in the system prior to deployment.

  The MD application is written in C and depends on the cFS Operating System
  Abstraction Layer (OSAL) and cFE components. There is additional MD application
  specific configuration information contained in the application's Doxygen
  user's guide available in https://github.com/nasa/MD/tree/master/docs.

  This software is licensed under the Apache 2.0 license.


Software Included
------------------
  Memory Dwell application (MD) 2.3.3


Software Required
------------------

 Operating System Abstraction Layer 5.0.0 or higher can be
 obtained at https://github.com/nasa/osal

 core Flight Executive 6.8.0 or higher can be obtained at
 https://github.com/nasa/cfe

 Note: An integrated bundle including the cFE, OSAL, and PSP can
 be obtained at https://github.com/nasa/cfs


About cFS
-----------
  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.
  This framework is used as the basis for the flight software for satellite data
  systems and instruments, but can be used on other embedded systems.  More
  information on the cFS can be found at http://cfs.gsfc.nasa.gov
"
140,nasa/WellClear,HTML,"![](docs/DAIDALUS.jpeg """")

**Current version of DAIDALUS v2 is available from [https://github.com/nasa/daidalus](https://github.com/nasa/daidalus)**
========

Detect and AvoID Alerting Logic for Unmanned Systems
========

This repository includes a prototype implementation written in Java and
C++ of DAIDALUS (Detect and Avoid Alerting Logic for Unmanned Systems).
DAIDALUS is a reference implementation of a detect and avoid concept
intended to support the integration of Unmanned Aircraft Systems into
civil airspace.
The repository also includes definitions in Matlab of a family of
well-clear violation volumes and the formal specification
of DAIDALUS core algorithms in the Prototype Verification System ([PVS](http://pvs.csl.sri.com)).

DAIDALUS is a
reference implementation of the detect and avoid (DAA) functional
requirements  described in
Appendix G of DO-365, the Minimum Operational Performance Standards (MOPS)
for Unmanned Aircraft Systems (UAS)  developed by RTCA
Special Committee 228 (SC-228). The current software release implements

*  detection logic,
*  alerting logic based on hazard and non-hazard volumes, and
*  multi-level instantaneous and kinematic maneuver guidance.

### Documentation

A draft of user guide is available at https://nasa.github.io/WellClear. 

Example programs [`DaidalusExample.java`](DAIDALUS/Java/src/DaidalusExample.java) and 
[`DaidalusExample.cpp`](DAIDALUS/C++/src/DaidalusExample.cpp) illustrate the main 
functional capabilities of DAIDALUS in Java and C++, respectively.

For technical information about the definitions and algorithms in this
repository, visit https://shemesh.larc.nasa.gov/fm/DAIDALUS.

### Current Release

The release in this repository is DAIDALUSv1.0.2.
**The current version of DAIDALUS v2 is available from [https://github.com/nasa/daidalus](https://github.com/nasa/daidalus).**

### License

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. 

### Contact

[Cesar A. Munoz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.

### Logo

The DAIDALUS logo was designed by 
[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC), NASA Langley Research Center.

### Copyright Notice

Copyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
141,nasa/AGTF30,MATLAB,"# AGTF30

The Advanced Geared Turbofan 30,000 (AGTF30) is a geared turbofan 
simulation that utilizes the Toolbox for the Modeling and Analysis 
of Thermodynamic Systems (T-MATS) to create a steady-state and 
dynamic engine model within MATLAB/Simulink. The engine model is 
based upon a futuristic geared turbofan concept and allows steady-state 
operation throughout the flight envelope. Dynamic operation is 
utilizes a baseline control system.
Design tools included within the package include system linearization and
automated plotting scripts.

AGTF30 installation:
1) Install Matlab and Simulink, Developed in MATLAB 2015aSP1
2) Install the Toolbox for the Modeling and Analyais of Thermodyanmic Systems,
developed with T-MATS version 1.3 (https://github.com/nasa/T-MATS/releases)

Running AGTF30 simulation:
1) Navigate to the AGTF30 folder
3) Run script ""setup_everything.m"",  this will generate all required variables within MATLAB 
(namely: bus variables and the engine parameter structure 'MWS'). 
4) Open AGTF30 system simualtion: dynamic(Dyn), Linearization (Lin), or steady-state (SS).
5) Run the specified model.

Changing run conditions:

Open setup_everyting and adjust Input.  structure as appropriate

or

Verify setup_everything sets Input.UseExcel = 1, update excel document define_inputs.xlsx

or

Verify setup_everything sets Input.UseExcel = 0, update define_inputs.m

System inputs are defined (within the MWS structure) as vectors with each value matching with a time vector.
The linearization and steady-state systems will only make use of the initial values, 
Note: Linearization and Dynamic systems will use the steady-state solver to generate initial conditions.

Outputs:
1) Steady state simulation outputs structure: out_SS, containing a large amount of data. Additionally it generate specific structures out_*
2) Linearization simulation outputs structure: out_Lin, containing a large amount of data. (ABCD matricies for state-space model are located within the structure)
3) Dynamic simulation outputs structure: out_Dyn, containing a large amount of data. Additionally it generate specific structures out_*

Auto plotting scripts:

The AGTF30 makes use of T-MATS autoplotting scrips. These are defined within PlotDyn.m and PlotSS.m.
After running the Dyn model or SS model, run the appropriate script to generate the appropriate plots.

References:
1) Jeffryes W. Chapman and Jonathan S. Litt. ""Control Design for an Advanced Geared Turbofan Engine"", 53rd AIAA/SAE/ASEE Joint Propulsion Conference, AIAA Propulsion and Energy Forum, (AIAA 2017-4820)
2) Jones, S.M., Haller, W.J., Tong, M.T., ÔøΩAn N+3 Technology Level Reference Propulsion SystemÔøΩ, NASA/TM-2017-219501, 2017. "
142,nasa/PyMKAD,Python,"# PyMKAD

The world-wide aviation system is one of the most complex dynamical systems ever developed and is generating data at an extremely rapid rate. Most modern commercial aircraft record several hundred flight parameters including information from the guidance, navigation, and control systems, the avionics and propulsion systems, and the pilot inputs into the aircraft. These parameters may be continuous measurements or binary/categorical measurements recorded in one second intervals for the duration of the flight. Currently, most approaches to aviation safety are reactive, meaning that they are designed to react to an aviation safety incident or accident. PyMKAD is a novel approach based on the theory of multiple kernel learning to detect potential safety anomalies in very large data bases of discrete and continuous data from world-wide operations of commercial fleets. This code address an anomaly detection problem which includes both discrete and continuous data streams, where we assume that the discrete streams influence on the continuous streams. We also assume that atypical sequence of events in the discrete streams can lead to off-nominal system performance.  

The objective of this project is to automate the analysis of flight safety incidents in a way that combines both analysis of discrete and continuous parameters. 

This repository contains the following files in its top level directory:

* [PythonCode](PythonCode)  
The source code of the repository includes: preprocessing modules, the main mkad code, and a post processing visualization tool. The code is uses a command line interface and a json file for configuring. 

* [documentation](documentation)  
Documents describing how to configure and run the program, as well as how to interpret the results. 


* [MKAD NOSA 2019.pdf](MKAD%20NOSA%202019.pdf)  
Licensing for MKAD




## Contact Info

NASA Point of contact: Nikunj Oza <nikunj.c.oza@nasa.gov>, Data Science Group Lead.

For questions regarding the research and development of the algorithm, please contact Bryan Matthews <bryan.l.matthews@nasa.gov>, Senior Research Engineer.


## Copyright and Notices

Notices:

Copyright ¬© 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

"
143,nasa/polyfit,C++,"# polyfit

This is a stand alone C++ program to fit a curve with a polynomial.

To compile: g++ -o Polyfit Polyfit.cpp

Inputs:

k: Degree of the polynomial
fixedinter: Fixed the intercept (coefficient A0)
wtype: Weight: 0 = none (default), 1 = sigma, 2 = 1/sigma^2
alphaval: Critical apha value
x[]: Array of x values to be fitted
y[]: Array of y values to be fitted
erry[]: Array of error of y (if applicable)


"
144,nasa/EdsLib,C,"# CCSDS SOIS Electronic Data Sheet Tool and Library

This repository contains an implementation of a tool and runtime library
for embedded software to create and interpret data structures defined 
using Electronic Data Sheets per CCSDS book 876.0.

The full specification for Electronic Data Sheets is available here:

[CCSDS 876.0-B-1](https://public.ccsds.org/Pubs/876x0b1.pdf)

This repository contains the basic tool to process EDS files, a
runtime library (EdsLib) for embedded software, and libraries 
to interoperate with JSON, Lua, and Python.

This software is also intended to work with the Core Flight System:

[cFS](https://github.com/nasa/cFS)

A set of patches to CFS to enable EDS features is also available.

## General Overview

The tool reads all EDS files and generates an in-memory document object model (DOM) 
structure which can then be queried by scripts which can in turn generate derived outputs 
based on the EDS information.

This DOM is conceptually similar to the Javascript DOM of HTML documents in a web browser,
but very different in terms of usage and implementation as it represents a very different
type of document.

Scripts currently exist for generating C header files and runtime data structures to work
with the accompanying EdsLib runtime library.  However, applications can supply additional
scripts and generate custom outputs from the same DOM.

## Components

The following subdirectories are contained within this source tree:

- `tools` contains the build tool to read EDS files, generate the DOM tree, and run scripts
- `edslib` contains the runtime C library for dealing with EDS-defined objects
- `doc` contains additional information about the DOM structure
- `cfecfs` contains additional bindings/libraries for use with Core Flight System

A separate CMake script is included in each subdirectory for building each component.

## Execution

Once built, the tool is executed by supplying a set of XML files and processing
scripts on the command line, such as:

```
$ sedstool MyEDSFile1.xml MyEDSFile2.xml MyScript1.lua MyScript2.lua ...
```

A few command line options are recognized:

- `-v` : Increase verbosity level.  Use twice for full debug trace.
- `-D NAME=VALUE` : Sets the symbolic NAME to VALUE for preprocessor substitutions


However, this tool is generally _not_ intended to be executed manually in a standalone
installation, but built and executed as part of a larger application build system, such
as Core Flight System (cFS).

The tool will first read _all_ the supplied XML files and build a DOM tree, and then it
will invoke each Lua script _in alphanumeric order_ of filename.  Ordering is
very important, as each script can build upon the results of the prior scripts.  To preserve
the intended order of operations, each script supplied with this tool contains a two digit
numeric prefix, which indicates the correct position in the set.  This way, when scripts are
executed in a simple alphanumeric order, and this will always produce the correct result.
Furthermore, additional scripts can be added into the sequence simply by choosing an appropriate
prefix number, without needing to specify explicit dependencies or complicated rules.
"
145,nasa/cFS-EDS-GroundStation,Python,"# cFS-EDS-GroundStation

This is a Python based ground station that interfaces with an Electronic Data Sheets supported Core Flight Systems instance.  
Also, there are several utility python scripts that provide basic Telemetry and Telecommand functionality in a non-GUI format.

The user's manual can be found at docs/cFS-EDS-GroundStation Users Manual.docx

## Prerequisistes

  -python3-dev
  -python3-pyqt5

## Configuring and running

It is recommended to use this software in conjunction with the cfe-eds-framework repository which can be found at:
https://github.com/jphickey/cfe-eds-framework
This software package requries the EdsLib and CFE_MissionLib python modules from that repository to run.  The build process
also automatically configues these files with the defined mission name.

##

First, the following cmake variables need to be turned on for both the GroundStation or assorted utilities to work.

EDSLIB_PYTHON_BUILD_STANDALONE_MODULE:BOOL=ON
CFE_MISSIONLIB_PYTHON_BUILD_STANDALONE_MODULE:BOOL=ON
CONFIGURE_CFS_EDS_GROUNDSTATION:BOOL=ON

The first two options will compile the python bindings for EdsLib and CFE_MissionLib respectively.
The third configures the python scripts in this folder with the mission name defined in the cFS build process
and output the scripts to the ${CFS_HOME}/build/exe/host/cFS-EDS-GroundStation/ folder.

The folder where the python modules get installed is:
${CFS_HOME}/build/exe/lib/python

This folder needs to be added to the PYTHON_PATH environment variable so the modules can be imported into Python.
The folder that contains EdsLib and CFE_MissionLib .so files also needs to be added to the LD_LIBRARY_PATH
enviroment variable so Python can load these libraries.  For example the following lines can be added to ~/.bashrc

CFS_HOME = /path/to/cfs/directory
export PYTHONPATH=$PYTHONPATH:$CFS_HOME/build/exe/lib/python
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CFS_HOME/build/exe/lib

With this set the cFS-EDS-GroundStation can be run with the following command

python3 cFS-EDS-GroundStation.py

# Utility python scripts

python3 cmdUtil.py

This script runs through several prompts to send a command to a core flight instance.  The user inputs the instance name,
topic, subcommand (if applicable), payload values (if applicable), and destination IP address.  The script will create, 
fill, pack, and send a command to the desired IP address.

python3 tlm_decode.py -p <port=5021>

This script will listen in on the specified port for telemetry messages.  When messages come in they are decoded
and the contents are displayed on the screen.

python3 convert_tlm_file.py -f <filename>       or
python3 convert_tlm_file.py --file=<filename>   (recommended as this allows tab completion of file names)

This script takes the binary telementry files from the Telemetry System of the cFS-EDS-GroundStation, reads through all
of the messages in the file, and writes the decoded packet information in a csv fie of the same base name.
"
146,nasa/Kodiak,C++,"![](logo/Kodiak.jpeg """")

*Kodiak*, a C++ Library for Rigorous Branch and Bound Computation
=================================================================

*Kodiak* is a C++ library that implements a generic branch and bound
algorithm for rigorous numerical approximations. Particular instances
of the branch and bound algorithm allow the user to refine and
isolate solutions to systems of nonlinear equations and inequalities,
global optimization problems, and bifurcation sets for systems of
ODEs. Kodiak utilizes interval arithmetic (via the *filib++* library)
and Bernstein enclosure (for polynomials and rational functions) as
self-validating enclosure methods. Symbolic operations support
procedures such as automatic partial differentiation.

### Authors

Cesar A. Munoz (cesar.a.munoz@nasa.gov), NASA Langley Research Center

Marco A. Feliu (marco.feliu@nianet.org), National Institute of Aerospace

### Other Contributors

Andrew P. Smith, formerly at National Institute of Aerospace, US.

Anthony Narkawicz, formerly at NASA Langley Research Center, US.

Mantas Markevicius, formerly at University of York, UK.

### Documentation

Currently, the main documentation is contained in this file.
Publications concerning the library and associated algorithms are in
preparation. There are also numerous comments in the source code.

### Support and Contribution

See the instructions in this file and the linked resources.
For further assistance, or to report problems and bugs, please
contact the authors. Contributions to the library are welcomed.

Obtaining *Kodiak*
------------------

The repository is located at:
[https://github.com/nasa/Kodiak/](https://github.com/nasa/Kodiak/)

License
-------

The Kodiak C++ Library is released under NASA's Open Source Agreement.
See the files `LICENSES/Kodiak-NOSA.pdf` and `LICENSES/Kodiak-BooleanChecker-NOSA.pdf`;
see also the copyright notice at the end of this file.

Installation and Usage Options
------------------------------

*Kodiak* is installed from source code.
*Kodiak* is run by encoding a problem in a C++ program file, then compiling and running it.

### 2. Prerequisites

It is recommended to run *Kodiak* on a Linux or Mac computer with the
GNU C++ compiler; so far it has been successfully tested on Ubuntu Linux
and Mac OS X. Use of Windows is not supported, although it ought to be
feasible, and the authors would welcome any report of successfully
running the software on Windows or any other system.

The following software should firstly be installed, if not already
present (please follow the links for instructions and support):

* *CMake* build tool (required):
  [https://cmake.org](https://cmake.org)

* *Boost* libraries (required):  
  [http://www.boost.org/users/download/](http://www.boost.org/users/download/)  
  In addition to the headers, you need at least the library
  `serialization`. This library can be installed using 
  `
  ./bootstrap.sh --with-libraries=serialization
  `
  and then `sudo ./b2 install`. Finally, you need to define the environment
  variable `BOOST_ROOT` to point to the directory where Boost's
  `include` and `lib` directories were installed, e.g., `/usr/local`.

* *filib++* interval library (required):
  [http://www2.math.uni-wuppertal.de/wrswt/software/filib.html](http://www2.math.uni-wuppertal.de/wrswt/software/filib.html)
  This library should be configured with the following options before making (`make` command)
  and installing it (maybe `sudo` will be needed prepended to the `make install` command in order to
  install the files in `/usr/local`):
  `
  ./configure CFLAGS=-fPIC CPPFLAGS=-fPIC CXXFLAGS=-fPIC
  `

### 2. Build Library and Examples

If necessary, unzip the zip file in order to extract the files.
The following files and directories should be present:

* In the working directory: this `README.md` file and a `CMakeListst.txt` file
* `LICENSES`: licenses and copyrights for *Kodiak*
* `logo`: *Kodiak*'s logo and credits
* `src`: source code for the library
* `examples`: example C++ files  (`.cpp`) containing several problems

If any of the prerequisite libraries were installed in non-standard
directories, then the file `CMakeLists.txt` should be modified accordingly.

Create a `build` directory to keep the compiled libraries and make it
the current working directory:
```
$ mkdir build
$ cd build
```

Now, run *CMake* for creating the build scripts:
```
$ cmake ..
```

Finally, build all targets by invoking the *CMake* build command:
```
$ cmake --build .
```

Using the Library
-----------------

The *Kodiak* library is used in your own C/C++ programs.
A good way to start is to take one of the existing `.cpp`
files in the `examples` directory and adapt it to your purposes.
You can either invoke the compiler directly with a link to the
*filib++* and *Kodiak* libraries, or else add a new entry to
the `examples/CMakeLists.txt` file.
For C programs, please use the `src/Adapters/Codiak.h` header file to invoke *Kodiak*'s routines.

Be aware that care must be taken with the order in which commands
are invoked. All variables should be declared before any variable
resolutions are set.

## Version

*Kodiak* ver. 2.0.2,  June 2020

## Logo
The Kodiak logo was designed by 
[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC) (NASA).

## License and Copyright Notice

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES).

<pre>

Notices:

Copyright 2017 United States Government as represented by the
   Administrator of the National Aeronautics and Space Administration.
   All Rights Reserved.

Disclaimers:

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY
WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,
INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE
WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO
THE SUBJECT SOFTWARE.  THIS AGREEMENT DOES NOT, IN ANY MANNER,
CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT
OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY
OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.
FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS
AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND
SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF
THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,
EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM
PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT
SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE
REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL
TERMINATION OF THIS AGREEMENT.

</pre>
"
147,nasa/gFTL,Fortran,"# The problem

Fortran only provides one type of container: *array*.  While Fortran
arrays are exemplary for their intended purpose such as numerical
algorinthms, they are poorly suited in many other contexts.  Arrays
can be thought of as a particular case of a ""container"" that holds
multiple entities.  As a container, arrays are well suited for random
access to a fixed number of objects.  (Yes, Fortran arrays are
technically dynamic, but ""growing"" an array involves multiple steps.)


Many other languages provide additional types of containers that
commonly arise in many contexts.  E.g., a vector (C++ STL) or List
(Java) are _growable_ containers of objects that automatically resize
when required to add a new object.  Another example is that of Map
which allows stores objects as key-value pairs, thereby allowing
retrieval of an object by providing it's key.


# The solution

This package, gFTL, provides a mechanism to easily create robust
containers and associated iterators which can be used within Fortran
applications.  The primary methods are intended to be as close to
their C++ STL analogs as possible.  We have found that these
containers are a powerful productivity multiplier for certain types of
software development, and hope that others find them to be just as useful.

Currently, the following three types of containers are provided.
* Vector (list)
* Set
* Map  (associated array)

Contributions of additional containers are very much welcomed.

## Initial developers

* Tom Clune
* Doron Feldman

# Related package

It is worth noting that there is a similar package
[FTL](https://github.com/robertrueger/ftl) which may be of interest.
gFTL was developed independently of FTL, but was not open-sourced in
time to claim the cooler name.


## Quick overview of gFTL vs FTL

I expect this section to grow a bit more after the authors of the two
packages have had time to discuss.  It is highly desired that this
section be factually correct.

### Similarities

* Both packages use the preprocessor that is built-in to essentially
  all modern Fortran compilers.

### Differences

* Naming conventions for gFTL are much closer to C++ STL.



# Request support

If you have any questions, please contact:

* Tom Clune  (Tom.Clune@nasa.gov)


"
148,nasa/HK,C,"core Flight System (cFS) Housekeeping Application (HK)
======================================================

Open Source Release Readme
==========================

HK Release 2.4.3

Date: 10/8/2020

Introduction
-------------
  The Housekeeping application (HK) is a core Flight System (cFS) application
  that is a plug in to the Core Flight Executive (cFE) component of the cFS.

  The HK application is used for building and sending combined telemetry messages
  (from individual system applications) to the software bus for routing. Combining
  messages is performed in order to minimize downlink telemetry bandwidth.
  Combining certain data from multiple messages into one message eliminates the
  message headers that would be required if each message was sent individually.
  Combined messages are also useful for organizing certain types of data. This
  application may be used for data types other than housekeeping telemetry. HK
  provides the capability to generate multiple combined packets (a.k.a. output
  packets) so that data can be sent at different rates (e.g. a fast, medium and
  slow packet).

  The HK application is written in C and depends on the cFS Operating System
  Abstraction Layer (OSAL) and cFE components.  There is additional HK application
  specific configuration information contained in the application user's guide
  available in https://github.com/nasa/HK/tree/master/docs/users_guide

  This software is licensed under the Apache 2.0 license. 


Software Included
------------------

  Housekeeping application (HK) 2.4.3


Software Required
------------------

 Operating System Abstraction Layer 5.0 or higher can be
 obtained at https://github.com/nasa/osal

 core Flight Executive 6.8.0 or higher can be obtained at
 https://github.com/nasa/cfe

 Note: An integrated bundle including the cFE, OSAL, and PSP can
 be obtained at https://github.com/nasa/cfs

About cFS
----------
  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.
  This framework is used as the basis for the flight software for satellite data
  systems and instruments, but can be used on other embedded systems.  More
  information on the cFS can be found at http://cfs.gsfc.nasa.gov

EOF
"
149,nasa/HS,C,"core Flight System (cFS) Health and Safety Application (HS) 
===========================================================

Open Source Release Readme
==========================

HS Release 2.3.2

Date: 
10/5/2020

Introduction
-------------
  The Health and Safety application (HS) is a core Flight System (cFS) 
  application that is a plug in to the Core Flight Executive (cFE) component 
  of the cFS.  
  
  The HS application provides functionality for Application Monitoring, 
  Event Monitoring, Hardware Watchdog Servicing, Execution Counter Reporting
  (optional), and CPU Aliveness Indication (via UART). 

  The HS application is written in C and depends on the cFS Operating System 
  Abstraction Layer (OSAL) and cFE components. There is additional HS 
  application specific configuration information contained in the application 
  user's guide available in: 
  https://github.com/nasa/HS/tree/master/docs/users_guide
  
  This software is licensed under the Apache 2.0 license.  
 
 
Software Included
------------------
  Health and Safety application (HS) 2.3.2
  
 
Software Required
------------------

 Operating System Abstraction Layer 5.0 or higher can be 
 obtained at https://github.com/nasa/osal

 core Flight Executive 6.8.0 or higher can be obtained at
 https://github.com/nasa/cfe

 Note: An integrated bundle including the cFE, OSAL, and PSP can 
 be obtained at https://github.com/nasa/cfs
  
About cFS
-----------
  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.
  This framework is used as the basis for the flight software for satellite data
  systems and instruments, but can be used on other embedded systems.  More
  information on the cFS can be found at http://cfs.gsfc.nasa.gov
                      
EOF                       
"
150,nasa/DS,C,"core Flight System (cFS) Data Storage Application (DS)
======================================================

Open Source Release Readme
==========================

DS Release 2.5.2

Date: 10/5/2020

Introduction
-------------
  The Data Storage application (DS) is a core Flight System (cFS) application 
  that is a plug in to the Core Flight Executive (cFE) component of the cFS.  
  The DS application is used for storing software bus messages in files. These 
  files are generally stored on a storage device such as a solid state recorder 
  but they could be stored on any file system. Another cFS application such as 
  CFDP (CF) must be used in order to transfer the files created by DS from 
  their onboard storage location to where they will be viewed and processed.

  The DS application is written in C and depends on the cFS Operating System
  Abstraction Layer (OSAL) and cFE components.  There is additional DS application
  specific configuration information contained in the application user's guide
  available in https://github.com/nasa/DS/tree/master/docs/users_guide

  This software is licensed under the Apache 2.0 license.


Software Included
------------------

  Data Storage application (DS) 2.5.2


Software Required
------------------

 Operating System Abstraction Layer 5.0 or higher can be
 obtained at https://github.com/nasa/osal

 core Flight Executive 6.8.0 or higher can be obtained at
 https://github.com/nasa/cfe

 Note: An integrated bundle including the cFE, OSAL, and PSP can
 be obtained at https://github.com/nasa/cfs

About cFS
----------
  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.
  This framework is used as the basis for the flight software for satellite data
  systems and instruments, but can be used on other embedded systems.  More
  information on the cFS can be found at http://cfs.gsfc.nasa.gov

EOF
"
151,nasa/MXMCPy,Python,"# MXMCPy
main: 
[![Build Status](https://travis-ci.com/nasa/MXMCPy.svg?branch=main)](https://travis-ci.com/nasa/MXMCPy) 
[![Coverage Status](https://coveralls.io/repos/github/nasa/MXMCPy/badge.svg?branch=main)](https://coveralls.io/github/nasa/MXMCPy?branch=main) 
[![Total alerts](https://img.shields.io/lgtm/alerts/g/nasa/MXMCPy.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/MXMCPy/alerts/)
[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/nasa/MXMCPy.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/MXMCPy/context:python)

develop: 
[![Build Status](https://travis-ci.com/nasa/MXMCPy.svg?branch=develop)](https://travis-ci.com/nasa/MXMCPy) 
[![Coverage Status](https://coveralls.io/repos/github/nasa/MXMCPy/badge.svg?branch=develop)](https://coveralls.io/github/nasa/MXMCPy?branch=develop) 

## General
MXMCPy is an open source package that implements many existing multi-model 
Monte Carlo methods (MLMC, MFMC, ACV) for estimating statistics from expensive,
high-fidelity models by leveraging faster, low-fidelity models for speedup.

## Getting Started

### Installation

MXMCPy can be easily installed using pip:
```shell
pip install mxmcpy
```

Alternatively, the MXMCPy repository can be cloned:
```shell
git clone https://github.com/nasa/mxmcpy.git
```
and the dependencies can be installed manually as follows. 

### Dependencies
MXMCPy is intended for use with Python 3.x.  MXMCPy requires installation of a 
few dependencies which are relatively common for optimization/numerical methods
with Python:
  - numpy
  - scipy
  - pandas
  - matplotlib
  - h5py
  - pytorch
  - pytest, pytest-mock (if the testing suite is to be run)
  
A `requirements.txt` file is included for easy installation of dependencies with 
`pip` or `conda`.

Installation with pip:
```shell
pip install -r requirements.txt
```

Installation with conda:
```shell
conda install --yes --file requirements.txt
```

### Documentation
See the [MXMCPy Read the Docs page](https://mxmcpy.readthedocs.io/).

## Running Tests
An extensive unit test suite is included with MXMCPy to help ensure proper 
installation. The tests can be run using pytest on the tests directory, e.g., 
by running:
```shell
python -m pytest tests 
```
from the root directory of the repository.

## Example Usage

The following code snippet shows the determination of an optimal sample
allocation for three models with assumed costs and covariance matrix using
the MFMC algorithm:

```python
import numpy as np
from mxmc import Optimizer

model_costs = np.array([1.0, 0.05, 0.001])
covariance_matrix = np.array([[11.531, 11.523, 12.304],
                              [11.523, 11.518, 12.350],
                              [12.304, 12.350, 14.333]])
                             
optimizer = Optimizer(model_costs, covariance_matrix)
opt_result = optimizer.optimize(algorithm=""mfmc"", target_cost=1000)

print(""Optimal variance: "", opt_result.variance)
print(""# samples per model: "", opt_result.allocation.get_number_of_samples_per_model())
```

For more detailed examples using MXMCPy including end-to-end construction of
estimators, see the scripts in the [examples directory](examples/) or the 
[end-to-end example](https://mxmcpy.readthedocs.io/en/main/ishigami_example.html)
in the documentation. 

## Contributing
1. Fork it (<https://github.com/nasa/mxmcpy/fork>)
2. Create your feature branch (`git checkout -b feature/fooBar`)
3. Commit your changes (`git commit -am 'Add some fooBar'`)
4. Push to the branch (`git push origin feature/fooBar`)
5. Create a new Pull Request

## Citing 

If you use MXMCPy for your work, please cite the following reference:

`
Bomarito, G. F., Warner, J. E., Leser, P. E., Leser, W. P., and Morrill, L.: Multi Model Monte Carlo with Python (MXMCPy). NASA/TM‚Äì2020‚Äì220585. 2020.
`

## Authors
  * Geoffrey Bomarito
  * James Warner
  * Patrick Leser
  * William Leser
  * Luke Morrill
  
## License 

Notices:
Copyright 2020 United States Government as represented by the Administrator of 
the National Aeronautics and Space Administration. No copyright is claimed in 
the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF 
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED 
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY 
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR 
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR 
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE 
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN 
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, 
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS 
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY 
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, 
IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE 
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY 
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY 
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, 
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S 
USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE 
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY 
PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR 
ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS 
AGREEMENT.


"
152,nasa/PrognosticsAlgorithmLibrary,MATLAB,"# Prognostics Algorithm Library

The Prognostics Algorithm Library is a suite of algorithms implemented in the MATLAB programming language for model-based prognostics (remaining life computation). It includes algorithms for state estimation and prediction, including uncertainty propagation. The algorithms take as inputs component models developed in Matlab, and perform estimation and prediction functions. The library allows the rapid development of prognostics solutions for given models of components and systems. Different algorithms can be easily swapped to do comparative studies and evaluations of different algorithms to select the best for the application at hand.

## Citation

Publications making use of software products obtained from this repository are requested to acknowledge the assistance received by using this repository. Please cite: ""M. Daigle; Prognostics Algorithm Library [Computer software]. (2016). Retrieved from https://github.com/nasa/PrognosticsAlgorithmLibrary"".

## Installation

Installation can be done in one of two ways. Either (1) use the MATLAB toolbox installer provided in the install folder, which will install the toolbox in your local toolboxes folder and add the folder to your MATLAB path, or (2) copy the source from the MATLAB folder to any desired directory, and add that directory to your MATLAB path. Do not add the subdirectories (the package directories) to your MATLAB path. If the first option is used, then the MATLAB add-on manager can be used to uninstall the package; otherwise, the installation can be removed manually by removing the directory from your MATLAB path and deleting the source.

## User Manual

Instructions for using the software can be found in the following sources:
- [Wiki](https://github.com/nasa/PrognosticsAlgorithmLibrary/wiki)
- [User Manual](https://github.com/nasa/PrognosticsAlgorithmLibrary/blob/master/docs/PrognosticsAlgorithmLibrary-UserManual.pdf)

## Dependencies

Some modules included in this library are dependent on the [Prognostics Model Library](https://github.com/nasa/PrognosticsModelLibrary).

## Compatibility

The PrognosticsModelLibrary has been tested with Matlab R2016a, but should work with older versions, down to at least R2012a.

## Contributions

All contributions are welcome. Issues may be opened using GitHub. To contribute directly, open a pull request against the ""develop"" branch. Pull requests will be evaluated and integrated into the next official release.

## License

This software is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/PrognosticsAlgorithmLibrary/blob/master/LICENSE.pdf).

## Notices

Copyright ¬©¬†2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.¬† No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

### Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED¬†""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.¬† FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT¬†""AS IS.""

Waiver and Indemnity:¬† RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.¬† IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.¬† RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

"
153,nasa/PrognosticsModelLibrary,MATLAB,"# Prognostics Model Library

The Prognostics Model Library is a modeling framework focused on defining and building models for prognostics (computation of remaining useful life) of engineering systems, and provides a set of prognostics models for select components developed within this framework, suitable for use in prognostics applications for these components. The library currently includes models for valves, pumps, and batteries. The Prognostics Model Library is implemented in MATLAB. The implementation consists of a set of utilities for defining a model (specifying variables, parameters, and equations), simulating the model, and embedding it within common model-based prognostics algorithms. A user can use existing models within the library or construct new models with the provided framework.

## Citation

Publications making use of software products obtained from this repository are requested to acknowledge the assistance received by using this repository. Please cite: ""M. Daigle; Prognostics Model Library [Computer software]. (2016). Retrieved from https://github.com/nasa/PrognosticsModelLibrary"".

## Installation

Installation can be done in one of two ways. Either (1) use the MATLAB toolbox installer provided in the install folder, which will install the toolbox in your local toolboxes folder and add the folder to your MATLAB path, or (2) copy the source from the MATLAB folder to any desired directory, and add that directory to your MATLAB path. Do not add the subdirectories (the package directories) to your MATLAB path. If the first option is used, then the MATLAB add-on manager can be used to uninstall the package; otherwise, the installation can be removed manually by removing the directory from your MATLAB path and deleting the source.

## User Manual

Instructions for using the software can be found in the following sources:
- [Wiki](https://github.com/nasa/PrognosticsModelLibrary/wiki)
- [User Manual](https://github.com/nasa/PrognosticsModelLibrary/blob/master/docs/PrognosticsModelLibrary-UserManual.pdf)

## Compatibility

The PrognosticsModelLibrary has been tested with Matlab R2016a, but should work with older versions, down to at least R2012a.

## Contributions

All contributions are welcome. Issues may be opened using GitHub. To contribute directly, open a pull request against the ""develop"" branch. Pull requests will be evaluated and integrated into the next official release.

## License

This software is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/PrognosticsModelLibrary/blob/master/LICENSE.pdf).

## Notices

Copyright ¬©¬†2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.¬† No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

### Disclaimers‚Ä®‚Ä®

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED¬†""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.¬† FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT¬†""AS IS.""

Waiver and Indemnity:¬† RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.¬† IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.¬† RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

"
154,nasa/spaceapps-phenomena_detection,Jupyter Notebook,"# Phenomena Detection Challenge Resources

This repository provides a list of resources for the phenomena detection challenge, including satellite imagery, labeled data, labeling tools, and example code for imagery downloading, processing, and machine learning. <br>

Selected pre-labeled data is available in the [labeled folder](data/labeled/), or participants may choose to use the resources in the [raw folder](data/raw/) to access a broader range of data and label it themselves or with our included tool ImageLabler. <br>

The [examples](examples/) folder contains example scripts for machine learning, as well as downloading satellite imagery.<br>

Best of luck with the challenge! If you need clarification on challenge details please see contact information in the [contacts](contacts/) folder.
"
155,nasa/World-Wind-Java,C++,"$Id$

This file explains the organization of the World Wind Subversion repository's trunk directories, and briefly outlines their contents.

trunk/WorldWind
The 'WorldWind' folder contains the World Wind Java SDK project. Many resources are available at http://oneworldwind.org to help you understand and use World Wind. Key files and folders in the World Wind Java SDK:
- build.xml: Apache ANT build file for the World Wind Java SDK.
- src: Contains all Java source files for the World Wind Java SDK, except the World Wind WMS Server.
- server: Contains the World Wind WMS Server Java source files, build file, and deployment files.
- lib-external/gdal: Contains the GDAL native binaries libraries that may optionally be distributed with World Wind.

trunk/WWAndroid
the 'WWAndroid' folder contains the World Wind Android SDK project. Many resource are available at http://oneworldwind.org/android to help you understand and use World Wind on Android. Key files and folders in the World Wind Android SDK:
- build.xml: Apache ANT build file for the World Wind Android SDK.
- src: Contains all Java source files for the World Wind Android SDK.
- examples: Contains example applications that use the World Wind Android SDK.

trunk/GDAL
The 'GDAL' folder contains the GDAL native library project. This project produces the GDAL native libraries used by the World Wind Java SDK (see WorldWind/lib-external/gdal). The GDAL native library project contains detailed instructions for building the GDAL native libraries on the three supported platforms: Linux, Mac OS X, and Windows.
"
156,nasa/icc,MATLAB,"# icc-dev

Contains MATLAB simulation and optimization code for spacecraft orbiting small Solar System Bodies.

## Quickstart

The examples folder contains example _main_ scripts, which demonstrate usage of the provided modules. The scripts should be executed from within this folder.

___

## Requirements

- [Mosek](https://www.mosek.com/) or [CPLEX](https://www.ibm.com/analytics/cplex-optimizer). Both Mosek and CPLEX provide free academic versions. If neither MOSEK nor CPLEX are available, it is possible to fall back to MATLAB's built-in `linprog` and `intlinprog`, but performance will be compromised.
- JPL's [Small Body Dynamics Toolkit (SBDT)](https://engineering.purdue.edu/people/kathleen.howell.1/Publications/Conferences/2015_AAS_SBDT.pdf). SBDT is not open-source. A license can be requested from [download.jpl.nasa.gov](download.jpl.nasa.gov) mentioning NTR-49005. 
- NASA's [SPICE](https://naif.jpl.nasa.gov/naif/aboutspice.html) MATLAB toolkit (MICE), available at [this link](https://naif.jpl.nasa.gov/naif/toolkit_MATLAB.html).
- In order to automatically download SPICE data, the [Expect/TCL](https://en.wikipedia.org/wiki/Expect) scripting language should be available. On Debian-based systems, you can `sudo apt-get install expect`. On MacOS, `brew install expect` with [Homebrew](https://brew.sh/). On Windows, either use the Windows Subsystem for Linux, or manually download SPICE data as discussed below..

### Setup

Set the following environment variables:

- `SBDT_PATH` should point to the folder where SBDT is installed.
- `NAIF_PATH` should point to the folder where NAIF data will be stored. This is typically the subfolder `utilities/spice` inside this repository.

Run the script `utilities/spice/download_SPICE_kernel.sh` to download required data from NASA's SPICE orbit database. If Expect/TCL is unavailable, manually download the orbit data for 433 EROS from [Horizons](https://ssd.jpl.nasa.gov/horizons.cgi) and save it as `${NAIF_PATH}/naif/generic_kernels/spk/asteroids/a433.bsp`.

To set environment variables:

- On Linux-based systems, add the line `export SBDT_PATH=""path/to/your/copy/of/sbdt""` and `export NAIF_PATH=""path/to/your/copy/of/naif""` to `~/.bashrc`. If you use a different shell (e.g. zsh), add the lines to the appropriate `.*rc` file.
- On MacOS, follow [these instructions](https://www.mathworks.com/matlabcentral/answers/170268-how-do-i-set-environment-variables-on-mac-os-x#answer_165094).
  > **WARNING** On MacOS, setting environment variables in `~/.bashrc` or `~/.zshrc` will _not_ affect MATLAB unless MATLAB is launched from the terminal.
- On Windows, follow [these instructions](https://www.architectryan.com/2018/08/31/how-to-change-environment-variables-on-windows-10/).

Apply the patches in `utilities/sbdt` to the files `harmonic_gravity.c` and `harmonic_gravity_complex_3.cpp` in SBDT and recompile the corresponding MEX files.
___

## Modules

### small_body_dynamics

Integrators to simulate the orbital dynamics of a spacecraft in orbit around a small body.

### network_flow_communication_optimizer

Optimize the communication flows between spacecraft, given a set of orbits, data production, and a communication model.

### relay_orbit_optimizer

Optimize the orbit of relay spacecraft, given the orbits of science spacecraft, their data production, and the orbit of the carrier.

### observed_points_optimizer

Optimize the observation of spacecraft, given the orbits of science spacecraft.

### monte_carlo_coverage_optimizer

Runs a monte carlo to select spacecraft orbits. Calls the observed points optimizer to calculate the coverage reward of each trial orbit set. 

### utilities

Common utilities and supporting functions for icc simulation and analysis.

### media

Various media.

### Structure of Optimization Modules

The optimizer modules are composed of a _main_ function (is called outside the module) and several supporting functions which should only be called from within the module. Functions supporting multiple modules are placed in _utilities_.

___

## Interfaces

The system is defined through two special classes, which serve as the primary interfaces (inputs and outputs) between the modules. 

1. _SpacecraftSwarm_: Defines the swarm of spacecraft.
2. _SphericalHarmonicsGravityIntegrator_SBDT_: Defines the small body that the spacecraft orbit around and integrates trajectories using JPL's Small Body Dynamics Integrator (SBDT).

See the _examples_ directory for usage examples.

___

## Troubleshooting

### Troubleshooting SBDT

SBDT is typically distributed as a `.zip` file. The archive contains tests in the `Tests` folder and examples in the `Demos` folder.

Core SBDT functions are implemented in C and must be compiled to MATLAB _mex_ files to enable MATLAB to call them. The SBDT distribution contains pre-compiled MEX files for Windows, MacOS, and Linux inside the folder `CompiledMEX`.
If you encounter issues trying to run SBDT, it may be advisable to recompile the MEX files from scratch. To do this,

- Navigate to the SBDT root folder.
- In MATLAB, run `compileMexSBDT()`. A number of files will be generated in the folder. The file extension is `mexw64` on Windows, `mexa64` on Linux, and `mexmaci64` on MacOS.
- Copy the newly generated mex files to the CompiledMEX folder, overwriting the previous ones.

### Missing function `setstructfields`

SBDT internally makes use of the MATLAB function `setstructfields`, which
is provided by the Signal Processing toolbox.
If you do not have access to the Signal Processing toolbox, the repository
contains [a clean-room reimplementation](utilities/misc/setstructfields_ICC.m)
of the function (based on [this description](https://stackoverflow.com/questions/38645/what-are-some-efficient-ways-to-combine-two-structures-in-matlab/23767610#23767610)).
To use it, rename the file from `setstructfields_ICC.m` to `setstructfields.m`
and ensure that it is in a folder on your MATLAB PATH.

### Warning: P-file is older than M-file

Depending on the SBDT distribution in use, you may receive the warning

```matlab
Warning: P-file $PATH/TO/FILE.p is older than M-file $PATH/TO/FILE.m.
$PATH/TO/FILE.p may be obsolete and may need to be regenerated.
Type ""help pcode"" for information about generating P-files from M-files.

```

You can get rid of this warning by navigating to the SBDT folder in the terminal and running

```bash
find . -type f -name ""*.p"" -exec touch {} +
```

This will reset the last-modified time of all .p files to the current date and time.

### Detailed instructions on how to set environment variables

#### MacOS

- Create a file named `environment_variables.plist` in `~/Library/LaunchAgents/` (the file name is immaterial, but the extension should be `.plist`).

- Edit the file to contain the following text:
```xml
<?xml version=""1.0"" encoding=""UTF-8""?>
 <!DOCTYPE plist PUBLIC ""-//Apple//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd"">
   <plist version=""1.0"">
   <dict>
   <key>Label</key>
   <string>Set environment variables that will be picked up by Spotlight</string>
   <key>ProgramArguments</key>
   <array>
     <string>/bin/launchctl</string>
     <string>setenv</string>
     <string>SBDT_PATH</string>
     <string>/Users/frossi/Documents/JPL/ICC/SBDT</string>
     <string>NAIF_PATH</string>
     <string>/Users/frossi/Documents/JPL/ICC/icc-dev/utilities/spice/</string>
   </array>
   <key>RunAtLoad</key>
   <true/>
 </dict>
 </plist>
```
  Ensure that the SBDT and NAIF paths are set to the proper path on your machine.
- Run 
```
launchctl load ~/Library/LaunchAgents/environment_variables.plist
launchctl start ~/Library/LaunchAgents/environment_variables.plist
```"
157,nasa/bingocpp,C++,"# BingoCpp #

![Bingo Logo](media/logo.png)

master: [![Build Status](https://travis-ci.com/nasa/bingocpp.svg?branch=master)](https://travis-ci.com/nasa/bingocpp) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingocpp/badge.svg?branch=master)](https://coveralls.io/github/nasa/bingocpp?branch=master)

develop: [![Build Status](https://travis-ci.com/nasa/bingocpp.svg?branch=develop)](https://travis-ci.com/nasa/bingocpp) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingocpp/badge.svg?branch=develop)](https://coveralls.io/github/nasa/bingocpp?branch=develop) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/ccd11c4092544eaca355722cea87272e)](https://www.codacy.com/app/bingo_developers/bingocpp?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=nasa/bingocpp&amp;utm_campaign=Badge_Grade)

## General ##

BingoCpp is part of the open source package Bingo for performing symbolic
regression.  BingoCpp contains the c++ implementation of a portion of the code
within bingo. 

## Getting Started ##

### Cloning ###

BingoCpp has 3 submodules: eigen, google test, and pybind.  To clone this
repository and include the submodules, run the following command:

```bash
git clone --recurse-submodules https://github.com/nasa/bingocpp
```

### Installation/Compiling with CMake ###

Installing from source requires git and a recent version of
[cmake](https://cmake.org/).

Installation can be performed using the typical out-of-source build flow:

```bash
mkdir <path_to_source_dir>/build
cd <path_to_source_dir>/build
cmake .. -DCMAKE_BUILD_TYPE=Release
make
```

### Python Bindings ###

The python bindings that are needed for integration with bingo can be made by
running the following commend from the build directory:

```bash
make bingocpp
```

A common error in the build of the python bindings is that the build must be
use the same version of python that will run your bingo scripts.  Pybind
usually finds the default python on your machine during build, so the easiest
way to ensure consistent python versioning is to build bingocpp in a Python 3
virtual environment.

### Documentation ###

Sphynx is used for automatically generating API documentation for bingo. The
most recent build of the documentation can be found in the repository at:
doc/_build/html/index.htm

## Running Tests ##

Several unit and integration tests can be performed upon building, to ensure a
proper install.  The test suite can be started by running the following command
from the build directory:

```bash
make gtest
```

## Usage Example ##

TODO

## Contributing ##

1. Fork it (<https://github.com/nasa/bingo/fork>)
2. Create your feature branch (`git checkout -b feature/fooBar`)
3. Commit your changes (`git commit -am 'Add some fooBar'`)
4. Push to the branch (`git push origin feature/fooBar`)
5. Create a new Pull Request

## Versioning ##

We use [SemVer](http://semver.org/) for versioning. For the versions available,
see the [tags on this repository](https://github.com/nasa/bingocpp/tags).

## Authors ##

* Geoffrey Bomarito
* Ethan Adams
* Tyler Townsend
  
## License ##

Copyright 2018 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration. No copyright is claimed in
the United States under Title 17, U.S. Code. All Other Rights Reserved.

The Bingo Mini-app framework is licensed under the Apache License, Version 2.0
(the ""License""); you may not use this application except in compliance with the
License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0 .

Unless required by applicable law or agreed to in writing, software distributed 
under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR 
CONDITIONS OF ANY KIND, either express or implied. See the License for the 
specific language governing permissions and limitations under the License.
"
158,nasa/stol-mode,Emacs Lisp,"**stol-mode** - Emacs major mode for Systems Test and Operations Language (STOL)
==========================================================

Written for use with STOL as used in [Integrated Test and Operations System
(ITOS)](https://itos.gsfc.nasa.gov/) 9.4.0.

## Features ##
* Keyword, directive, and constant highlighting

## Goals ##
* Indentation
* Insertion of double semi-colon (;;) when lines automatically break

## Usage ##
See the [Lisp Libraries page in the GNU Emacs
manual](https://www.gnu.org/software/emacs/manual/html_node/emacs/Lisp-Libraries.html) for information on how
to get Emacs to load this library. Once loaded, the `M-x stol-mode` command activates stol-mode.

## Notices

Copyright ¬© 2020 United States Government as represented by the Administrator of the National Aeronautics and
Space Administration.  No copyright is claimed in the United States under Title 17, U.S. Code. All Other
Rights Reserved.

### Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED,
IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO
SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION,
IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE
PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS
CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE
RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES
FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY
AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE
IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
159,nasa/common-mapping-client,JavaScript,"## Welcome to the Common Mapping Client!

[![CircleCI](https://circleci.com/gh/nasa/common-mapping-client.svg?style=shield)](https://circleci.com/gh/nasa/common-mapping-client)
[![Dependencies Status](https://david-dm.org/nasa/common-mapping-client/status.svg)](https://david-dm.org/nasa/common-mapping-client)
[![Coverage Status](https://coveralls.io/repos/github/nasa/common-mapping-client/badge.svg?branch=master)](https://coveralls.io/github/nasa/common-mapping-client?branch=master)
[![license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)


[![Preview](https://raw.githubusercontent.com/nasa/common-mapping-client/master/docs/core-docs/resources/screenshot_core.jpg)](https://nasa.github.io/common-mapping-client/branches/master/)

### Overview
The Common Mapping Client (CMC) is a foundation for web-based mapping applications that
leverages, combines, and abstracts certain commonly used mapping functionalities,
enabling developers to spend less time reinventing the wheel and more time solving
their specific problems. Sitting somewhere between a starter-kit and a framework,
CMC aims fulfill the basic needs of a mapping application without getting in the
way of any given project's unique goals.

Over the years, there have been many projects that try to solve the same issue:
put data on a map and explore it. Unfortunately, there is an increasingly steep
hill to climb when it comes to actually starting one of these projects. All of
them have to decide: Which framework should I start from? Which library
will give me the features I need? How to I glue all these together with 
a code layout that I won't want to burn next week? CMC solves this by bundling
together a solid and modular base framework with robust mapping libraries,
a well thought out structure, and a thick shield against feature-creep
to let you start building the cool stuff faster.

We maintain a list of further example projects and projects that began from CMC
so that you can easily get examples of building intricate and detailed workflows
on top of this tool.

View our [live demo](https://nasa.github.io/common-mapping-client/branches/master/).

### Features
* 2D/3D Map library abstraction
* Map layer controls
* Map layer ingestion and merging (from JSON and WMTS capabilities XML)
* Highly customizable user analytics
* Configurable map projection
* Basic vector layer support
* Map extent synchronization across all maps (2D and 3D)
* Geodesic map geometry synchronization across all maps (2D and 3D)
* Global time widget, and interactive timeslider
* Adjustable map layer cache
* Shareable/loadable application state via url parameters with share widget (facebook, twitter, email, google plus)
* Core set of UI elements necessary for most basic applications
* Basemap switching
* Basic shape drawing tools in 2D and 3D
* Basic geodesic measurement (distance and area) tools in 2D and 3D
* Display help documentation from markdown files
* A preconfigured testing framework
* A preconfigured build process
* Handy development tools
  * Hot reloading
  * Local dev and production node servers
  * BrowserSync

### Quickstart

##### Install
1. Install `NodeJS`
2. Get the code
   1. Option A: Grab a tag that suits you
   2. Option B: Fork the repo into your new project and get the cutting edge goodness
   3. Option C: Clone the repo, contribute back, and get the cutting edge goodness
3. `npm install`: install node dependencies
4. `npm start`: build and server development code bundle
5. Start building.

##### Build
* `npm run build`: build production code bundle
* `npm run open:dist`: serve production code bundle

##### Test
* `npm run test`: Single run of tests
* flags
  * `--includecoretests`: Include cmc-core tests as well
  * `--nowebgl`: Run tests in phantomJS and skip tests that require webgl
  * `--watch`: Run tests with code watch

### Contributing to the Project

*Contributing Directly to CMC-Core*

You are more than welcome to create forks to fix bugs, add features or
enhancements or otherwise improve the tool. Please submit these changes through
pull-requests for review.

*Contributing to the CMC Ecosystem*

If you use CMC for your own project, please let us know so that we may list it
under our [Example Projects](docs/core-docs/EXAMPLE_PROJECTS.md) for others to find inspiration in.

If you create a particularly useful or robust widget in your own CMC descendant,
please create an example project demonstrating just that widget so that others
who have the same problem down the road may find a solution faster.

### Documentation Shortcut

* [Walkthrough](https://github.com/AaronPlave/common-mapping-client-walkthrough)
* [Developer Guide](docs/core-docs/DEVELOPER_MANUAL.md)
* [Example Projects](docs/core-docs/EXAMPLE_PROJECTS.md)
"
160,nasa/ccmc-swpc-cat-web,Python,"# ccmc-swpc-cat-web


ccmc-swpc-cat-web is a new CCMC web implementation of the NOAA's SWPC_CAT IDL program.  

The project page is hosted by the Community Coordinated Modeling Center (CCMC) at NASA Goddard Space Flight Center.

Official site page https://ccmc.gsfc.nasa.gov/swpc_cat_web/

ccmc-swpc-cat-web source code is hosted on github under a permissive NASA open source license:

https://github.com/nasa/ccmc-swpc-cat-web/

## INSTALLATION
ccmc-swpc-cat-web is a plotly-dash based web application, the following steps will start a python simple server:

### prerequisites:
```
python3.7 already installed
virtualenv already installed
``` 

### 1. create venv using python 3.7
```
NOTE: ""/usr/bin/python3.7"" is an example path, you might need to change this path
virtualenv -p /usr/bin/python3.7 venv
```

### 2. install requirements.txt
```
source venv/bin/activate
pip3 install -r requirements.txt 
```

### 3. start server
```
source venv/bin/activate
python __SWPC_CAT__.py
Visit: http://127.0.0.1:8050/ in a web browser (preferably chrome)
"
161,nasa/cumulus-message-adapter-python,Python,"# cumulus-message-adapter-python

[![CircleCI]](https://circleci.com/gh/nasa/cumulus-message-adapter-python)
[![PyPI version]](https://badge.fury.io/py/cumulus-message-adapter-python)

## What is Cumulus?

Cumulus is a cloud-based data ingest, archive, distribution and management
prototype for NASA's future Earth science data streams.

Read the [Cumulus Documentation]

## What is the Cumulus Message Adapter?

The Cumulus Message Adapter is a library that adapts incoming messages in the
Cumulus protocol to a format more easily consumable by Cumulus tasks, invokes
the tasks, and then adapts their response back to the Cumulus message protocol
to be sent to the next task.

## Installation

```plain
pip install cumulus-message-adapter-python
```

## Task definition

In order to use the Cumulus Message Adapter, you will need to create two
methods in your task module: a handler function and a business logic function.

The handler function is a standard Lambda handler function which takes two
parameters (as specified by AWS): `event` and `context`.

The business logic function is where the actual work of your task occurs. It
should take two parameters: `event` and `context`.

The `event` object contains two keys:

* `input` - the task's input, typically the `payload` of the message, produced
  at runtime
* `config` - the task's configuration, with any templated variables resolved

The `context` parameter is the standard Lambda context as passed by AWS.

The return value of the business logic function will be placed in the
`payload` of the resulting Cumulus message.

Expectations for input, config, and return values are all defined by the task,
and should be well documented. Tasks should thoughtfully consider their inputs
and return values, as breaking changes may have cascading effects on tasks
throughout a workflow. Configuration changes are slightly less impactful, but
must be communicated to those using the task.

## Cumulus Message Adapter interface

The Cumulus Message adapter for python provides one method:
`run_cumulus_task`. It takes four parameters:

* `task_function` - the function containing your business logic (as described
  above)
* `cumulus_message` - the event passed by Lambda, and should be a Cumulus
  Message, *or* a CMA parameter encapsulated message (see [Cumulus Workflow
  Documentation](https://nasa.github.io/cumulus/docs/workflows/input_output)):

  ```json
  {
     ""cma"": {
        ""event"": ""<cumulus message object>"",
        ""SomeCMAConfigKey"": ""CMA configuration object>""
     }
  }
  ```

* `context` - the Lambda context
* `schemas` - optional: a dict with `input`, `config`, and `output` properties.
  Each should be a string set to the filepath of the corresponding JSON schema
  file. All three properties of this dict are optional. If ommitted, the message
  adapter will look in `/<task_root>/schemas/<schema_type>.json`, and if not
  found there, will be ignored.
* `taskargs` - Optional. Additional keyword arguments for the `task_function`

## Example

Simple example of using this package's `run_cumulus_task` function as a wrapper
around another function:

```python
from run_cumulus_task import run_cumulus_task

# simple task that returns the event
def task(event, context):
    return event

# handler that is provided to aws lambda
def handler(event, context):
    return run_cumulus_task(task, event, context)
```

For a full example see the [example folder](./example).

## Creating a deployment package

Tasks that use this library are just standard AWS Lambda tasks. See
[creating release packages].

## Usage in a Cumulus Deployment

For documenation on how to utilize this package in a Cumulus Deployment, view
the [Cumulus Workflow Documenation].

## Development

### Dependency Installation

```plain
$ pip install -r requirements-dev.txt
$ pip install -r requirements.txt
```

### Logging with `CumulusLogger`

Included in this package is the `cumulus_logger` which contains a logging class
`CumulusLogger` that standardizes the log format for Cumulus. Methods are
provided to log error, fatal, warning, debug, info, and trace.

**Import the `CumulusLogger` class:**

```python
from cumulus_logger import CumulusLogger
```

**Instantiate the logger inside the task definition (name and level are
optional):**

```python
logger = CumulusLogger(event, context)
```

**Use the logging methods for different levels:**

```python
logger.trace('<your message>')
logger.debug('<your message>')
logger.info('<your message>')
logger.warn('<your message>')
logger.error('<your message>')
logger.fatal('<your message>')
```

**It can also take additional non-keyword and keyword arguments as in Python
Logger.**

The `msg` is the message format string, the `args` and `kwargs` are the
arguments for string formatting.

If `exc_info` in `kwargs` is not `False`, the exception information in the
`exc_info` or `sys.exc_info()` is added to the message.

```python
logger.debug(msg, *args, **kwargs)
```

**Example usage:**

```python
import os
import sys

from run_cumulus_task import run_cumulus_task
from cumulus_logger import CumulusLogger

# instantiate CumulusLogger
logger = CumulusLogger()

def task(event, context):
    logger.info('task executed')

    # log error when an exception is caught
    logger.error(""task formatted message {} exc_info "", ""bar"", exc_info=True)

    # return the output of the task
    return { ""example"": ""output"" }

def handler(event, context):
    # make sure event & context metadata is set in the logger
    logger.setMetadata(event, context)
    return run_cumulus_task(task, event, context)
```

### Running Tests

Running tests requires [localstack](https://github.com/localstack/localstack).

Tests only require localstack running S3, which can be initiated with the
following command:

```plain
$ SERVICES=s3 localstack start
```

And then you can check tests pass with the following nosetests command:

```plain
$ CUMULUS_ENV=testing nosetests -v -s --with-doctest
```

### Linting

```plain
$ pylint run_cumulus_task.py
```

## Why?

This approach has a few major advantages:

1. It explicitly prevents tasks from making assumptions about data structures
   like `meta` and `cumulus_meta` that are owned internally and may therefore
   be broken in future updates. To gain access to fields in these structures,
   tasks must be passed the data explicitly in the workflow configuration.
1. It provides clearer ownership of the various data structures. Operators own
   `meta`. Cumulus owns `cumulus_meta`. Tasks define their own `config`,
   `input`, and `output` formats.
1. The Cumulus Message Adapter greatly simplifies running Lambda functions not
   explicitly created for Cumulus.
1. The approach greatly simplifies testing for tasks, as tasks don't need to
   set up cumbersome structures to emulate the message protocol and can just
   test their business function.

## License

[Apache 2.0](LICENSE)

[circleci]:
  https://circleci.com/gh/nasa/cumulus-message-adapter-python.svg?style=svg
[pypi version]:
  https://badge.fury.io/py/cumulus-message-adapter-python.svg
[Cumulus Documentation]:
  https://nasa.github.io/cumulus/
[creating release packages]:
  https://docs.aws.amazon.com/lambda/latest/dg/deployment-package-v2.html
[cumulus workflow documenation]:
  https://nasa.github.io/cumulus/docs/workflows/input_output
"
162,nasa/ECI,C,"# Overview

The cFS (Core Flight System) ECI (External Code Interface) is a software abstraction layer which allows the interfacing of externally-generated task/mission-specific code to the cFS via a generic set of wrapper code. The ECI enables direct integration of existing or autogenerated code without the need for hand-written interface code and allows access to cFS API's including table services, time services, the software bus, event services, and fault reporting. 

The ECI accomplishes this by compiling with a interface definition (defined as a header file) which contains the data structures needed to manage the CFS interfaces for the external code. The result of compiling the ECI, the interface header file, and the external code is a full CFS application. This process eliminates the need for hand edits to integrate generated code, allowing quicker integration of code and reducing the probability of human error in the integration process.

# Getting Started

The ECI has been used with CFE 6.5 on various missions and is tested against the CFE located [here](https://github.com/nasa/cFE). There are no known incompatibilities with older (or newer) versions of the CFE, but they have not been tested.

# Status

[![build passing](https://travis-ci.com/nasa/ECI.svg?branch=master)](https://travis-ci.com/nasa/ECI/)

# Feedback

Please submit bug reports and feature requests via Github issues. Feature requests will be considered as they align with the requirements of NASA missions which are using the ECI."
163,nasa/PrognosticsMetricsLibrary,MATLAB,"# PrognosticsMetricsLibrary

This library provides MATLAB software that calculates prognostics metrics. The package is titled ‚Äú+PrognosticsMetrics‚Äù, and a standalone MATLAB file ‚ÄúTester.m‚Äù illustrates how to use the package. This tester creates dummy-output of a prognostic algorithm and explains how the software can be used to compute the performance metrics.

## Prognostics
After diagnosing the faults of a system, performing Prognostics is useful for determining the evolution of these faults. Analysis of fault development is critical when predicting the Remaining Useful Life (RUL) of a system and setting criteria for landmarks or thresholds in the future. Whether data-driven or physics-based, models are utilized as representations of a system for Prognostic algorithms to be performed.

## Prognostic Metrics
Prognostic algorithms are implemented in order to determine the Remaining Useful Life (RUL) of a system.  Prognostic Metrics are incorporated into the health management decision making process by analyzing the performance of these algorithms.  Establishing standard methods for performance assessment is essential in uniformly comparing certain aptitudes or measures across several algorithms.  Statistical evaluation methods are often advantageous when dealing with large datasets.

Some sources of error that could be associated with a system include model inaccuracy, data noise, observer faults, etc. Additional considerations when determining which Prognostic Metrics to adopt are logistics, saftey, reliability, mission criticality, and economic viability (estimating cost savings expected from prognostic algorithm deployment).

## Prognostic Framework Terminology
* PDF = Probability Density Function
* UUT = Unit Under Test
* PA = Prognostic Algorithm
* PHM = Prognostic Health Management
* RUL = Remaining Useful Life
* EoL = End of Life
* EoP = End of Prediction, time index of last prediction before EoL
* EoUP = End of Useful Predictions, time index where RUL is no longer useful to update
* HI = Health Index
* PoF = Probability of Failure
* FT = Failure Threshold, UUT is no longer usable
* RtF = Run to Failure, allow system to fail
* ROI = Return On Investment
* F = time index when fault becomes detectable
* D = time index when fault is detected
* P = time index of first prognostic prediction
* PH = Prognostic Horizon


## Inputs to the Package

The present version of metrics considers only the case when the algorithm predictions are described in terms of samples. Different predictions are available at different time instants. Let ‚Äút‚Äù denote the number of such time instants for which the algorithm‚Äôs predictions are available. At each such time instant, ‚ÄúN‚Äù samples of predictions (both EOL ‚Äì End of Life, and RUL ‚Äì Remaining Useful Life) are available. The samples also have weights associated with them; it is important to specify weights (the sum of weights should be equal to 1). If the samples are unweighted, then it is implied that the weight of each sample is 1/N.

To calculate all the possible metrics provided within this software package, a MATLAB structure ‚ÄúprognosisData‚Äù needs to be provided. The elements of this structure include:

1.	prognosisData.time = (1 x t) time vector of prediction time points
2.	prognosisData.EOL.true = (1 x t) true EOL values at each prediction time
3.	prognosisData.EOL.values = (N x t) EOL prediction values at each prediction time.
4.	prognosisData.EOL.weights = (N x t) EOL prediction weights at each prediction time.
5.	prognosisData.RUL.true = (1 x t) true RUL values
6.	prognosisData.RUL.values = (N x t) RUL prediction values at each prediction time.
7.	prognosisData.RUL.weights = (N x t) RUL prediction weights at each prediction time.
8.	prognosisData.cputime = (1 x t) EOL/RUL computation times (optional)


If unscented transform sampling [1] is used for prediction, then the samples have so-called sigma-points associated with them. These sigma-points need to be specified as a two-dimensional vector; note that only two sigma points are used within this MATLAB software package. For example, 
> sigma=[0.5, 0.5] 

is a valid assignment. This assignment can be ignored and omitted if unscented transform sampling is not used.

Also, the alpha and beta performance requirement levels [2] need to be defined by the user, as a two-dimensional vector. For example, 
> alphaBeta=[0.1, 0.5] 

is a valid assignment. Both the above numbers need to be between 0 and 1. The first argument is an allowed accuracy window; smaller the value, more stringent the requirement. The second argument is related to precision; larger the value, more stringent the requirement.
 
## How to load and run the package contents?

In MATLAB, a package is represented by using ‚Äú+‚Äù before the name of the folder containing the package. To load the package use the command: 
> import PrognosticsMetrics.*

This package contains one simple function that calculates all the prognostics metrics. This function can be called using the code: 
> computePrognosisMetrics(prognosisData,alphaBeta,sigma)

While the first argument is the aforementioned structure, the second argument is the 2-dimensional vector containing alpha and beta requirements. These two arguments are mandatory for the function. The third optional argument contains the 2-dimensional vector of sigma-points, and needs to be provided only if unscented transform sampling is used for prediction.

## Description of Output Metrics

**Alpha-Lambda Performance** <br />
Alpha-Lambda Performance is a binary metric that outputs either 1 or 0. At a specific time index, lambda, it questions whether the prediction remains within a cone of accuracy (marked by alpha bounds) as the system approaches EoL.  If a desired condition is met, indicated by the probability mass being greater than Beta, minimal acceptable probability, at lambda time then the output is 1. Otherwise, the return is 0.

**Relative Accuracy** <br />
Relative Accuracy is the measure of error in RUL prediction relative to the actual RUL. Because it indicates how accurately the algorithm is performing at a certain time, Prognostic algorithms with larger Relative Accuracies are more desirable.

**Convergence** <br />
The Convergence Metric determines the rate at which a Metric (ex: accuracy or precision) improves over time. As more information accumulates along with the system progression, it is assumed that algorithm performance also improves.  Convergence is calculated by finding the distance between the origin and centroid of area under the curve for a metric.

Smaller the distance, faster the convergence. Faster the convergence indicates higher confidence in keeping the Prediction Horizon as large as possible.

## Alpha-Lambda Performance Plot

In addition to computing the metrics, this software package can also plot the alpha-lambda performance [2] plot, using the command
> plotAlphaLambda(prognosisData,alphaBeta(1),alphaBeta(2))

Note that there are three arguments, and all three are mandatory. The first is the input structure, the second and the third are the alpha and beta values respectively [2].


## References

1. M. Daigle, A. Saxena, and K. Goebel, ‚ÄúAn efficient deterministic approach to model-based prediction uncertainty estimation,‚Äù in Annual Conference of the Prognosticsand Health Management Society, 2012, Minneapolis, MN, USA.
2. Saxena, A., Celaya, J., Saha, B., Saha, S., & Goebel, K. Metrics for Offline Evaluation of Prognostic Performance. International Journal of Prognostics and Health Management, Vol. 1, No. 1,  21 pages, 2010.
"
164,nasa/ISS_Camera_Geolocate,Jupyter Notebook,"ISS Camera Geolocate README
----------------------------
This is a Python software library that facilitates the geolocation of photographs and video frames from the International Space Station (ISS). The library provides functions that take camera and pointing information, along with publicly available ISS position information, and then will geolocate every pixel of the photograph in latitude and longitude. This enables geospatial analysis of astronaut photography from Earth, including pictures of clouds, lightning, coastlines, city lights, etc. Many images available from https://earth.jsc.nasa.gov/ can be fully geolocated using this software.

The code now also enables geolocation of the ISS Lightning Imaging Sensor (LIS) background imagery datasets. These data are available from http://dx.doi.org/10.5067/LIS/ISSLIS/DATA206 and http://dx.doi.org/10.5067/LIS/ISSLIS/DATA207.

ISS Camera Geolocate Installation
---------------------------------
ISS Camera Geolocate works under Python 3.6+ on most Mac/Linux setups. Windows installation and other Python versions are currently untested.

In the main source directory:  
`python setup.py install`

The following dependencies need to be installed first:

- A robust version of Python w/ most standard scientific packages (e.g., `numpy`, `datetime`, `astropy`, etc.) - Get one for free [here.](https://store.continuum.io/cshop/anaconda/)
- [SGP4](https://pypi.python.org/pypi/sgp4/)
-Cartopy
-Cython
-Xarray

Using ISS Camera Geolocate
--------------------------
To access everything:
```
import iss_camera_geolocate as icg
```

Demonstration notebooks are in the notebooks directory.

Latest release info:
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2585824.svg)](https://doi.org/10.5281/zenodo.2585824)
"
165,nasa/data-nasa-gov-frontpage,CSS,"# data-nasa-gov-frontpage

Data.nasa.gov holds metadata harvested from more than 88 different NASA sites that share public data.

This front-end only page serves as an easy to change front-page for the system that hosts the catalog of datasets in data.nasa.gov (which is not on github).

## Contributing
If you find any factual errors or places where additions could be made, please add it as an issue or submit a pull request. We accept pull requests from the public. 

## Potentially Reusable Code Assets Leveraged by this Repository

1. This page uses nasawds-2.0.7, which you can find here: https://github.com/nasa/nasawds which is based on GSA's <a href=""https://github.com/uswds/uswds""> US web design service</a>. Both are open-source projects on Github.

2. The <a href=""https://github.com/nasa/data-nasa-gov-frontpage/tree/master/non-nasawds-assets/footer"">footer</a> is reused across several open-innovation pages.

3. The page data_visualization.html contains a treemap that displays an aggregate data visualization of contents of data.nasa.gov data catalog. The data is represented by rectangles scaled by the number of datasets. Each rectangle reflects a unique combination of source, category, and keyword. This data is extracted from the metadata in data.json.

## The Treemap Data Visualization Page
### Data Source:
The aggregate data visualization code depends on a JSON file that follows a valid data.json schema as defined by the open data project here: https://project-open-data.cio.gov/ and here https://project-open-data.cio.gov/v1.1/schema/.

Theoretically, you should be able to drop in any data.json from any federal agency. Your milage may vary, however. The visualization may not look as nice if the structure is different. For example, if source is shown as the first breakdown and 95% of your datasets are from a single source, than that rectange will take up the majority of the space. This won't look very nice.

### Data Processing:
`data_processing.py` is responsible for processing the data and can be found inside the `data_processing_scripts` folder. To run it, make sure your data source is in the same folder as the script and run `python3 data_processing.py`. This will produce `processed_data.json`, which is formatted as follows: 
```
{
   ""name"": ""dataset"",
   ""children"": [
       {
           ""name"": <source>,
           ""children"": [
               {
                   ""name"": <category>,
                   ""children"": [
                       {
                           ""name"": <keyword>,
                           ""value"": <count>
                       }, ...
                   ]
               }, ...
           ]
       }, ...
   ]
}
```
**NOTE**: Category refers to ""theme"" in the original schema.

#### Duplicates:
If your source data has a duplicate source, you may add it to `duplicates.json` found inside the `data_processing_scripts` folder.

Example:
```
{
  ""National Aeronautics Space Administration"": ""NASA"",
  ...
}
```
This example will group both ""National Aeronautics Space Administration"" and ""NASA"" under the source name ""NASA"" in `data_processing.py`.

#### Ignoring Data:
If your source data has any source, category or keyword you want ignored, you may add it to `ignoreData.json` found inside the `data_processing_scripts` folder.

Example: 
```
{
  ""source"": [""NASA""],
  ""category"": [""Earth Science"", ""Geospatial""],
  ""keyword"": []
}
```
This example will ignore all entries in which ""NASA"" is the source, and ignore all entires with ""Earth Science"" and ""Geospatial"" are categories. 
**NOTE**: Category refers to ""theme"" in the original schema

#### Keyword Count Minimum:
`keyword_count_threshold` sets the minimum number a keyword count must be to be added to `processed_data.json` and can be changed inside `data_processing.py` [here](https://github.com/nasa/data-nasa-gov-frontpage/blob/master/data_processing_scripts/data_processing.py#L3).

### Data Visualization:
The code used to visualize the data is `visualizations.js` and can be found inside `nasawds-2.07/js`. 

#### Expanding Acronyms to be Displayed in Treemap Legend:
In `acronyms.json`, found inside `nasawds-2.0.7/json`, you may select acronyms you wish to be expanded for the purpose of displaying them in the treemap legend. Each acronym must have a `type` (either source or category), and a `name` (the acronym's expansion).

Example: 
```
{
  ""NASA"": { ""type"": ""source"", ""name"": ""National Aeronautics Space Administration""},
  ""GPM"": {""type"": ""category"", ""name"": ""Global Precipitation Measurement""}
}
```

#### Treemap Rectangle and Legend Key Links:
When a user clicks on either a treemap rectangle or a legend key, they are redirected to data.nasa.gov's data catalog page. Changing where a user is redirected can be done inside of `nasawds-2.07/js/visualizations.js`. Setting the treemap rectangle link is done [here](https://github.com/nasa/data-nasa-gov-frontpage/blob/master/nasawds-2.0.7/js/visualizations.js#L55), and setting the legend key link is done [here](https://github.com/nasa/data-nasa-gov-frontpage/blob/master/nasawds-2.0.7/js/visualizations.js#L120).

#### Functions:
`create_treemap(<data>, <format>)` function takes the `processed_data.json` as the first argument, and the treemap format (d3.treemapSquarify, d3.treemapBinary, d3.treemapSlice, d3.treemapDice, d3.treemapSliceDice) as the second. This function is responsible for rendering and appending the treemap to the site. 

`changeNesting(<data>)` function takes the `processed_data.json` as an argument, and returns back the same data except it changes the grouping/nesting order from `source -> category -> keyword` to `category -> source -> keyword`.

`clean_data_treemap(<data>)` function takes the `processed_data.json` as an argument, and is responsible for removing sources and categories that have no children, as well as swap out source and category acronyms listed in `acronyms.json` (found inside `nasawds-2.0.7/json`) using `swap_acronyms()`. This new cleaned dataset it returned. 

`swap_acronyms(<acronym>)` function takes an acronym string and returns its expantion if listed in `acronyms.json`. If no expantion is found, the original acronym string is returned.


#### Other D3.js visualizations that will work with same processed data format:
[Collapsible Tree](https://observablehq.com/@d3/collapsible-tree)

[Circle Packing](https://observablehq.com/@d3/circle-packing)

[Sunburst](https://observablehq.com/@d3/sunburst)

[Icicle](https://observablehq.com/@d3/icicle)

[Cluster Dendrogram](https://observablehq.com/@d3/cluster-dendrogram)


"
166,nasa/K2CE,Python,"# K2CE

## The Kepler-K2 Cadence Events Application

Since early 2018, the Kepler/K2 project has been performing a uniform global reprocessing of data from K2 Campaigns 0 through 14. Subsequent K2 campaigns (C15-C19) are being processed using the same processing pipeline. One of the major benefits of the reprocessing effort is that, for the first time, short-cadence (1-min) light curves are produced in addition to the standard long-cadence (30-min) light curves. Users have been cautioned that the Kepler pipeline detrending module (PDC), developed for use on original Kepler data, has not been tailored for use on short-cadence K2 observations. Systematics due to events on fast timescales,  such as thruster firings, are sometimes poorly corrected for many short-cadence targets. A Python data visualization and manipulation tool, called Kepler-K2 Cadence Events, has been developed that identifies and removes cadences associated with problematic thruster events, thus producing better light curves. Kepler-K2 Cadence Events can be used to visualize and manipulate light curve files and target pixel files from the Kepler, K2, and TESS missions.  We anticipate this software will be available from <http://code.nasa.gov> 

## Prequisites

The Kepler-K2 Cadence Events application should run on any computer with Python 3 .  The application was developed using the open-source Anaconda Distribution of Python 2.7 which is available at <https://www.anaconda.com/distribution>.  K2CE has now been ported to Python 3.7.3.

The application uses Lightkurve (<http://docs.lightkurve.org/>) which is a community-developed, open-source Python package which offers a ""beautiful and user-freindly way to analyze astronomical flux time seris data"".  It was specifically designed to analyze the pixels and lightcurves obtained by NASA's Kepler, K2, and TESS exoplanet missions. Installation instructions are given at the following webpage: <http://docs.lightkurve.org/about/install.html>.

## Running the Code

### As a Stand-Alone Application from the Command Line

(1) Copy the Python application (k2\_cadence\_events.py) into the local directory.

(2) Make sure that the code is executable by typing (using MacOS X or Linux)

	chmod u+x k2_cadence_events.py

(3) The application can be run without any arguments.  

If you type the following command from the command line, you will see a demonstration of the application:

	./k2_cadence_events.py
	
You should see something like this:

<pre>
./k2_cadence_events.py 
**********************************************
Kepler K2 Cadence Events (k2ce): Version 0.79
**********************************************

*******************************************************
***** Use --help to see the command line options. *****
*******************************************************


Using default target (exoplanet K2-99b):

  from_archive=True
  target=212803289
  campaign=17
  cadence=short

filename=/Users/kmighell/.lightkurve-cache/mastDownload/K2/ktwo212803289-c17_sc/ktwo212803289-c17_slc.fits

Kepler/K2 short cadence Light Curve File

K2 Campaign 17

target: EPIC 212803289

98490 cadences

Using default bitmask value of 1605636.

The bitmask value of 1605636 = 110001000000000000100

translates as

['Coarse point', 'Possible thruster firing', 'Thruster firing', 'No fine point']

</pre>

A plot should appear on your screen like this:

![](README_fig1.png)

Once you are ready to leave the application, close the plot window (MacOS X: click on the red button on the upper-right corner of the plot window).

The application ends by writing a little more information:

<pre>
1635 events

1635 cadences flagged
</pre>

The demo plot shows the short cadence observations of EPIC 212803289
(exoplanet: K2-99b) that were obtained during K2 Campaign 17.

The short-cadence K2 light curve file was downloaded automatically from
the Barbara A. Mikulski Archive for Space Telescopes (MAST) at the Space
Telescope Science Institute (STScI).

The **red points** are the observations that had a SAP_QUALITY value with either Bit21
(1048576 = (2\*\*20)) and/or Bit20 (524288 = (2\*\*19)) and/or Bit16 (32768 =
(2\*\*15)) and/or (4 = (2\*\*2)) set -- which indicates (1) an actual or
(2) probable thruster firing event, or the ***Kepler*** spacecraft was in (3) coarse point or (4) not in fine point **during an individual cadence observation**.

The **blue points** in the scatter plot are the ""**good observations**"" that ***did not*** have at least one of these bits set.

Of the total of 98490 short-cadence observations, 1635 (1.66%) of the cadences were flagged as being a ""cadence event"".

The grey vertical bars near the top of the plot show events.
The darker the grey, the greater the number of events in time.

The name of the Kepler/K2 short cadence light curve file analysed is shown
on the right side of the plot. Note that this is where the file is stored
locally.

#### Command-line arguments 

To see the command-line arguments, type

	k2_cadence_events.py --help
	
or 
	
	k2_cadence_events.py -h

You should see something like this:
	
<pre>
./k2_cadence_events.py 
  [-h] [--help]  
  [--filename FILENAME]    
  [--bitmask BITMASK]  
  [--from_archive FROM_ARCHIVE]                            
  [--target TARGET] 
  [--cadence CADENCE]
  [--campaign CAMPAIGN] 
  [--tag TAG]
  [--plotfile PLOTFILE] 
  [--scatter SCATTER]
  [--bars_yy BARS_YY] 
  [--xlim XLIM]
  [--ylim YLIM] 
  [--SAP_FLUX SAP_FLUX]
  [--report REPORT]
  [--report_filename REPORT_FILENAME]
  [--n_before N_BEFORE] 
  [--n_after N_AFTER]
  [--bitmask_decode BITMASK_DECODE]
  [--bitmask_flags BITMASK_FLAGS]
  [--show_plot SHOW_PLOT]
  [--new_filename NEW_FILENAME]
  [--overwrite OVERWRITE] [--useTPF USETPF]
  [--xcut XCUT] 
  [--ycut YCUT]

optional arguments:

  -h, --help            show this help message and exit

  --filename FILENAME   Filename of the K2 light curve file (ktwo*llc.fits) to
                        be analyzed [default: None]

  --bitmask BITMASK     Bitmask value (integer) specifying quality flag
                        bitmask of cadences to *show* events. See Table 2-3 of
                        the Kepler Archive Manual (KDMC-10009-006) for more
                        information [default: None]

  --from_archive FROM_ARCHIVE
                        If True, get the data from the Mikulski Archive for
                        Space Telescopes (MAST) at the Space Telescope Science
                        Institute (STScI) [default=True]

  --target TARGET       Target name or EPIC number [default: None]

  --cadence CADENCE     Type of K2 cadence: 'short' or 'long' [default:
                        'long']

  --campaign CAMPAIGN   K2 campaign number [default: None]

  --tag TAG             String written at the start of the title of the plot
                        [default: None]

  --plotfile PLOTFILE   Filename of the output plotfile (if any) [default:
                        None]

  --scatter SCATTER     If True: the data is plotted as a scatter plot. If
                        False: the data is plotted as a line plot
                        [default=True]

  --bars_yy BARS_YY     Used to set the Y axis location of the gray vertical
                        lines showing events [default: None]

  --xlim XLIM           User-defined right and left limits for the X axis.
                        Example: xlim='(3360,3390)' [default: None]

  --ylim YLIM           User-defined bottom and top limits for the Y axis.
                        Example: ylim='(0.0,1.1)' [default: None]
  --SAP_FLUX SAP_FLUX   If True, flux is SAP_FLUX. If False, flux is
                        PDCSAP_FLUX [default: True]

  --report REPORT       If True, print out the time, flux, cadence number and
                        the QUALITY value fore each event [default: False]

  --report_filename REPORT_FILENAME
                        Filename of the report (if any) [default: None]

  --n_before N_BEFORE   Number of observations (cadences) before an event to
                        mark as bad [default: 0]

  --n_after N_AFTER     Number of observations (cadences) after an event to
                        mark as bad [default: 0]

  --bitmask_decode BITMASK_DECODE
                        If True, Decodes (translate) the bitmask value to K2
                        Quality Flag Events [default: False]

  --bitmask_flags BITMASK_FLAGS
                        If True, show the QUALITY bit flags. See Table 2-3 of
                        the Kepler Archive Manual (KDMC-10008-006) for more
                        information. [default=False]

  --show_plot SHOW_PLOT
                        If True, show the plot [default=True]

  --new_filename NEW_FILENAME
                        Filename of the new long (llc) or short (slc) file
                        with the event cadences and the bad cadences removed
                        [default: None].

  --overwrite OVERWRITE
                        If True, overwrite (""clobber"") an existing output file
                        [default: False].

  --useTPF USETPF       If False, return a KeplerLightCurveFile object. If
                        True, return a KeplerTargetPixelFile object [default:
                        False].

  --xcut XCUT           Cadences with time (X axis) values within the xcut
                        limits will be flagged for removal. Example:
                        xcut='(3510,3590)' [default: None]

  --ycut YCUT           Cadences with normalized flux (Y axis) values within
                        the ycut limits will be flagged for removal. Example:
                        ycut='(0.0,0.9)' [default: None]
</pre>

The above was reformatted for easier reading.

### As a part of a Python script

(1) Using your favorite editor, cut-and-paste the following text to create a
Python script called **spud.py**:

<pre>
#!/usr/bin/env python
import sys
import os
import k2_cadence_events as k2ce
nargs = len(sys.argv) - 1
if (nargs == 0):
    # interactive mode: no arguments
    k2ce.k2_cadence_events()
else: 
    # non-interactive mode: first argument is the name of the plotfile (e.g., foo.png)
    plotfile=sys.argv[1]  
    if (os.path.isfile(plotfile)):  # abort if file already exists
        print '***** ERROR ***** Requested file already exists: ',plotfile
    else:
        k2ce.k2_cadence_events(plotfile=plotfile)
        print
        print 'Show the docstring for k2_cadence_events() :'
        print k2ce.k2_cadence_events.__doc__
#EOF
</pre>

(2) Make sure that the script is executable by typing 

	chmod u+x spud.py
	
(3) You can execute the script in its interactive mode (using no arguments):

	./spud.py
	
(4) You can execute the script in its non-interactive mode using the first argument as the name of the output plotfile (e.g., spud_plot.png):

	./spud.py spud_plot.png
	
After the plot is written the Python docstring for k2\_cadence\_envents() is shown.

## Explanation of QUALITY and SAP_QUALITY bit values

<pre>
Name     Value : Explanation (Kepler/K2)
Bit01        1 : Attitude tweak
Bit02        2 : Safe mode
Bit03        4 : Coarse point
Bit04        8 : Earth point
Bit05       16 : Zero crossing
Bit06       32 : Desaturation event
Bit07       64 : Argabrightening
Bit08      128 : Cosmic ray in optimal aperture
Bit09      256 : Manual exlude
Bit10      512 : This bit not used by ***Kepler***
Bit11     1024 : Sudden sensitivity dropout
Bit12     2048 : Impulsive outlier
Bit13     4096 : Argabrightening on CCD
Bit14     8192 : Cosmic ray in collateral data
Bit15    16384 : Detectpr amp,a;u
Bit16    32768 : No fine point
Bit17    65536 : No data
Bit18   131072 : Rolling band in optimal aperture
Bit19   262144 : Rolling band in full mask
Bit20   524288 : Possible thruster Firing
Bit21  1048576 : Thruster firing
</pre>

## Contact

Kepler-K2 Cadence Events was created by Kenneth J. Mighell and supported by the Kepler/K2 Science Office.  You can contact the author at kenneth dot j dot mighell at nasa dot gov or kmighell at seti dot org.


## More information about using the application

You can learn more about the many options of the k2\_cadence\_events application by using running its demo Jupyter notebook called **k2\_cadence\_events.ipynb**.

	jupyter notebook k2_cadence_events.ipynb
	
## Notices

Copyright ¬© 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

NASA acknowledges the SETI Institute‚Äôs primary role in authoring and producing the Kepler-K2 Cadence Events application under Cooperative Agreement Number NNX13AD01A

## Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

##### 2019SEP10
"
167,nasa/FM,C,"core Flight System (cFS) File Manager Application (FM) 
======================================================

Open Source Release Readme
==========================

FM Release 2.5.3

Date: 
3/19/2020

Introduction
---------------

  The File Manager application (FM) is a core Flight System (cFS) application 
  that is a plug-in to the Core Flight Executive (cFE) component of the cFS.  
  
  The FM application provides onboard file system management services by 
  processing ground commands for copying, moving, and renaming files, 
  decompressing files, creating directories, deleting files and directories, 
  providing file and directory informational telemetry messages, and providing 
  open file and directory listings.

  The FM application is written in C and depends on the cFS Operating System 
  Abstraction Layer (OSAL) and cFE components. There is additional FM application 
  specific configuration information contained in the application user's guide
  available in https://github.com/nasa/FM/tree/master/docs/users_guide

  This software is licensed under the NASA Open Source Agreement. 
  http://ti.arc.nasa.gov/opensource/nosa
 
 
Software Included
--------------------

  File Manager application (FM) 2.5.3
  
 
Software Required
--------------------

  Operating System Abstraction Layer 5.0 or higher can be 
  obtained at https://github.com/nasa/osal
 
  core Flight Executive 6.7.0 or higher can be obtained at
  https://github.com/nasa/cfe

  Note: An integrated bundle including the cFE, OSAL, and PSP can 
  be obtained at https://github.com/nasa/cfs

About cFS
------------

  The cFS is a platform and project independent reusable software framework and
  set of reusable applications developed by NASA Goddard Space Flight Center.  
  This framework is used as the basis for the flight software for satellite data 
  systems and instruments, but can be used on other embedded systems.  More 
  information on the cFS can be found at http://cfs.gsfc.nasa.gov

                      
EOF                       
"
168,nasa/georef_deploy,Ruby,"Installation
============

Requirements
~~~~~~~~~~~~

Our reference platform for GeoRef is Ubuntu Linux 14.04 LTS,
running Python 2.7.6 and Django 1.9.2.  For development we use Django's
built-in development web server MySQL database.  

We develop using a VagrantBox VM running a Ubuntu Linux inside a Mac OS X host machine.
Vagrant VM is strictly optional and only necessary if you are not running directly from a Ubuntu Linux Machine.

Our image view is rendered using the OpenSeadragon open source image viewer. (openseadragon.github.io/)

(Optional) Set up a Vagrant VM
~~~~~~~~~~~~~~~~~~~~
If you are running on a mac, we highly encourage you to use Vagrant to set up 
a Ubuntu Development Instance. Our set up script works best within the Vagrant 
environment running on Mac OSX.

Install VirtualBox. We have found that VirtualBox Version 4.3.10 works best with Vagrant.
We highly recommend you download VirtualBox 4.3.10.
Install the latest version of vagrant: ‚Äãhttp://www.vagrantup.com/downloads


Set Up an Install Location
~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's create a directory to hold the whole GeoRef installation
and capture the path in an environment variable we can use
in the instructions below::

  export GEOCAM_DIR=$HOME/projects/geocam # or choose your own
  mkdir -p $GEOCAM_DIR
  

Get the Source
~~~~~~~~~~~~~~

Check out our latest source revision with::

  cd $GEOCAM_DIR
  git clone https://github.com/nasa/georef_deploy.git


For more information on the Git version control system, visit `the Git home page`_.
You can install Git on Ubuntu with::

  sudo apt-get install git-all

.. _the Git home page: http://git-scm.com/


Run the Setup Script
~~~~~~~~~~~~~~~~~~~~~
The ""setup_site_vagrant.sh"" script initializes the vagrant box and it clones 
all the submodules that are needed::

    # go into the georef_deploy directory
    cd georef_deploy
    
    # if you are running inside a Vagrant VM do
    setup_site_vagrant.sh


If you are running directly on a Ubuntu Linux Machine, you can skip the above shell
script and run the following::
    sudo python $GEOCAM_DIR/georef_deploy/setup_site.py
    
    # You need to manually create couple symlinks if not running on vagrant
    sudo ln -s /home/geocam/georef_deploy georef_deploy
    sudo ln -s gds/georef/ georef


Override settings.py
~~~~~~~~~~~~~~~~~~~~~~~

In the ``settings.py`` file, modify the ``DATABASES`` field to point to
your Django MySQL database::

    DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.mysql',
            'NAME': 'georef',
            'USER': 'root',
            'PASSWORD': 'vagrant',
            'HOST': '127.0.0.1',
            'PORT': '3306',
        }
    }


Setup the Data Directory
~~~~~~~~~~~~~~~~~~~~~~~~~~
You must manually create the data directory and its sub folders. GeoRef will 
write the image tiles to this directory.

1. Create a data directory
    ``mkdir $GEOCAM_DIR/georef/data``
2. Create the overlays directory
    ``mkdir -p $GEOCAM_DIR/georef/data/geocamTiePoint/overlay_images``
3. Set the permissions
    ``chmod -R 777 $GEOCAM_DIR/georef/data``


Setup GeoRef
~~~~~~~~~~~~

If your development environment is set up inside Vagrant, cd into the georef_deploy 
directory and do::
    vagrant ssh
And then run the following commands.


You must create the following directory and files::

 # If you are not using Vagrant, do
     mkdir -p $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/ & touch $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/deepzoom.exception.log

 # If you are using Vagrant, do
     # deepzoom directory needs to be owned by www-data. Put it in /home/vagrant so that it can be owned by www-data (and not by user)
     mkdir -p /home/vagrant/deepzoom 
     # create a symlink to deepzoom in the data dir
     ln -s /home/vagrant/deepzoom /home/vagrant/georef/data/deepzoom


Install Earth Engine by following the instructions below: 
    https://developers.google.com/earth-engine/python_install_manual


To install Python dependencies, render icons and collect media for the
server, run::

  cd $GEOCAM_DIR/georef_deploy/georef
  ./manage.py bootstrap --yes
  source $GEOCAM_DIR/georef_deploy/georef/sourceme.sh genSourceme genSettings
  ./manage.py collectstatic  
  ./manage.py prep

You'll need to source the ``sourceme.sh`` file every time you open a new
shell if you want to run GeoCam-related Python scripts such as starting
the Django development web server.  The ``sourceme.sh`` file will also
take care of activating your virtualenv environment in new shells (if
you were in a virtualenv when you ran ``setup.py``).


To initialize the database
    ``$GEOCAM_DIR/georef/manage.py makemigrations deepzoom``

    ``$GEOCAM_DIR/georef/manage.py makemigrations geocamTiePoint``

    ``$GEOCAM_DIR/georef/manage.py migrate``

Note that the path to manage.py may be different if you are running inside Vagrant.


Create a User Account  
~~~~~~~~~~~~~~~~~~~~~
User name and password are required to use GeoRef. To create one, do::
    
    ./manage.py createsuperuser

And follow the prompts.



Try It Out
~~~~~~~~~~
Now you're ready to try it out!  

Restart the Apache server ``sudo apachectl restart``

Point your browser to ‚Äãhttp://10.0.3.18/


.. o  __BEGIN_LICENSE__
.. o  Copyright (C) 2008-2010 United States Government as represented by
.. o  the Administrator of the National Aeronautics and Space Administration.
.. o  All Rights Reserved.
.. o  __END_LICENSE__
"
169,nasa/Three-Dimensional-Nozzle-Design-Code,C++,"# Three-Dimensional-Nozzle-Design-Code

This package contains 3 programs written for the Microsoft Visual C++ compiler:

1) 2D Method of Characteristics, MOC_GRID_BDE
	- Computes a planar or axisymmetric two-dimensional nozzle contour based on user inputs using method of characteristics
	
2) Streamline Tracing Tool, STT2001
	- Performs streamline tracing on a supplied nozzle solution grid (usually generated by MOC_GRID_BDE)

3) 3D Method of Characteristics Design Tool, 3D_MOC
	- Computes three-dimensional flowfield properties for a given 3D nozzle contour. This could be a modified version of a nozzle contour generated by MOC_GRID_BDE.

	

Each program is contained in a separate folder containing source code and sample outputs. These programs are designed to use the Microsoft Foundation Class (MFC) libraries to generate the GUI windows for user input.

Detailed descriptions of the mathematical basis and operation of each program are provided in the attached report: 

**[Rice, T., ""2D and 3D Method of Characteristic Tools for Complex Nozzle Development,"" JHU/APL Report RTDC-TPS-481. 2003.](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20030067852.pdf)**
"
170,nasa/simplegrid,Python,"
==========
Simplegrid
==========

-------------------------------------------------------------------------
Simple regional grid creation and refinement for ocean circulation models
-------------------------------------------------------------------------

.. inclusion-marker-start-of-intro

simplegrid is a Python package for creating, refining, and joining horizontal
quadrilateral grids that are used in connection with the `MIT General
Circulation Model (MITgcm) <http://mitgcm.org/>`_.

simplegrid is based on equal great circle arc subdivision (hence the name) with
geodesic computations provided by `pyproj/PROJ
<https://pypi.org/project/pyproj/>`_, and implements both Python-callable and
command-line functionality for embedded and scripted solutions. It also contains
several useful utilities for grid manipulation, file i/o, and coincident edge
detection.

Grid point location data are specified in decimal longitude and latitude, with
resulting MITgcm grid data in units of meters.

.. inclusion-marker-end-of-intro

.. inclusion-marker-start-of-installation

------------
Installation
------------

Requirements
^^^^^^^^^^^^

simplegrid is compatible with Python 3.5+, with numpy, pyproj, and xESMF package
dependencies.

If xESMF is not already installed, the simplest approach is via an `anaconda
<https://anaconda.org/>`_ distribution using any one of several procedures
outlined in the `xESMF installation documentation
<https://xesmf.readthedocs.io/en/latest/installation.html>`_.  simplegrid may
then be installed within the base distribution or in a virtual environment that
has been granted access to the global site packages.


Installing from github
^^^^^^^^^^^^^^^^^^^^^^

simplegrid is under active development. To obtain the latest development version
you may clone the repository and install it::

    git clone https://github.jpl.nasa.gov/gmoore/simplegrid.git
    cd simplegrid
    pip install .


Building the documentation
^^^^^^^^^^^^^^^^^^^^^^^^^^

Source and build files are included to generate html-based documentation using
`Sphinx <https://www.sphinx-doc.org/>`_.
From the simplegrid git clone root directory, just::

    make --directory=./simplegrid/docs html

And point your browser to the resulting collection at::

    <simplegrid_git_clone_root>/simplegrid/docs/build/html/index.html

.. inclusion-marker-end-of-installation

.. inclusion-marker-start-of-examples

--------------------
Quick Start Examples
--------------------

Creating a grid
^^^^^^^^^^^^^^^

The following creates a simple 10x10 grid over a one-by-one degree region with
northwest/southeast lon/lat corners at (1.,2.)/(2.,1.):

from Python::

    import simplegrid as sg
    (newgrid,newgrid_ni,newgrid_nj) = sg.mkgrid.mkgrid(
        lon1=1., lat1=2.,
        lon2=2., lat2=1.,
        lon_subscale=10, lat_subscale=10)

from the command line::

    sgmkgrid                            \
        --lon1 1.                       \
        --lat1 2.                       \
        --lon2 2.                       \
        --lat2 1.                       \
        --lon_subscale 10               \
        --lat_subscale 10               \
        --outfile newgrid.mitgrid

All sixteen horizontal grid quantities (XC, YC, DXF, DYF, RAC, XG, YG, DXV, DYU,
RAZ, DXC, DYC, RAW, RAS, DXG, and DYG) are either written to a dictionary with
corresponding key/value pairs in the case of Python or, in the command-line
case, to a contiguous binary file in mitgridfile format.

Refining a grid
^^^^^^^^^^^^^^^

The following performs a 10x10 refinement of the llc 90 model, tile005,
""southwest"" corner cell. In the following, northwest/southeast input corner
lat/lon values for the region of interest have been determined by inspection and
need not be as precisely defined as shown; simplegrid will perform
nearest-neighbor checks to determine the closest existing corner grids:

from Python::

    import simplegrid as sg
    (newgrid,newgrid_ni,newgrid_nj) = sg.regrid.regrid(
        strict=True, verbose=False,
        mitgridfile='./data/tile005.mitgrid',
        ni=270, nj=90,
        lon1=-127.73445435, lat1=67.56064719,
        lon2=-128., lat2=67.40168504,
        lon_subscale=10, lat_subscale=10)

from the command line::

    sgregrid                            \
        --mitgridfile tile005.mitgrid   \
        --ni 270                        \
        --nj  90                        \
        --lon1 -127.73445435            \
        --lat1   67.56064719            \
        --lon2 -128.0                   \
        --lat2   67.40168504            \
        --lon_subscale 10               \
        --lat_subscale 10               \
        --outfile regrid005.mitgrid     \
        --strict

As in the preceding mkgrid case, all horizontal grid quantities are either
written to a dictionary of name/value pairs in Python or, in the command-line
case, to a contiguous binary file in mitgrid file format.

In addition to the mitgrid file input, the Python and command line interfaces to
regrid also support binary and comma-separated value (csv) input options; such
files would have been produced had an mitgrid file been read into matlab, for
example, with XG and YG corner grid matrix output (the only mitgrid file
quantities, in fact, used by regrid) to intermediate files.

from Python::

    import simplegrid as sg
    # both *.bin and *.csv supported:
    (newgrid,newgrid_ni,newgrid_nj) = sg.regrid.regrid(
        xg_file='./data/tile005_XG.bin',
        yg_file='./data/tile005_YG.bin',
        ni=270, nj=90,
        lon1=-127.73445435, lat1=67.56064719,
        lon2=-128., lat2=67.40168504,
        lon_subscale=10, lat_subscale=10)

and, from the command line::

    # both *.bin and *.csv supported:
    sgregrid                            \
        --xg_file tile005_XG.csv        \
        --yg_file tile005_YG.csv        \
        --ni 270                        \
        --nj  90                        \
        --lon1 -127.73445435            \
        --lat1   67.56064719            \
        --lon2 -128.0                   \
        --lat2   67.40168504            \
        --lon_subscale 10               \
        --lat_subscale 10               \
        --outfile regrid005.mitgrid

Determining boundary grid terms
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In most cases, mitgrid data that spans tracer cells is undefined along
boundaries (for example, ""U"" cell quantities RAW and DXV along a tile's western
and eastern edges).  ""addfringe"" functionality can compute this boundary, or
fringe, grid data if an adjacent tile is provided.

The following augments a 2x2 tile with boundary data computed using an eastern
edge-adjacent 2x2 tile:

from Python::

    import simplegrid as sg
    (tilea_edge,tileb_edge,new_tilea_grid) = sg.addfringe.addfringe(
        strict=True,
        tilea='./data/tile_A_2x2.mitgrid',nia=2,nja=2,
        tileb='./data/tile_B_E_2x2.mitgrid',nib=2,njb=2)

new_tilea_grid is a dictionary of mitgrid name/value pairs containing tile_A
input data, augmented with eastern edge data computed using tile_B.  tilea_edge
and tileb_edge are integer indicators confirming the A and B edge matches: 0==N,
1==S, 2==E, and 3==W (in this example, tilea_edge will be eqal to 2, and
tileb_edge, 3).

from the command line::

    sgaddfringe                         \
        --tilea tile_A_2x2.mitgrid      \
        --nia 2                         \
        --nja 2                         \
        --tileb tile_B_E_2x2.mitgrid    \
        --nib 2                         \
        --njb 2                         \
        --outfile addfringe_A_EW_2x2.mitgrid \
        --strict

As in the Python example, the output file contains tile_A grid quantities,
augmented with eastern edge data computed using tile_B.  Output is to a combined
binary file in mitgrid file format.

Joining grids
^^^^^^^^^^^^^

Joining, or ""stitching"", two tiles together produces a single entity, assigning
common-edge boundary quantities as appropriate.  The following joins two 2x2
tiles that match on their northern and southern edges, respectively, resulting
in a 2x4 mitgrid:

from Python::

    (newgrid,newgrid_ni,newgrid_nj) = sg.stitch.stitch(
        strict=True, verbose=False,
        tilea='./data/tile_A_2x2.mitgrid',nia=2,nja=2,
        tileb='./data/tile_B_N_2x2.mitgrid',nib=2,njb=2)

As in the previous examples, newgrid is a dictionary of mitgrid name/value
pairs, and newgrid_ni and newgrid_nj provide the tracer cell row and column
counts for the combined grid.

from the command line::

    sgstitch                            \
        --tilea tile_A_2x2.mitgrid      \
        --nia 2                         \
        --nja 2                         \
        --tileb tile_B_N_2x2.mitgrid    \
        --nib 2                         \
        --njb 2                         \
        --outfile stitch_AB_NS_2x4.mitgrid \
        --strict

Computing open boundary conditions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Global, or parent, grid simulation results can be interpolated to the boundaries
of regional grids for use in connection with MITgcm's OBCS package for regional
modeling. The ""getobcs"" capability in simplegrid supports the generalized
methods for describing open boundaries described in MITgcm's `OBC physical
parameterization package
<https://mitgcm.readthedocs.io/en/latest/phys_pkgs/obcs.html>`_, and
automatically generates boundary matrices for all results, depths, and times
found in the global simulation results directory.

The following computes open boundary conditions for the (default) outer tracer
cell edges of a 20x16 regional grid using MITgcm's lab_sea verification model
results, storing the N, S, E, and W sets of boundary matrices in the directory
./run_obcs:

from Python::

    # regional grid (at 2x global grid resolution):
    (mg_region, ni_region, nj_region) = sg.regrid.regrid(
        mitgrid_matrices=parent_mitgrid,
        lon1=290., lat1=70., lon2=310., lat2=54.,
        lon_subscale=2, lat_subscale=2)

    # interpolate global simulation results to regional boundaries:
    sg.getobcs.getobcs(
        parent_mitgrid_matrices=parent_mitgrid,
        parent_resultsdir = './run',
        regional_mitgrid_matrices = mg_region,
        verbose=True)


.. inclusion-marker-end-of-examples

"
171,nasa/S4P,Perl,"S4P 
===

S4P is the Simple, Scalable Script-based Science Processor.

The aim is to develop a simplified processing system to accommodate the increased load expected with the advent of reprocessing and launch of a new satellites.

INSTALLATION

To install this module type the following:

   perl Makefile.PL
   make
   make test
   make install

Disclaimer: We will update the software but not maintain the pull requests.

COPYRIGHTS
==========
Copyright ¬© 2002-2011 United States Government as represented by Administrator for The National Aeronautics and Space Administration. All Rights Reserved.
"
172,nasa/S4PA,Perl,"S4PA 
====

S4PA is the Simple, Scalable Script-based Science Processing Archive.
The aim is to develop a simple data management system for disk-based
archives of science data.

INSTALLATION

To install this module type the following:

   perl Makefile.PL
   make
   make test
   make install

COPYRIGHTS
==========
Copyright ¬© 2002-2011 United States Government as represented by Administrator for The National Aeronautics and Space Administration. All Rights Reserved.
"
173,nasa/aos-dr,C,"
## AOS-DR: Autonomy Operating System (AOS) Diagnostic Reasoner (DR)
-----------------------
### About
AOS-DR is software that performs runtime diagnosis of a system of interest. That is, it can determine when part of the system fails (fault/failure detection) and determine what part failed (fault/failure isolation). This software was first developed as part of the Autonomy Operating System project at NASA Ames Research Center. However, AOS-DR is general software that may be adapted to diagnose any system. The code and documentation will typically only use Diagnostic Reasoner, or DR, as the name of the software. \
\
The software is implemented as an application that runs within Core Flight Executive (cFE). cFE is open-source flight software created by NASA, and maintained separately [here](https://github.com/nasa/cFE). AOS-DR cannot run without being part of the cFE framework. It also currently uses another cFE app called [Limit Checker (LC)](https://github.com/nasa/LC) to classify data from the system into pass/fail results, which are the input to AOS-DR. cFE runs on Linux and VxWorks; however, AOS-DR has only been used on Linux.\
\
AOS-DR uses a dependency matrix (D-matrix) approach for performing diagnosis. The D-matrix is system-specific, and maps pass/fail results into system failure modes. A user of AOS-DR will need to create a D-matrix as well as several other tables in order to adapt it for their system. For more details on how AOS-DR works, please see the user manual in the doc directory.

-----------------------
### Contact
If you have questions about AOS-DR, please contact Adam Sweet <<adam.sweet@nasa.gov>> or Chris Teubert <<christopher.a.teubert@nasa.gov>>.

-----------------------
### Contributing

Contributions to AOS-DR are welcome! Contributors will need to sign and submit a ""Contributor License Agreement"" (CLA), included in this source. 

-----------------------
### Copyright and Notices
The AOS-DR code is released under the NASA Open Source Agreement Version 1.3 license. A copy of the license is distributed with the source code.\
\
Copyright ¬© 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers:

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.



"
174,nasa/MISR-Toolkit,C,"
Welcome to the MISR Toolkit
---------------------------

The MISR Toolkit is a simplified programming interface to access MISR L1B2, L2, and MISR-HR,
conventional and ancillary data products. It is an interface built upon HDF-EOS
that knows about MISR data products. It has the ability to:

   - Specify regions to read based on geographic location and extent or the
     more traditional path and block range
   - Map between path, orbit, block, time range and geographic location
   - Automatically stitch, unpack and unscale MISR data while reading
   - Perform coordinate conversions between lat/lon, SOM x/y, block/line/sample
     and line/sample of a data plane, which means geolocation can be computed
     instantly without referring to an ancillary data set lookups
   - Retrieve pixel acquistion time from L1B2 product files
   - Read a slice of a multi-dimensional field into an 2-D data plane (eg. RetrAppMask[0][5])
   - Convert MISR product files to IDL ENVI files

The MISR Toolkit has been tested on Linux CentOS 7, Mac OS X 10.14.6 and Windows 10. 
It's core interface is C. There are also bindings for Python (2.7 and 3.x supported) and IDL.
Note that Python 2.7 is end of life and deprecated in MISR Toolkit.


Complete documentation and function reference
---------------------------------------------

Use a browser to view online documentation: https://nasa.github.io/MISR-Toolkit/html/index.html

Offline documentation packages are also available for download on the github releases page.


Third Party Library Dependencies
--------------------------------

The MISR Toolkit depends on the following libraries. Download links are provide
for your reference, but it is preferred to use the script ""download_libraries""
in the source bundle ""scripts"" directory. Python users will still need to
download NumPy and install it according to NumPy instructions.

   - HDF-EOS2.18v1.00 (https://observer.gsfc.nasa.gov/ftp/edhs/hdfeos/previous_releases)
   - HDF4.2.10        (https://support.hdfgroup.org/ftp/HDF/releases/HDF4.2.10/src)
   - hdf5-1.8.16      (https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.16/src)
   - netcdf-4.4.0     (ftp://ftp.unidata.ucar.edu/pub/netcdf/)
   - jpegsrc.v6b      (https://observer.gsfc.nasa.gov/ftp/edhs/hdfeos/previous_releases)
   - zlib-1.2.5       (https://observer.gsfc.nasa.gov/ftp/edhs/hdfeos/previous_releases)
   - NumPy 1.15 or later Python module (http://www.numpy.org/)


Binary Installation Instructions
--------------------------------

Binary installations that do not require compilation are now available for Python 3.6, IDL, and C library users
on Linux, Mac OS X, and Windows.
Python 2.7 binaries are also available but deprecated on Linux and Mac OS X.

For Python users, the most convenient option is downloading MISR Toolkit directly from PyPI using pip.
- python -m pip install -U pip
- python -m pip install -U wheel numpy
- python -m pip install -U MisrToolkit

A downloadable python binary wheel distribution is also available on the releases page.

For IDL users, a IDL Dynamically Loadable Module (DLM) distribution is available on the releases page.
To use it, download the package, extract it to a convenient location, and then follow the instructions in the README
to set the IDL_DLM_PATH enviornment variable or use IDL's PREF_SET to tell IDL where you extracted the package.


Source Installation Instructions
--------------------------------

Linux and MacOS X

The recommended location for the HDF-EOS/HDF libraries and the MISR Toolkit is /usr/local/hdfeoslibs
and /usr/local/Mtk-1.5.X respectively, because this location provides convenient access for multiple user
accounts.  It is not necessary, however, to install in /usr/local. The MISR Toolkit and HDF-EOS can be
installed anywhere.  Of course installing in /usr/local requires root or sudo privileges. Use the sudo
commands below if you are a sudoer or simply become root before installing the HDF-EOS/HDF libraries
and before the ""make install"" step for the MISR Toolkit.

1) Create a working directory

   mkdir Mtk_tmp
   cd Mtk_tmp

2) Extract Mtk-src-1.5.X.tar.gz

   tar xzvf Mtk-src-1.5.X.tar.gz      (if not done already)

3) Extract Mtk testdata (substitute Mtk-testdata-1.5.X with your version)

   tar xzvf Mtk-testdata-1.5.X.tar.gz

4) Download HDF-EOS/HDF, JPEG and ZLIB libraries

   cd Mtk-src-1.5.X
   scripts/download_libraries

   <Hit ""return"" for defaults>

5) Install HDF-EOS/HDF, JPEG and ZLIB libraries (using the following script
   is necessary because it applies patches which are required for some systems,
   like Mac Universal Binaries, Linux and Linux64)

   ** The next step requires root privileges to install into 
   /usr/local/hdfeoslibs, otherwise choose another location and disregard sudo

   sudo scripts/install_hdf+hdfeos

   <Hit ""return"" for defaults (/usr/local/hdfeoslibs)>

6) Build, test and install Mtk (substitute Mtk-src-1.5.X with your version)

    Setup your HDF/HDFEOS environment
    for csh:

       source <your-step5-path>/bin/hdfeos_env.csh

    for bash:

      source <your-step5-path>/bin/hdfeos_env.sh

    Set MTK_INSTALLDIR environment variable (ex. /usr/local/Mtk-1.5.X)
    for csh:

       setenv MTK_INSTALLDIR <your-path>/Mtk-1.5.X

    for bash:

       export MTK_INSTALLDIR=<your-path>/Mtk-1.5.X

    Set IDL_DIR environment variable (if applicable)
    for csh:

       setenv IDL_DIR <path-to-idl>/harris/idl

    for bash:

      export IDL_DIR=<path-to-idl>/harris/idl

    To build, test and install (choose which libraries to build)

       Everything - C, command-line utilities, IDL and Python

          make
          make testall (If you have Mtk testdata installed and would like
                        to test C, IDL and Python)

          ** The next step requires root privileges to install into
             /usr/local/Mtk-1.5.X, otherwise choose another location
             and disregard sudo

          sudo MTK_INSTALLDIR=$MTK_INSTALLDIR make install

       Or - C library and command-line utilites

          make lib
          make cmdutil
          make test (If you have Mtk testdata installed and would like to
                     test only C routines)

          ** The next step requires root privileges to install into
             /usr/local/Mtk-1.5.X, otherwise choose another location
             and disregard sudo

          sudo make install

       Or - IDL library

          make idl
          make testidl (If you have Mtk testdata installed and would like to
                        test only IDL routines - requires user interaction)

          ** The next step requires root privileges to install into
             /usr/local/Mtk-1.5.X, otherwise choose another location
             and disregard sudo

          sudo make install

       Or - Python library

          make python
          make testpython (If you have Mtk testdata installed and would like to
                           test only Python routines)

          ** The next step requires root privileges to install into
             /usr/local/Mtk-1.5.X, otherwise choose another location
             and disregard sudo

          sudo make install

       Other build targets and options

          make help (for other build targets)
          make clean (to clean everything)

       If the build complains about .d files ( This occurs when the HDF-EOS
       environment is not set)

          scripts/rmdepend.sh (to remove dependency files)
          make

7) To use Mtk

   The HDF/HDFEOS environment is already set above

   Setup your Mtk environment (pick any):

      For csh:
         source $MTK_INSTALLDIR/bin/Mtk_c_env.csh       for C
         source $MTK_INSTALLDIR/bin/Mtk_idl_env.csh     for IDL
         source $MTK_INSTALLDIR/bin/Mtk_python_env.csh  for Python (Don't need
            to do if installed in site-packages (see step 8))

      For bash:
         . $MTK_INSTALLDIR/bin/Mtk_c_env.sh       for C
         . $MTK_INSTALLDIR/bin/Mtk_idl_env.sh     for IDL
         . $MTK_INSTALLDIR/bin/Mtk_python_env.sh  for Python (Don't need to do
            if installed in site-packages (see step 8))

   For C examples: $MTK_INSTALLDIR/examples/C
   For IDL examples: $MTK_INSTALLDIR/examples/idl
   For Python examples: $MTK_INSTALLDIR/examples/python
   For C command-line utilities source code: Mtk-src-1.5.X/src
   For IDL tests source code: Mtk-src-1.5.X/wrappers/idl

8) Optional - You may want to install the MisrToolkit into you Python
   site-packages directory

   cd Mtk-src-1.5.X/wrappers/python
   sudo python setup.py install
   setenv LD_LIBRARY_PATH $MTK_INSTALLDIR/lib

9) Optional - After installing the Mtk_tmp directory, it's contents is
   not needed, unless for reference and may be removed

   cd ../..
   rm -rf Mtk_tmp

Windows

See win64/README.txt in the source or binary bundle.

Known Problems
--------------

The IDL routine MTK_FIND_FILELIST() does not work properly.
By default OS X swallows click-events on change of focus.
  If IDL test windows on MacOS X 10.14 don't respond to clicks, do this command 
    ""defaults write org.x.x11 wmclickthrough -bool true""
  Then, once at a shell prompt, restart X11/XQuartz and retry
  ""make testidl"".

IDL is not compatible with XQuartz versions greater than 2.7.9
  If IDL tests return an error of ""Error: attempt to add non-widget child ""dsm"" 
    to parent ""idl"" which supports only widgets"""" then you can either downgrade
	to XQuartz 2.7.9 or follow the procedure described at goo.gl/RvXOXy to add
	/opt/X11/lib/flat_namespace to the DYLD_LIBRARY_PATH set by your idl
	launcher script (e.g. /Applications/harris/idl87/bin/idl)
"
175,nasa/Rapid-Model-Import-Tool,Python,"<!DOCTYPE html>
    <html>
    <head>
        <meta http-equiv=""Content-type"" content=""text/html;charset=UTF-8"">
        <title>RMIT Repo</title>
        
        <link rel=""stylesheet"" href=""https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css"">
        <link rel=""stylesheet"" href=""https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css"">
        
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        
    </head>
    <body>
        <h1 id=""RMIT-Repo"">RMIT Repo</h1>
<p>RMIT(Rapid Model Import Tool) is a python executable that processes high fidelity CAD models for use in game engines such as Unity. This repository serves as the code base for this project.</p>

<li><strong>William Little</strong> (Civil Service)</li>
<li><strong>Joseluis Chavez</strong>(Civil Service)</li>
<li><strong>Arjun Ayyangar</strong>(NIFS intern)</li>
<li><strong>Tanya Gupta</strong>(NIFS intern)</li>
<li><strong>Ben Fairlamb</strong>(NIFS intern)</li>
<li><strong>Kyle Mott</strong>(NIFS intern)</li>
<li><strong>Talyor Waddell</strong>(Civil Service)</li>

</ul>

    </body>
    </html>"
176,nasa/MultiDop,C,"# MultiDop README

<b>Note: MultiDop has been superseded by PyDDA (https://github.com/openradar/PyDDA). It is recommended that you install and use PyDDA rather than MultiDop.</b>

MultiDop is a Python-based interface between the C-based DDA code developed at
University of Oklahoma and the Python Atmospheric Radiation Measurement
Radar Toolkit (Py-ART) software developed at Argonne National Lab. Use this
software to perform 3D wind analyses using 2-3 independent Doppler radars.

To install:
1. Edit the Makefile in the src/ directory to match the compiler and path
   information relevant to your system.
2. ‚Äúpython setup.py install‚Äù in the master directory.

The installation will compile the DDA binary, which is the analysis engine, as
well as install the overall Python package. To see how to run the software,
view the example Jupyter notebook in the examples/ directory.

MultiDop has been tested using Python 2.7 and Python 3.6.

A conference presentation describing MultiDop and how to use it can be found at https://ams.confex.com/ams/97Annual/webprogram/Paper306647.html

Get Py-ART here: http://arm-doe.github.io/pyart/

MultiDop also requires numpy and xarray.
Get xarray here: http://xarray.pydata.org/en/stable/

Original C code developed by Corey Potvin (OU/CIMMS) and Alan Shapiro (OU). Subsequent major C code improvements and real-data functionality added by Daniel Betten (OU) and Gordon Carrie (OU).

Python code and C code modifications to ingest Py-ART grids were done by Timothy Lang (timothy.j.lang@nasa.gov).  

> If you use this software to produce an analysis for a presentation or
publication, you *must* cite the following papers:
http://journals.ametsoc.org/doi/abs/10.1175/2009JTECHA1256.1
(Shapiro et al. 2009, JTECH)
http://journals.ametsoc.org/doi/abs/10.1175/JTECH-D-11-00019.1
(Potvin et al. 2012, JTECH)

Latest release info:
[![DOI](https://zenodo.org/badge/84335317.svg)](https://zenodo.org/badge/latestdoi/84335317)
"
177,nasa/openmct-demo,CSS,"# Open MCT Live Demo

__NOTE__: This demo is available online at https://openmct-demo.herokuapp.com. This guide is intended for those wishing to setup and run the Open MCT demo locally. 

## Dependencies
The Open MCT demo depends on [node.js](https://nodejs.org/en/). Life is also easier with [git](https://git-scm.com/downloads), but it's not a requirement for installing and running the demo.

## Setup and Installation
Either download or clone the `openmct-demo` repository. Git is the preferred way of working with the Open MCT Demo repository, but it's not a requirement to get the demo running. If you're using git, the steps necessary to download and install the Open MCT demo are included below

```
git clone https://github.com/nasa/openmct-demo.git
cd openmct-demo
npm install
npm start
```

If you're not using git, you can download a [zip of the repository](https://github.com/nasa/openmct-demo/archive/master.zip) and after unzipping it and switching to the directory it was unzipped to, run `npm install` and `npm start`.

## Overview
This is a functional demo of the [Open MCT](https://github.com/nasa/openmct) mission operations framework using a combination of real and mock data. The real data is historical weather data taken from the REMS instrument on the Curiosity rover, which is kindly made available via a web service provided by the Centro de Astrobiolog√≠a of the Spanish National Research Council, to whom we are eternally grateful http://cab.inta-csic.es/rems/wp-content/plugins/marsweather-widget/api.php

## Dependency on Open MCT Tutorials
This demo depends on the [Open MCT Tutorial](https://github.com/nasa/openmct-tutorial) project. The demo reuses the realtime and historical servers and their associated adapters, but uses a customized dictionary and spacecraft object. This dependency should be considered when making significant code changes to the tutorials.
"
178,nasa/SCH,C,"# Scheduler

NASA core Flight System Scheduler Application

## Description

The Scheduler application (SCH) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

The SCH application provides a method of generating software bus messages at pre-determined timing intervals. This allows the system to operate in a Time Division Multiplexed (TDM) fashion with deterministic behavior. The TDM major frame is defined by the Major Time Synchronization Signal used by the
cFE TIME Services (typically 1 Hz). The Minor Frame timing (number of slots executed within each Major Frame) is also configurable.

## License

This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa
"
179,nasa/MiniWall,JavaScript,"<!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta charset=""utf-8"">
    <meta http-equiv=""refresh"" content=""0; URL=index.html"" />
    <title>MiniWall Software</title>
  </head>
  <body>
    <p>
      Read the <a href=""index.html"">index.html</a> file for information on how the
      MiniWall software works and how to use it.
    </p>
  </body>
</html>
"
180,nasa/kepler-pipeline,C,"# Kepler Science Data Processing Pipeline

The Kepler telescope launched into orbit in March 2009, initiating
NASA‚Äôs first mission to discover Earth-size planets orbiting Sun-like
stars. Kepler simultaneously collected data for ‚àº160,000 target stars
over its four-year mission, identifying over 4700 planet candidates,
2300 confirmed or validated planets, and 2100 eclipsing binaries.
While Kepler was designed to discover exoplanets, the long term,
ultra-high photometric precision measurements it achieved also make it
a premier observational facility for stellar astrophysics, especially
in the field of asteroseismology, and for variable stars, such as RR
Lyrae stars. The Kepler Science Operations Center (SOC) was developed
at NASA Ames Research Center to process the data acquired by Kepler
starting with pixel-level calibrations all the way to identifying
transiting planet signatures and subjecting them to a suite of
diagnostic tests to establish or break confidence in their planetary
nature. Detecting small, rocky planets transiting Sun-like stars
presents a variety of daunting challenges, including achieving an
unprecedented photometric precision of ‚àº20 ppm on 6.5-hour timescales,
supporting the science operations, management, and repeated
reprocessing of the accumulating data stream.

The scientific objective of the Kepler Mission is to explore the
structure and diversity of planetary systems. This is achieved by
surveying a large sample of stars to:

* Determine the abundance of terrestrial and larger planets in or near
the habitable zone of a wide variety of stars;
* Determine the distribution of sizes and shapes of the orbits of these planets;
* Estimate how many planets are in multiple-star systems;
* Determine the variety of orbit sizes and planet reflectivities,
radii, masses and densities of short-period giant planets;
* Identify additional members of each discovered planetary system
  using other techniques; and
* Determine the properties of those stars that harbor planetary systems.

This repository contains the source code of the Science Data
Processing Pipeline. Please note that it is not expected that the
reader will be able to build or run this this software due to
third-party licensing restrictions and dependencies and other
complications.

The top-level directory contains the following files:

* [kscrm.pdf](kscrm.pdf)  
The Kepler Source Code Road Map. This document contains most of the
information normally found in GitHub README files. Please read this
first.
* [source-code](source-code)  
The source code itself.
* [parameters](parameters)  
The configuration details for the last run of the Kepler Science Data
Processing Pipeline.
* [MATHWORKS-LIMITED-LICENSE.docx](MATHWORKS-LIMITED-LICENSE.docx)  
The license for files from MathWorks.
* [NASA-OPEN-SOURCE-AGREEMENT.doc](NASA-OPEN-SOURCE-AGREEMENT.doc)  
The license for every other file.

## Contact Info

For questions on the science, algorithms, and MATLAB code, please
contact Jon Jenkins \<<Jon.Jenkins@nasa.gov>\>, Co-Investigator for
Data Processing.

For questions on the ""plumbing"" and Java code, please contact Bill
Wohler \<<Bill.Wohler@nasa.gov>\>, Senior Software Engineer.

## Copyright and Notices

The Kepler Science Data Processing Pipeline code is released under the
[NASA Open Source Agreement Version 1.3
license](NASA-OPEN-SOURCE-AGREEMENT.doc).

Code provided by MathWorks is released under the [MathWorks Limited
License](MATHWORKS-LIMITED-LICENSE.docx).

Copyright ¬© 2017 United States Government as represented by the
Administrator of the National Aeronautics and Space Administration.
All Rights Reserved.

NASA acknowledges the SETI Institute‚Äôs primary role in authoring and
producing the Kepler Data Processing Pipeline under Cooperative
Agreement Nos. NNA04CC63A, NNX07AD96A, NNX07AD98A, NNX11AI13A,
NNX11AI14A, NNX13AD01A & NNX13AD16A.

This file is available under the terms of the NASA Open Source Agreement
(NOSA). You should have received a copy of this agreement with the
Kepler source code; see the file NASA-OPEN-SOURCE-AGREEMENT.doc.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY
WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,
INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE
WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM
TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER,
CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT
OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY
OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.
FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS
AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND
SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT'S USE OF
THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,
EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM
PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT
SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT'S SOLE
REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL
TERMINATION OF THIS AGREEMENT.
"
181,nasa/cumulus-message-adapter,Python,"# Cumulus Message Adapter

[![CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-message-adapter)

`cumulus-message-adapter` is a command-line interface for preparing and outputting Cumulus Messages for Cumulus Tasks. `cumulus-message-adapter` helps Cumulus developers integrate a task into a Cumulus Workflow.

Read more about how the `cumulus-message-adapter` works in the [CONTRACT.md](./CONTRACT.md).

## Releases

### Release Versions

Please note the following convention for release versions:

X.Y.Z: where:

* X is an organizational release that signifies the completion of a core set of functionality
* Y is a major version release that may include incompatible API changes and/or other breaking changes
* Z is a minor version that includes bugfixes and backwards compatible improvements

### Continuous Integration

[CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter) manages releases and release assets.

Whenever CircleCI passes on the master branch of cumulus-message-adapter and `message_adapter/version.py` has been updated with a version that doesn't match an existing tag, CircleCI will:

* Create a new tag with `tag_name` of the string in `message_adapter/version.py`
* Create a new release using the new tag, with a name equal to `tag_name` (equal to version).
* Build a `cumulus-message-adapter.zip` file and attach it as a release asset to the newly created release. The zip file is created using the [`Makefile`](./Makefile) in the root of this repository.

These steps are fully detailed in the [`.circleci/config.yml`](./.circleci/config.yml) file.

## Development

### Dependency Installation

```shell
pip install -r requirements-dev.txt
pip install -r requirements.txt
```

### Running Tests

Running tests requires [localstack](https://github.com/localstack/localstack).

Tests only require localstack running S3, which can be initiated with the following command:

```shell
SERVICES=s3 localstack start
```

And then you can check tests pass with the following nosetests command:

```shell
CUMULUS_ENV=testing nosetests -v -s
```

### Linting

```shell
pylint message_adapter
```

### Contributing

If changes are made to the codebase, you can create the cumulus-message-adapter zip archive for testing libraries that require it:

```shell
make clean
make cumulus-message-adapter.zip
```

Then you can run some integration tests:

```shell
./examples/example-node-message-adapter-lib.js
```

### Troubleshooting

* Error: ""DistutilsOptionError: must supply either home or prefix/exec-prefix ‚Äî not both"" when running `make cumulus-message-adapter.zip`
  * [Solution](https://stackoverflow.com/a/24357384)
"
182,nasa/resample_GLISTIN_DEMs,Python,"# resample_GLISTIN_DEMs
This set of codes is a Python toolbox to download and resample GLISTIN-A radar interferometer digital elevation models from NASA's Oceans Melting Greenland campaign. 

The official OMG GLISTIN-A data product is hosted on NASA's <a href=""https://podaac.jpl.nasa.gov/"">PO.DAAC</a>: 
<a href=""https://podaac.jpl.nasa.gov/dataset/OMG_L3_ICE_ELEV_GLISTINA"">OMG_L3_ICE_ELEV_GLISTINA</a>

<br>
<b>Swath Locations and Numbers</b>
<img src=""GLISTIN-A_DEM_Index_Domains.jpg"" width=300 title=""GLISTIN Swath Locations"">

<br>
<b>Example gridded swath after resampling</b>
<img src=""https://podaac.jpl.nasa.gov/Podaac/thumbnails/OMG_L3_ICE_ELEV_GLISTINA.jpg"" width=300  title=""GLISTIN Swath Locations"">

<br><br>

## Getting Started
Required Python packages: ```numpy```,```scipy```, ```argparse```, ```requests```, ```pyresample```, ```utm```, ```netCDF4```, ```xarray```, ```osr```

From a fresh anaconda environment, it is recommended to use this installation sequence:
```
pip install requests
conda install -c conda-forge pyresample
pip install netCDF4
pip install xarray
conda install -c conda-forge gdal
pip install pyproj
pip install utm
pip install bs4
conda install -c conda-forge scipy
```

After the installing the required packages,
1. Determine a directory on your local machine where the Level 1 data and resampled output products will be stored. This directory is referred to as ""dataFolder"" in the scripts provided.
2. Determine the indicies of the GLISTIN-A DEMs to downloaded. Refer to the <a href=""GLISTIN-A_DEM_Index_Domains.pdf"">Swath Location Map</a> to see the indices of all swaths.

## Steps to Generate Resampled GLISTIN-A DEMs in NetCDF format

1. Download the Level-2 GLISTIN-A DEMs and associated metadata using **download_level_2_data.py**
2. (Optional) Download a geoid layer to reference the elevation measurements to mean sea level
3. Resample the Level-2 data using **resample_GLISTIN_DEMs.py** 

### Step 1: Download the Level-2 data GLISTIN-A DEMs and associated metadata using download_level_2_data.py

Use **download_level_2_data.py** to download level 2 data from the [UAVSAR website](https://uavsar.jpl.nasa.gov/).

Keywords:
- dataFolder (-d): (Required) Directory where resampled data will be stored on local machine.

- swathIndices (-i): (Optional) Set of swath indices to resample, separated by spaces.  Default is -1: resample all swaths.
 
- years (-y): (Optional) Set of years to download, separated by spaces. Default: -1 (download swaths in all years 2016-2019). 

Example command to download only years 2016 and 2017 for swath indices 1,2,3,4, and 52:
```
python download_level_2_data.py -d '/path/to/dataFolder' -y 2016 2017 -i 1 2 3 4 52
```


Example command to download all available years of data for swath index 52:
```
python download_level_2_data.py -d '/path/to/dataFolder' -i 52 
```

After downloading the above example (swath 52, all years), you will have a directory structure with files that should look like the following:
```
> cd /path/to/dataFolder
> find .
.
./Raw
./Raw/2017
./Raw/2017/Data
./Raw/2017/Data/greenl_17914_17037_011_170321_ALTTBB_HH_04.hgt.grd
./Raw/2017/Metadata
./Raw/2017/Metadata/greenl_17914_17037_011_170321_ALTTBB_HH_04_metadata.txt
./Raw/2019
./Raw/2019/Data
./Raw/2019/Data/greenl_17914_19022_009_190413_ALTTBB_HH_01.hgt.grd
./Raw/2019/Metadata
./Raw/2019/Metadata/greenl_17914_19022_009_190413_ALTTBB_HH_01_metadata.txt
./Raw/2018
./Raw/2018/Data
./Raw/2018/Data/greenl_17914_18014_005_180315_ALTTBB_HH_01.hgt.grd
./Raw/2018/Metadata
./Raw/2018/Metadata/greenl_17914_18014_005_180315_ALTTBB_HH_01_metadata.txt
./Raw/2016
./Raw/2016/Data
./Raw/2016/Data/greenl_17914_16037_013_160330_ALTTBB_HH_03.hgt.grd
./Raw/2016/Metadata
./Raw/2016/Metadata/greenl_17914_16037_013_160330_ALTTBB_HH_03_metadata.txt
```

### Step 2 (Optional): Download a geoid layer to reference the elevation measurements to mean sea level 

To include a geoid file with your resampling, you must include a directory called 'Geoid' within your specified dataFolder that contains your geoid file. Feel free to use any geoid that suits your purposes.  Here use the
<a href=https://link.springer.com/article/10.1007/s10712-016-9406-y>GOC05c geoid</a> of Fetcher et al. [2017].

To obtain the GOCO05c geoid and prepare it for use in resample_GLISTIN_DEMs, follow the following steps:
1. Go to http://icgem.gfz-potsdam.de/calcgrid
2. Under Model Selection, choose Longtime Model -> GOCO05c.
3. Under Functional Selection, choose geoid.
4. Under Geographic Selection, set longitude bounds to -75.9 to -9.8, latitude bounds to 55.2 to 86.8, and Grid Step to 0.1.
5. Leave all other parameters as their default valies, and 'start computation'.
6. When file is complete, click Download Grid and save to **dataFolder/Geoid**
7. Finally, convert this file to a netCDF file using the **geoid_grid_to_nc.py** function.

Example command to convert the geoid grid to a NetCDF file:
```
python geoid_grid_to_nc.py -d '/path/to/dataFolder' -g 'GOCO05c_383e72b1d9fbea44d4c550a7446ff8fcb6a57aba0bfdd6293a3e4b72f86030aa.gdf'
```

To use a different geoid, you will likely have to modify **geoid_grid_to_nc.py**.


### Step 3: Resample the Level-2 data using **resample_GLISTIN_DEMs.py**

To resample data, use **resample_GLISTIN_DEMs.py**

Keywords:
- dataFolder (-d): (Required) Directory where resampled data will be stored on local machine.

- resolution (-r): (Optional) Horizontal resolution in meters of the new grid used for the resampling. Default: 50 (meters).

- swathIndices (-i): (Optional) Set of swath indices to resample, separated by spaces.  Default is -1: resample all swaths.

- years (-y): (Optional) Set of years to resample, separated by spaces. Default: -1 (resample swaths in all years 2016-2019). 

- projection (-p): (Optional) The projection of the output DEM. Input with an EPSG reference code as EPSG:XXXX or 'UTM'.  Default: 'UTM', the UTM zone that corresponds to the center lat/long of the swath. This data spans UTM zones 19N to 27N.

- addGeoid (-g): (Optional) Choose 1 if you would like to add a geoid correction to the file, 0 otherwise. Default: 0, do not include a geoid file.  Geoid field must be downloaded manually prior to running this script using the instructions provided on github.com/NASA/glistin). 


Example command to resample all DEMs for all years at 50m resolution, exluding the optional geoid layer:
```
python resample_GLISTIN_DEMs.py -d '/path/to/dataFolder' 
```

Example command to resample the DEMs for swath indices 1 2 3 4 and 52 in years 2018 and 2019 at 500m resolution, including the geoid layer:
```
python resample_GLISTIN_DEMs.py -d '/path/to/dataFolder' -i 1 2 3 4 52 -y 2018 2019 -g 1 -r 500
```

#### Benchmarks
The time required to resample the GLISTIN-A DEM data is a function of both resolution and the size of the initial swath. The resample time will increase with the requested grid spacing and number of points in the initital swath.

For example, swath 16 is a relatively small swath and the following wall-clock times were required for the resample:
- 50 m: 79 seconds
- 100 m: 173 seconds
- 500 m: 3332 seconds

As another example, swath 1 is a relatively large swath and the following wall-clock times were required for the resample:
- 50 m: 162 seconds
- 100 m: 276 seconds
- 500 m: 3842 seconds
"
183,nasa/CrisisMappingToolkit,Python,"# NASA Ames Crisis Mapping Toolkit

The Crisis Mapping Toolkit is a collection of algorithms and utilities for creating maps in response to crisis.
The CMT relies on [Google Earth Engine (EE)](https://earthengine.google.org/) for much of its data processing.
The CMT is released under the Apache 2 license.

The CMT is developed by the NASA Ames Intelligent Robotics Group, with generous support from the Google Crisis
Response Team and the Google Earth Engine Team.

The CMT currently provides:

- Algorithms to determine **flood extent from MODIS data**, such as
  multiple thresholding techniques, learned approaches, Dynamic Nearest
  Neighbor Search, and more.
- Algorithms to determine **flood extent from SAR data**, such as
  histogram thresholding and active contour.
- Algorithms to detect water and clouds in LANDSAT images.
- Various helpful utilities, such as:
    - An **improved visualization UI**, with a drop down menu of layers
      similar to the EE javascript playground.
    - **Local EE image download** and processing, for the occasional operation
      that cannot be done efficiently in EE.
    - A **configurable domain specification** to define problem domains and
      data sources in XML.
    - Functions for searching image catalogs.

The CMT is still under development, so expect changes and additional features.
Please contact Brian Coltin (brian.j.coltin at nasa dot gov) with any questions.

## Installation

- Follow the instructions for installing [Google Earth Engine for Python](https://developers.google.com/earth-engine/python_install).
- Download the CMT source code from [Github](https://github.com/bcoltin/CrisisMappingToolkit).
- Install PyQt4.
- Install the CMT with 
  ```
  python setup.py install
  ```

## Documentation

Before calling any CMT function, you must initialize EE, either by calling
ee.Initialize, or by using the cmt ee\_authenticate package:

```python
from cmt import ee_authenticate
ee_authenticate.initialize()
```

### Using the CMT UI

To use the CMT UI, replace your import of the EE map client:

```python
from ee.mapclient import centerMap, addToMap
```

with 

```python
from cmt.mapclient_qt import centerMap, addToMap
```

then use the centerMap and addToMap functions exactly as before.
That's it!

### Using the CMT LocalEEImage

See the documentation in local_ee_image.py. When you construct a LocalEEImage,
the image is downloaded from EE with the specified scale and bounding box
using getDownloadURL. You can then access individual pixels or bands as PIL Images.
Images are cached locally so if you are testing on the same image you do not need to
wait to download every time. We recommend using LocalEEImage sparingly, only for
operations which cannot be performed through EE, as downloading the entire image
is expensive in both time and bandwidth.

## Data Access

Data used in our examples has been uploaded as Assets in Earth Engine and should be
accessible without any special effort.  Unfortunately some datasets, such as 
TerraSAR-X, cannot be uploaded.  If you find any missing data sets, contact the
project maintainers to see if we can get it uploaded.

## Licensing

The Crisis Mapping Toolkit is released under the Apache 2 license.

Copyright (c) 2014, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.
"
184,nasa/Coordinate-systems-class-library,C++,"# Coordinate Systems Class Library
Library of classes representing various coordinate systems and providing the transformations between them. Coordinate systems represented are: East-North-Up (ENU), Downrange-Crossrange-Above (DCA), Latitude-Longitude-Altitude (LLA), Earth-Centered-Fixed (ECF), and Azimuth-Elevation-Range (AER).

## Notices
Copyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.
 
## Disclaimers
**No Warranty:** THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
**Waiver and Indemnity:**  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
185,nasa/rbf,Fortran,"# rbf
Mode shape interpolation via radial basis functions

See Examples/hirenasd for scripts showing all steps required to prepare FEM modes for use in FUN3D. The scripts will have to be adjusted for your PBS envirnoment. Also make sure that fun3d and tecplot are in your path. 

The following help is shown by typing ./rbf without any arguments.
<pre>
rbf Version: 2.3.0-2019.10.02

 Purpose:  Interpolate mode shapes from FEM modes to CFD surface.
   Usage:  ./rbf -s fem_mode_shape -d cfd_mesh -i cfd_mode_shape [options] 
 Options:  
           -s  source_fem_mode_shape_file
           -d  destination_mesh_file
           -i  interpolated_mode_shape_file
           -iz ignore points whose values are zero
           -nk number of source (fem) nodes to keep
           -pk percent of source (fem) nodes to keep
           -wp write fem_source_points for debugging
           -p  primary_surface_file
           -b  radial_blend_distance_in_grid_units for use with -p
           -x  xsym_blend_distance_in_grid_units
           -y  ysym_blend_distance_in_grid_units
           -z  zsym_blend_distance_in_grid_units
           -cs compute spring connectivity from rbf.nml
Requires:  
          1) ascci formatted fem mode shapes with 
             variables x,y,z,f1,f2,f3,f4,f5,f6
Examples:  
           ./rbf -s fem/mode001.txt -d project_ddfdrive_body1.dat \
             -i project_body1_mode1.dat -nk 250 -y 4
           ./rbf -s fem/mode001.txt -d ddfdrive_allsurf.dat \
             -i project_body1_mode1.dat -p ddfdrive_wing_tail.dat -b 3 -pk 25
   Built:  Fri Oct  4 11:20:17 PDT 2019 on Linux 4.12.14-95.19.1.20190617-nasa
 Version:  2.3.0 (2019.10.02)
   About:  Steven.J.Massey@nasa.gov 
</pre>
Notices:
Copyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
The following license and copyright notices govern the noted 3rd party software package KDTREE2  included in the NASA SOFTWARE:
 
The KDTREE2 software is licensed under the terms of the Academic Free Software License, listed herein.  In addition, users of this software must give appropriate citation in relevant technical documentation or journal paper to the author, Matthew B. Kennel, Institute For Nonlinear Science, preferably via a reference to the www.arxiv.org repository of this document, {\tt www.arxiv.org e-print: physics/0408067}.  This requirement will be deemed to be advisory and not mandatory as is necessary to permit the free inclusion of the present software with any software licensed under the terms of any version of the GNU General Public License, or GNU Library General Public License.
 
Academic Free License
Version 1.1
 
This Academic Free License applies to any original work of authorship (the ""Original Work"") whose owner (the ""Licensor"") has placed the following notice immediately following the copyright notice for the Original Work: ""Licensed under the Academic Free License version 1.1.""
 
Grant of License. Licensor hereby grants to any person obtaining a copy of the Original Work (""You"") a world-wide, royalty-free, non-exclusive, perpetual, non-sublicenseable license (1) to use, copy, modify, merge, publish, perform, distribute and/or sell copies of the Original Work and derivative works thereof, and (2) under patent claims owned or controlled by the Licensor that are embodied in the Original Work as furnished by the Licensor, to make, use, sell and offer for sale the Original Work and derivative works thereof, subject to the following conditions.
 
Right of Attribution. Redistributions of the Original Work must reproduce all copyright notices in the Original Work as furnished by the Licensor, both in the Original Work itself and in any documentation and/or other materials provided with the distribution of the Original Work in executable form.
 
Exclusions from License Grant. Neither the names of Licensor, nor the names of any contributors to the Original Work, nor any of their trademarks or service marks, may be used to endorse or promote products derived from this Original Work without express prior written permission of the Licensor.
 
WARRANTY AND DISCLAIMERS. LICENSOR WARRANTS THAT THE COPYRIGHT IN AND TO THE ORIGINAL WORK IS OWNED BY THE LICENSOR OR THAT THE ORIGINAL WORK IS DISTRIBUTED BY LICENSOR UNDER A VALID CURRENT LICENSE FROM THE COPYRIGHT OWNER. EXCEPT AS EXPRESSLY STATED IN THE IMMEDIATELY
PRECEEDING SENTENCE, THE ORIGINAL WORK IS PROVIDED UNDER THIS LICENSE ON AN ""AS IS"" BASIS, WITHOUT WARRANTY, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, THE WARRANTY OF NON-INFRINGEMENT AND WARRANTIES THAT THE ORIGINAL WORK IS MERCHANTABLE OR FIT FOR A
PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY OF THE ORIGINAL WORK IS WITH YOU. THIS DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO LICENSE TO ORIGINAL WORK IS GRANTED HEREUNDER EXCEPT UNDER THIS DISCLAIMER.
 
LIMITATION OF LIABILITY. UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, WHETHER TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE, SHALL THE LICENSOR BE LIABLE TO ANY PERSON FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER ARISING
AS A RESULT OF THIS LICENSE OR THE USE OF THE ORIGINAL WORK INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PERSON SHALL HAVE BEEN INFORMED OF THE
POSSIBILITY OF SUCH DAMAGES. THIS LIMITATION OF LIABILITY SHALL NOT APPLY TO LIABILITY FOR DEATH OR PERSONAL INJURY RESULTING FROM SUCH PARTY'S NEGLIGENCE TO THE EXTENT APPLICABLE LAW PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.
 
License to Source Code. The term ""Source Code"" means the preferred form of the Original Work for making modifications to it and all available documentation describing how to access and modify the Original Work. Licensor hereby agrees to provide a machine-readable copy of the Source Code of the Original Work along with each copy of the Original Work that Licensor distributes. Licensor reserves the right to satisfy this obligation by placing a machine-readable copy of the Source Code in an information repository reasonably calculated to permit inexpensive and convenient access by You for as long as Licensor continues to distribute the Original Work, and by publishing the address of that information repository in a notice immediately following the copyright notice that applies to the Original Work.
 
Mutual Termination for Patent Action. This License shall terminate automatically and You may no longer exercise any of the rights granted to You by this License if You file a lawsuit in any court alleging that any OSI Certified open source software that is licensed under any license containing this ""Mutual Termination for Patent Action"" clause infringes any patent claims that are essential to use that software.
 
This license is Copyright (C) 2002 Lawrence E. Rosen. All rights reserved. Permission is hereby granted to copy and distribute this license without modification. This license may not be modified without at the express written permission of its copyright owner.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
186,nasa/Kepler-FLTI,Python,"# Kepler-FLTI
Kepler-FLTI.py - Illustrate using the Flux-Level Transit Injection (FLTI) Tests of TPS for Data Release 25.  The FLTI test output is in the FITs file format.  This code generates the figures in the documentation of FLTI
    
Burke, C.J. & Catanzarite, J. 2017, ""Planet Detection Metrics: Per-Target Flux-Level Transit Injection Tests of TPS for Data Release 25"", KSCI-19109-001
       
**Assumptions**: python packages astropy, numpy, and matplotlib are available and file kplr007702838_dr25_5008_flti.fits is available in the same directory as Kepler-FLTI.py
      
**Running**: python Kepler-FLTI.py
    
**Output**: Displays a series of figures and generates hardcopy

Notices:

Copyright ¬© 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

NASA acknowledges the SETI Institute‚Äôs primary role in authoring and producing the Plotting Program for Kepler Planet Detection Efficiency Products under Cooperative Agreement Number NNX13AD01A.


Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
187,nasa/podaac_tools_and_services,Python,"podaac_tools_and_services
=========================
This is a meta-repository which lists locations of code related to all tools and services software for `NASA JPL's Physical Oceanography Distributed Active Archive Center (PO.DAAC) <https://podaac.jpl.nasa.gov>`__.

|image7|

What is PO.DAAC?
----------------
The `PO.DAAC <https://podaac.jpl.nasa.gov>`__ is an element of the Earth Observing System Data and Information System (`EOSDIS <https://earthdata.nasa.gov/>`__). The EOSDIS provides science data to a wide community of users for NASA's Science Mission Directorate. `PO.DAAC <https://podaac.jpl.nasa.gov>`__ has become the premier data center for measurements focused on ocean surface topography (OST), sea surface temperature (SST), ocean winds, sea surface salinity (SSS), gravity, ocean circulation and sea ice.

What's in this repository?
--------------------------
This repository reflects an active catalog of all tools and services software pertaining to `PO.DAAC data access <https://podaac.jpl.nasa.gov/dataaccess>`__. If you have a suggestion for a new tool or would like to update the content here, please `open an issue <https://github.com/nasa/podaac_tools_and_services/issues>`__ or `send a pull request <https://github.com/nasa/podaac_tools_and_services/pulls>`__.

Where do I find detailed information on tools and services included in this repository?
---------------------------------------------------------------------------------------
Each repository has it's own README file e.g. `data_animation/README.rst <https://github.com/nasa/podaac_tools_and_services/blob/master/data_animation/README.rst>`__

Keeping Git submodules up-to-date
---------------------------------
In order to keep the submodules as defined in [.gitmodules](https://github.com/nasa/podaac_tools_and_services/blob/master/.gitmodules) up-to-date it is necessary to periodically push updates. You can safely execute this command to do so::


    $ git submodule foreach git pull origin master
    $ git status //you will then see the changes which have been mode
    $ git add -A
    $ git commit -m ""Update submodules""
    $ git push origin master


License
-------
| Unless noted explicitly, all code in this repository is licensed permissively under the `Apache License
  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.
| A copy of that license is distributed with each software project.

Copyright and Export Classification
-----------------------------------

::

    Copyright 2019, by the California Institute of Technology. ALL RIGHTS RESERVED. 
    United States Government Sponsorship acknowledged. Any commercial use must be 
    negotiated with the Office of Technology Transfer at the California Institute 
    of Technology.
    This software may be subject to U.S. export control laws. By accepting this software, 
    the user agrees to comply with all applicable U.S. export laws and regulations. 
    User has the responsibility to obtain export licenses, or other export authority 
    as may be required before exporting such information to foreign countries or 
    providing access to foreign persons.

.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png
"
188,nasa/podaacpy,Python,"podaacpy
========

|DOI| |license| |PyPI| |documentation| |Travis| |Coveralls| |Requirements Status| |Anaconda-Server Version| |Anaconda-Server Downloads| 

|DeepSource|

|image7|

A python utility library for interacting with NASA JPL's
`PO.DAAC <https://podaac.jpl.nasa.gov>`__


Software DOI
------------

If you are using Podaacpy in your research, please consider citing the software |DOI|. This DOI represents all versions, and will always resolve to the latest one. If you wish to reference actual versions, then please find the appropriate DOI's over at Zenodo.


What is PO.DAAC?
----------------

| The Physical Oceanography Distributed Active Archive Center (PO.DAAC)
  is an element of the
| Earth Observing System Data and Information System
  (`EOSDIS <https://earthdata.nasa.gov/>`__).
| The EOSDIS provides science data to a wide community of users for
  NASA's Science Mission Directorate.

What does podaacpy offer?
-------------------------

The library provides a Python toolkit for interacting with all
`PO.DAAC Web Services v3.2.2 APIs <https://podaac.jpl.nasa.gov/ws>`__, namely

-  `PO.DAAC Web Services <https://podaac.jpl.nasa.gov/ws/>`__: services
   include
-  `Dataset
   Metadata <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__
   - retrieves the metadata of a dataset
-  `Granule
   Metadata <https://podaac.jpl.nasa.gov/ws/metadata/granule/index.html>`__
   - retrieves the metadata of a granule
-  `Search
   Dataset <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__
   - searches PO.DAAC's dataset catalog, over Level 2, Level 3, and
   Level 4 datasets
-  `Search
   Granule <https://podaac.jpl.nasa.gov/ws/search/granule/index.html>`__
   - does granule searching on PO.DAAC level 2 swath datasets
   (individual orbits of a satellite), and level 3 & 4 gridded datasets
   (time averaged to span the globe)
-  `Image
   Granule <https://podaac.jpl.nasa.gov/ws/image/granule/index.html>`__ -
   renders granules in the PO.DAAC's catalog to images such as jpeg
   and/or png
-  `Extract
   Granule <https://podaac.jpl.nasa.gov/ws/extract/granule/index.html>`__
   - subsets a granule in PO.DAAC catalog and produces either netcdf3 or
   hdf4 files

-  | `Metadata Compliance
     Checker <https://podaac-uat.jpl.nasa.gov/mcc>`__: an online tool and
     web
   | service designed to check and validate the contents of netCDF and
     HDF granules for the
   | Climate and Forecast (CF) and Attribute Convention for Dataset
     Discovery (ACDD) metadata conventions.

-  | `Level 2 Subsetting 
      <https://podaac-tools.jpl.nasa.gov/hitide/>`__: allows users to subset 
      and download popular PO.DAAC level 2 (swath) datasets.

-  | `PO.DAAC Drive <https://podaac-tools.jpl.nasa.gov/drive/>`__: an HTTP based 
      data access service. PO.DAAC Drive replicates much of the functionality 
      of FTP while addressing many of its issues.

Additionally, Podaacpy provides the following ocean-related data services 

- `NASA OceanColor Web <https://oceancolor.gsfc.nasa.gov>`_:

- `File Search <https://oceandata.sci.gsfc.nasa.gov/api/file_search>`_ -  locate publically available files within the NASA Ocean Data Processing System (ODPS)
- `Bulk data downloads via HTTP <https://oceancolor.gsfc.nasa.gov/forum/oceancolor/topic_show.pl?pid=12520>`_ - mimic FTP bulk data downloads using the `HTTP-based data distribution server <https://oceandata.sci.gsfc.nasa.gov>`_.

Installation
------------

From the cheeseshop

::

    pip3 install podaacpy
    
or from conda

::

    conda install -c conda-forge podaacpy    

or from source

::

    git clone https://github.com/nasa/podaacpy.git && cd podaacpy
    python3 setup.py install

Quickstart
----------
Check out the **examples** directory for our Jupyter notebook examples.

Tests
-----

| podaacpy uses the popular
  `nose <http://nose.readthedocs.org/en/latest/>`__ testing suite for
  unit tests.
| You can run the podaacpy tests simply by running

::

    nosetests

Additonally, click on the build sticker at the top of this readme to be
directed to the most recent build on
`travis-ci <https://travis-ci.org/nasa/podaacpy>`__.

Documentation
-------------

You can view the documentation online at

http://podaacpy.readthedocs.org/en/latest/

Alternatively, you can build the documentation manually as follows

::

    cd docs && make html

Documentation is then available in docs/build/html/

Community, Support and Development
----------------------------------

| Please open a ticket in the `issue
  tracker <https://github.com/nasa/podaacpy/issues>`__.
| Please use
  `labels <https://help.github.com/articles/applying-labels-to-issues-and-pull-requests/>`__
  to
| classify your issue.

License
-------

| podaacpy is licensed permissively under the `Apache License
  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.
| A copy of that license is distributed with this software.

Copyright and Export Classification
-----------------------------------

::

    Copyright 2016-2019, by the California Institute of Technology. ALL RIGHTS RESERVED. 
    United States Government Sponsorship acknowledged. Any commercial use must be 
    negotiated with the Office of Technology Transfer at the California Institute 
    of Technology.
    This software may be subject to U.S. export control laws. By accepting this software, 
    the user agrees to comply with all applicable U.S. export laws and regulations. 
    User has the responsibility to obtain export licenses, or other export authority 
    as may be required before exporting such information to foreign countries or 
    providing access to foreign persons.

.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1751972.svg
   :target: https://doi.org/10.5281/zenodo.1751972
.. |license| image:: https://img.shields.io/github/license/nasa/podaacpy.svg?maxAge=2592000
   :target: http://www.apache.org/licenses/LICENSE-2.0
.. |PyPI| image:: https://img.shields.io/pypi/v/podaacpy.svg?maxAge=2592000?style=plastic
   :target: https://pypi.python.org/pypi/podaacpy
.. |documentation| image:: https://readthedocs.org/projects/podaacpy/badge/?version=latest
   :target: http://podaacpy.readthedocs.org/en/latest/
.. |Travis| image:: https://img.shields.io/travis/nasa/podaacpy.svg?maxAge=2592000?style=plastic
   :target: https://travis-ci.org/nasa/podaacpy
.. |Coveralls| image:: https://coveralls.io/repos/github/nasa/podaacpy/badge.svg?branch=master
   :target: https://coveralls.io/github/nasa/podaacpy?branch=master
.. |Requirements Status| image:: https://requires.io/github/nasa/podaacpy/requirements.svg?branch=master
   :target: https://requires.io/github/nasa/podaacpy/requirements/?branch=master
.. |Anaconda-Server Version| image:: https://anaconda.org/conda-forge/podaacpy/badges/version.svg
   :target: https://anaconda.org/conda-forge/podaacpy
.. |Anaconda-Server Downloads| image:: https://anaconda.org/conda-forge/podaacpy/badges/downloads.svg
   :target: https://anaconda.org/conda-forge/podaacpy
.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png
.. |DeepSource| image:: https://static.deepsource.io/deepsource-badge-light.svg
    :target: https://deepsource.io/gh/nasa/podaacpy/?ref=repository-badge

"
189,nasa/terraform-aws-cumulus-thin-egress-app,HCL,
190,nasa/VIIRS-demo,,"The documentation for the VIIRS Demonstration Software is contained in
documentation/NTR_1571760412_2019-11-14.pdf, documentation/SectionI.pdf
and documentation/SectionII.pdf.

"
191,nasa/SIL,MATLAB,"# Overview

The Simulink Interface Layer (SIL) is an extention of the Simulink Coder generation tool which allows it to generate code which is compatible with the cFS ECI (External Code Interface). The ECI is a software abstraction layer which allows the interfacing of task/mission-specific code generated from Simulink (or other sources) to the Core Flight System (cFS) via a generic set of wrapper code. The SIL and ECI combined enables direct integration of code from the Simulink autocoding pipeline into the cFS without the need for interface code and allows access to cFS API's including table services, time services, the software bus, event services, and fault reporting. 

The SIL accomplishes this by extending Simulink's code generation pipeline to produce a description of the model's interfaces which is utilized by generic wrapper code to initialize the model and the appropriate cFS interfaces. The SIL simplifies the integration of code as a CFS application and eliminates the need for hand edits to generated code, allowing quicker integration of code and reducing the probability of human error in the integration process.

# Getting Started

The SIL is intended to be compatible with Matlab/Simulink 2017b and above on all Matlab-supported platforms. 

Usage of the SIL requires the following Matlab Toolboxes:

- Simulink
- Embedded Coder
- Simulink Coder
- Matlab Coder

Please see the Simulink integration guide (located in the [doc](doc/) directory) for specific instructions on generating and integrating SIL-compatible code.

# Testing [![build passing](https://travis-ci.com/nasa/SIL.svg?branch=master)](https://travis-ci.com/nasa/SIL/)

The SIL has been used with 2017b and 2018b and is currently tested with 2018b on Windows and Ubuntu. There are no known incompatibilities with supported versions or platforms, but they are not actively tested. 

# Feedback

Please submit bug reports and feature requests via Github issues. Feature requests will be considered as they align with the requirements of NASA missions which are using the SIL and to the extent that they minimize deviation from standard Matlab/Simulink analysis/development workflows and features."
192,nasa/QuIP,C,"
QuIP - Quick Image Processing

2/9/2012	converting to git repositories
3/25/2016	installed on guthub

How to compile without optimization?

"
193,nasa/MCT-Plugins,Java,"MCT-Plugins
===========

Each plug-in may be built in one of two ways. If building MCT from source, these may be included alongside other projects and packaged by making appropriate changes to pom.xml within an assembly. As a convenience, ant scripts are included to build as stand-alone plugins. An existing platform build (such as the evaluation version) is required and may be specified as a property ""mct.dir"", i.e.:

ant -Dmct.dir=/Applications/MCT


Ancestor View:

A plug-in for viewing a graph of referencing components within MCT. Select ""Ancestor View"" to see a graph indicating which components (such as collections) refer to this component, with more information further up the tree.


**Chronology:**

A set of interfaces used for communicating time-stamped information between plug-ins. 


**Notebook:**

A plug-in for making and maintaining notes within MCT. Notes may be annotated with other objects, such as telemetry elements, by dragging and dropping them into the note's text field. Notes are also time-stamped, so they can be viewed in time-enabled views (such as timelines). Depends upon Chronology.


**Timeline:**

A plug-in for viewing time-stamped information (notes, events) in a graphical timeline. The ""Timeline"" view shows event sequences, such as notebook entries, horizontally in relation to their occurrence in time. These events may be reorganized using drag and drop if the event sequence permits changes. Depends upon Chronology.


**Earth View:**

A plug-in for viewing state vectors relative to the Earth. To view, create an ""Orbit"" object from the Create menu. You may set initial vectors (units are km and km/s respectively, and position is relative to Earth's center; orbits are approximated at an accelerated rate and are not physically accurate). The resulting collection of state vectors can be viewed as spatial coordinates using the ""Orbit"" view.

Contains a true-color image of the Earth, owned by NASA, from the Visible Earth catalog. 

http://visibleearth.nasa.gov/view.php?id=73909

R. Stockli, E. Vermote, N. Saleous, R. Simmon and D. Herring (2005). The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observatory. Corresponding author: rstockli@climate.gsfc.nasa.gov


**Quickstart Persistence:**

Provides a simple in-memory persistence service populated with a small number of components and displays. To use, the compiled jar should be placed in the resources/platform of an MCT installation, in lieu of databasePersistence-1.1.0.jar. Note that the example plugin may need to be moved from resources/plugins to resources/platform as well, as this quickstart persistence service is pre-populated with example telemetry components.


**SatelliteTracker:**

This plug-in allows users to create satellites in MCT and track their orbits, in real-time, through various views:
 <UL>
   <LI> Satellite Orbits in 3D via the Earth View plug-in.
   <LI> Real-time locations on a 2D Mercator Projection (A new to MCT; created within this plug-in).
   <LI> All of the views that come standard with the core-MCT distribution (i.e.: MultiColumn View, Plots over time, Alpha, etc.).
 </UL> 

Along with adding an interesting data-source to MCT, the true purpose of this plug-in is that the SatelliteTracker serves as a concrete example on how to write a plug-in for MCT.  As SatelliteTracker is a flagship example for plug-in development, comments have been added throughout the source-files to guide the developer on the design-style and requirements put-forth by MCT's structure (and similarly, a section on the Wiki concerning developing-with MCT references the source contained within SatelliteTracker).

Similar to EarthView, this plug-in contains two true-color image of the Earth (one with and one without snow), owned by NASA, from the Visible Earth catalog.

http://visibleearth.nasa.gov/view.php?id=73909

R. Stockli, E. Vermote, N. Saleous, R. Simmon and D. Herring (2005). The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observatory. Corresponding author: rstockli@climate.gsfc.nasa.gov
"
194,nasa/ADOPT,Python,"# ADOPT

Although aviation accidents are rare, safety incidents occur more frequently and require a careful analysis to detect and mitigate risks in a timely manner. Analyzing safety incidents using operational data and producing event-based explanations is invaluable to airline companies as well as to governing organizations such as the Federal Aviation Administration (FAA) in the United States. However, this task is challenging because of the complexity involved in mining multi-dimensional heterogeneous time series data, the lack of time-step-wise annotation of events in a flight, and the lack of scalable tools to perform analysis over a large number of events. We propose a precursor mining algorithm: Automatic Discovery of Precursors in Time series data (ADOPT) that identifies events in the multidimensional time series that are correlated with the safety incident. Precursors are valuable to systems health and safety monitoring and in explaining and forecasting safety incidents. Current methods suffer from poor scalability to high dimensional time series data and are inefficient in capturing temporal behavior. We propose an approach by combining multiple-instance learning (MIL) and deep recurrent neural networks (DRNN) to take advantage of MIL's ability to learn using weakly supervised data and DRNN's ability to model temporal behavior. 


The objective of this project is to automate the analysis of flight safety incidents in a way that scales well and offers explanations. These explanations include:

* When the degraded states start to appear?
* What are the degraded states?
* What is the likelihood of the event is to occur?
* What corrective actions can be taken?

This project aims to:

* Create a novel deep temporal multiple-instance learning (DT-MIL) framework that combines multiple-instance learning with deep recurrent neural networks suitable for weakly-supervised learning problems involving time series or sequential data. 
* Provide a novel approach to explaining safety incidents using precursors mined from data.
* Deliver a detailed evaluation of the DT-MIL model using real-world aviation data and comparison with baseline models. 
* Perform a precursor analysis and explanation of high speed exceedance safety incident using flight data from a commercial airline







This repository contains the following files in its top level directory:

* [source](source)  
The source code of the repository, this includes the ADOPT model, GUI configuration tools, and a command line program that utilizes the model.

* [documentation](documentation)  
Documents describing how to configure and run the program, as well as how to interpret the results. 

* [datasets](datasets)  
A directory containing a sample dataset. Other datasets may also be added here by the user.

* [requirements.txt](requirements.txt)   
General module requirements for the program. A more specific requiremnts.txt can be found in [source](source).


* [ADOPT NASA Open Source Agreement.pdf](ADOPT%20NASA%20Open%20Source%20Agreement.pdf)  
Licensing for ADOPT
* [ADOPT Individual CLA.pdf](ADOPT%20Individual%20CLA.pdf)  
NASA Individual Contributor License Agreement
* [ADOPT Corporate CLA.pdf](ADOPT%20Corporate%20CLA.pdf)   
NASA Corporate Contributor License Agreement




## Contact Info

NASA Point of contact: Nikunj Oza <nikunj.c.oza@nasa.gov>, Data Science Group Lead.

For questions regarding the research and development of the algorithm, please contact Bryan Matthews <bryan.l.matthews@nasa.gov>, Senior Research Engineer.

For questions regarding the source code, please contact Daniel Weckler <daniel.i.weckler@nasa.gov>, Software Engineer.


## Copyright and Notices

Notices:

Copyright ¬© 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

"
195,nasa/utm-docs,Shell,"# The UTM Documentation Repository

This repository is for maintaining publicly available NASA UTM documentation.  Read on for further details on why this repo exists.

## Repository Status
As of the time of this edit (21 Mar 2018), this repo is in a formation stage.  It should NOT be used as 'truth' for the state of UTM documentation by any entity at this time.

The repository will not ever be a comprehensive site for all UTM documentation.  There are many NASA UTM documents that are not yet ready for public release.  As those mature to a state where public release is reasonable, we will endeavor to include them here if they add to understanding the core UTM System concepts and implementation.

## Audience
The primary audience for this repository are current NASA UTM partners or those partners that are currently in the on-boarding process.  There may be value to others as well.

## Externally Available Documentation Collections
In this section we link to important documentation already available elsewhere.

| External location        | Description |
| ------------------------ | ------------- |
| [NASA UTM Publication Repository](https://utm.arc.nasa.gov/documents.shtml) | This the full repository for all conference publications, NASA technical memos, and similar documents. We'll try to pull some of the key docs directly into this repository. |
| [NASA UTM APIs](https://github.com/nasa/utm-apis) | A github repository representing the truth in terms of the current state of NASA APIs. |
| [SwaggerHub API Documentation](https://app.swaggerhub.com/search?owner=utm)  | A convenience view of the NASA UTM APIs. *NOTE THAT THE GITHUB REPO IS TRUTH FOR THE APIs*. It is possible for the SwaggerHub documentation and our github API repo to drift. Developers must always reference the github repo for building compliant systems. Swagger hub is nice for navigating and discussing the API with others. | 

## Primary Concept Documents
| Document        | Pub Date | Description |
| ------------------------ | --- | ------------- |
| [UTM: Enabling Low-Altitude Airspace and UAS Operations](UTM-Original-TM-20140013436.pdf) | 2014 April | The original NASA Technical Memo describing UTM. |
| [Unmanned Aircraft System Traffic Management (UTM) Concept of Operations](Aviation2016_UTMConOps_AsPublished_v2.pdf) | 2016 June | The first version of the UTM ConOps. |

## Documentation Supporting Software Development

## Working Group Documents
This section will contain publicly-releaseable documents that originated from within each working group within NASA UTM.  Again, we emphasize that these document lists do not represent the depth and breadth of work since not all documents are able to be publicly released.  We will endeavor to keep this list up to date, but also encourage folks to submit a github issue if you think something is missing or have a suggestion for a helpful doc to include.

The structure of the working groups is somewhat driven by the [NASA-FAA Research Transition Team for UTM](https://www.faa.gov/uas/research/utm/) organization structure.  Please see the [UTM RTT Plan](FAA_NASA_UAS_Traffic_Management_Research_Plan.pdf) for more details on the Research Transition Team.

### Concepts Working Group
The CWG will define the concept of UTM in terms of overall conceptual principles and
assumptions, including those associated with operations, supporting architecture,
information flows and exchanges, and FAA and UAS operator roles and responsibilities.
This scoping and definition will: (1) ensure consistent messaging of a coordinated
FAA/NASA view of UTM; (2) guide the efforts of other UTM RTT working groups; and
(3) support the development of the UTM pilot program.

### Data and Information Architecture Working Group
The objective of the DWG is to identify and collaboratively research and develop
technical capabilities for the data/information exchange needed across stakeholders to
support UAS operations that meet NAS service expectations. The data exchange and
information architecture subgroup will work in conjunction with the Concepts &
Scenarios, SAA, and Communications & Navigation subgroups to identify the data
exchange and information gaps and/or deltas associated with the UTM concepts,
strategies, and system capabilities to support safe expansion of the UAS operating
envelope across the NAS structure.

| Document        | Pub Date | Description |
| ------------------------ | --- | ------------- |
| [UAS Reports (UREPs): Enabling Exchange of Observation Data Between UAS Operations](Rios_NASA-TechMemo_219462_UTM_UREP_20170214.pdf) | 2017 Feb | Description of the UREP concept and data schema. |
| [UTM Data Working Group Demonstration 1: Final Report](Rios_NASA-Tech-Memo-2017-219494v2.pdf) | 2017 April | Description and results from a collaborative simulation in support of TCL2. |


### Sense and Avoid Working Group
The objective for this subgroup is to explore operator solutions to ensure that unmanned
aircraft do not collide with other aircraft (unmanned or manned).

### Communications and Navigation Working Group
The objective for this subgroup is to explore operator solutions to ensure that UA are
under operational control of the pilot (to the degree appropriate to the scenario) and
remain within a defined area (around a planned trajectory or as a defined area).
"
196,nasa/PointCloudsVR,C++,"# PointCloudsVR
PointCloudsVR is a C++ Windows 10 desktop application that displays Point Clouds in Virtual Reality (VR) using [OpenVR](https://github.com/ValveSoftware/openvr) and [SteamVR](https://store.steampowered.com/steamvr).  It is built upon a number of open source libraries: [OpenSceneGraph](http://www.openscenegraph.org/), [OpenFrames](https://github.com/ravidavi/OpenFrames), [liblas](https://liblas.org/), and several others, as described in the [PointCloudsVR User Manual](doc/PointCloudsVR_User_Manual.pdf).  It can display several Point Cloud file formats out of the box: .las, .ply, and others supported by OpenSceneGraph, as well as custom formats, as defined in the C++ code for this project.  It has various 3D analysis tools, such as the ability to draw 3D lines in space.  It can be used for science data analysis that uses Point Clouds for its data format.  PointCloudsVR has been used to display and analyze: Lidar of trees, Lidar of snow, Gaia (an ESA spacecraft) star data, and the solar wind flowing around Mars, all in Virtual Reality.  The user may navigate 3D space using one's VR headset and controllers. PointCloudsVR has been tested with both the HTC Vive / Vive Pro, and Oculus Rift.

### Build and Run Instructions
See [PointCloudsVR User Manual](doc/PointCloudsVR_User_Manual.pdf).  The manual also shows 2 screenshots of different applications (Gaia star data, and an .las file showing a tree) toward the end of the document.

### Copyright Notice
Copyright ¬© 2019 United States Government as represented by the Administrator of
the National Aeronautics and Space Administration. No copyright is claimed in the
United States under Title 17, U.S. Code. All Other Rights Reserved.

### Contact Information
    Thomas Grubb
    NASA Goddard Space Flight Center
    Code 587
    Greenbelt, MD 20771
    
    Thomas.G.Grubb@nasa.gov

### License Information

This software is licensed under the [NASA Open Source Agreement](LICENSE.txt).

### Example of PointCloudsVR showing an .las Lidar file in VR:

![Sample VR Screenshot](data/images/Screenshot%202017-12-12%2016.21.59.png ""Sample VR Screenshot"")
"
197,nasa/Analysis_SDK,,
198,nasa/BrightComets,Python,"BrightComets Module
===================

The BrightComets Module detects bright comets in fits images taken by WISE
and in the future NEOCAM. These images can be downloaded from irsa.ipac.caltech.edu.

The repository may be downloaded and easily modified to detect objects in your own fits or jpg image dataset. E.g., detecting stars, galaxies, planets, or even non-astronomical data like street lights or people.

The code handles fits and jpg images. All region files must be compatible with SAO Image DS9 and have the .reg extension. 

WARNING: The code has been tested with an anaconda3 installation of python 3.6.5. It has not been tested with other installations of python

Authors: Nathan Blair, ncblair@me.com, Joe Masiero, Emily Kramer, Jana Chelsey, Adeline Paiment

Quick Installation for Comet Detection
--------------------------------------

Use this installation if you only want to use the package for detecting comets in wise images

Preferably, do this in a python 3.6.5 virtual environment. If you have anaconda3, run:
::
    conda create -n brightcomets python=3.6.5 anaconda
    source activate brightcomets

And when you are finished using the program, you can deactivate your environment by closing the terminal window or running:
::
    source deactivate

If you ever want to delete your virtual environment:
::
    conda remove --name brightcomets --all

In a terminal window:
**WARNING: MAKE SURE YOUR PATH HAS NO SPACES OR SPECIAL CHARACTERS**
::
    git clone https://github.com/nasa/BrightComets
    pip install -e BrightComets

Detection
---------

In a python environment (you may need to run python with the command ""pythonw"" if you get an error saying Python is not installed as a framework):

the detector can take in 12 micrometer wavelength 
range infrared images taken by WISE/NEOWISE by specifying 
that you want to look detect in band_3 band_3

.. code:: python 

    from brightcomets import pipeline
    pipeline.detect(""path/to/12um_fits_file"", im_type=""band_3"")

or, if you have access to all 4 bands

.. code:: python 

    from brightcomets import pipeline

    # takes a list of fits files, from low ir bands to high ir bands.
    # intended for bands 2-4 (i.e., the list should include bands 1-4 or 2-4)
    pipeline.detect([""path/to/low_band_fits_file"", ""path/to/next_band_fits_file"", 
                    ""path/to/next_band_fits_file"", ""path/to/high_band_fits_file""],
                    im_type=""composite"")


Try this out on the given example fits data:

.. code:: python

    # From BrightComets
    pipeline.detect(""brightcomets/WISE_data/00808a064/00808a064-w3.fits"", im_type=""band_3"", do_show=True)
    import os
    pipeline.detect([f for f in os.listdir(""brightcomets/WISE_data/00808a064"") if f.endswith("".fits"")], im_type=""composite"", do_show=True)


Long Installation for Custom Training/Retraining
------------------------------------------------

**WARNING: IMAGE AND REGION FILES POINTED AT BY THE RETRAINING SCRIPT MAY BE ALTERED OR DELETED BY THE PROGRAM. KEEP A COPY OF YOUR FILES ELSEWHERE ON YOUR COMPUTER**

**WARNING: IF YOU ALREADY INSTALLED A COPY OF BRIGHTCOMETS IN THE SHORT INSTALLATION, IT IS A GOOD IDEA TO REMOVE YOUR INSTALLATION BEFORE THE LONG INSTALLATION WITH ""pip uninstall brightcomets""**

0. (Optional, but highly recommended) Activate a virtual environment to install dependencies for the project. 
    If you have conda, you can run:
    ::
        conda create -n brightcomets python=3.6.5 anaconda
        source activate brightcomets
    If you do not have conda, consult an online tutorial, but make sure that python version is 3.6.5

    When you want to stop working on this project, don't forget to run
    ::
        source deactivate

    If you ever want to delete your virtual environment:
    ::
        conda remove --name brightcomets --all


1. Get the code, install requirements. 
    In a terminal window: 
    ::
        git clone https://github.com/joemasiero/BrightComets
        cd BrightComets
        pip install -r requirements.txt


2. Install the tf object_detection library in the BrightComets directory
    First, go to https://github.com/google/protobuf/releases and download protobuf-all-3.6.1.tar.gz
    ::
        # From the location where you downloaded protobuf (possibly Downloads)
        tar -xvf protobuf-all-3.6.1.tar.gz
        cd protobuf-3.6.1
        ./configure
        # This may take a while
        make
        sudo make install
        protoc --version # check installation worked
    Then, run the following commands:
    ::
        # From path/to/BrightComets
        git clone https://github.com/tensorflow/models.git
        cd models
        git checkout 3a05570f8d5845a4d56a078db8c32fc82465197f
        cd ..
        git clone https://github.com/cocodataset/cocoapi.git
        cd cocoapi/PythonAPI
        make
        cp -r pycocotools ../../models/research
        cd ../../models/research
        protoc object_detection/protos/*.proto --python_out=.

    More info about this step can be found here_:

    .. _here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md

3. Run the file_organization script. 
    This will make some compatibility changes to the object_detection library and also add a line to your ~/.bash_profile file so that the object_detection library can be properly imported. 
    ::
        # From BrightComets/brightcomets
        # cd ../../brightcomets
        python file_organization.py
    If this command gives you an error, you may not have a ~/.bash_profile file. And you will have to manually type the following line whenever you open a new terminal window and want to run the retraining script.
    ::
        # If the previous command gave an error
        # From BrightComets/models/research/
        export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
    Now, the library should be installed and prepared for retraining. 


4. Download `SAO Image DS9`_ if you do not already have it

    .. _SAO Image DS9: http://ds9.si.edu/site/Download.html

5. Compile training data
    **WARNING: IMAGE AND REGION FILES POINTED AT BY THE RETRAINING SCRIPT MAY BE ALTERED OR DELETED BY THE PROGRAM. KEEP A COPY OF YOUR FILES ELSEWHERE ON YOUR COMPUTER**

    Change to your BrightComets/brightcomets directory

    Compile a folder with .fits and .reg files in it. For example, take a look at the WISE_data folder. The files don't need to be structured in any specific way, however matching fits and regions files should have the same name.

    File naming conventions:

    A. Image files may either be of type .jpg or .fits. All .jpg files will be converted to .fits. 

    B. Image files will be resized to the shape 512 by 512. If they are not 512x512, they will be stretched to 512x512. 

    C. All image files should have unique names. 

    D. Regions are specified as .reg files and use the syntax conventions from SAO Image DS9 region files. They must have the .reg file extension. 

    E. All region files should correspond to a fits file by having the same name as that image file. 

    F. The dash (""-"") character is an important keyword
        i. They allow the user to specify that two images represent different channels of the same image. 

        ii. For example, the files image1-w1, image1-w2, and image1-w3 will all be placed in the image1 folder, and can be used to make a composite image with w1 representing the red channel, w2 representing the green channel, and w3 representing the blue channel. 

        iii. The program supports up to 4 channels of the same image. They should be named w1, w2, w3, and w4. 

        iv. When training, you will have the option to train on all channels separately, just channel 3 (w3), or a composite of the three highest channels. 

        v. If there are two dash (""-"") characters in the name of a file, the second dash and everything after it will be truncated. i.e. image1-w1-int1-abc.fits will be truncated to image1-w1 and placed into the w1 folder

        vi. A region file corresponding to the composite of images in a folder will have the extension -comp. For example, a file may be named image1-comp.reg

        vii. Be very careful when having dashes in your file names. They should only be used when followed by either w1, w2, w3, w4 or comp. And, they will signify that the images are different channels of the same image. 

        viii. When region and fits files are matched into folders, they are matched only by the uniqueness of the name before the first dash. Once the files have been organized, regions and fits files of corresponding channel will be matched during runtime. For example, image1-w2.fits will be matched with image1-w2.reg

    **What if I don't have region files?**

    You will be prompted upon running the retraining script to create your own regions

    **What if I have region files without corresponding fits files?**

    Those region files will be deleted. Make sure to keep a copy elsewhere on your computer! 

6. Use config.py to set hyperparameters for training. 
    A. Open BrightComets/brightcomets/config.py

    B. Open SAO Image DS9, and keep it open for the rest of the training process.

    C. Go to File > XPA > Information

    D. In the config.py file, copy the text after XPA_METHOD and set the variable FITS_XPA_METHOD = ""YOUR_XPA_METHOD""

    E. In the config.py file, set pyversion to the command that you type in the terminal to invoke python 3.6.5. This is probably just ""python"", but could also be ""pythonw"" or ""python3"", for example. This is necessary because the program makes system calls to run the training and evaluation scripts. 

    F. If you are using custom training data, set color_key to a dictionary describing how your annotations are labelled, or how you wish them to be labelled if they are not yet labelled. Use default_color_key as a reference. 

    G. Default image resizing is to 512x512. If you change the image_size parameter here, it will be changed everywhere for all networks. It is recommended that you do not change this parameter, as changing the image size has not been heavily tested. 

7. Run the retraining script
    In a terminal window: 

    This will display all the retraining options you have
    ::
        # From BrightComets/brightcomets
        python retrain.py -h

    If you get an error saying python is not installed as a framework, try
    ::
        # replace all future calls to python with pythonw
        pythonw retrain.py -h

        # Also, change your pyversion variable in the config.py file. 

    The command you will run will look something like
    ::
        # From BrightComets/brightcomets
        # Make sure to replace Your_Custom_Data folder with the path to your folder
        python retrain.py --im_type band_3 --update_records --data_folders Your_Custom_Data --retrain --train_iterations 5000 --classes comet

    This script does training, evaluation, and metrics all at once. 

    You can see the training and evaluation progress by going to a web browser while the script is running and searching **localhost:6006**

8. Once you are satisfied with your model, you can download it. 
    From the BrightComets directory
    ::
        # Inside the BrightComets Directory
        pip install -e .
    This will allow it be available everywhere on your computer. If you intend to make more changes locally. I recommend uninstalling it till you are finished making changes. Otherwise, you will have to update your installation every time you make changes for testing. 
    Uninstall with:
    ::
        pip uninstall brightcomets
    This will not get rid of your local copy of the repository. 

Objects with comets already annotated (and some stars, planets, and defects annotated):
These annotations are stored in regions-static and will always be checked. However, annotations given by the user will always be prioritized. 
comets = [""C/2006 W3"", ""C/2007 Q3"", ""65P"", ""29P"", ""30P"", ""81P"", ""116P"", ""10P"", ""118P"", ""P/2010 H2"", ""C/2007 N3""]
not_comets = [""mars"", ""jupiter"", ""alpha boo"", ""R dor""]

FAQ
===

**Can you train multiple different kinds of models?**

Yes! You can, for example a model that looks at band_3 stars, a model that looks at band_3 comets, a model that looks at composite comets. You cannot train two different band_3 comet models however. 

Note that the config.py file is global to all models, while each call to retrain.py will only retrain a single model. The retrain script may be slow as it preprocesses the images every time it is run.

**Will this program run on all operating systems?**

This module is built for mac. I make no gurantees that it will work on other operating systems. 

**My program isn't working. What do I do?**

Are you using the right version of python (3.6.5)? Do you have all necessary libraries installed (if not, install with pip)? Do you have your XPA_METHOD and pyversion properly set in your config file? Are your data files properly named, with dashes in the correct places? Did you reinstall the library with pip after making changes (You have to reinstall it every time you make changes locally). Make sure none of your filepaths have spaces or special characters like backslashes. 


Brief Folder and File Descriptions
==================================

1. BrightComets/references.md
    Some websites I referenced during the creation of the project, some comments point to these references. 
2. BrightComets/requirements.txt
    All dependencies. Install requirements with pip install -r requirements.txt
3. BrightComets/setup.py
    File that allows the project to be installed via pip. pip install -e BrightComets
4. BrightComets/brightcomets
    a. data
        Where tensorflow records and label files are stored for all neural networks. These files are binary files, not human readable, but store all fits and regions files after preprocessing. 
    b. master_data
        Fits and Regions files organized into test and train datasets
    c. models
        Tensorflow object detection models
        Check here for more models: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
        This is where all tensorflow computational graphs and weights are stored for each model you train. 
    d. regions-static
        All the regions that are provided upfront
    e. unused
        Files that I did not end up using but still have interesting code in them. The files are mostly intended to skirt os.system calls in the retraining script
    f. WISE_data
        An example of how training data can be organized.
    g. __init__.py
        A short file for making BrightComets a python package
    h. config.py
        Configuration file where the user sets training hyperparameters
    i. eval.py
        (Created by Google Tensorflow) Tensorflow object_detection file that calls does evaluation
    j. export_inference_graph.py
        (Created by Google Tensorflow) Tensorflow object_detection file that exports trained neural nets
    k. file_organization.py
        File that does all organization of training data, file movement, TFRecord creation, etc.
    l. image_methods.py
        File that has the main object detection algorithm, preprocessing algorithms, and fits/regions handling algorithms. 
    m. pipeline.py
        File that allows user to use the detection algorithm
    n. retrain.py
        File that allows the user to retrain and organize/annotate training data
    o. tests.py
        Mostly deprecated file with some tests, mostly for image_methods functions
    p. train.py
        (Created by Google Tensorflow) Tensorflow object_detection file that initiates neural network training, called by retrain.py
    q. utils.py
        static methods and utils
"
199,nasa/PyGNSS,Jupyter Notebook,
200,nasa/OpenSPIFe,Java,"## Scheduling and Planning Interface for Exploration
The OpenSPIFe user interface is designed to be a highly adaptable and user-customizable framework for viewing and manipulating plan and schedule data. In order to achieve this, SPIFe employs a composable plug-in architecture based on the open source Eclipse Rich Client Platform (RCP).

## Target Platform
Open SPIFe requires the Eclipse Rich Client Platform in order for it to run and includes third party software codes that are subject to the licenses and notices set forth below.
- Java SE 8 JDK
- Eclipse 4.3.2 (Modeling Tools)
- Eclipse Nebula features
- Eclipse Orbit third party libraries
- Eclipse RCP delta pack for multi-platform exports (only required to build multi-platform product artifacts)

## Links
- [Project Homepage][home]
- [Code Repository][repo]
- [Bug Tracker][bugs]

## How to submit patches
One or more of:

- [Fork the code](http://help.github.com/forking/) on GitHub and send a [pull request](http://github.com/guides/pull-requests).
- File them on the [bug tracker][bugs]
- Send them to the [mailing list][list]

If your contribution is significant, you may be asked to sign a contributor agreement, either as an [individual][clai] or as a [corporation][clac].


## Mailing List
- Send your messages to [open-spife@lists.nasa.gov][list].

## License

See the license files for the original and updated contributions.  The initial release of Open SPIFe to open source is given by the NASA Open Source Agreement and third-party licenses including Apache License 2.0, Eclipse Public License 1.0, Mozilla Public License 2.0, and GNU General Public License 3.0.

Copyright (c) 2019 United States Government as represented by the Administrator for The National Aeronautics and Space Administration.  All Rights Reserved.
 

[home]: https://github.com/nasa/OpenSPIFe/wiki
[repo]: https://github.com/nasa/OpenSPIFe
[bugs]: https://github.com/nasa/OpenSPIFe/issues
[list]: mailto:open-spife@lists.nasa.gov
[clac]: http://ti.arc.nasa.gov/m/project/nasa-vision-workbench/VW-CLA-Corp.pdf
[clai]: http://ti.arc.nasa.gov/m/project/nasa-vision-workbench/VW-CLA-Individual.pdf
"
201,nasa/openmct-tutorial,JavaScript,"# Open MCT Integration Tutorials

These tutorials will walk you through the simple process of integrating your telemetry systems with Open MCT.  In case you don't have any telemetry systems, we've included a reference implementation of a historical and realtime server.  We'll take you through the process of integrating those services with Open MCT.

## Tutorial Prerequisites

* [node.js](https://nodejs.org/en/)
    * Mac OS: We recommend using [Homebrew](https://brew.sh/) to install node.
    ```
    $ brew install node
    ```
    * Windows: https://nodejs.org/en/download/
    * linux: https://nodejs.org/en/download/
* [git](https://git-scm.com/)
    * Mac OS: If XCode is installed, git is likely to already be available from your command line. If not, git can be installed using [Homebrew](https://brew.sh/).
    ```
    $ brew install git
    ```
    * Windows: https://git-scm.com/downloads
    * linux: https://git-scm.com/downloads

Neither git nor node.js are requirements for using Open MCT, however this tutorial assumes that both are installed. Also, command line familiarity is a plus, however the tutorial is written in such a way that it should be possible to copy-paste the steps verbatim into a POSIX command line.

## Installing the tutorials

```
git clone https://github.com/nasa/openmct-tutorial.git
cd openmct-tutorial
npm install
npm start
```

This will clone the tutorials and install Open MCT from NPM.  It will also install the dependencies needed to run the provided telemetry server. The last command will start the server. The telemetry server provided is for demonstration purposes only, and is not intended to be used in a production environment.

At this point, you will be able to browse the tutorials in their completed state.  We have also tagged the repository at each step of the tutorial, so it is possible to skip to a particular step using git checkout.

eg.

```
git checkout -f part-X-step-N
```

Substituting the appropriate part and step numbers as necessary.

The recommended way of following the tutorials is to checkout the first step (the command is shown below), and then follow the tutorial by manually adding the code, but if you do get stuck you can use the tags to skip ahead. If you do get stuck, please let us know by [filing in issue in this repository](https://github.com/nasa/openmct-tutorial/issues/new) so that we can improve the tutorials.

All source files that you create while following this tutorial should be created in the `openmct-tutorial` directory, unless otherwise specified.

## Part A: Running Open MCT
**Shortcut**: `git checkout -f part-a`

We're going to define a single `index.html` page.  We'll include the Open MCT library, configure a number of plugins, and then start the application.

[index.html](https://github.com/nasa/openmct-tutorial/tree/part-b-step-1/index.html)
```html
<!DOCTYPE html>
<html>
<head>
    <title>Open MCT Tutorials</title>
    <script src=""node_modules/openmct/dist/openmct.js""></script>
    <script src=""lib/http.js""></script>
</head>
<body>
    <script>
        openmct.setAssetPath('node_modules/openmct/dist');
        openmct.install(openmct.plugins.LocalStorage());
        openmct.install(openmct.plugins.MyItems());
        openmct.install(openmct.plugins.UTCTimeSystem());
        openmct.time.clock('local', {start: -15 * 60 * 1000, end: 0});
        openmct.time.timeSystem('utc');
        openmct.install(openmct.plugins.Espresso());

        openmct.start();
    </script>
</body>
</html>
```

We have provided a basic server for the purpose of this tutorial, which will act as a web server as well as a telemetry source. This server is for demonstration purposes only. The Open MCT web client can be hosted on any http server. 

If the server is not already running, run it now -

```
npm start
```

If you open a web browser and navigate to http://localhost:8080/ you will see the Open MCT application running. Currently it is populated with one object named `My Items`. 

![Open MCT](images/openmct-empty.png)

In this tutorial we will populate this tree with a number of objects representing telemetry points on a fictional spacecraft, and integrate with a telemetry server in order to receive and display telemetry for the spacecraft.

# Part B - Populating the Object Tree
## Introduction
In Open MCT everything is represented as a Domain Object, this includes sources of telemetry, telemetry points, and views for visualizing telemetry. Domain Objects are accessible from the object tree 

![Domain Objects are accessible from the object tree](images/object-tree.png)

The object tree is a hierarchical representation of all of the objects available in Open MCT. At the root of an object hierarchy is a root object. For this tutorial, we are going to create a new root object representing our spacecraft, and then populate it with objects representing the telemetry producing subsystems on our fictional spacecraft.

## Step 1 - Defining a new plugin
**Shortcut:** `git checkout -f part-b-step-1`

Let's start by creating a new plugin to populate the object tree. We will include all of the code for this plugin in a new javascript file named `dictionary-plugin.js`. Let's first create a very basic plugin that simply logs a message indicating that it's been installed.

[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-2/dictionary-plugin.js)
```javascript
function DictionaryPlugin() {
    return function install() {
        console.log(""I've been installed!"");
    }
};
```
Note that when the `install` function is invoked, the [Open MCT API will be provided as the first parameter](https://github.com/nasa/openmct/blob/master/API.md#defining-and-installing-a-new-plugin). In this simple case we don't use it, so it has been left out.

Next, we'll update index.html to include the file:

[index.html](https://github.com/nasa/openmct-tutorial/blob/part-b-step-2/index.html)
```html
<!DOCTYPE html>
<html>
<head>
    <title>Open MCT Tutorials</title>
    <script src=""node_modules/openmct/dist/openmct.js""></script>
    <script src=""lib/http.js""></script>
    <script src=""dictionary-plugin.js""></script>
</head>
<body>
    <script>
        openmct.setAssetPath('node_modules/openmct/dist');
        openmct.install(openmct.plugins.LocalStorage());
        openmct.install(openmct.plugins.MyItems());
        openmct.install(openmct.plugins.UTCTimeSystem());
        openmct.time.clock('local', {start: -15 * 60 * 1000, end: 0});
        openmct.time.timeSystem('utc');
        openmct.install(openmct.plugins.Espresso());

        openmct.install(DictionaryPlugin());

        openmct.start();
    </script>
</body>
</html>
```

If we reload the browser now, and open a javascript console, we should see the following message 

```
I've been installed.
```

The process of opening a javascript console differs depending on the browser being used. Instructions for launching the browser console in most modern browsers are [available here](http://webmasters.stackexchange.com/a/77337).

In summary, an Open MCT plugin is very simple: it's an initialization function which receives the Open MCT API as the single argument.  It then uses the provided API to extend Open MCT.  Generally, we like plugins to return an initialization function so they can receive configuration.

[Learn more about plugins here](https://github.com/nasa/openmct/blob/master/API.md#plugins)

## Step 2 - Creating a new root node
**Shortcut:** `git checkout -f part-b-step-2`

To be able to access our spacecraft objects from the tree, we first need to define a root. We will use the Open MCT API to define a new root object representing our spacecraft. 

[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-3/dictionary-plugin.js)
```javascript
function DictionaryPlugin() {
    return function install(openmct) {
        openmct.objects.addRoot({
            namespace: 'example.taxonomy',
            key: 'spacecraft'
        });
    }
};
```

A new root is added to the object tree using the `addRoot` function exposed by the Open MCT API. `addRoot` accepts an object identifier - defined as a javascript object with a `namespace` and a `key` attribute. [More information on objects and identifiers](https://github.com/nasa/openmct/blob/master/API.md#domain-objects-and-identifiers) is available in our API.

If we reload the browser now, we should see a new object in the tree.
 
 ![Open MCT](images/openmct-missing-root.png)
 
Currently it will appear as a question mark with `Missing: example.taxonomy:spacecraft` next to it. This is because for now all we've done is provide an identifier for the root node. In the next step, we will define an __Object Provider__, which will provide Open MCT with an object for this identifier. A [basic overview of object providers](https://github.com/nasa/openmct/blob/master/API.md#object-providers) is available in our API documentation.

## Step 3 - Providing objects
**Shortcut:** `git checkout -f part-b-step-3`

Now we will start populating the tree with objects. To do so, we will define an object provider. An Object Provider receives an object identifier, and returns a promise that resolve with an object for the given identifier (if available).  In this step we will produce some objects to represent the parts of the spacecraft that produce telemetry data, such as subsystems and instruments. Let's call these telemetry producing things __telemetry points__. Below some code defining and registering an object provider for the new ""spacecraft"" root object:

[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-4/dictionary-plugin.js)
```javascript
function getDictionary() {
    return http.get('/dictionary.json')
        .then(function (result) {
            return result.data;
        });
}

var objectProvider = {
    get: function (identifier) {
        return getDictionary().then(function (dictionary) {
            if (identifier.key === 'spacecraft') {
                return {
                    identifier: identifier,
                    name: dictionary.name,
                    type: 'folder',
                    location: 'ROOT'
                };
            }
        });
    }
};

function DictionaryPlugin() {
    return function install(openmct) {
        openmct.objects.addRoot({
            namespace: 'example.taxonomy',
            key: 'spacecraft'
        });
        
        openmct.objects.addProvider('example.taxonomy', objectProvider);
    }
};
```

If we reload our browser now, the unknown object in our tree should be replaced with an object named ""Example Spacecraft"" with a folder icon. 

![Open MCT with new Spacecraft root](images/openmct-root-folder.png)

The root object uses the builtin type `folder`. For the objects representing the telemetry points for our spacecraft, we will now register a new object type.

Snippet from [dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-c/dictionary-plugin.js#L65-L69)
```javascript
openmct.types.addType('example.telemetry', {
    name: 'Example Telemetry Point',
    description: 'Example telemetry point from our happy tutorial.',
    cssClass: 'icon-telemetry'
});
```

Here we define a new type with a key of `example.telemetry`. For details on the attributes used to specify a new Type, please [see our documentation on object Types](https://github.com/nasa/openmct/blob/master/API.md#domain-object-types)
 
Finally, let's modify our object provider to return objects of our newly registered type. Our dictionary plugin will now look like this:

[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-4/dictionary-plugin.js)
```javascript
function getDictionary() {
    return http.get('/dictionary.json')
        .then(function (result) {
            return result.data;
        });
}

var objectProvider = {
    get: function (identifier) {
        return getDictionary().then(function (dictionary) {
            if (identifier.key === 'spacecraft') {
                return {
                    identifier: identifier,
                    name: dictionary.name,
                    type: 'folder',
                    location: 'ROOT'
                };
            } else {
                var measurement = dictionary.measurements.filter(function (m) {
                    return m.key === identifier.key;
                })[0];
                return {
                    identifier: identifier,
                    name: measurement.name,
                    type: 'example.telemetry',
                    telemetry: {
                        values: measurement.values
                    },
                    location: 'example.taxonomy:spacecraft'
                };
            }
        });
    }
};

function DictionaryPlugin() {
    return function install(openmct) {
        openmct.objects.addRoot({
            namespace: 'example.taxonomy',
            key: 'spacecraft'
        });
        
        openmct.objects.addProvider('example.taxonomy', objectProvider);
    }
};
```

Although we have now defined an Object Provider for both the ""Example Spacecraft"" and its children (the telemetry measurements), if we refresh our browser at this point we won't see any more objects in the tree. This is because we haven't defined the structure of the tree yet.

## Step 4 - Populating the tree
**Shortcut:** `git checkout -f part-b-step-4`

We have defined a root node in [Step 2](https://github.com/nasa/openmct-tutorial/blob/part-b-step-3/dictionary-plugin.js) and we have provided some objects that will appear in the tree. Now we will provide structure to the tree and define the relationships between objects in the tree. This is achieved with a __[Composition Provider](https://github.com/nasa/openmct/blob/master/API.md#composition-providers)__.

Snippet from [dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-c/dictionary-plugin.js#L36-L52)
```javascript
var compositionProvider = {
    appliesTo: function (domainObject) {
        return domainObject.identifier.namespace === 'example.taxonomy' &&
               domainObject.type === 'folder';
    },
    load: function (domainObject) {
        return getDictionary()
            .then(function (dictionary) {
                return dictionary.measurements.map(function (m) {
                    return {
                        namespace: 'example.taxonomy',
                        key: m.key
                    };
                });
            });
    }
};

openmct.composition.addProvider(compositionProvider);
```
A Composition Provider accepts a Domain Object, and provides identifiers for the children of that object. For the purposes of this tutorial we will return identifiers for the telemetry points available from our spacecraft. We build these from our spacecraft telemetry dictionary file.

Our plugin should now look like this:

[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-c/dictionary-plugin.js)
```javascript
function getDictionary() {
    return http.get('/dictionary.json')
        .then(function (result) {
            return result.data;
        });
}

var objectProvider = {
    get: function (identifier) {
        return getDictionary().then(function (dictionary) {
            if (identifier.key === 'spacecraft') {
                return {
                    identifier: identifier,
                    name: dictionary.name,
                    type: 'folder',
                    location: 'ROOT'
                };
            } else {
                var measurement = dictionary.measurements.filter(function (m) {
                    return m.key === identifier.key;
                })[0];
                return {
                    identifier: identifier,
                    name: measurement.name,
                    type: 'example.telemetry',
                    telemetry: {
                        values: measurement.values
                    },
                    location: 'example.taxonomy:spacecraft'
                };
            }
        });
    }
};

var compositionProvider = {
    appliesTo: function (domainObject) {
        return domainObject.identifier.namespace === 'example.taxonomy' &&
               domainObject.type === 'folder';
    },
    load: function (domainObject) {
        return getDictionary()
            .then(function (dictionary) {
                return dictionary.measurements.map(function (m) {
                    return {
                        namespace: 'example.taxonomy',
                        key: m.key
                    };
                });
            });
    }
};

function DictionaryPlugin() {
    return function install(openmct) {
        openmct.objects.addRoot({
            namespace: 'example.taxonomy',
            key: 'spacecraft'
        });
    
        openmct.objects.addProvider('example.taxonomy', objectProvider);
    
        openmct.composition.addProvider(compositionProvider);
    
        openmct.types.addType('example.telemetry', {
            name: 'Example Telemetry Point',
            description: 'Example telemetry point from our happy tutorial.',
            cssClass: 'icon-telemetry'
        });
    };
};
```

At this point, if we reload the page we should see a fully populated object tree. 

![Open MCT with spacecraft telemetry objects](images/telemetry-objects.png)

Clicking on our telemetry points will display views of those objects, but for now we don't have any telemetry for them. The tutorial telemetry server will provide telemetry for these points, and in the following steps we will define some telemetry adapters to retrieve telemetry data from the server, and provide it to Open MCT. 

# Part C - Integrate/Provide/Request Telemetry
**Shortcut:** `git checkout -f part-c`

Open MCT supports receiving telemetry by requesting data from a telemetry store, and by subscribing to real-time telemetry updates. In this part of the tutorial we will define and register a telemetry adapter for requesting historical telemetry from our tutorial telemetry server. Let's define our plugin in a new file named `historical-telemetry-plugin.js`

[historical-telemetry-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-d/historical-telemetry-plugin.js)
```javascript
/**
 * Basic historical telemetry plugin.
 */

function HistoricalTelemetryPlugin() {
    return function install (openmct) {
        var provider = {
            supportsRequest: function (domainObject) {
                return domainObject.type === 'example.telemetry';
            },
            request: function (domainObject, options) {
                var url = '/history/' +
                    domainObject.identifier.key +
                    '?start=' + options.start +
                    '&end=' + options.end;
    
                return http.get(url)
                    .then(function (resp) {
                        return resp.data;
                    });
            }
        };
    
        openmct.telemetry.addProvider(provider);
    }
}
```

The telemetry adapter above defines two functions. The first of these, `supportsRequest`, is necessary to indicate that this telemetry adapter supports requesting telemetry from a telemetry store. The `request` function will retrieve telemetry data and return it to the Open MCT application for display.

Our request function also accepts some options. Here we support the specification of a start and end date.

With our adapter defined, we need to update `index.html` to include it.

[index.html](https://github.com/nasa/openmct-tutorial/blob/part-d/index.html)
```html
<!DOCTYPE html>
<html>
<head>
    <title>Open MCT Tutorials</title>
    <script src=""node_modules/openmct/dist/openmct.js""></script>
    <script src=""lib/http.js""></script>
    <script src=""dictionary-plugin.js""></script>
    <script src=""historical-telemetry-plugin.js""></script>
</head>
<body>
    <script>
        openmct.setAssetPath('node_modules/openmct/dist');
        openmct.install(openmct.plugins.LocalStorage());
        openmct.install(openmct.plugins.MyItems());
        openmct.install(openmct.plugins.UTCTimeSystem());
        openmct.time.clock('local', {start: -15 * 60 * 1000, end: 0});
        openmct.time.timeSystem('utc');
        openmct.install(openmct.plugins.Espresso());

        openmct.install(DictionaryPlugin());
        openmct.install(HistoricalTelemetryPlugin());

        openmct.start();
    </script>
</body>
</html>
```

At this point If we refresh the page we should now see some telemetry for our telemetry points. For example, navigating to the ""Generator Temperature"" telemetry point should show us a plot of the telemetry generated since the server started running.

# Part D - Subscribing to New Telemetry
**Shortcut:** `git checkout -f part-d`

We are now going to define a telemetry adapter that allows Open MCT to subscribe to our tutorial server for new telemetry as it becomes available. The process of defining a telemetry adapter for subscribing to real-time telemetry is similar to our previously defined historical telemetry adapter, except that we define a `supportsSubscribe` function to indicate that this adapter provides telemetry subscriptions, and a `subscribe` function for subscribing to updates. This adapter uses a simple messaging system for subscribing to telemetry updates over a websocket. 

Let's define our new plugin in a file named `realtime-telemetry-plugin.js`.

[realtime-telemetry-plugin.js](https://github.com/nasa/openmct-tutorial/blob/master/realtime-telemetry-plugin.js)
```javascript
/**
 * Basic Realtime telemetry plugin using websockets.
 */
function RealtimeTelemetryPlugin() {
    return function (openmct) {
        var socket = new WebSocket(location.origin.replace(/^http/, 'ws') + '/realtime/');
        var listener = {};
    
        socket.onmessage = function (event) {
            point = JSON.parse(event.data);
            if (listener[point.id]) {
                listener[point.id](point);
            }
        };
        
        var provider = {
            supportsSubscribe: function (domainObject) {
                return domainObject.type === 'example.telemetry';
            },
            subscribe: function (domainObject, callback) {
                listener[domainObject.identifier.key] = callback;
                socket.send('subscribe ' + domainObject.identifier.key);
                return function unsubscribe() {
                    delete listener[domainObject.identifier.key];
                    socket.send('unsubscribe ' + domainObject.identifier.key);
                };
            }
        };
        
        openmct.telemetry.addProvider(provider);
    }
}
```

The subscribe function accepts as arguments the Domain Object for which we are interested in telemetry, and a callback function. The callback function will be invoked with telemetry data as they become available.

With our realtime telemetry plugin defined, let's include it from `index.html`.

[index.html](https://github.com/nasa/openmct-tutorial/blob/master/index.html)
```html
<!DOCTYPE html>
<html>
<head>
    <title>Open MCT Tutorials</title>
    <script src=""node_modules/openmct/dist/openmct.js""></script>
    <script src=""lib/http.js""></script>
    <script src=""dictionary-plugin.js""></script>
    <script src=""historical-telemetry-plugin.js""></script>
    <script src=""realtime-telemetry-plugin.js""></script>
</head>
<body>
    <script>
        openmct.setAssetPath('node_modules/openmct/dist');
        openmct.install(openmct.plugins.LocalStorage());
        openmct.install(openmct.plugins.MyItems());
        openmct.install(openmct.plugins.UTCTimeSystem());
        openmct.time.clock('local', {start: -15 * 60 * 1000, end: 0});
        openmct.time.timeSystem('utc');
        openmct.install(openmct.plugins.Espresso());

        openmct.install(DictionaryPlugin());
        openmct.install(HistoricalTelemetryPlugin());
        openmct.install(RealtimeTelemetryPlugin());

        openmct.start();
    </script>
</body>
</html>
```

If we refresh the page and navigate to one of our telemetry points we should now see telemetry flowing. For example, navigating to the ""Generator Temperature"" telemetry point should show us a plot of telemetry data that is now updated regularly.
"
202,nasa/georef,Python,"Installation
============

Requirements
~~~~~~~~~~~~

Our reference platform for GeoRef is Ubuntu Linux 14.04 LTS,
running Python 2.7.6 and Django 1.9.2.  For development we use Django's
built-in development web server MySQL database.  

We develop using a VagrantBox VM running a Ubuntu Linux inside a Mac OS X host machine.
Vagrant VM is strictly optional and only necessary if you are not running directly from a Ubuntu Linux Machine.

Our image view is rendered using the OpenSeadragon open source image viewer. (openseadragon.github.io/)

(Optional) Set up a Vagrant VM
~~~~~~~~~~~~~~~~~~~~
If you are running on a mac, we highly encourage you to use Vagrant to set up 
a Ubuntu Development Instance. Our set up script works best within the Vagrant 
environment running on Mac OSX.

Install VirtualBox. We have found that VirtualBox Version 4.3.10 works best with Vagrant.
We highly recommend you download VirtualBox 4.3.10.
Install the latest version of vagrant: ‚Äãhttp://www.vagrantup.com/downloads


Set Up an Install Location
~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's create a directory to hold the whole GeoRef installation
and capture the path in an environment variable we can use
in the instructions below::

  export GEOCAM_DIR=$HOME/projects/geocam # or choose your own
  mkdir -p $GEOCAM_DIR
  

Get the Source
~~~~~~~~~~~~~~

Check out our latest source revision with::

  cd $GEOCAM_DIR
  git clone https://github.com/nasa/georef_deploy.git


For more information on the Git version control system, visit `the Git home page`_.
You can install Git on Ubuntu with::

  sudo apt-get install git-all

.. _the Git home page: http://git-scm.com/


Run the Setup Script
~~~~~~~~~~~~~~~~~~~~~
The ""setup_site_vagrant.sh"" script initializes the vagrant box and it clones 
all the submodules that are needed::

    # go into the georef_deploy directory
    cd georef_deploy
    
    # if you are running inside a Vagrant VM do
    setup_site_vagrant.sh


If you are running directly on a Ubuntu Linux Machine, you can skip the above shell
script and run the following::
    sudo python $GEOCAM_DIR/georef_deploy/setup_site.py
    
    # You need to manually create couple symlinks if not running on vagrant
    sudo ln -s /home/geocam/georef_deploy georef_deploy
    sudo ln -s gds/georef/ georef


Override settings.py
~~~~~~~~~~~~~~~~~~~~~~~

In the ``settings.py`` file, modify the ``DATABASES`` field to point to
your Django MySQL database::

    DATABASES = {
        'default': {
            'ENGINE': 'django.db.backends.mysql',
            'NAME': 'georef',
            'USER': 'root',
            'PASSWORD': 'vagrant',
            'HOST': '127.0.0.1',
            'PORT': '3306',
        }
    }


Setup the Data Directory
~~~~~~~~~~~~~~~~~~~~~~~~~~
You must manually create the data directory and its sub folders. GeoRef will 
write the image tiles to this directory.

1. Create a data directory
    ``mkdir $GEOCAM_DIR/georef/data``
2. Create the overlays directory
    ``mkdir -p $GEOCAM_DIR/georef/data/geocamTiePoint/overlay_images``
3. Set the permissions
    ``chmod -R 777 $GEOCAM_DIR/georef/data``


Setup GeoRef
~~~~~~~~~~~~

If your development environment is set up inside Vagrant, cd into the georef_deploy 
directory and do::
    vagrant ssh
And then run the following commands.


You must create the following directory and files::

 # If you are not using Vagrant, do
     mkdir -p $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/ & touch $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/deepzoom.exception.log

 # If you are using Vagrant, do
     # deepzoom directory needs to be owned by www-data. Put it in /home/vagrant so that it can be owned by www-data (and not by user)
     mkdir -p /home/vagrant/deepzoom 
     # create a symlink to deepzoom in the data dir
     ln -s /home/vagrant/deepzoom /home/vagrant/georef/data/deepzoom


Install Earth Engine by following the instructions below: 
    https://developers.google.com/earth-engine/python_install_manual


To install Python dependencies, render icons and collect media for the
server, run::

  cd $GEOCAM_DIR/georef_deploy/georef
  ./manage.py bootstrap --yes
  source $GEOCAM_DIR/georef_deploy/georef/sourceme.sh genSourceme genSettings
  ./manage.py collectstatic  
  ./manage.py prep

You'll need to source the ``sourceme.sh`` file every time you open a new
shell if you want to run GeoCam-related Python scripts such as starting
the Django development web server.  The ``sourceme.sh`` file will also
take care of activating your virtualenv environment in new shells (if
you were in a virtualenv when you ran ``setup.py``).


To initialize the database
    ``$GEOCAM_DIR/georef/manage.py makemigrations deepzoom``

    ``$GEOCAM_DIR/georef/manage.py makemigrations geocamTiePoint``

    ``$GEOCAM_DIR/georef/manage.py migrate``

Note that the path to manage.py may be different if you are running inside Vagrant.


Create a User Account  
~~~~~~~~~~~~~~~~~~~~~
User name and password are required to use GeoRef. To create one, do::
    
    ./manage.py createsuperuser

And follow the prompts.



Try It Out
~~~~~~~~~~
Now you're ready to try it out!  

Restart the Apache server ``sudo apachectl restart``

Point your browser to ‚Äãhttp://10.0.3.18/


.. o  __BEGIN_LICENSE__
.. o  Copyright (C) 2008-2010 United States Government as represented by
.. o  the Administrator of the National Aeronautics and Space Administration.
.. o  All Rights Reserved.
.. o  __END_LICENSE__
"
203,nasa/PyTDA,Jupyter Notebook,"PyTDA README
------------

This software provides Python functions that will estimate turbulence from
Doppler radar data. It is tested and working under Python 2.7-3.7.

For help see `HELP` file. For license see `LICENSE.md`.


Installation
------------

Install [Py-ART](https://github.com/ARM-DOE/pyart).

Run `python setup.py install` from the command line in the main PyTDA folder.


Using PyTDA
-----------
```
import pytda
```

PyTDA (among other modules) is discussed in this [conference presentation]
(https://ams.confex.com/ams/95Annual/webprogram/Paper262779.html)

See the notebooks directory for a demonstration Jupyter notebook.
"
204,nasa/UQ-Kernel-Mini-App,Python,"# uq kernel miniapp

A kernel for uncertainty quantification (UQ) codes at NASA.

# Getting Started

1) Clone or download this repository
* It is best to work with and modify the source code using Git ([install/update Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)). Using Git, clone the repository to your computer using `git clone https://github.com/nasa/UQ-Kernel-Mini-App.git`. Otherwise, the repository can be downloaded manually with the ""Clone or download"" button above. 
2) Add the repository to your Python path
* On Mac/Linux, add `export PYTHONPATH=$PYTHONPATH:Path/To/Your/Repo/` to your ~/.bashrc file (click [here](https://stackoverflow.com/questions/3402168/permanently-add-a-directory-to-pythonpath) for more details). For Windows, see this [link](https://stackoverflow.com/questions/3701646/how-to-add-to-the-pythonpath-in-windows-so-it-finds-my-modules-packages). This makes it possible to run the code in this repository from anywhere on your computer.
3) Make sure you have the required Python modules
* The repository uses a few external Python modules (currently just `numpy` and `scipy`). Either install them manually, or using pip: `pip install -r requirements.txt` or Anaconda: `conda install --yes --file requirements.txt` from the top directory of the repository.
4) Test that everything is working correctly
* Navigate to the `tests/` directory in the repository and type `python engine_check.py`. If things are working as expected, you should see an output similar to:
```
Engine Check Results:
  Output is Correct!
  Theoretical execution time was 9.40658189601207
  Actual execution time was 9.961344003677368
  Efficiency was 0.9443
```

# VT Capstone project

## Challenge
Devise  an  interface  (front end)  and  computational  engine  (back end)  
that  increases  the  usability, efficiency, and scalability of NASA 
open-source uncertainty quantification software.

### Sub-challenge 1: interface
 How can we best allow users from all disciplines to ‚Äúplug‚Äù their own 
 computational model into our Python-based UQ software?

#### Interface Requirements

Currently, NASA UQ software requires that a user develops a Python class
for the model they'd like to analyze that has a standardized interface. 
The primary requirement of this interface is that their Python class has
an `evaluate` function that receives an array of input parameters for their
model and returns an array of outputs:

 ```python
def evaluate(self, inputs):
	<Python code to implement an internal simulation or execute an external simulation>
	return output
```

Secondarily, all Python models must have a `cost` member variable that contains
the approximate time to execute the model. The base interface for a `UQModel` 
is defined in `uq_kernel/model.py`.  It illustrates these parts of a model that 
must be exposed to the UQ framework. 

#### Challenge problem focus

The challenge problem should focus on the most general type of `UQModel` 
and how best to streamline the process of creating one to use with 
NASA UQ software. This is the situation where a user has a simulation
executable (developed in any programming language) that can be run
on the command line by providing an input file and, upon completion, writes 
an output file with any simulation output data of interest. 
 
In this case, the `UQModel` a user develops to use the NASA UQ
software will always follow the general form:
```python
def evaluate(self, inputs):
	self.write_input_file_to_disc(inputs)
	self.execute_model()
	output = self.parse_output_file()
	return output
```
where the `write_input_file_to_disc` function generates a problem-specific 
input file based on the `inputs` array and stores it, `execute_model` will
make a system call in Python to execute the user's simulation for the 
input file written previously and write an output file when it has completed,
and `parse_output_file` will load the output file and extract the relevant 
outputs to return. 

In general, a user will know the command to execute their simulation at the
command line and the format of their input file along with where the particular 
parametersin the `inputs` array should go within it. From here, they need to 
wrap this functionality inside of a `UQModel` Python class. We want to make 
this process as easy as possible by capitalizing on any code, procedures, patterns
that are shared between most or all applications and simulations.
 
#### UQModel Example 

There is a simple example of a simulation and corresponding Python `UQModel`
in the `examples/spring_mass_uq_model/` to make this challenge more concrete.

<img src=""/imgs/spring_mass.png"" width=""200"">

The `spring_mass_simulation.py` implements a simple spring-mass system and 
calculates the maximum displacement in the system for given parameters like
mass and acceleration due to gravity. For our purposes, we will assume this
is a ""blackbox"" executable, we don't have access to the code inside,
we just know we can run it on the command line using:

`python spring_mass_simulation.py <name-of-input-file> <name-of-output-file>`

where the input file contains values for mass, stiffness, etc. and the output
file will contain a single number defining the maximum displacement. An example
input file is provided (`spring_mass_inputs.txt`) so that the code can be 
tested by typing `python spring_mass_simulation.py spring_mass_inputs.txt output.txt`.

Now let's say that the user wants to use NASA UQ software to study how 
uncertainty in the spring stiffness effects their prediction of the 
maximum displacement. They need to define a `UQModel` class with and
`evaluate` function whose `inputs` array contains one value for stiffness 
and the `output` array contains one value for the maximum displacement.
An example of such a class can be seen in the `spring_mass_UQ_model.py`.
A sample script that initializes and evaluates the model for a few
different stiffness values is provided in `run_spring_mass_uq_model.py`
for illustration.

How can we help a user produce code/classes like inside of 
`spring_mass_UQ_model.py` to ""plug"" their simulation into the NASA
UQ software? The user will have information about how to execute their
simulation from the command line, the structure of their input file, and the
names of pertinant inputs/outputs. Note that it is important to distinguish 
between the parameters that will change from simulation to simulation (stiffness 
in this example) versus those that will remain fixed (gravity, mass, etc.).
The main procedure of 1) writing input file, 2) executing model, and 3) parsing 
output file / returning output(s) of interest will generally be followed, but
these individual steps will require customization depending on the user's 
simulation, input/output files, parameters of interest, etc. 

 
### Implementation
A starting interface for a `UQModel` is defined in `uq_kernel/model.py`.  It 
illustrates the parts of a model that must be exposed to the UQ framework. 
Defining implementations of this interface to account for different possible 
models is one option to pursue this challenge.  Alternative solutions are 
encouraged as well.  Everything, including the basic interface in 
`uq_kernel/model.py` can be modified to suit the chosen approach.

### Sub-challenge 2: engine
**A load balancing issue for parallel computing:** How can we devise a strategy 
to execute a set of computational models with *varying run times* an arbitrary 
number of times each in an efficient and scalable manner.

#### Problem Definition
**Given**
 * A set of Python models (like the ones developed in sub-challenge 1)
 * The estimated cost (run time) for each model
 * The number of times to execute each model (with the inputs to the models 
 for each evaluation)
 * Number of processors to run on

**Determine**

A strategy for spreading the executions of all models across the processors 
such that all processors have a similar amount of work to do (minimize idle 
time for processors)

#### Implementation

Develop an engine function with an interface that accepts a list of `UQmodel` 
objects and a list of `numpy` arrays defining the inputs for each model,

<img src=""/imgs/inputs.png"" width=""600"">

note that in general the individual numpy arrays will have different lengths
(number of inputs). This function returns a list of numpy arrays defining
the outputs

<img src=""/imgs/outputs.png"" width=""600"">

where each array has been generated by evaluating the appropriate `UQmodel`,
for example: 

<img src=""/imgs/run_model.png"" width=""600"">

assuming that `model-1` has `N1` inputs to evaluate. 

If the approximate cost of each model is `C1`, ..., `CM`, then the total time 
to run all of the models on one processor is `T-serial = N1 * C1 + N2 * C2 + ... NM * CM`. 
The specific goal is to write an engine function that has an execution time that 
is as close to `T-parallel = T1 / P` as possible when run on `P` processors. An example of 
a serial engine function can be seen in the `tests/engine_check.py` in the 
`simple_serial_engine` function. 

The implementation of the engine is completely open; however, a solution that 
works in a distributed memory system is preferred. `mpi4py` is one example of a 
distributed processing tool that could be helpful, though others exist as well.

A script for checking implementations of the engine is included in the miniapp: 
`tests/engine_check.py`. The script runs the engine, checks its outputs, and 
computes its efficiency.  The script is configurable to test many different 
possibilities of model number, run times, etc.  Note that it is likely that the 
script will need to be modified to work in the context of the engine you 
develop.  It should prove useful, nonetheless.

#### Running a model in parallel with mpi4py

An example script demonstrating basic usage of `mpi4py` to evaluate a Python model
in parallel is given in `examples/parallel/run_model_with_mpi.py`. In order to run
this script, `mpi4py` must be installed, which requires a working installation of MPI on your computer (look for [online resources](https://mpi4py.readthedocs.io/en/stable/install.html) if there are issues). Once installed properly, the script can be run with `mpirun -np <number of processors> python run_model_with_mpi.py` or `mpiexec -n <number of processors> python run_model_with_mpi.py` depending on your version of MPI. 
"
205,nasa/Reinforcement-Learning-Benchmarking,Shell,"# Reinforcement Learning Benchmarks

This project provides scripts for running several OpenAI Baselines algorithms on all MuJoCo or Roboschool gym environments in order to compare algorithm performance.

## Prerequisites

The following Python packages are required:

[gym](https://github.com/openai/gym)

[roboschool](https://github.com/openai/roboschool) (Only required if you want to run Roboschool environments)

[baselines](https://github.com/openai/baselines) (Note: baselines must be installed from source.  A pip install will give you an outdated version)

matplotlib

argparse

os

The MuJoCo software, [mjpro150](https://www.roboti.us/index.html) is required to run the MuJoCo environments.  The MuJoCo submodule for gym is also required, which can be installed via

```bash
pip install gym[mujoco]
```

## How to run

Run the bash script by typing

```bash
./runExperiments.sh
```

The bash script accepts the following positional command line arguments:

1: Subdirectory to save training data in.  Default is data.

2: mujoco or roboschool - Choose which environment set to run.  Default is roboschool.

3: Number of timesteps to run each algorithm.  Defualt is 1000000.

4: Number of random seeds to run for each algorithm.  Default is 3.

For example, to save data to a subdirectory called ""test"", run the MuJoCo environments for 10000 timesteps with each algorithm, and run 5 different seeds for each algorithm, you would type

```bash
./runExperiments.sh test mujoco 10000 5
```

## Viewing results

Plots of all learning curves are saved as pdf files in a subfolder inside the data folder called plots.  These plots show the episode reward vs. number of timesteps, and the smoothing function from the baselines plot utility is applied.  The shaded regions represent the variations across all seeds, and the solid line represents the mean across all seeds. 

---

__**Notices**__:

_Copyright **2019** United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved._

__**Disclaimers**__

_No Warranty_: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
_Waiver and Indemnity_:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
206,nasa/vsm,Python,
207,nasa/Giovanni,Perl,"# Giovanni:     The Bridge Between Data And Science 
https://giovanni.gsfc.nasa.gov/giovanni/

Giovanni is an online (Web) environment for the display and analysis of geophysical parameters in which the provenance (data lineage) can easily be accessed. 

GES DISC is making our Giovanni (currently at version 4.31)  code base available on github

<h4> Getting Started with the code </h4>
At the <a href=""https://disc.gsfc.nasa.gov/"">GES DISC</a>, development of Giovanni is split into several repositories:

Each subdirectory has either a Makefile or Perl Makefile.PL
<br/><b>agiovanni</b>
<br/><b>agiovanni_algorithms</b>
<br/><b>agiovanni_data_access</b>
<br/><b>agiovanni_www</b> 
<br/><b>agiovanni_shapes</b>
<br/><b> agiovanni_giovanni</b><br/>
<br/><b>AESIR</b><br/>refers to Giovanni's variable metadata database in SOLR. Giovanni's earth science data file database is <a href=""https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository"">CMR</a>

<br/><b>agiovanni/Dev-Tools/other/rpmbuild</b><br/> Contains  a build script and RPM spec file that gives an indication as to Giovanni's software dependencies.


<b>Disclaimer:We will update the software but not maintain the pull requests.</b>

<br/>Direct comments and questions to the Giovanni Development Team at: <b>gsfc-help-disc@lists.nasa.gov</b>

<br/>To give more indication of Giovanni's dependencies:
<br/>Giovanni is powered by:
<br/><a href=""http://nco.sourceforge.net/"">NCO netCDF Operator</a>
<br/><a href=""https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository"">CMR Common Metadata Repository</a>
<br/><a href=""http://developer.yahoo.com/yui/"">YUI</a>
<br/><a href=""http://openlayers.org/"">OpenLayers</a>
<br/><a href=""http://www.mapserver.org/ogc/"">MapServer</a>
<br/><a href=""http://opendap.org/"">OPeNDAP</a>





"
208,nasa/api-docs-stage,,
209,nasa/gen_msgids,C,"# Core Flight System : Framework : Tool : Generate Message Ids

This repository contains NASA's Generate Message Ids Tool (gen_msgids), which is a framework component of the Core Flight System.

This lab application is a ground utility to Generate Message Ids used by cFE for cFS. It is intended to be located in the `tools/gen_msgids` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.

## Release Notes

gen_msgids version 1.0a is released as part of cFE 6.6.0a under the Apache 2.0 license, see [LICENSE](LICENSE-18128-Apache-2_0.pdf).

## Known issues

This ground utility has not been updated to use the cmake framework.  The include path defined in Makefile will likely need to be updated for distributions.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.

Official cFS page: http://cfs.gsfc.nasa.gov

"
210,nasa/meshNetwork,C++,"# meshNetwork

The Mesh Network Communication System is a peer-to-peer communication network architecture that enables communication between network nodes of various types.  The initial primary goal of the system was to enable communication between small formations of cubesats or other small satellites, but the basic mesh architecture is applicable to data exchange between network assets of any type.  The system has been flight tested on formations of small unmanned aerial systems (sUAS) and shown to provide low latency data throughput for dynamic flight environments.

### Documentation

The full documentation is in the doc/build/html directory.

### License

The Mesh Network software is released under the NASA Open Source Agreement Version 1.3 [license](LICENSE).


### Python dependencies
pyserial
crcmod
"
211,nasa/GTM_DesignSim,MATLAB,"#GTM_DesignSim: The Generic Transport Model

The GTM_DesignSim is a batch simulation intended for design and analysis of flight control laws.  It models the
dynamics of a 5.5% scale model of a generic transport aircraft.  It was developed to allow fault accomodating control
algorithms to be developed and refined in simulation before being tested on an experimental subscale model.  

See:

Cunningham, K., Cox, D. E., Murri, D. G., and Riddick, S. E., *‚ÄúA Piloted Evaluation of Damage Accommodating Flight Control Using a Remotely Piloted Vehicle,‚Äù* **AIAA-2011-6451**, AIAA Guidance, Navigation, and Control Conference and Exhibit, Portland Oregon, August 2011.""   

and the references therein for a summary of the research.  

The simulation, however, was released as open-source and has been found useful in many other applications where a
non-linear large-envelope flight dynamics simulation is required.

"
212,nasa/MLMCPy,Python,"MLMCPy - **M**ulti-**L**evel **M**onte **C**arlo with **Py**thon
===================================================================

<a href='https://travis-ci.com/nasa/MLMCPy'><img src='https://travis-ci.com/nasa/MLMCPy.svg?branch=master' alt='Build Status' /></a> <a href='https://coveralls.io/github/lukemorrill/MLMCPy?branch=master'><img src='https://coveralls.io/repos/github/lukemorrill/MLMCPy/badge.svg?branch=master' alt='Coverage Status' /></a>

General
--------

MLMCPy is an open source Python implementation of the Multi-Level Monte Carlo (MLMC) method for uncertainty propagation. Once a user defines their computational model and specifies the uncertainty in the model input parameters, MLMCPy can be used to estimate the expected value of a quantity of interest to within a specified precision. Support is available to perform the required model evaluations in parallel (if mpi4py is installed) and extensions of the MLMC method are provided to calculate more advanced statistics (e.g., covariance, CDFs). 

Dependencies
--------------

MLMCPy is intended for use with Python 2.7 and relies on the following packages:

* numpy
* scipy
* mpi4py (optional for running in parallel)
* pytest (optional for running unit tests)

A requirements.txt file is included for easy installation of dependecies with pip:

```
pip install -r requirements.txt
```

Example Usage
---------------

```python
'''
Simple example of propagating uncertainty through a spring-mass model using MLMC. 
Estimates the expected value of the maximum displacement of the system when the spring 
stiffness is a random variable. See the /examples/spring_mass/from_model/ for more details.
'''

import numpy as np
import sys

from MLMCPy.input import RandomInput
from MLMCPy.mlmc import MLMCSimulator

# Add path for example SpringMassModel to sys path.
sys.path.append('./examples/spring_mass/from_model/spring_mass')
import SpringMassModel

# Step 1 - Define random variable for spring stiffness:
# Need to provide a sampleable function to create RandomInput instance in MLMCPy
def beta_distribution(shift, scale, alpha, beta, size):
    return shift + scale*np.random.beta(alpha, beta, size)

stiffness_distribution = RandomInput(distribution_function=beta_distribution,
                                     shift=1.0, scale=2.5, alpha=3., beta=2.)

# Step 2 - Initialize spring-mass models. Here using three levels with MLMC.
# defined by different time steps
model_level1 = SpringMassModel(mass=1.5, time_step=1.0)
model_level2 = SpringMassModel(mass=1.5, time_step=0.1)
model_level3 = SpringMassModel(mass=1.5, time_step=0.01)
models = [model_level1, model_level2, model_level3]

# Step 3 - Initialize MLMC & predict max displacement to specified error (0.1).
mlmc_simulator = MLMCSimulator(stiffness_distribution, models)
[estimates, sample_sizes, variances] = mlmc_simulator.simulate(epsilon=1e-1)

```

Getting Started
----------------
MLMCPy can be installed via pip from [PyPI](https://pypi.org/project/MLMCPy/):

```
pip install mlmcpy
```

MLMCPy can also be installed using the `git clone` command:

```
git clone https://github.com/nasa/MLMCPy.git
```

The best way to get started with MLMCPy is to take a look at the scripts in the examples/ directory. A simple example of propagating uncertainty through a spring mass system can be found in the ``examples/spring_mass/from_model`` directory. There is a second example that demonstrates the case where a user has access to input-output data from multiple levels of models (rather than a model they can directly evaluate) in the ``examples/spring_mass/from_data/`` directory. For more information, see the source code documentation in ``docs/MLMCPy_documentation.pdf`` (a work in progress).

Tests
------
The tests can be performed by running ""py.test"" from the tests/ directory to ensure a proper installation.

Developers
-----------

UQ Center of Excellence <br />
NASA Langley Research Center <br /> 
Hampton, Virginia <br /> 

This software was funded by and developed under the High Performance Computing Incubator (HPCI) at NASA Langley Research Center. <br /> 

Contributors: James Warner (james.e.warner@nasa.gov), Luke Morrill, Juan Barrientos


License
--------

Copyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
 
"
213,nasa/astrobee_gds,Java,"# Astrobee Ground Data System - Control Station software
Astrobee is a free-flying robot designed to operate as a payload inside the International 
Space Station. (Source code for Astrobee Flight software is available
 [here](https://github.com/nasa/astrobee))

The **Astrobee Control Station** is an Eclipse RCP application that can command and monitor up to three 
Astrobee robots or simulators. Astrobees are commanded via prewritten 
 **Plans**, or manually via single commands in **Teleoperate** mode.  Plans consist 
 of locations called Stations, tasks done at each Station, and Segments that connect the Stations.
  The Control Station also commands the Astrobee Docking Station to wake up Astrobees that are hibernating.

The Astrobee Control Station is an extension of the Visual Environment for Remote Virtual Exploration (VERVE) that has been customized to operate the Astrobee robot on the International Space Station (ISS). 

Astrobee Control Station Copyright ¬© 2019, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.

Verve Copyright ¬© 2011, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.

The Astrobee Control Station platform is licensed under the Apache License, Version 2.0 (the 
""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. 
 
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

Astrobee Control Station includes a number of third party open source software listed below.  Find the complete listing of third-party software notices and licenses, see the separate ‚ÄúAstrobee Control Station Listing of Notices/Licenses, Including Third Party Software‚Äù pdf document in the LICENSE folder.
1.	A) Eclipse Core Runtime, B) Eclipse Core Filesystem, C) Eclipse Modeling Framework (EMF) ECore, and D) Eclipse Modeling Framework XMI, http://www.eclipse.org  
2.	Ardor3D , https://github.com/Renanse/Ardor3D  
3.	JOGL (Java OpenGL), http://jogamp.org/ 
4.	Apache Log4j, http://logging.apache.org/log4j/ 
5.	Codehaus Jackson https://github.com/codehaus/jackson  


The **Overview** tab summarizes the states of known Astrobees, and sends wake, grab control, and hibernate commands. The **Guest Science** tab sends Plans and some teleoperate commands to as many as three Astrobees simultaneously. The **Run Plan** and **Teleoperate** tabs allow detailed control and monitoring of a single Astrobee. These four tabs are also available to astronauts as the Crew Control Station.

The Engineering Control Station includes six additional tabs. The **Plan Editor** provides 
a graphical interface to create a Plan for one Astrobee. The **Advanced Guest Science** tab,
 like the simplified Guest Science tab available to crewmembers, commands and monitors three 
 Astrobees simultaneously, but it also allows the user to customize commands and to see Guest
  Science telemetry in greater detail. The **Advanced** and **Advanced 2** tabs display 
  detailed engineering telemetry from one Astrobee, including faults, operating limits, 
  power and disk usage, etc. The **Modeling** tab provides a graphical interface to edit 
  keepout/keepin files, which are sent to the robot and displayed in the Control Station 3d 
  view, and handrail files, which position handrail models within the Control Station 3d 
  view. The Modeling tab also includes a CSV to Fplan Converter to facilitate the construction 
  of repetitive plans with known coordinates. The **Debugging** tab displays engineering data 
  helpful for debugging DDS communication with the robot.

[Install instructions](docs/SETUP.md)

[Usage instructions and documentation](docs/USAGE.md)
"
214,nasa/conduit_cms,,"# conduit_cms

Source code will be checked in soon...

# README

Detailed design documentation is available on the wiki at https://wiki.earthdata.nasa.gov/display/EDDEV/Conduit.

Database seeds for development are in db/seeds/development.rb.  Test pages for the following rake tasks(db:reset, edtopnav:ingest) 
will create pages in ""TEST"" project.

Run `rake db:reset` to load the test data.

Run `rake edtopnav:ingest` to create and load pages and redirects required for conduit web Top Navigation.  
This is a required step after every database reset.  

Run `rake edtopnav:delete` to delete pages and redirects required for conduit web Top Navigation.

Page and redirect information required for ED top navigation is configured in ""spec/fixtures/ed_top_navigation.json"".  
Any changes to the json file should be preceded by  running `rake edtopnav:delete` 
After updating the json file run  `rake edtopnav:ingest`


By default, this will make the local workstation user an admin in the app; if
your local username does not match your AUID, you can override this by
setting the AUID_USER environment variable. For example:

    AUID_USER=my_auid rake db:reset

You could also add `export AUID_USER=foo` to your .bash_profile to make this
change permanently.

Conduit requires PostgreSQL 9.2 or above (and will soon require 9.3 or above).

##Testing Launchpad Authentication in Dev Environment

To test authentication with Launchpad, run off any of these instead of the usual (localhost:3000):
	https://conduit.dev.earthdata.nasa.gov
	https://conduit.sit.earthdata.nasa.gov
	https://conduit.uat.earthdata.nasa.gov
	https://conduit.earthdata.nasa.gov

Add this line your /etc/hosts file depending on the environment you're currently testing.

```
127.0.0.1	conduit.dev.earthdata.nasa.gov
```

You'll also need to alter your nginx.conf file. Add

```
	ssl_certificate /usr/local/etc/nginx/cert.crt;
    ssl_certificate_key /usr/local/etc/nginx/cert.key;

    server {
        listen 443 ssl;

        server_name conduit.dev.earthdata.nasa.gov; # Change this line for a different environment

        ssl on;
        ssl_session_cache shared:SSL:1m;
        ssl_session_timeout 5m;
        ssl_ciphers HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers on;
        location / {
            root html;
            index index.html index.htm;

            proxy_pass http://conduit.dev.earthdata.nasa.gov:3000; # Change this line for a different environment

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
```
To start the server run ""rails s -b 0.0.0.0"" then go to https://conduit.dev.earthdata.nasa.gov

If you change the environment you're testing you'll have to update the following values in the application.yml file.
	SAML_SP_ISSUER_BASE
	SAML_SP_ISSUER
	SAML_SP_ACS_URL


For details about custom fields in pages that may need processing before rendering
see [API Documentation](docs/api.md)


"
215,nasa/PyAMPR,Jupyter Notebook,"***************************
IF YOU ARE USING PYAMPR FOR IPHEX DATA, GO BACK TO THE GHRC SERVER AND GET THE
LATEST VERSION OF THE DATASET, AS WE HAVE FIXED THE 37 GHZ CHANNEL A AND B SWAP ISSUE.
***************************

Title/Version
-------------
Python AMPR Data Toolkit (PyAMPR) v1.7.1 
Last changed 08/07/2019  


Lead Author
-----------
Timothy Lang  
NASA MSFC  
timothy.j.lang@nasa.gov  
(256) 961-7861  


Contributing Authors
--------------------
Brent Roberts  
NASA MSFC  
jason.b.roberts@nasa.gov  
(256) 961-7477  


Overview
--------
The Advanced Microwave Precipitation Radiometer (AMPR) is an airborne 
passive microwave radiometer managed by NASA Marshall Space Flight Center.
Download AMPR data from http://ghrc.nsstc.nasa.gov.
AMPR brightness temperature data from NASA field projects
are in ASCII or netCDF format. This python script defines a class that will 
read in single file from an individual aircraft flight and pull out
timing, brightness temperatures from each channel, geolocation, and
other information and store them as attributes using numpy 
arrays of the appropriate type. The file is read and the data are populated when
the class is instantiated with the full path and name of an AMPR file.
Numerous visualization methods are provided, including track plots,
strip charts, and Google Earth KMZs. In addition, polarization
deconvolution is available.


Installation and Use
--------------------
Dependencies: Python 2.7 thru 3.7,  `numpy`,  `matplotlib`,  `cartopy`,
              `os`,  `time`,  `simplekml`,  `datetime`,  `calendar`, 
              `codecs`,  `gzip`,  `netCDF4`
Most of these are provided with standard Python distributions.
You may need to install `cartopy` via your Python distribution's
package manager. The `simplekml` package can be found [here.](https://pypi.python.org/pypi/simplekml/ )

In the same directory as this `README` is `setup.py`, to install this
package enter the following command at the prompt:
```
python setup.py install
```

Then to import, in your python program include:
```
import pyampr
```

To read an AMPR TB file type:
```
ampr_data = pyampr.AmprTb('FILE_NAME_HERE', project='PROJECT_NAME_HERE')
```

Then the ampr_data object will have access to all the plotting and analysis 
methods. Use `help(pyampr.AmprTb)` to find out more.

In particular, `help(pyampr.AmprTb.read_ampr_tb_level2b)` will give a full 
rundown on the data structure.

A demonstration IPython notebook can be found in the notebooks directory.

A simple interactive testing notebook is available in the test directory.

This [conference presentation](https://ams.confex.com/ams/95Annual/webprogram/Paper262779.html) describes PyAMPR (among other modules).


"
216,nasa/xasgo,Julia,
217,nasa/abaverify,Python,"# abaverify
A python package built on [`unittest`](https://docs.python.org/2.7/library/unittest.html) for running verification tests on Abaqus user subroutines. Basic familiarity with unittest and Abaqus user subroutine development is assumed in this readme.

This software may be used, reproduced, and provided to others only as permitted under the terms of the agreement under which it was acquired from the U.S. Government. Neither title to, nor ownership of, the software is hereby transferred. This notice shall remain on all copies of the software.

Copyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

For any questions, please contact the developers:
- Andrew Bergan | [andrew.c.bergan@nasa.gov](mailto:andrew.c.bergan@nasa.gov) | (W) 757-864-3744
- Frank Leone   | [frank.a.leone@nasa.gov](mailto:frank.a.leone@nasa.gov)     | (W) 757-864-3050

## Getting-Started
This package assumes that you have `python 2.x` and `git` installed. This packaged is designed for Abaqus 2016 and it has been used successfully with v6.14; it may or may not work with older versions. It also assumes that you have an Abaqus user subroutine in a git repository with a minimum directory structure as shown here:
```
repo_dir/
    .git/
    for/
        usub.for
    tests/
        testOutput/
        abaqus_v6.env
        test_model1.inp
        test_model1_expected.py
        ...
        test_runner.py
    .gitignore
```

The user subroutine is stored in the `for/` directory and the verification tests are stored in the `<your_userSubroutine_repo_dir>/tests/` directory.

### Install `abaverify`
Abaverify can be installed using the python utility `pip` (8.x+). The following sections provide a short summary of how to use `pip` to install `abaverify`. Clone `abaverify` into a convenient directory:
```
$  git clone https://github.com/nasa/abaverify.git
```
Then install using the `-e` option:
```
$  pip install -e path/to/abaverifyDir
```

That's it.

If install fails with errors indicating an issue with `paramiko` or `cryptography`, see the [`paramiko` installation guide](http://www.paramiko.org/installing.html) for troubleshooting.

The remainder of this section describes how to build your own tests using `abaverify` (e.g., what goes inside the `test_model1.inp`, `test_model1_expected.py`, and `test_runner.py`) files. For a working example, checkout the sample verification test in the `abaverify/tests/tests/` directory. You can run the sample test with the command `python test_runner.py` from the `abaverify/tests/tests/` directory. Note, the default environment file (`abaverify/tests/tests`) is formatted for windows; linux users will need to modify the default environment file to the linux format.

### Create `.inp` and `.py` files for each test model
A model file (`*.inp` or `*.py`) and corresponding results file (`*_expected.py`) with the same name must be created for each test case. These files are placed in the `tests/` directory. The model file is a typical Abaqus input deck or python script, so no detailed discussion is provided here (any Abaqus model should work). When tests are executed (with the command `python test_runner.py`), the models in the `tests/` directory are run in Abaqus.

The `*_expected.py` file defines the assertions that are run on the `odb` output from the analysis. After each analysis is completed, the script `abaverify/processresults.py` is called to collect the quantities defined in the `*_expected.py` file. The `*_expected.py` file must contain a list called `""results""` that contains an object for each result of interest. A typical result quantity would be the maximum stress for the stress component `S11`. For example:
```
parameters = {
    ""results"":
        [
            # Simple example to find max value of state variable d2
            {
                ""type"": ""max"",                                      # Specifies criteria to apply to the output quantity
                ""identifier"":                                       # Identifies which output quantity to interrogate
                    {
                        ""symbol"": ""SDV_CDM_d2"",
                        ""elset"": ""ALL"",
                        ""position"": ""Element 1 Int Point 1""
                    },
                ""referenceValue"": 0.0                               # This the value that the result from the model is compared against
            },
            {
            ...
            <Additional result object here>
            ...
            }
        ]
}   
```
The value found in the `odb` must match the reference value for the test to pass. In the case above, the test is simply to say that `SDV_CDM_d2` is always zero, since the range of `SDV_CDM_d2` happens to be between 0 and 1. Any history output quantity can be interrogated using one of the following criteria defined in the `type` field: `max`, `min`, `continuous`, `xy_infl_pt`, `disp_at_zero_y`, `log_stress_at_failure_init`, `slope`, or `finalValue`. Here's a more complicated example:
```
parameters = {
    ""results"":
        [
            # More complicated case to find the slope of the stress strain curve within the interval 0.0001 < x < 0.005
            {
                ""type"": ""slope"",
                ""step"": ""Step-1"",                                   # By default all steps are used. You can specify any step with the step name
                ""identifier"": [                                     # The identifier here is an array since we are looking for the slope of a curve defined by x and y
                    { # x
                        ""symbol"": ""LE11"",
                        ""elset"": ""ALL"",
                        ""position"": ""Element 1 Int Point 1""
                    },
                    { # y
                        ""symbol"": ""S11"",
                        ""elset"": ""ALL"",
                        ""position"": ""Element 1 Int Point 1""
                    }
                ],
                ""window"": [0.0001, 0.005],                          # [min, max] in x        
                ""referenceValue"": 171420,                           # Reference value for E1
                ""tolerance"": 1714                                   # Require that the calculated value is within 1% of the reference value
            }
        ]
}
```
The results array can contain as many result objects as needed to verify that the model has performed as designed. Assertions are run on each result object and if any one fails, the test is marked as failed.

### Create a `test_runner.py` file
The file `sample_usage.py` gives an example of how you call your newly created tests. By convention, this file is named as `test_runner.py`. This file must include:

1. `import abaverify`.
2. classes that inherit `abaverify.TestCase` and define functions beginning with `test` following the usage of `unittest`. See the `sample_usage.py` for an example.
3. call to `runTests()` which takes one argument: the relative path to your user subroutine (omit the `.f` or `.for` ending, the code automatically appends it).

Functionality in `unittest` can be accessed via `abaverify.unittest`. One example of the use case for this is that `unittest` decorators can be applied to functions and classes in the `test_runner.py` file.

### Running your tests
Before running tests, make sure you place an Abaqus environment file in your project's `tests/` directory. At a minimum, the environment file should include the options for compiling your subroutine. If you do not include your environment file, `abaverify` will give an error.

You can run your tests with the syntax defined by `unittest`. To run all tests, execute the following from the `tests` directory of your project:
```
tests $  python test_runner.py
```
All of the tests that have been implemented will be run. The last few lines of output from these commands indicate the number of tests run and `OK` if they are all successful.

To run a single test, add the class and test name. For example for the input deck `test_CPS4R_tension.inp` type:
```
tests $  python test_runner.py SingleElementTests.test_CPS4R_tension
```

## Command Line Options
Various command line options can be used as described below.
- `-A` or `--abaqusCmd` can be used to override the abaqus command to specify a particular version of abaqus. By default, the abaqus command is `abaqus`. Specify a string after the option to use a different version of abaqus. For example:
```
tests $  python test_runner.py SingleElementTests.test_C3D8R_simpleShear12 -A abq6123
```
- `-c` or `--preCompileCode` can be specified to use `abaqus make` to compile the code into a binary before running one or more tests. A function that compiles the code must be provided to `abaverify` as an argument to the `runTests` function call in `test_runner.py`. The `usub_lib` option must be defined in the environment file.
- `-C` or `--cpus` can be used to run abaqus jobs on more than one cpu. By default, abaqus jobs are run on one cpu. To specify more than one cpu, use a command like:
```
tests $  python test_runner.py SingleElementTests.test_C3D8R_simpleShear12 -C 4
```
- `-d` or `--double` can be specified to run explicit jobs with double precision.
- `-e` or `--useExistingBinaries` can be specified to reuse the most recent compiled version of the code.
- `-i` or `--interactive` can be specified to print the Abaqus log data to the terminal.
- `-n` or `--doNotSaveODB` can be used to disable saving of x-y data to the odb. This is sometimes helpful when debugging post processing scripts in conjunction with `-r`
- `-r` or `--useExistingResults` can be specified to reuse the most recent test results. The net effect is that only the post-processing portion of the code is run, so you don't have to wait for the model to run just to debug a `_expected.py` file or `processresults.py`.
- `-R` or `--remoteHost` can be specified to run the tests on a remote host, where the host information is passed as `user@server.com[:port][/path/to/run/dir]`. The default run directory is `<login_dir>/abaverify_temp/`. Looks for a file in the `tests/` directory called `abaverify_remote_options.py`, which can be used to set options for working with the remote server. An example of this file is available `abaverify/tests/tests/abaverify_remote_options.py`. Usage example:
```
tests $  python test_runner.py -R username@server.sample.com
```
- `-s` or equivalently `--specifyPathToSub` can be used to override the relative path to the user subroutine specified in the the call `abaverify.runTests()` in your `test_runner.py` file.
- `-t` or `--time` can be specified to print the run times for the compiler, packager, and solver to the terminal. For example:
```
tests $  python test_runner.py SingleElementTests.test_C3D8R_simpleShear12 --time
```
- `-V` or `--verbose` can be specified to print the Abaqus log data to the terminal.
- `-x` or `--expiration` can be specified to set a duration (in seconds) after which the job is killed to prevent long running jobs that may occur as a result of an error in the subroutine. A job-specific expiration can be set by adding `""expiration"": <time in seconds>` to the parameters dictionary in a `*_expected.py` file.


## Results `type`
A variety of different types of results can be extracted from the odbs and compared with reference values. A list of each support type and brief explanation are provided below:
- `max`: finds the maximum value of an xy data set
- `min`: finds the minimum value of an xy data set
- `continuous`: finds the maximum delta between two sequential increments an xy data set
- `xy_infl_pt`: finds an inflection point in xy data set
- `disp_at_zero_y`: finds the displacement (implied as x value) where the y value is zero in an xy data set
- `log_stress_at_failure_init`: finds stress at failure (intended for checking failure criteria)
- `slope`: finds the slope of an xy data set
- `finalValue`: finds the y value at the last increment in the xy data set
- `x_at_peak_in_xy`: finds the x-value corresponding to the absolute peak in the y-value
- `tabular`: compares the values for a list of tuples specifying x, y points [(x1, y1), (x2, y2)...]. See example for further details

### Eval Statement

Every abaverify results type (max, min, tabular, etc.) has a default usage where either
one history object is specified (using the identifier dictionary object) or two history
objects (x and y) are specified. Essentially, a value like displacement (u) and reaction force (rf)
can be extracted from the abaqus results file and asserted against directly. 

There are times when this is limiting and some combination of Abaqus quantities is more convenient to
be asserted against. If the results type specifies only a single a single history object then an *evalStatement*
key may be specified within the results specifier dictionary. If there results type specifies an x and y history
identifier then two separate evalStatements ( *xEvalStatement* and *yEvalStatement* ). 

An eval statement is generally an arithmetic combination of results history objects. An eval
statement is a string where previously defined history objects maybe referenced (via there assigned *label* key).
An example of an eval statement is given below:

```
    ""evalStatement"": ""(d['n1_U1'] + d['n2_U1']) / 10.0""
```

In the example above, *n1_U1* and *n2_U2* would be identifying dictionaries with those very labels (labels should
therefore be unique) which are summed and then divided by 10.0.  

NOTE: To access a result defined in an identifier object the general syntax is d[<label>]. This is because
internally a dictionary of results (d) is created where label is made to be key. Therefore, to access the data
in the dictionary d one must use the python syntax to do so (d[<label>]).

#### Tabular Example Using Eval Statement

The tabular example by default uses the two identifier dict objects to define x and y data respectively (which is thusly compared to a
list of tuples specified as referenceValue). Additionally, a more advanced usage is allowed within the tabular option to specify a pythonic statement for combinging
multiple identifier results into a set of x values (and y values). This is best seen by way of example:

```
    length = 10
    area = 100
    ...
    ""results"": [
        {
            ""type"": ""tabular"",
            ""identifier"": [
                {   ""label"": ""x1"",
                    ""symbol"": ""U2"",
                    ""nset"": ""LOADAPP""
                },
                {   ""label"": ""x2"",
                    ""symbol"": ""U2"",
                    ""position"": ""Node 4"",
                    ""nset"": ""LOADFOLLOWERS""
                },
                {   ""label"": ""y"",
                    ""symbol"": ""RF2"",
                    ""nset"": ""LOADAPP""
                }
            ],
            # Use eval statements to calculate a reference strain and stress val from abaqus output of force and disp
            ""xEvalStatement"": ""(d['x1'] + d['x2']) / (2 * {length})"".format(length=length),
            ""yEvalStatement"": ""d['y']/ {area}"".format(area=area),
            ""referenceValue"": [
                            (0.0, 0.0), 
                            (0.000582908, 1.49516), 
                            (0.000944326, 2.4222), 
                            (0.00138836, 3.56113)
                            ],
            ""tolerance"": (0.0001, 0.350)
        }
    ]
```

In the example above *label*s are given to identifier dictionaries (for subsequent use in evaluation statements). 
Then a *xEvalStatement* and *yEvalStatement* is provided which can be any pythonic evaluatable expression (generally,
some combination of the xy history results specified by the labeled identifier objects). In this example, two displacements
are extracted from the odb (labeled *x1* and *x2*). They are averaged together and then normalized by some length to determine 
a reference strain value. Because this combination is defined in the *xEvalStatement* these points will become the basis for
the x's. Similarly y points are defined by normalizing force by area for reference stress determination. After
the definition of x and y points through eval statements the comparison for test is identical to the
default tabular implementation (comparison to referenceValue within specified tolerance). 

## Custom assertions in the test_runner.py file
User-defined assertions can be added without modifying the abaverify code as follows. Optional arguments `func` and `arguments` are provided in the `self.runTest()` function call. `func` is a python function that receives three positional arguments: the abaverify object, the jobname, and the object passed to `arguments`. The user-defined code in `func` may implement any logic necessary and then use the abaverify object to make the necessary assertions.

## Automatic testing
Abaverify has the capability to run a series of tests, generate a report, and plot run times against historical run times. See `automatic.py` and `automatic_testing_script.py` for details.
"
218,nasa/gen_sch_tbl,C,"# Core Flight System : Framework : Tool : Generate Schedule Table

This repository contains NASA's Generate Schedule Table Tool (gen_sch_tbl), which is a framework component of the Core Flight System.

This lab application is a ground utility to generate the binary schedule table for cFS. It is intended to be located in the `tools/gen_sch_tbl` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.

See readme.txt for more information.

## Release Notes

gen_sch_tbl version 1.0a is released as part of cFE 6.6.0a under the Apache 2.0 license, see [LICENSE](LICENSE-18128-Apache-2_0.pdf).

NOTE - there may be other schedule table management schemes which may be more applicable for modern missions.  Contact the community as detailed below for more information.

## Known issues

This ground utility was developed for a specific mission/configuration, and may not be applicable for general use.

## Getting Help

For best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.

Official cFS page: http://cfs.gsfc.nasa.gov

"
219,nasa/MINX,Prolog,"# MINX (MISR INteractive eXplorer)
---
MINX is an interactive visualization and analysis program written in IDL and designed to make MISR data more accessible to science users.
Its principal use is to retrieve heights and motion for aerosol plumes and clouds using stereoscopic methods.

MINX is platform independent and has been tested on Mac OS X, MS Windows, and Linux.

#### Binaries for Mac OS X, MS Windows, and Linux can be found under [MINX Releases](https://github.com/nasa/MINX/releases ""MINX Releases"")
##### Full Documentation can be found under the [webdoc](https://github.com/nasa/MINX/blob/master/webdoc ""MINX documentation"") directory

  

# How to run MINX from source
---

### In the IDL Development Environment / IDL Workbench Import the Project
* File Menu -> Import
* Existing Projects into Workspace
* Next Button
* Browse to Select Root Directory
* Navigate to the root MINX directory containing MINX.prj and click the Open button
* Ensure the MINX project is displayed in the projects listing
* Finish Button

### Build the Project
* Select the MINX project in Project Explorer
* Project Menu -> Build Project

### Run the Project
* Select the MINX project in Project Explorer
* Run Menu -> Run Project MINX

# How to build MINX binaries
---

### In the IDL Development Environment / IDL Workbench Import the Project
* File Menu -> Import
* Existing Projects into Workspace
* Next Button
* Browse to Select Root Directory
* Navigate to the root MINX directory containing MINX.prj and click the Open button
* Ensure the MINX project is displayed in the projects listing
* Finish Button

### Build the Project
* Select the MINX project in Project Explorer
* Project Menu -> Build Project

### Use MAKE_RT at the IDL Console to Create a MINX Stand-alone IDL Runtime Distribution

#### For Mac OS X
* `MAKE_RT, 'MINX4', '/Users/<username>/<MINX_package_destination>', SAVEFILE='/<MINX Source Path>/MINX/minx.sav', /VM, /MACINT64, /HIRES_MAPS`

#### For Windows
* `MAKE_RT, 'MINX4', 'C:\Users\<username>\<MINX_package_destination>', SAVEFILE='C:\<MINX Source Path>\MINX\minx.sav', /VM, /WIN32, /HIRES_MAPS`

#### For Linux
* `MAKE_RT, 'MINX4', '/home/<username>/<MINX_package_destination>', SAVEFILE='/<MINX Source Path>/MINX/minx.sav', /VM, /LIN32, /HIRES_MAPS`

*Note: Depending on your version of IDL and the current OS, you may also be able to specify the /WIN64 (for Windows 64-bit), /MACINT32 (for OS X 32-bit), or /LIN64 (for Linux 64-bit) keywords to build MINX packages*

"
220,nasa/GPU_SDR,HTML,"GPU SDR
=======
<p>This project enable the readout of superconductive resonators using Ettus&reg USRP SDR platforms and Nvidia&reg GPUs. The goal of the project is to allow users to modify and customize every aspect of the data acquisition: from the realtime signal generation and demodulation to the analysis techniques.</p>

![jpl-logo](server_docs/images/jpl-logo.png)

<p>This project has been developed and tested in various laboratories at JPL and Caltech in the context of the JPL Flexible radio-frequency readout for multiplexed submillimeter-wave detectors RT&D</p>

The documentation for this software is available at [this page](http://www.its.caltech.edu/~minutolo/gpu_sdr_doc.html)

Quickstart
----------
In order to test and use the system you need to:
  1. Gather all the required hardware: see the shopping list section section [here](http://www.its.caltech.edu/~minutolo/server_docs/build/html/dd/d8d/md_server_docs_01_installation.html)
  2. Compile the server using the instruction provided [here](http://www.its.caltech.edu/~minutolo/server_docs/build/html/dd/d8d/md_server_docs_01_installation.html)
  3. Launch examples of the python API or follow the quickstart guide [here](http://www.its.caltech.edu/~minutolo/lib_docs/lib_docs_index.html)
"
221,nasa/DdsJs,C++,"# Building and Packaging

## Prerequisites

1. ANTLR (http://www.antlr.org/) version 4.4 or higher. Note location of ANTLR JAR file.
2. StringTemplate (http://www.stringtemplate.org) version 4.0 or higher. Note location of StringTemplate JAR file.
3. CommonsCLI (http://commons.apache.org/proper/commons-cli/). Version 1.2 or higher. Note location of commons-cli 
   JAR file.
4. CMake (http://www.cmake.org). Version 3.11.0 or higher. A version may be provided by Linux distributions.
5. Node.js (http://www.nodejs.org). Any NodeJS version belonging to major version 8. Note the actual version number.
6. `cmake-js` (installed via NPM).
7. A properly-licensed DDS provider distribution. Currently, only CoreDX (http://www.twinoaks.com). Version 4.2.0 or higher is supported.
8. Java Development Kit (http://java.oracle.com). Version 1.8 or higher. OpenJDK may also work if provided by your
   Linux distribution.

## Build Preparation

**DDS.js** requires that the files required to build Node.js native add-ons be already installed. This can be done
by first installing `cmake-js`, then letting it install the build files:

    npm install -g cmake-js
    cmake-js install

## Preferred Location of JAR Dependencies

The **DDS.js** build files prefer that the JAR files it depends on (ANTLR, StringTemplate, and CommonsCLI) be 
installed in the `/usr/local/lib` directory. They may be installed in other locations, but doing so may make configuring the build system more difficult.

## DDS Environment Settings

### CoreDX

The **DDS.js** build files expect the following CoreDX-related environment variables be properly configured:

*  `COREDX_TOP`: Should point to the top-level installation directory of CoreDX
*  `COREDX_TARGET`: Should indicate the platform specification of the target on which **DDS.js** will run. Usually
   this is the same as the platform building the software.

### OpenSplice

TBD

### OpenDDS

TBD

## Build Configuration

The recommended way to build **DDS.js** is to create a build directory inside the top-level source directory (i.e., a ""build tree"") and perform the build from that directory.

    cd <top-level>
    mkdir build
    cd build
    
**DDS.js** uses CMake as its build system. The command line parameters passed to CMake provide information regarding
where to find all the prerequisites. The available command line parameters are:

*  `-DANTLR_VERSION=<ver>`: Version number of the ANTLR v4 JAR file. Should be evident from the JAR file name. This
   option may only be used if the ANTLR JAR file is installed in `/usr/local/lib`.
*  `-DCOMMONS_CLI_VERSION=<ver>`: Version number of the Commons-CLI JAR file. Should be evident from the JAR file 
   name. This option may only be used if the Commons-CLI JAR file is installed in `/usr/local/lib`, or if the 
   location of the JAR file is specified via `COMMONS_CLI_HOME`.
*  `-DCOMMONS_CLI_HOME=<dir>`: Location of the Commons-CLI JAR file. Not necessary if the JAR file is installed in
   `/usr/local/lib`.
*  `-DST_VERSION=<ver>`: Version number of the StringTemplate JAR file. Should be evident from the JAR file name.
   This option may only be used if the StringTemplate JAR file is installed in `/usr/local/lib`.
*  `-DNODEJS_VERSION=<ver>`: Version number of Node.js being used.
*  `-DWITH_DDS=<provider>`: Configure **DDS.js** to use a particular DDS provider. Currently, only `CoreDX` is supported.
*  `-DCMAKE_BUILD_TYPE=(Debug|Release)`: Type of build. Labels are self-explanatory.
*  `-DCMAKE_INSTALL_PREFIX=<directory>`: Directory where **DDS.js** will be installed. On user builds, it defaults to
   `${HOME}/Install`. Production builds usually are rooted in `/opt`.

For example, to configure the **DDS.js** build with ANTLR version 4.5, StringTemplate version 4.0.8, Commons-CLI
version 1.2, and using CoreDX DDS, with all JARs installed in `/usr/local/lib`, and Node.js version 4.2.4, one would issue the
following command (assuming the other prerequisites are met, and the command is being run from the build directory):

    cmake -DCMAKE_BUILD_TYPE=Release \
          -DANTLR_VERSION=4.5 \
          -DCOMMONS_CLI_VERSION=1.2 \
          -DST_VERSION=4.0.8 \
          -DNODEJS_VERSION=4.2.4 \
          -DWITH_DDS=CoreDX \
          ..

The last argument (`..`; parent directory reference) is important, as it tells CMake where to find the 
`CMakeLists.txt` file.

As of this writing, **DDS.js** works with the following package versions:

| Package  | Version  |
| -------- | -------- |
| ANTLR  | 4.5.1  |
| Commons CLI  | 1.3.1  |
| StringTemplate  | 4.0.8  |
| NodeJS  | 8.11.4  |

## Building

After CMake successfully completes the build environment configuration, building **DDS.js** may be done by
simply issuing a `make`.

## Packaging

Once the build is complete, CMake may be used to create installable packages. On Intel/Linux systems, the follwing
command produces two (2) installable DEB archives. On other Linux systems, it produces two (2) TAR-GZ archives:

    make package

This default behavior can be circumvented by calling the CPack tool (part of CMake) directly with a package generator specification. For example, to generate TAR-GZ archives even in Intel/Linux systems, the following command must be run from the build tree:

    cpack -G TGZ

The resulting archives will bear in their name the **DDS.js** package name, version, target operating system, target
system processor, and whether the archive contains runtime files or files used for development. For example, a
set of TAR-GZ archives built for a Linux x86-64 host would look like this:

*  `ddsjs-1.0.0-Linux-x86_64-Bin.tar.gz`: Archive containing the shared libraries that are needed at runtime.
*  `ddsjs-1.0.0-Linux-x86_64-Dev.tar.gz`: Archive containing header files and build scripts that are needed when
   building a NodeJS add-on with **DDS.js**.

A set of DEB archives would look as follows (when using CoreDX as the DDS provider):

*  `ddsjs-coredx-bin_1.0.0-custom_amd64.deb`: Archive containing the shared libraries that are needed at runtime.
*  `ddsjs-coredx-dev_1.0.0-custom_amd64.deb`: Archive containing header files and build scripts that are needed when
   building a NodeJS add-on with **DDS.js**.


## Installation

When installing from TAR-GZ archives, simply unpacking then on a target directory will create a
directory for **DDS.js** and unpack all the files in their appropriate location. When `CMAKE_INSTALL_PREFIX` specified
a global location, it is expected that the archive be unpacked at the root of the file system:

    tar -C / -zxvf ddsjs-1.0.0-Linux-x86_64-Bin.tar.gz

By default, `CMAKE_INSTALL_PREFIX` is configured to be `/opt/ASTRA`.

## Post-installation

**DDS.js** must have the shared libraries in its ""Bin"" package be registered with the system's dynamic linker. On Linux
systems, this usually involves adding an entry to the `/etc/ld.so.conf` file pointing to the directory where the
shared libraries were installed.

# Using DDS.js

## Workspace Preparation

The DDS.js package works best when used in conjunction with the CMake.js
(https://www.npmjs.com/package/cmake-js) NPM package. A typical DDS.js enabled
add-on would have the following files in its project root directory:

*  `CMakeLists.txt`: CMake file containing the build instructions for the
   add-on.
*  `package.json`: JSON file that describes the NodeJS add-on.
*  `index.js`: Top-level, or ""main"", JavaScript file for the add-on.
*  **IDL File**: The DDS IDL file describing the data types to use in the
   add-on.

In order for users of the add-on to be able to re-compile the source files, the
`package.json` file must contain an `install` script fragment that calls
`cmake-js compile`, as well as contain dependencies on `cmake-js` and (highly
recommended) `bindings`.

Refer to the `examples` directory in the source distribution in order to glean
what the typical Node.js add-on package source tree should look like. In it,
there is a very simple IDL file that will serve as the basis for the examples
that follow:

    module HostMonitor {

    typedef string<256> HostNameType;

    struct OverallInformation {
        HostNameType hostName;
        float cpuUtilization;
        float memoryUtilization;
    };

    };

An excellent idea for exploring **DDS.js** is to copy the contents of the 
`examples` directory onto a different location, thus any experimentation does
not affect the **DDS.js** source area.

## Building Node.js Add-on

After the aforementioned files are created, the command `cmake-js build` should
build the add-on.

## Distributing the Add-on

Once the custom add-on is built using the **DDS.js** tools and libraries, it may be
packaged for distribution using the `npm pack` command (run from the source
tree). The aforementioned command will produce a `*.tgz` archive that can then
be installed onto the target application as a Node.js module. The `Bin`-variants
of **DDS.js** must also be distributed with the add-on.

# Using the DDS.js Built Add-on

With the NPM package containing the **DDS.js** built add-on installed on the
target application, the API provided by said add-on can be used as any other
Node.js module. Following is a sample Node.js ""console"" application that
subscribes to the `OverallInformation` samples defined in the example module's
`HostMonitor.idl` file:

    const DDS = require(""dds-hostmonitor"").DDS;
    const HostMonitor = require(""dds-hostmonitor"").HostMonitor;

    var particip = DDS.createDomainParticipant(
        /* Domain ID */0
    );

    var subscriber = particip.createSubscriber();

    var overallInfoTopic = particip.createTopic(
        HostMonitor.OverallInformationTopic
    );

    var overallInfoReader = subscriber.createDataReader(
        overallInfoTopic
    );

    /* Callable that receives samples from ""take()"" */
    var printSamples = (error, samples, sampleInfos) => {
        if (error) {
            console.out(""ERROR in take(): "" + error);
        } else {
            console.out(JSON.stringify(samples));
        }
    };

    /* Do a ""take()"" every 1000 milliseconds */
    var timerObj = setInterval(
        () => {
            overallInfoReader.take(
                /* Max sample count */ 100,
                /* Unused */ 0,
                /* Callback */ printSamples
            );
        },
        1000
    );

    process.on(""SIGINT"", () => {
        /* Cleanup after detecting Ctrl+C */
        clearInterval(timerObj);
        timerObj = null;
        particip.deleteContainedEntities();
        overallInfoReader = null;
        overallInfoTopic = null;
        subscriber = null;
        particip = null;
    });

Inclusion of the **DDS.js** module is by the name given in `package.json`. The
lone JS file in the example directory, `index.js`, basically relays the
inclusion to the module via the `bindings` package. The aforementioned file can
contain additional content at the developer's discretion.

The module produced by **DDS.js** will contain *two* namespaces within it. The
first namespace, called `DDS`, will contain the standard DDS calls and
definitions (such as QoS structures). The only use of this namespace in the
previous code appears when creating the DDS Domain Participant with
`DDS.createDomainParticipant()`. The call is required to specify the DDS domain
ID as the first argument. The second argument is optional and, if specified,
must be an instance of the `DDS.DomainParticipantQos`. The recommended way to
acquire an instance of this structure is to call
`DDS.getDefaultParticipantQos()`. The fields in the returned object reflect
those of the QoS structure defined in standard DDS.

From the participant spawn several of the standard objects. In this simple
example, the participant is used to create a `Subscriber` instance via the
`createSubscriber()` method in the returned participant object. The participant
object also creates the topic, but creation of IDL-specific topics requires a
discussion of the other produced namespace.

The second namespace in the module will take its name directly from the
top-level IDL module name in the provided source IDL file. In this case, the
top-level module is called `HostMonitor`, thus the produced namespace uses the
same name. This module contains definitions for all of the data structures and
topics defined in the IDL file. In this case, only one structure/topic called
`OverallInformation` is defined. In order to create a topic instance, a helper
object that follows the pattern `<TopicName>Topic` is created for every topic
worthy data structure found in the IDL file. For `OverallInformation`, thus, the
name of the helper object is `OverallInformationTopic`. Passing this helper to
the DDS participant's `createTopic()` call takes care of both topic creation and
type registration. The default type name is used when registering the type
(by specifying `nullptr` in the underlying `register_type()` C++ call).

The sample code should run, attempting to `take()` samples every second, and
printing whatever it received. At the time of this writing, **DDS.js** does not
support event-driven sample ingest, such as those that could be defined via
either a `WaitSet` or via callbacks. The code should stop upon receiving a
`SIGINT` event, usually via `Ctrl+C`.

# Bindings Reference

The following table illustrates the API bindings available as of this writing to
JavaScript developers, and their proper analogue in the standard DDS C++
bindings. Note that only a small fraction of standard binding API calls area
currently avaialable in this JavaScript implementation. Any calls not scoped to
a class in the JavaScript column reside directly in the `DDS` namespace. All
symbols in the C++ column reside within the `DDS::` namespace. As a general
rule, any C++ calls that use *snake case* to define their symbol names were
transformed to *camel case* in order to better abide by JavaScript conventions.

| JS API call  | Equivalent C++ call  |
| ------------ | -------------------- |
| `createDomainParticipant()`  | `DomainParticipantFactory::create_domain_participant()`  |
| `DomainParticipant.enable()`  | `DomainParticipant::enable()`  |
| `DomainParticipant.createPublisher()`  | `DomainParticipant::create_publisher()`  |
| `DomainParticipant.createSubscriber()`  | `DomainParticipant::create_subscriber()`  |
| `DomainParticipant.createTopic()`  | `DomainParticipant::create_topic()`  |
| `DomainParticipant.getDiscoveredParticipants()`  | `DomainParticipant::get_discovered_participants()`  |
| `DomainParticipant.getDiscoveredParticipantData()`  | `DomainParticipant::get_discovered_participant_data()`  |
| `DomainParticipant.deleteContainedEntities()`  | `DomainParticipant::delete_contained_entities()`  |
| `DomainParticipant.addTransport()`  | `DomainParticipant::add_transport()`<sup>*</sup>  |
| `Subscriber.createDataReader()`  | `Subscriber::create_datareader()`  |
| `Subscriber.getDefaultDataReaderQos()`  | `Subsriber::get_default_datareader_qos()`  |
| `Publisher.createDataWriter()`  | `Publisher::create_datawriter()`  |
| `Publisher.getDefaultDataWriterQos()`  | `Publisher::get_default_datawriter_qos()`  |
| `DataReader.take()`  | `DataReader::take()`  |
| `DataReader.getStatusChanges()`  | `DataReader::get_status_changes()`  |
| `DataReader.getLivelinessChangedStatus()`  | `DataReader::get_liveliness_changed_status()`  |
| `DataReader.getSubscriptionMatchedStatus()`  | `DataReader::get_subscription_matched_status()`  |
| `DataReader.getSampleLostStatus()`  | `DataReader::get_sample_lost_status()`  |
| `DataReader.getRequestedIncompatibleQosStatus()`  | `DataReader::get_requested_incompatible_qos_status()`  |
| `DataReader.getSampleRejectedStatus()`  | `DataReader::get_sample_rejected_status()`  |
| `DataReader.getMatchedPublications()`  | `DataReader::get_matched_publications()`  |
| `DataReader.getMatchedPublicationData()`  | `DataReader::get_matched_publication_data()`  |
| `DataWriter.write()`  | `DataWriter::write()`  |
| `DataWriter.getStatusChanges()`   | `DataWriter::get_status_changes()`  |
| `DataWriter.getMatchedSubscriptions()`  | `DataWriter::get_matched_subscriptions()`  |
| `DataWriter.getMatchedSubscriptionData()`  | `DataWriter::get_matched_subscription_data()`  |
| `DataWriter.registerInstance()`  | `DataWriter::register_instance()`  |
| `DataWriter.unregisterInstance()`  | `DataWriter::unregister_instance()`  |
| `DataWriter.dispose()`  | `DataWriter::dispose()`  |

<sup>*</sup> Denotes a feature only available with **CoreDX**.

As far as the IDL productions fed through to **DDS.js**, no name alterations are
done. The data types specified in the productions are mapped as follows:

| IDL Type(s)  | Mapped in JS As  |
| ------------ | ---------------- |
| `struct`  | `Object`  |
| `long`, `short`, `octet`, `float`, `double`  | `Number`  |
| `string` (bounded and unbounded)  | `String`<sup>1</sup>  |
| `sequence` (bounded and unbounded)  | `Array`<sup>2</sup>  |

<sup>1</sup> Any bounds specified in the IDL are not currently enforced in
JavaScript.

<sup>2</sup> Only element homogeneity specified in the IDL is enforced in
JavaScript, and the enforcement only manifests upon calls to the **DDS.js** API.

Namespaces found in the IDL file(s) processed are turned into Node.js modules,
observing any hierarchy specified in the source IDL.

# Applicable Licenses

## ANTLR and StringTemplate

_[The BSD License]_
Copyright (c) 2012 Terence Parr and Sam Harwell
All rights reserved.
*  Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
*  Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
*  Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
*  Neither the name of the author nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"
222,nasa/CF,C,"# CCSDS File Delivery Protocol

NASA core Flight System File Transfer Application

## Description

The CCSDS File Delivery Protocol (CFDP) application (CF) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

The CF application is used for transmitting and receiving files. To transfer files using CFDP, the CF application must communicate with a CFDP compliant peer.

CF sends and receives file information and file-data in Protocol Data Units (PDUs) that are compliant with the CFDP standard protocol defined in the CCSDS 727.0-B-4 Blue Book. The PDUs are transferred to and from the CF application via CCSDS packets on the cFE's software bus middleware.

## License

This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa
"
223,nasa/CFS-101,,"# CFS-101

1. Read the README.txt file for instructions on getting started.

Tip: ""Download"" option seems to be faster then ""Clone"" option.

-------------------------------------------------------------------------------

CHANGE LISTS:

2019/06/18 - Correct & cleanup the Training Guide.
             Convert VmWare VM image to VirtualBox VM image.
			 (We will be maintaining VirtualBox VM image from here on.)
			 Update instructions in README.txt file relating to running the new VM image. 
			 
2019/02/05 - Add note on extracting the VM from .zip files as reported in issue #8.
             (Thank you for reporting it, desmfryan.)			 
2019/01/30 - Add fixes to the CFS-101 Guide as reported in issues #4 & #6.
			 (Many thanks to users, xpromache & cscase, for reporting them.)

2018/10/17 - Update the training guide such that code snippets can be copy&paste.
             (Many thanks to Kevin McCluney!)
"
224,nasa/MCMCPy,OpenEdge ABL,"MCMCPy - **M**arkov **C**hain **M**onte **C**arlo **S**ampling with **Py**thon 
==========================================================================
Python module for uncertainty quantification using a Markov chain Monte
Carlo sampler.

MCMCPy is a wrapper around the popular PyMC package (https://github.com/pymc-devs/pymc)
for Python 2.7. The purpose of the MCMCPy module is to (1) standardize the
format of the input and output of the underlying PyMC code and (2) reduce the
inherent complexity of PyMC by pre-defining a statistical model of a commonly-used
form. The MCMCPy module was originally released as part of the SMCPy code
(https://github.com/nasa/SMCPy), but, in some cases, it is possible to isolate
MCMCPy and use it directly without calling SMCPy's primary module.
 
To operate MCMCPy, the user supplies a computational model built in Python 2.7,
defines prior distributions for each of the model parameters to be estimated, and
provides data to be used for calibration. These are roughly the same steps required
to operate SMCPy. Markov chain Monte Carlo sampling can be conducted with ease
through instantiation of the MCMCSampler class and a call to the sample() method.
The output of this process is an approximation of the parameter posterior probability
distribution conditioned on the data provided.

This software was funded by and developed under the High Performance Computing Incubator
(HPCI) at NASA Langley Research Center.

----------------------------------------------------------------------------------------------
Notices:
Copyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
 
"
225,nasa/perfutils-java,,"# Core Flight System : Framework : Tool : Performance Utility

This repository contains NASA's Performance Utility Tool (perfutils-java), which is a framework component of the Core Flight System.

This lab application is a ground utility to analyze performance information generated by cFS. It is intended to be located in the `tools/perfutils-java` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.

See CPM\ Users\ Guide.docx for more information.

## Release Notes

perfutils-java version 1.0.0a release is pending.

## Known issues

This version has known compatibility issues.  The expectation is this repo will be updated once through the release processes.  Contact the community below for more information.

## Getting Help

The cFS community page http://coreflightsystem.org should be your first stop for getting help. Please post questions to http://coreflightsystem.org/questions/. There is also a forum at http://coreflightsystem.org/forums/ for more general discussions.

Official cFS page: http://cfs.gsfc.nasa.gov

"
226,nasa/multipath-tcp-tools,C++,"# multipath-tcp-tools

## A collection tools for analysis and configuration of Multipath Transmission Control Protocol (MPTCP)

### network-traffic-analysis-tools
---
The network-traffic-analysis-tools directory contains a collection of applications written in C to help both *analyze* and *visualize* MPTCP packet traces.

### mptcp-over-ppp-links
---
The mptcp-over-ppp-links directory contains the scripts and files needed to support the type of MPTCP over PPP tests used in this experiment.

This resulting system leverages MPTCP to provide long-lived, responsive TCP connections that would have previously stalled or timed out using MLPPP.  The system as a whole provides better fairness, stability, and responsiveness, allowing multiple data flows to share the available resources equally and automatically.  It provides a more efficient and reliable communication channel.  This can be done without impacting the scientific payloads directly, changing only the ground station and aircraft gateway.

Packages of these files are available in the downloads/releases section.  However, if you need the source or are curious as to which files are modified, this is the place to look.

### multipath-udp-proxy
--- 
The multipath-udp-proxy directory contains a python script used to proxy a tun interface to the available PPP links, effectively creating a ""Multipath-UDP"" that adapts to link outages.

### mptcp-kernel-patches
---
The mptcp-kernel-patches directory contains the patches made to the 3.18 and 4.4 MPTCP kernels used in these builds.
"
227,nasa/ARC-SGE-interns-19,CSS,"
# Summary of CASA Spring Internship Project 2019
By Andrew Li and Roshni Biswas\
Mentor: Dr. Christopher Potter
"
228,nasa/RHEAS,Python,"# RHEAS  [![Build Status](https://travis-ci.org/nasa/RHEAS.svg?branch=master)](https://travis-ci.org/nasa/RHEAS)

[![Join the chat at https://gitter.im/nasa/RHEAS](https://badges.gitter.im/nasa/RHEAS.svg)](https://gitter.im/nasa/RHEAS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)

The Regional Hydrologic Extremes Assessment System (RHEAS) is a modular hydrologic modeling framework that has been developed at JPL. At the core of the system lies a hydrologic model that can be run both in nowcasting (i.e. estimation of the current time period) and forecasting (i.e. estimation for future time periods) modes. The nowcasting simulations are constrained by assimilating a suite of Earth Science satellite observations, resulting in optimal estimates of directly and indirectly observed water variables. The latter nowcast estimates can then be used to initialize the 30- to 180-day forecasts. Datasets are automatically fetched from various sources (OpenDAP, FTP etc.) and ingested in a spatially-enabled PostGIS database, allowing for easy dissemination of maps and data.

Documentation for RHEAS can be found at [Read the Docs](http://rheas.readthedocs.org/en/latest/).

A peer-reviewed journal article on RHEAS is also [available](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0176506).
"
229,nasa/georef_imageregistration,Python,"The Image Registration module is intended to help automate the process of registering
images to the correct location on the ground.  It consists of two main components:
A - Fetching RGB satellite imagery of Earth to use for comparison.
B - Searching for the correct image registration parameters.


Step A is performed using Google's Earth Engine platform.
To install Earth Engine, follow these steps:
1 - Apply for a beta signup: https://docs.google.com/forms/d/17-LSoJQcBUGIwfplrBFLv0ULYhOahHJs2MwRF2XkrcM/viewform
  - Your application should be accepted quickly.
2 - Follow the steps for Python installation of Earth Engine: https://developers.google.com/earth-engine/python_install
3 - Make sure the path in imageregistration/ImageFetcher/ee_authenticate.py is pointed to the credentials file created in step 2.

Step B is performed using a C++ program relying on OpenCV 3.0
To install, follow these steps:
1 - Build OpenCV 3.0 with the contributor modules package.
	(follow tutorial here: http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/)
  - Use the below cmake (assumes opencv_contrib is in /usr/local/lib/opencv_contrib)
	sudo cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D OPENCV_EXTRA_MODULES_PATH=/usr/local/lib/opencv_contrib/modules -D BUILD_EXAMPLES=ON ..
	And make sure to do ""make install"" as well as ""make""!!!
		  
  - Building OpenCV may not go smoothly, so we will have to update this file with more specific instructions
    as we go.
  - Sample install instructions here may be useful: http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/
  
2 - Build the ImageRegistration C++ code.
  - I used the following CMake line to do this:
    cmake ..  -DOPENCV_INSTALL_DIR=/home/smcmich1/programs/opencv_install/
    (for grace it's: cmake ..  -DOPENCV_INSTALL_DIR=/usr/local/)
  - Once this is built, everything should be ready to use.

==============================
Grace's notes:
Two step process A&B
A: center point + zoom level are used to fetch new images
B: Tries to align the images together and returns a transformation matrix (image to lat lon)

Also gives a three part confidence estimate (I should take the ""high"" and ignore the others)

Takes optional fields (referencedImagePath and referencedGeoTransform), which take the similar image and the transform. If you do this step, it will use the given image as a reference image and will skip the first step. 
This is good when there are sequence of images of the same area.

registerImage.py is the main function.

==============================

Offline processing TODO:

---- How to connect to georef on the stage machine -----

ssh -L 1234:127.0.0.1:443  geocam-stage.jsc.nasa.gov
In web browser: https://localhost:1234 (add exception if needed)


DB change requests:
    - Make issMRF a unique value in all tables where we use it.
    - Do something with the registration info in the overlays table.
        - Move simple numbers out of the extras field (size, rotation, focal length, center, nadir, bounds, etc).
        - Add a ""writtenToFile"" field.
        - By mirroring our existing registration table fields we can make searches easier using joins.

- TODO: Images which are in our DB but not in the input DB file are not supported.

- Add UNIQUE flag if not already done.

- Disable GUI writing of gtiff files.  ---> Go ahead and start running the tools, ignoring this step!
- Verify offline tools output folder.
- Set up automatic offline tools.
    - How should they be running?
- Verify that offline tools are running.
- Test out the GUI, make sure everything works properly.
- More testing

- Double check why our RMS fit error is so high.
- Switch from prints to logging


Verify local is working:
iss027 - 005051, 50

No local:
MISSION	-->	TOTAL	NONE	LOW	HIGH	HIGH_FRACTION
ISS027	-->	1710	638 	499	573 	0.34
Local: 
MISSION	-->	TOTAL	NONE	LOW	HIGH	HIGH_FRACTION
ISS027	-->	1710	990	    15	705	    0.41



Examples for demo:
43-122588 = -22.2, -67.8  --> Dist = 4000m
44-903    = -15.5, 123.2  --> Dist = 3600m
44-868    = -21.7, 115.1  --> Dist = 5600m
44-1998   =  34.7,  10.8  --> Dist = 6600m



--> Final idealized design
    = Multiple asynchronous processes that all feed into the same SQL database.
        - Image detector = Add new images to the DB.
        - Metadata fetcher = Fetch ISS metadata.
        - Geosense fetcher = Add Geosense metadata.
        - Image matcher = Perform image alignment, generate GCP list.
        - Output generator = Use GCPs to generate final output image.
        --> The georef GUI will only edit SQL rows and flag the output generator.

-> Use a common info fetching function for frame, similar to what we have now.
    - Each tool can access just the info it needs.


- Switch from SQLite to MYSQL
    - Waiting on new DB to stabilize

- Port functionality from command line tool to seperate command line async processes.

- Expand/improve DB wrapper classes so all tools and GUI can use the wrapper.
    - Started new MySQL wrapper




- File describing the input data system?

- Handle overwrite options better, including re-fetch

- Tune the uncertainty constants

- IP registration improvements:    
    - Small amounts of clouds cause all the match image IP to fall on them!
      What can we do to alleviate this effect?
    - Snow covered images have a similar effect.
    - Large cities and some other detailed regions fail because the IP are scattered around the entire image
      and we don't have enough density to find the small ISS image without a massive run time.
      - Could just use a huge amount of IP...
      - Try a first-pass registration at a lower resolution?
      - Try a second-pass registration based on an initial low-confidence registration?

    - Check image saturation and re-process if too much white?  
  
    - Performance is MUCH better on images which have similar lighting/color
      conditions.  Could possibly get significant improvements by improving
      our image preprocessing steps.
  

- Double check batch local matching
- Verify that we can process from each mission














"
230,nasa/TrickFMI,C,"# TrickFMI: A Functional Mockup Interface (FMI) Standard Implementation for Trick Base Models and Simulations

## Brief Abstract:

This software supports FMI based model exchange with Trick based simulations.

Simulation is a key technology used in the conception, design, development and operation of human space systems.  As these space system become more and more complex, so do the models used in the simulations of these systems.  This software provides a practical method for exchanging models between NASA, its contractors and its international partners.

The Functional Mockup Interface (FMI) standard was developed in partnership with governmental, academic and commercial entities in the European Union.  This standard is used to support the exchange of component models for complex system simulations throughout Europe and the United States.  Trick simulations are used all across NASA for simulations that support human spaceflight activities.  However, until now, there were no means to use FMI based models in a Trick based simulation or a method for providing Trick based models that were FMI compliant.  This software provides implementation software to do both.

There are two principal components to the software:
- A C based software implementation for wrapping Trick based C models that provide an FMI compliant interface;
- A collection of C++ classes that can be used in a Trick based simulation to use an FMI compliant model (FMU).

## Copyright:
Copyright 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.

---

TrickFMI is released under the NASA Open Source Agreement Version 1.3 [license](https://github.com/nasa/trickfmi/blob/master/LICENSE).
"
231,nasa/Kepler-PyKE,Python,"# PyKE
***A suite of Python/PyRAF tools to analyze Kepler data.***

For more information and documentation,
visit http://keplerscience.arc.nasa.gov/software.html#pyke

## Installation with PyRAF

The easiest way to install PyKE is through the astroconda-iraf channel:
http://astroconda.readthedocs.io/en/latest/installation.html#legacy-software-stack-with-iraf

After that, run the following commands on a terminal of your preference:

1. ``mkiraf``
2. ``pyraf``
3. ``kepler``


## Acknowledgement
If you find this code useful in your research, please consider [citing](http://adsabs.harvard.edu/abs/2012ascl.soft08004S):
Title:	PyKE: Reduction and analysis of Kepler Simple Aperture Photometry data
```
Authors:          Still, Martin; Barclay, Tom
Publication:      Astrophysics Source Code Library, record ascl:1208.004
Publication Date: 08/2012
```

*This package was mostly developed by Tom Barclay ([@mrtommyb](http://www.github.com/mrtommyb)) and Martin Still.*

## Support
Users are welcome to open [issues](https://github.com/KeplerGO/PyKE/issues) involving any aspects of this software.

Feel free to contact us also through: keplergo@mail.arc.nasa.gov
"
232,nasa/cratous,C,"# CRATOUS
CRoss-Application Translator for Operational Unmanned Systems
(CRATOUS) is a software bridge that enables the communication between
[UxAS](https://github.com/afrl-rq/OpenUxAS) and
[ICAROUS](https://github.com/nasa/icarous).

### License and Copyright Notice

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSE`](LICENSE).

<pre>

Notices:

Copyright 2018 United States Government as represented by the Administrator
of the National Aeronautics and Space Administration. All Rights Reserved.

Disclaimers:

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY
OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT
LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO
SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE
SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF
PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN
ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR
RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR
ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER,
GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING
THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES
IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST
THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS
ANY PRIOR RECIPIENT. IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN
ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S
USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT'S SOLE REMEDY FOR
ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS
AGREEMENT.

</pre>

---

## Installation Guide

### Requirement installation

(This was tested on an Ubuntu 32 bit system)

1. Install [UxAS](https://github.com/afrl-rq/OpenUxAS) and verify it is running correctly.
2. Switch to the ```ICAROUS_integration``` branch of UxAS and recompile UxAS.
3. Install [ICAROUS](https://github.com/nasa/icarous) and verify it is running correctly.
4. Switch ICAROUS branch to `cratous`:
```
git checkout cratous
```


### CRATOUS Installation
1. Copy the cratous repository into the `cFS/apps` folder relative to your ICAROUS installation with:

```
git clone http://github.com/nasa/cratous cFS/apps/cratous
```

2. Rebuild icarous from the top folder (icarous):

```
mkdir build
cd build
cmake -DCRATOUS=ON ..
make -j9 cpu1-install
```

3. At this point it should be good to run. To test, run the examples found in `OpenUxAS/example/07_.../`. The file `runDemo.sh` in each of these should run the example without any other input.
Note: You may need to set the value in `/proc/sys/fs/mqueue/max_msg` to `1000` so that ICAROUS can run without sudo.
"
233,nasa/FPRoCK,Python,"FPRoCK
------

*FPRoCK v-0.1* (February 2019)

*FPRoCK* (Floating-Point and Real ChecKer) is a prototype tool that decides the satisfiability of a mixed real and floating-point formula.
To this aim, it transforms a mixed formula into an equi-satisfiable one over the reals.
This transformed formula is then input to different off-the-shelf SMT solvers for resolution.
FPRoCK returns an assignment satisfying the formula if it exists; otherwise, it returns `unsat` indicating that the set is unsatisfiable.

The input to *FPRoCK* is a text file containing the following information:

* the list of floating-point variables with their precision (single or double) and *optional* initial range, for example:

```
Float: X[double] = [-2000.0;2000.0],
       Y[double] = [-2000.0;2000.0]
```

* the list of real variables with their *optional* initial range, for example:

```
Real: Real_X[real] = [-2000.0;2000.0],
      Real_Y[real] = [-2000.0;2000.0],
      Err_X[real],
      Err_Y[real]
```

* the set of constraints/formulas to check for satisfiability, for example:

```
Err_X >= abs(Real_X - X)
Err_Y >= abs(Real_Y - Y)
Err_X = 1.1368683772161603e-13
Err_Y = 1.1368683772161603e-13
```

In its current version, *FPRoCK* supports conjuntions (and), negations (not), equalities and disequalities (=, !=, <, >, <=, >=)  basic arithmetic operators over the reals (+,-,\*,/) and over the floats (+FP,-FP,\*FP,/FP), and the absolute value operator (abs).
Constant integer values are required to be written with a .0 at the end, e.g. 2.0. Scientific format is also accepted, e.g. 1.1368683772161603e-13.

Prerequisites
-------------

To install *FPRoCK* you will need to install:

* Python 2.7 and pip
    * install the `bigfloat` library with the command `pip install bigfloat`
* MathSAT (http://mathsat.fbk.eu/)
* Z3 (https://github.com/Z3Prover/z3)
* CVC4 (http://cvc4.cs.stanford.edu/web/)
* Colibri (https://soprano-project.fr/download_colibri.html)

The SMT solvers and the python interpreter should be available in the `PATH` environment variable.


Installation and use
--------------------

To install *FPRoCK* the repository has to be copied to a desired location. For example, with `git` we can clone the repository in the `/fprock` folder:

```
git clone https://github.com/nasa/FPRoCK.git /fprock
```

To execute *FPRoCK* the `executor.py` script has to be called from the root folder of the repository:

```
cd /fprock
python executor.py
```

The last command will display some usage help.

For example, to check the benchmark `eps-line/ceb_5.fpr` (which is `unsat`) you have to specify a timeout, a rounding mode and an input file in sequence, as in:

```
python executor.py 10 rnd-to-zero  benchmarks/eps_line/ceb_5.fpr
```

*FPRoCK* will create a directory `results` with one file of results for each combination of SMT solver and search strategy used.
The result of *FPRoCK* is the content of those files.
For example, to check the result of the previous example you can execute a command like:

```
cat results/*
```

And you would get an output like the following telling you that the formula is `unsat`:

```
unsat
(error ""Cannot get the current model unless immediately preceded by SAT/INVALID or UNKNOWN response."")
unsat
(error ""model generation not enabled"")
unsat
(error ""model generation not enabled"")
unsat
(error ""model generation not enabled"")
unsat
(error ""line 13029 column 10: model is not available"")
unsat
(error ""line 4344 column 10: model is not available"")
unsat
(error ""line 6580 column 10: model is not available"")
```

Some result files maybe empty because the SMT timed out. The different results should all agree. If two different results `sat`/`unsat` are found for a problem it should be reported since it would imply an error in *FPRoCK* or in the underlying SMT solvers used.


Using Docker
------------

A `Dockerfile` is provided in the `/fprock/Docker/` folder in order to build a *Docker* image that can be use to install and run automatically *FPRoCK*.
The [Docker](https://www.docker.com/) tool has to be installed in order to use this approach.
The following commands build and run a *Docker* image with *FPRoCK*:

```
cd /fprock/Docker
docker build -t fprock .
docker run -it fprock /bin/bash
```

When running an *FPRoCK* image the tool is executed identically:

```
cd  /fprock
python executor.py 10 rnd-to-zero  benchmarks/eps_line/ceb_5.fpr
```


Contact information
-------------------

[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam)

### License and Copyright Notice

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES).

<pre>

Notices:

Copyright 2019 United States Government as represented by the
   Administrator of the National Aeronautics and Space Administration.
   All Rights Reserved.

Disclaimers:

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY
WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,
INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE
WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM
INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO
THE SUBJECT SOFTWARE.  THIS AGREEMENT DOES NOT, IN ANY MANNER,
CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT
OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY
OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.
FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES
REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS
AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND
SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF
THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,
EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM
PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT
SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE
REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL
TERMINATION OF THIS AGREEMENT.

</pre>

"
234,nasa/kepler-robovetter,C++,"# The Kepler DR25 Robovetter

The DR25 Kepler Robovetter is a robotic decision-making code that dispositions each Threshold Crossing Event (TCE) from the Kepler pipeline into Planet Candidates (PCs) and False Positives (FPs). The Robovetter also provides four major flags to designate each FP TCE as Not Transit-Like (NTL), a Stellar Eclipse (SS), a Centroid Offset (CO), and/or an Ephemeris Match (EM). It also produces a score ranging from 0.0 to 1.0 that indicates the Robovetter's disposition confidence, where 1.0 indicates strong confidence in PC, and 0.0 indicates strong confidence in FP. Finally, the Robovetter provides comments in a text string that indicate the specific tests each FP TCE fails and provides supplemental information on all TCEs as necessary.

More information can be found at: https://exoplanetarchive.ipac.caltech.edu/docs/PurposeOfKOITable.html#q1-q17dr25


## Compiling and Running the Code

The DR25 Robovetter code is provided, along with the necessary input files that contain every metric the Robovetter uses to make its decisions, and the resultant output files, so that users may validate their implementation. (Note that input files are compressed using tar/gzip and and should be uncompressed via ""tar xvzf FILENAME"" before being used.)


### Prerequisites

The code is written in C++11 and only requires the standard C++ library (specifically, the required libraries are iomanip, iostream, fstream, vector, sstream and random). It has been tested on Linux and Mac to work with:
  - The g++ complier (Minimum version 4.7.2 tested - earlier versions unlikely to work.)
  - The clang++ compiler (Versions 3.4 and 3.5 tested. Version 3.3 may work, but untested. Earlier than 3.3 will not work.)
  - The Intel icpc compiler (Version 17 tested. Earlier versions as far back as 11 very likely to work, but untested.)

Note that Danley Hsu was able to compile and run the code on Windows, and he has graciously provided detailed instructions in the file https://github.com/nasa/kepler-robovetter/blob/master/windows-compile-instructions. These instructions have not been verified to work on another Windows machine, but we provide them in case anyone using a Windows machine finds them helpful.


### Compiling

To compile the code, use your available C++ compiler with the -std=c++11 option, and a recommended O2 level of optimization. For example, use one of the commands below based on your available compiler:

```
g++     -std=c++11 -O2 -o robovet Kepler-RoboVetter-DR25.cpp
```
```
clang++ -std=c++11 -O2 -o robovet Kepler-RoboVetter-DR25.cpp
```
```
icpc    -std=c++11 -O2 -o robovet Kepler-RoboVetter-DR25.cpp
```

### Running the Code

Run as ""./robovet INPUTFILE  OUTFILE  NMC""

INPUTFILE is the name of the input file for a particular run. OBS is the Observed run. INV is the Inverted run. SCR1 is the Scrambled (ordering #1) run. SCR2 is the Scrambled (ordering #2) run. SCR3 is the Scrambled (ordering #3) run. INJ1 is the Injected group 1 run (on-target injected planets.) INJ2 is the Injected group 2 run (off-target planets.) INJ3 is the Injected group 3 run (Eclipsing Binaries). For more details on each run, see https://exoplanetarchive.ipac.caltech.edu/docs/KSCI-19114-001.pdf

OUTFILE is the name of the output file, as desired by the user.

NMC is the number of Monte Carlo iterations desired for computing the scores. A value of at least 100 is recommended for useable scores, and preferably at least 1,000. A value of 10,000 was used for calculating the scores archived at NExScI and the output files provided here.

For example:

```
./robovet kplr_dr25_obs_robovetter_input.txt kplr_dr25_obs_robovetter_output.txt 10000
```

will run the Robovetter on the OBS data, and perform 10,000 Monte Carlo runs to compute the score. The resulting output file should exactly match that provided in this GitHub repository. To replicate the other data sets (INV, SCR1, SCR2, SCR3, INJ1, INJ2, INJ3) one would use the following commands:

```
./robovet kplr_dr25_inv_robovetter_input.txt kplr_dr25_inv_robovetter_output.txt 10000
```
```
./robovet kplr_dr25_scr1_robovetter_input.txt kplr_dr25_scr1_robovetter_output.txt 10000
```
```
./robovet kplr_dr25_scr2_robovetter_input.txt kplr_dr25_scr2_robovetter_output.txt 10000
```
```
./robovet kplr_dr25_scr3_robovetter_input.txt kplr_dr25_scr3_robovetter_output.txt 10000
```
```
./robovet kplr_dr25_inj1_robovetter_input.txt kplr_dr25_inj1_robovetter_output.txt 10000
```
```
./robovet kplr_dr25_inj2_robovetter_input.txt kplr_dr25_inj2_robovetter_output.txt 10000
```
```
./robovet kplr_dr25_inj3_robovetter_input.txt kplr_dr25_inj3_robovetter_output.txt 10000
```


### Terminal output

In addition to the output file that is created, the code writes to the command line the currently executing task (e.g., reading in the data or vetting the TCEs) and the current Monte-Carlo iteration number so that users can monitor progress.


## Acknowledgments

Please reference Thompson et al. 2018, ApJS, 235, 38 (http://adsabs.harvard.edu/abs/2018ApJS..235...38T) if you make use of this code or the files provided.


## Notices

Copyright ¬© 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

NASA acknowledges the SETI Institute‚Äôs primary role in authoring and producing the Kepler Robovetter under Cooperative Agreement Number NNX13AD01A


## Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
235,nasa/DualPol,Jupyter Notebook,"DualPol README
--------------
This is an object-oriented Python module that facilitates precipitation retrievals (e.g., hydrometeor type, precipitation rate, precipitation mass, particle size distribution information) from polarimetric radar data. It leverages existing open source radar software packages to perform all-in-one QC and retrievals that are then easily visualized or saved using existing software.

DualPol Installation
--------------------
DualPol works under Python 2.7  and 3.4-3.6 on most Mac/Linux setups. Windows installation and other Python versions are currently untested.

In the main source directory:  
`python setup.py install`

The following dependencies need to be installed first:

- A robust version of Python 2.7  or 3.4-3.6 w/ most standard scientific packages (e.g., `numpy`, `matplotlib`, `pandas`, etc.) - Get one for free [here.](https://store.continuum.io/cshop/anaconda/)
- [The Python Atmospheric Radiation Measurement (ARM) Radar Toolkit (Py-ART)](https://github.com/ARM-DOE/pyart)
- [CSU_RadarTools](https://github.com/CSU-Radarmet/CSU_RadarTools)
- [SkewT](https://pypi.python.org/pypi/SkewT) - a Python 3 version can be found [here.](https://github.com/tjlang/SkewT)

Specific import calls in the DualPol source code:

```
from __future__ import print_function
import numpy as np
import warnings
import time
import pyart
import matplotlib.colors as colors
from pyart.io.common import radar_coords_to_cart
from skewt import SkewT
from csu_radartools import (csu_fhc, csu_liquid_ice_mass, csu_blended_rain,
                            csu_dsd, csu_kdp)
```

Using DualPol
-------------
To access everything:
```
import dualpol
```
A demonstration notebook is in the notebooks directory.

Release info:
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2585820.svg)](https://doi.org/10.5281/zenodo.2585820)
"
236,nasa/SCA,C,"# Stored Command Absolute 

NASA Core Flight System Stored Command Absolute (SCA) Application

## Description

The Stored Command Absolute (SCA) application is a core Flight System (cFS) application that is a plug in to 
the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed 
by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems 
and instruments, but can be used on other embedded systems. More information on the cFS can be found at 
[http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

The SCA application allows a system to be commanded 24 hours a day using sequences of absolute time tagged
sequences.  Each command has a time tag associated with it, permitting the command to be released for distribution at
predetermined times.  SCA allows 5 absolute time tagged command sequences to be run concurrently.  Unlike the Stored Command
(SC) application, SCA relies on a text-based format to specify command sequences.  This application is a prototype.

## License

This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa"
237,nasa/lager,C++,"Lager
-----------

Light-weight Accumulator Gathering Efficiently in Real-time   
   
Lager is a light-weight logging system.
   
Master   
[![pipeline status](https://js-er-code.jsc.nasa.gov/lager/lager/badges/master/pipeline.svg)](https://js-er-code.jsc.nasa.gov/lager/lager/commits/master) [![coverage report](https://js-er-code.jsc.nasa.gov/lager/lager/badges/master/coverage.svg)](https://artifacts.jsc.nasa.gov/artifactory/doc/lager/lager/master/coverage/index.html)   
   
Develop   
[![pipeline status](https://js-er-code.jsc.nasa.gov/lager/lager/badges/develop/pipeline.svg)](https://js-er-code.jsc.nasa.gov/lager/lager/commits/develop) [![coverage report](https://js-er-code.jsc.nasa.gov/lager/lager/badges/develop/coverage.svg)](https://artifacts.jsc.nasa.gov/artifactory/doc/lager/lager/develop/coverage/index.html)   

### Documentation

* [Design Document](doc/design.md)

### Dependencies Linux

Package dependencies   

`sudo apt install -y git cmake build-essential uuid-dev libxerces-c-dev cppcheck`   

[CMake](https://cmake.org) >= 3.1.3
[XercesC](https://xerces.apache.org/xerces-c/) >= 3.1.1   
[ZeroMQ](https://github.com/zeromq/libzmq) >= 4.2.2 required.  To install from source:   

```
cd
git clone https://github.com/zeromq/libzmq
cd libzmq
git checkout v4.2.2
mkdir build
cd build
cmake ..
make
sudo make install
cd
git clone https://github.com/zeromq/cppzmq
cd cppzmq
git checkout v4.2.2
sudo cp zmq.hpp /usr/local/include/
```

### Building Linux

1. `mkdir build`   
2. `cd build`   
3. `cmake ..`   
4. `make`   

### Dependencies Windows

1. Clone [libzmq](https://github.com/zeromq/libzmq) and use build directions located in `./builds/msvc/`.   
2. Clone [cppzmq](https://github.com/zeromq/cppzmq) and copy the `*.hpp` files into your `libzmq/include` directory.   
3. Set an environment variable `ZeroMQ_ROOT_DIR` to the full path to the libzmq directory.

### Building Windows

Replace `Visual Studio 15 2017` with your appropriate version.   
   
1. `mkdir build`   
2. `cd build`   
3. `cmake -G ""Visual Studio 15 2017"" ..`   
4. `cmake --build .`   

### Examples

See `src/tap_test_main.cpp`, `src/mug_test_main.cpp`, and `src/bartender_main.cpp`

### Run examples

Open a terminal window and run the bartender   
   
`./test_bartender`   
   
Open a second terminal window and run the tap   
   
`./test_tap`   
   
Open a third terminal window and run the mug   
   
`./test_mug`  
"
238,nasa/DERT,Java,"![ ](https://github.com/nasa/DERT/blob/master/dert/html/images/dert_small.png)

# Desktop Exploration of Remote Terrain 

Desktop Exploration of Remote Terrain (DERT) is a software tool for exploring large Digital Terrain Models (DTM) in 3D. It aids in understanding topography and spatial relationships of terrain features, as well as performing simple analysis tasks relevant to the planetary science community.

DERT was developed by the Autonomous Systems and Robotics Area of the Intelligent Systems Division at NASA Ames Research Center. It leverages techniques implemented for science planning support applications provided to a number of NASA missions including Phoenix Mars Lander (PML) and Mars Science Laboratory (MSL). 

DERT was funded by the Mars Reconnaissance Orbiter (MRO) mission and developed in collaboration with members of the MRO Context Camera (CTX) science team. DERT is licensed under the NASA Open Source Agreement (NOSA).

See demonstrations of DERT [here](https://github.com/nasa/DERT/wiki/Demonstrations).

[Find out more . . .](https://github.com/nasa/DERT/wiki)

Prebuilt releases are available on the [releases](https://github.com/nasa/DERT/releases) tab for this site. Additionally, versions that include the JPL SPICE kernels as well as some prebuilt landscapes are available [here](https://github.com/nasa/DERT/wiki/Download). These releases are built with the Java 1.8 JDK so the corresponding JRE is required to run them.

## Contributions

We are not adding contributors at this time.  However, we welcome your feedback. Please let us know of any issues you find by submitting them to this site.

## LICENSE

DERT is a viewer for digital terrain models created from data collected during NASA missions.

DERT is released under the NASA Open Source Agreement (NOSA) found in the ‚ÄúLICENSE‚Äù folder where you downloaded DERT.

DERT includes 3rd Party software. The complete copyright notice listing for DERT is:

Copyright ¬© 2015 United States Government as represented by the Administrator of the National Aeronautics and Space
Administration.  No copyright is claimed in the United States under Title 17, U.S.Code. All Other Rights Reserved.

Desktop Exploration of Remote Terrain (DERT) was written with the aid of a number of free, open source
libraries. These libraries and their notices are listed below. Find the complete third party license listings in the
separate ‚ÄúDERT Third Party Licenses‚Äù pdf document found where you downloaded DERT in the LICENSE folder.
 
 
**JogAmp Ardor3D Continuation**, Copyright ¬© 2008-2012 Ardor Labs, Inc.

 
**JOGL**, Copyright 2010 JogAmp Community. All rights reserved.
	 
	JOGL Portions Sun Microsystems, Copyright ¬© 2003-2009 Sun Microsystems, Inc. All Rights Reserved. 
	JOGL Portions Silicon Graphics, Copyright ¬© 1991-2000 Silicon Graphics, Inc. 
	Light Weight Java Gaming Library Project (LWJGL), Copyright ¬© 2002-2004 LWJGL Project All rights reserved. 
	Tile Rendering Library - Brain Paul, Copyright ¬© 1997-2005 Brian Paul. All Rights Reserved. 
	OpenKODE, EGL, OpenGL, OpenGL ES1 & ES2, Copyright ¬© 2007-2010 The Khronos Group Inc. 
	Cg, Copyright ¬© 2002, NVIDIA Corporation. 
	Typecast - David Schweinsberg, Copyright ¬© 1999-2003 The Apache Software Foundation. All rights reserved. 
	PNGJ - Herman J. Gonzalez and Shawn Hartsock, Copyright ¬© 2004 The Apache Software Foundation. All rights reserved. 
	Apache Harmony - Open Source Java SE, Copyright ¬© 2006, 2010 The Apache Software Foundation.

 
 **GlueGen**, Copyright ¬© 2010 JogAmp Community. All rights reserved.
 
	GlueGen Portions - Sun Microsystems, Copyright ¬© 2003-2005 Sun Microsystems, Inc. All Rights Reserved.

 
**Guava**, Copyright ¬© 2010 The Guava Authors.


**XStream**, Copyright ¬© 2003-2006, Joe Walnes, Copyright ¬© 2006-2009, 2011 XStream Committers. All rights reserved.


**SPICE**, Copyright ¬© 2003, California Institute of Technology. U.S. Government sponsorship acknowledged.

 
**LibTIFF**, Copyright ¬© 1988-1997 Sam Leffler, Copyright ¬© 1991-1997 Silicon Graphics, Inc.

 
**PROJ.4**, Copyright ¬© 2000, Frank Warmerdam.
 

### Disclaimers

	No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND,
	EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY
	THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF
	MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY
	WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT
	DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT
	DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY
	PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR
	ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER,
	GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY
	SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

	Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED
	STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
	RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES,
	DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES
	FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE,
	RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS
	CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT
	PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE,
	UNILATERAL TERMINATION OF THIS AGREEMENT.
"
239,nasa/AprilNav,C++,"<img src=""AprilNavLogo.jpg"" align=""right"" />

## AprilNav

[![license](https://img.shields.io/badge/license-LGPL%202.1-blue.svg)](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html)

## Overview

AprilNav is a mobile indoor real-time landmark navigation system. Using printable 2D barcodes, a HD
camera, and a computer, AprilNav is a low cost, scalable, and accurate system for vehicular autonomous
navigation and localization. Matrices (or 2D barcodes) mounted on the ceiling of a room act as a landmark for a camera located anywhere in the same room with up to 5cm of accuracy. AprilNav has sundry of potential applications ranging from robotics education to manufacturing and warehouse vehicles and facilities.

Our team at NASA - MSFC has adapted code (AprilTags) originally created by Edward Olson at University of Michigan and adapted by additional authors at Carnegie Mellon and MIT under the LGPL 2.1 license. AprilTags outputs the location of multiple 2D barcodes located anywhere in 3D space. AprilNav expands upon the AprilTags library estimating the pose of a camera given known coordinates of unique tags. 

AprilNav has been tested and run on MacOS 10.4, Ubuntu 12.04, and Rasbian.

### Prerequisites

Install the following dependencies to run the program (The following was tested on Ubuntu 12.04 and Raspberry Pi running Raspian: 

```
sudo apt-get install subversion cmake libopencv-dev libeigen3-dev libv4l-dev xterm
```

To install on Mac OS using homebrew:
```
sudo port install pkgconfig opencv eigen3
```


### Installing

Once installed, navigate to the AprilNav directory and compile with:

```
make
```

After compiling, run with:

```
./build/bin/AprilNav
```

***Note: Be sure to specify the propper tag size using the -S flag. Display a list of additional customizations using the -h flag

## Authors

* **Tristan Schuler** - *NASA MSFC* 
* **Greta Studier** - *NASA MSFC* 

## Project History

AprilTags was developed by Professor Edwin Olson of the University of
Michigan.  His Java implementation is available on this web site:
  http://april.eecs.umich.edu.

Olson's Java code was ported to C++ and integrated into the Tekkotsu
framework by Jeffrey Boyland and David Touretzky:
  http://wiki.tekkotsu.org/index.php/AprilTags

Michael Kaess (kaess@mit.edu) and Hordur Johannson (hordurj@mit.edu) of MIT
further modified the code and made a standalone library for AprilTags:
  http://people.csail.mit.edu/kaess/apriltags/
"
240,nasa/sitepod,PHP,"# Sitepod

[![Latest Stable Version](https://poser.pugx.org/nasa/sitepod/v/stable)](https://packagist.org/packages/nasa/sitepod)
[![Total Downloads](https://poser.pugx.org/nasa/sitepod/downloads)](https://packagist.org/packages/nasa/sitepod)
[![Latest Unstable Version](https://poser.pugx.org/nasa/sitepod/v/unstable)](https://packagist.org/packages/nasa/sitepod)
[![License](https://poser.pugx.org/nasa/sitepod/license)](https://packagist.org/packages/nasa/sitepod)
[![composer.lock](https://poser.pugx.org/nasa/sitepod/composerlock)](https://packagist.org/packages/nasa/sitepod)
[![Travis](https://img.shields.io/travis/nasa/podaacpy.svg?maxAge=2592000?style=plastic)](https://www.travis-ci.org/nasa/sitepod)
[![Requirements Status](https://requires.io/github/nasa/sitepod/requirements.svg?branch=master)](https://requires.io/github/nasa/sitepod/requirements/?branch=master)
[![Quality Gate](https://sonarqube.com/api/badges/gate?key=nasa:sitepod)](https://sonarcloud.io/dashboard?id=nasa:sitepod)

Sitepod; A Sitemap Generator written in PHP. Sitepod is build using the [Fat-Free Framework](https://fatfreeframework.com), a powerful yet easy-to-use PHP micro-framework designed to build dynamic and robust web applications.

# Installation from the Composer PHP Package Manager
Sitemap can be installed using the [Composer](https://getcomposer.org/) 
dependency manager for PHP as follows
```
$ composer require nasa/sitepod
```
This will install Sitepod into a directory structure ```vendor/nasa/sitepod/``` relative to 
wherever the command was executed.

Alternatively, you can install the master branch from source

# Installation from Source

See [Sitepod Installation](https://github.com/nasa/sitepod/wiki/Sitepod-Installation)

# Configuration and Usage

See [Sitepod Setup and Usage](https://github.com/nasa/sitepod/wiki/Setup-and-Usage)

# Community, Support and Development
Please open a ticket in the [Sitepod issue tracker](https://github.com/nasa/sitepod/issues).
Please use labels to classify your issue.

# License
This code has been forked from the now unmaintained [phpSitemapNG](http://enarion.net/tools/phpsitemapng/)
The code is licensed permissively under the [GPLv3 License](https://www.gnu.org/licenses/gpl-3.0.en.html) which respects the origin authors work e.g. Tobias Kluge, enarion.net.
A copy of the GPLv3 License can be found below and is also included as part of this software.
```
This file is part of Sitepod.

Sitepod is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Sitepod is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Sitepod.  If not, see <http://www.gnu.org/licenses/>.
```
"
241,nasa/CFS_IO_LIB,C,"# Input/Output Library

NASA core Flight System Input/Output Library

## Description

The I/O Library (IO_LIB) is a collection of protocol libraries to be called by the CI/TO application custom implementations.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at http://cfs.gsfc.nasa.gov

## License

This software is licensed under the NASA Open Source Agreement. 
http://ti.arc.nasa.gov/opensource/nosa
"
242,nasa/georef_geocamutilweb,Python,"
``geocamUtilWeb`` is a set of utilities used by Django web apps in the
GeoCam Share app collection.  It includes the following utilities.

MultiSettings
~~~~~~~~~~~~~

A settings container object built out of an ordered list of child
settings objects.  When you request the value of an attribute, it
returns the value found in the first child that defines that attribute.

We typically use ``MultiSettings`` when apps extend Django settings by
defining new app-specific variables.  For example, if you have an app
``geocamAwesome`` you can put the following in
``geocamAwesome/defaultSettings.py``::

  GEOCAM_AWESOME_ENABLED = True

and in ``geocamAwesome/__init__.py``::

  import django.conf.settings
  from geocamUtil.MultiSettings import MultiSettings
  from geocamAwesome import defaultSettings
  
  settings = MultiSettings(django.conf.settings, defaultSettings)

then you can run::

  $ ./manage.py shell
  >>> from geocamAwesome import settings
  >>> settings.GEOCAM_AWESOME_ENABLED
  True

but if a site administrator adds this line to their site-level
``settings.py``::

  GEOCAM_AWESOME_ENABLED = False

you would see::

  $ ./manage.py shell
  >>> from geocamAwesome import settings
  >>> settings.GEOCAM_AWESOME_ENABLED
  False

The advantage of this approach is that site administrators don't need to
add all of your app's extended settings to their ``settings.py`` file if
they like the defaults, but they can override any setting in a uniform
way.

Actually, ``MultiSettings`` does not depend on Django at all.  It will
work with any kind of child container object as long as its fields can
be accessed using dot notation.

models.UuidField
~~~~~~~~~~~~~~~~

A Django model field that stores a `universally unique identifier`_.
When you first save a model with a ``UuidField``, if the UUID value is
not already set, it is automatically populated with a random (or ""type
4"") UUID encoded as a ``CharField`` in the standard UUID display format,
which is a series of hex digits separated by hyphens.

.. _universally unique identifier: XXX

You might want to use a ``UuidField`` if you have multiple instances of
your Django app on different hosts and you need to identify the same
object across instances.  We typically do *not* use a ``UuidField`` as the
primary key for a model to avoid a performance penalty.

forms.UuidField
~~~~~~~~~~~~~~~

A Django form field corresponding to the same-name model field.
Validates that the user entered a sequence of hex digits separated by
hyphens.

models.ExtrasField
~~~~~~~~~~~~~~~~~~

A Django model field for storing extra schema-free data.  You can get
and set arbitrary properties on the extra field, which can be comprised
of strings, numbers, dictionaries, arrays, booleans, and ``None``.
These properties are stored in a database ``TextField`` as a
JSON-encoded set of key-value pairs.

.. o  __BEGIN_LICENSE__
.. o  Copyright (C) 2008-2010 United States Government as represented by
.. o  the Administrator of the National Aeronautics and Space Administration.
.. o  All Rights Reserved.
.. o  __END_LICENSE__"
243,nasa/georef_geocamtiepoint,Python,"Coordinate Systems
==================

GeoRef uses two main coordinate systems:

 * The image coordinate system measures position in pixels (x, y) where
   (0, 0) is the upper-left corner of the image, x increases to the
   right, and y increases down.
 * The Spherical Mercator coordinate system expresses position on the
   Earth's surface. (x, y) coordinates. Roughly speaking, x increases to
   the east and y increases to the north. The origin matches the origin
   in lat/lon coordinates. The scale of the units approximates
   displacement in meters.  This system is also known as EPSG:3857 or
   EPSG:900913.

Two-way conversions between lat/lon and Spherical Mercator can be found
in the ``latLonToMeters`` and ``metersToLatLon`` functions:

 * `JavaScript coordinate conversions <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/static/geocamTiePoint/js/coords.js>`_
 * `Python coordinate conversions <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/quadTree.py>`_

Some other references:

 * `Google Maps Coordinates, Tile Bounds, and Projection <http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/>`_
 * `PROJ.4 FAQ: Google Mercator <http://trac.osgeo.org/proj/wiki/FAQ#ChangingEllipsoidWhycantIconvertfromWGS84toGoogleEarthVirtualGlobeMercator>`_

Export Format
=============

Exporting an overlay produces a gzip-compressed tar archive containing
following files: 

[imageid]-no_warp.tif

-this is a GeoTIFF version of the photo that is unmodified (unwarped) in an image sense, but contains a bunch of metadata header fields indicating the list of 
	control/tie/correspondence points found for alignment and some alignment fit uncertainty measures.  This version gives an end user all they'd need to create
	their own aligned image from the embedded control points.

[imageid]-warp.tif

-this is a GeoTIFF version of the photo that is actually modified to be warped/aligned to a map, with transparency around the warped photo to fit inside a 
	rectangular image as usual.  It does not contain a header with the list of tie points, but it contains fields with alignment fit measures.

[imageid]-no_warp_metadata.txt

-this is a text file containing a formatted dump of the header fields present in the -no_warp.tif version so that someone can get the important data without 
	retrieving the image.

[imageid]-warp_metadata.txt

-same as no_warp_metadata.txt but instead corresponding to the -warp.tif version.

[imageid]-uncertainty-no_warp.tif

-This is a special synthetic image (single channel floating point) where the number at each pixel represents the uncertainty (standard deviation) in meters we 
	estimate for our fit at that pixel.  This provides data to do automated analysis of the relative accuracy of our alignment at each pixel -- 
	it will be more accurate near tie points, and worse further away.

[imageid]-uncertainty-no_warp.tif

-analogous to the -uncertainty-no_warp.tif file, this is a warped/aligned version of the uncertainty image.  It contains two floating point channels, the first 
	is the uncertainty as in the unwarped version, and the second is a ""mask"" that is 0 where there is no uncertainty data (the warped image doesn't exist there) 
	or 255 if there is.


Meta-Data Format: meta.json
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``transform`` field represents a best-fit transform that maps image
coordinates to Spherical Mercator coordinates. Depending on the number
of tie points specified, the transform can be expressed in two forms:

 * ``""type"": ""projective""``. This is a 2D projective transform. Used when
   fewer than 7 tie point pairs are specified. The ``matrix`` field is a
   3x3 transformation matrix ``M`` specified in row-major order. To apply
   the transform:

   * Start with image coordinates ``(x, y)``.

   * Convert to a length-3 column vector ``u`` in homogeneous coordinates: ``u = (x, y, 1)``

   * Matrix multiply ``(x0, y0, w) = M * u``.

   * Normalize homogeneous coordinates: ``x' = x0 / w``, ``y' = y0 / w``.

   * The resulting Spherical Mercator coordinates are ``(x', y')``.

 * ``""type"": ""quadratic2""``. This transform is similar to the projective
   transform but adds higher-order terms to achieve a better fit when
   the overlay image uses a different map projection from the base
   layer. Used when 7 or more tie point pairs are specified. Please
   refer to the code for full details. Some points of interest:

   * Note that despite the name, this transform is *not* exactly
     quadratic. In order to ensure the transform has a simple analytical
     inverse, corrections are applied serially, which incidentally
     introduces some 4th-order and 6th-order terms.

   * The ``matrix`` field has essentially the same interpretation as for
     the 'projective' transform.

   * In order to help with numerical stability during optimization, the
     last step of the transform is to scale the result by 1e+7.  Because
     of this, the matrix entries will appear much smaller than those in
     the projective transform.

   * The coefficients for higher-order terms are encoded in the
     ``quadraticTerms`` field. If all of those terms are 0, the
     ``quadratic2`` transform reduces to a ``projective`` transform.

See the alignment transform reference implementations in the
``ProjectiveTransform`` and ``QuadraticTransform2`` classes:

 * `JavaScript alignment transforms <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/static/geocamTiePoint/js/transform.js>`_
 * `Python alignment transforms <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/transform.py>`_

.. o __BEGIN_LICENSE__
.. o Copyright (C) 2008-2010 United States Government as represented by
.. o the Administrator of the National Aeronautics and Space Administration.
.. o All Rights Reserved.
.. o __END_LICENSE__
"
244,nasa/pyCMR,Python,"# Python CMR Client Library

Python client library that abstracts CMR API calls for search, ingest, update, and deletion of collections and granules.



## Setup

Run:

    $ python setup.py install

Or

    $ pip install -e .

### Usage

Populate your CMR credentials into a `cmr.cfg` file, using `cmr.cfg.example` as a template. Alternatively, on instantiation, if no CFG file is provided then the CMR object will load credentials from these environment variables: `CMR_PROVIDER`, `CMR_USERNAME`, `CMR_PASSWORD`, and `CMR_CLIENT_ID`.

Then, run:

```python
$ python
>>> from pyCMR.pyCMR import CMR
>>> cmr = CMR('/path/to/my/cmr.cfg')  # or `cmr = CMR()` to load from env vars
```

## Test

```bash
python setup.py test
```
"
245,nasa/Computational-Materials-Miniapp1,,"
This mini-app is a simple test code for calculating potential energy, 
forces and stresses of an atomistic structure using EAM potential for Al.
(Mishin, Y., Farkas, D., Mehl, M.J., Papaconstantopoulos, D.A., 1999. 
Interatomic potentials for monoatomic metals from experimental data and 
ab initio calculations. Phys. Rev. B., 59, 3393-3407.)

-----------------------------------------------------------------------
Requirements:
* Linux
* FORTRAN compiler (2003 or newer)
-----------------------------------------------------------------------
Edit the provided makefile for specific compiler option of your choice
-----------------------------------------------------------------------
Compilation: use the provided makefile with the following options:
> make            ! compiles with -O3 optimization on !
> make DEBUG=TRUE ! compiles with check and warning flags on !

-----------------------------------------------------------------------
Example (test) directory:
./CM_MINI1.test

Running a tets case:
run CM_mini1.bat script from CM_MINI1.test directory:
> CM_mini1.bat

-----------------------------------------------------------------------
Required input files:

pot.dat - interatomic potential setup file
./NiCoAl_dat  - folder with potential files as described in pot.dat
structure.plt - input atomic structure file

--- Available test structures ---
in STR directory:
Ni85Al15_N4k.plt - 4,000 atoms Ni3Al crystal
Ni85Al15_N64k.plt - 64,000 atoms Ni3Al crystal
Ni85Al15_N256k.plt - 256,000 atoms Ni3Al crystal
 
Use any of the above structures by linking them to structure.plt, e.g.,
ln -s ./STR/Al_N500_T100K.plt structure.plt
 
-----------------------------------------------------------------------
Execution:   

cd ../CM_MINI1.test
./CM_mini1

Execution options: (see also ./CM_MINI1.test/CM_mini1.bat script)

./CM_mini1           # run, using default options - see below
./CM_mini1 -n 10     # do 10 itterations (default: -n 1)
./CM_mini1 -n 10 -r  # do 10 itterations with random changes
                     # (default: no randomization)
./CM_mini1 -v 1      # run version 1 (default: -v 0) - gives a possibility
                     # to test different versions of a subroutine.
                     # currently: -v 0 executes the code using
                     # subroutine get_neighbors (CM_mini1.f)
                     # while -v 1 executes the code using
                     # subroutine get_neighbors_vect (CM_mini1.f)
                     # see subroutine force_global(iver):
-----------------------------------------------------------------------
       select case (iver)
        case(0) 
         call get_neighbors
        case(1) 
         call get_neighbors_vect
        case default 
         call get_neighbors
       end select
=======================================================================
 Notices:
 Copyright 2018 United States Government as represented by the 
 Administrator of the National Aeronautics and Space Administration. 
 All Rights Reserved.
 
 Disclaimers:
 No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY
 WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, 
 INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE 
 WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF 
 MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM 
 INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR 
 FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM 
 TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT,IN ANY MANNER, 
 CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT 
 OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS 
 OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. 
 FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES 
 REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,
 AND DISTRIBUTES IT ""AS IS.""‚Ä®
 
 Waiver and Indemnity:
 RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES
 GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR 
 RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY 
 LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH
 USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, 
 RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND
 HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND 
 SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT
 PERMITTED BY LAW. RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL 
 BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.

"
246,nasa/nasapress-companion,PHP,"# NASAPress Companion

Uses Memcache to cache API calls to nasa.gov and technology.nasa.gov.

Search the plugin for `todo-config`. These comments mark the locations where you'll likely need to make customizations for your site.

## Shortcodes

category-list

children-list

portal-posts

spinoff-posts
"
247,nasa/content-guide,CSS,"## NASA Glenn Content Guide

This is the repository for the [NASA Glenn Content Guide](https://nasa.github.io/content-guide/). This guide was developed for NASA Glenn employees, but we hope it‚Äôs a useful reference for anyone. It is a fork of the [18F Content Guide](https://pages.18f.gov/content-guide/)

### Editing the Guide
The pages in the guide are broken into four sections corresponding to four folders in the [```_pages```](https://github.com/nasa/content-guide/tree/master/_pages) folder. The markdown files within each section can be edited using GitHub. If you are new to editing files in GitHub [this guide](https://help.github.com/articles/editing-files-in-your-repository/) should help. Just ignore the part about creating a new branch and commit your changes directly to the master branch. If you are new to using markdown [this guide](https://guides.github.com/features/mastering-markdown/) should help. When you commit your changes to the master branch CircleCI will automatically push your changes to the [site on GitHub Pages](https://nasa.github.io/content-guide/). It may take up to a few minutes for your changes to be viewable on the site.

### Public domain

This project is in the worldwide [public domain](LICENSE.md).

> As a work of the United States government, this project is in the public domain within the United States.

> Additionally, we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication.
"
248,nasa/scifen-solver,C,"This mini-app is a simple interface to PETSc Krylov subspace iterative solvers 
that are suitable for linear-elastic finite element models. The executable 
simply loads a system matrix, load vector (RHS), and optional reference 
solution files that were saved in PETSc binary format, then solves it. It is 
intended for single-core performance comparisons.

Requirements:
* Linux
* MPI
* PETSc (versions 3.6.3 and 3.9.3 tested) 
* A suitable C compiler such as cc, gcc, or icc.

********************************** IMPORTANT **********************************
Edit make-local.inc to set PETSC and MPI directories correctly, and to choose 
a compiler.

In ksp-solve.h, the preprocessor macro PETSC_HACK_POGX is set to ""NULL"" or 
to ""NULL,NULL"", for compatibility with different versions of PETSc.
*******************************************************************************

Usage:
The makefile is the easiest way to compile, test, and obtain timings with this 
mini-app. Timing logs are saved in a directory named after the executing system 
(hostname). The bash shell script provided, log-parse.sh, is used in the 
makefile to parse the output from several solutions into a convenient comma-
delimited (CSV) data file with a similar name.

make exe        : recompile the executable, ksp-solve
make test-solve : short (100 iteration) test
make test       : runs several tests
make time-all   : loops through several tolerances and solvers
make solve-lu   : solves the system by preconditioning with an LU decomposition

The executable, ksp-solve, requires at least a matrix [k] and vector [p] to
solve [k][u]=[p] for unknown [u]. The command-line options below are required.

ksp-solve
 -rmat <input_file> : read matrix file [k]
 -rrhs <input_file> : read load vector file [p]

The remaining are optional.
 -v <verbosity>     : default is 1
 -ruin <input_file> : read initial guess solution file [u]
 -rref <input_file> : read reference solution file [u]
 -wsol <output_file>: write solution file [u]
 -wcsv <output_file>: write data to a CSV file

When -rref is given, the true (not preconditioned) norm of the residual between 
the computed solution and the provided reference solution will be reported. 
PETSc options may also be provided to ksp-solve, though only some are used.
The PETSc options below are applied for the examples in the makefile. See the
PETSc documentation for more information about these options.

 -ksp_converged_reason            : prints convergence information
 -ksp_norm_type preconditioned    : use preconditioned residual norm
 -log_view  :<filename>           : full path to save PETSc log file
 -ksp_rtol   <relative tolerance> : default is PETSC_DEFAULT
 -ksp_max_it <max iterations>     : default is size of RHS vector [p]
 -pc_type    <preconditioner>     : jacobi, lu
 -ksp_type   <solver>             : (See table)

Suitable FEM System Solvers:
---------------------------------------------------------------------------
 <solver>                      Method                        Sym    Unsym  
---------------------------------------------------------------------------
    cg                   Conjugate Gradient                   ‚úì            
   fcg               Flexible Conjugate Gradient              ‚úì            
    cr          (Preconditioned) Conjugate Residuals          ‚úì            
                                                                           
  symmlq                       SYMMLQ                         ‚úì            
   cgs               Conjugate Gradient Squared               ‚úì       ‚úì    
   lcd                left conjugate direction                ‚úì       ‚úì    
                                                                           
   bicg            Biconjugate gradient (Unstable)                         
   bcgs      BiCGStab (Stabilized BiConjugate Gradient)               ‚úì    
  ibcgs             IBiCGStab (Improved BiCGStab)                     ‚úì    
  bcgsl            Variant of Enhanced BiCGStab(L)                    ‚úì    
                                                                           
  minres                  Minimum Residual                                 
  gmres             Generalized Minimal Residual                      ‚úì    
  lgmres          Augmented w/ error approximation                    ‚úì    
  dgmres                   Deflated GMRES                             ‚úì    
                                                                           
  tfqmr      transpose free QMR (quasi minimal residual)                   

Solve by LU Decomposition
---------------------------------------------------------------------------
  preonly                preconditioner only

The following do not support left preconditioning or precoditioned residual 
norm calculation.
---------------------------------------------------------------------------
   cgr      Preconditioned Generalized Conjugate Residual     ‚úì       ‚úì    
  fbcgsr           Equivalent Variant of FBiCGSTab                    ‚úì    
  fbcgs                  Flexible BiCGStab                            ‚úì    
  fgmres        Flexible Generalized Minimal Residual                 ‚úì    


===============================================================================
Notices:
Copyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.
 
Disclaimers
No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""
 
Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
249,nasa/cumulus-circleci-image,Shell,"# Base CircleCI image for testing Cumulus Core

[![CircleCI](https://circleci.com/gh/nasa/cumulus-circleci-image.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-circleci-image)

### Local Build

     $ docker build . -t cumuluss/circleci:node-8.11

### Local Test

     $ docker run --rm -it -p 20:20 -p 21:21 -p 47400-47470:47400-47470 -p 3030:3030 -p 2222:2222 cumuluss/circleci:8.11 /bin/bash

### Provider Server

To run this image as a provider server with the test data:

    $ docker run -it -e ON_AWS=true -p 20:20 -p 21:21 -p 47400-47470:47400-47470 -p 3030:3030 cumuluss/circleci:node-8.11 start

In the above example the test-data is loaded from the latest version of @cumulus/test-data package on npm.

If you need to load test-data other than the ones published to npm make sure to set `TEST_DATA_S3_PATH` and point to the location of test files. For example:

    $ docker run -it -e ON_AWS=true -e TEST_DATA_S3_PATH=s3://cumulus-data-shared/@cumulus/test-data/ -p 20:20 -p 21:21 -p 47400-47470:47400-47470 -p 3030:3030 cumuluss/circleci:node-8.11 start

## Credit

- [Bogem FTP Docker Image](https://github.com/bogem/dockerfiles/tree/master/ftp)
- [atmoz SFTP Docker Image](https://github.com/atmoz/sftp)"
250,nasa/TCML,HTML,"readme_TCML

TCML version 1.0.0
Date of most recent update: 01/04/2018
Developer: Jonathan Kratz (NASA Glenn Research Center)

GENERAL INFORMATION

The Tip Clearance Modeling Library (TCML) was developed to model the dynamic variation of high pressure 
turbine (HPT) tip clearance in aero-engine turbomachinery. The TCML blockset contains blocks to predict
the dynamic and steady-state tip clearance using a method similar to what is described in the references
below:

[1] Chapman, J., Kratz, J., Guo, T.H., and Litt, J., ÔøΩIntegrated Turbine Tip Clearance and Gas Turbine 
    Engine Simulation,ÔøΩ Proceedings of the 52nd AIAA/ASME/SAE/ASEE Joint Propulsion Conference, Salt Lake 
    City, UT, 2016.
[2] Kratz, J., Chapman, J., and Guo, T.H., ÔøΩA Parametric Study of Actuator Requirements for Active Turbine
    Tip Clearance Control of a Modern High Bypass Turbofan Engine,ÔøΩ Proceedings of the 2017 ASME Turbo Expo,
    Charlotte, NC, 2017.
[3] Kratz, J., and Chapman, J., ÔøΩActive Turbine Tip Clearance Control Trade Space Analysis of an Advanced
    Geared Turbofan Engine,ÔøΩ Proceedings of the 54nd AIAA/ASME/SAE/ASEE Joint Propulsion Conference, 
    Cincinnati, OH, 2018. To be published.

Most resently this blockset has been used in a HPT tip clearance study conducted for an advanced geared
turbofan with a compact gas turbine (CGT). See Ref. [3].

The library is conducive to use with dynamic engine models, particulary those created with the Toolbox 
for Modeling and Analysis of Thermodynamic Systems (T-MATS). TCML could be viewed as a compliment to 
T-MATS. Infact, the TCML blockset uses a few block from T-MATS version 1.1.3.13. The links to the T-MATS
library have been broken to enable use of TCML without a T-MATS installation.

The tip clearance modeling method is a physics-based system level approach. The help menu's of the blocks
provide a description of the method. In additon, the user is referred to the references listed above.

Note that TCML was developed using MATLAB/Simulink R2015a on a Windows PC with the Windows 7 operating
system. TCML is not gauranteed to work on different operating systems or different versions of MATLAB.



TCML ORGANIZATION

With the ""Trunk"" folder are files to install and uninstall TCML. Two folders ""TCML_Library"" and 
""TCML_Examples"". ""TCML_Library"" contains the library file ""lib_TipClearance.slx"" and the folders
""Support"" and ""MATLAB_Scripts"". ""Support"" containts the help files for the Simulink blocks and 
""MATLAB_Scripts"" contains MATLAB functions that support the Simulink blocks. The folder ""TCML_Examples""
contains an example to illustrate the usage of the blocks inside the library.



INSTALLING TCML

1. Download TSAT from the GIT server PUT URL HERE, click the green download button for the latest version, 
   and extract the files to a folder that can be accessed by MATLAB, ensuring there are no spaces in the 
   path name.
2. Open up MATLAB and navigate to the directory that TSAT was saved.
3. Run install_TCML.m. This will setup the paths for TSAT. A temporary install should only save the paths 
   for the current MATLAB session while the permanent install option will save the paths to MATLAB until 
   the uninstall script is ran or the paths are manually removed. If the user does not have elevated 
   privileges, the paths may not be saved properly. If the paths have not been saved, new paths must be 
   manually added to the pathdef.m file. To do this click on the ÔøΩSet PathÔøΩ icon in the MATLAB toolbar and 
   add the following paths by navigating to them and selecting them: 
	ÔøΩ Trunk\TCML_Library
	ÔøΩ Trunk\TCML_Library\Support
	ÔøΩ Trunk\TCML_Library\MATLAB_Scripts
   Save the paths before exiting the dialog box.
4. Open up Simulink and verify that the TCML library shows up in the library browser.
5. Open up one of the examples in the ÔøΩTrunk\TCML_ExamplesÔøΩ folder and attempt to execute it to verify 
   that the TCML library is on the path and the library blocks can be used.



UNINSTALLING TCML

1. Run uninstall_TCML.m. This will remove the paths that were added during the TCML install.
	a. If the paths were added manually during installation then they must be removed manually using 
 	   the ÔøΩSet PathÔøΩ tool in MATLAB.
"
251,nasa/TSAT,HTML,"readme_main.txt

TSAT version 1.0.0
Date of most recent update: 02/22/2018
Developer: Jonathan Kratz (NASA Glenn Research Center)

The Thermal Systems Analysis Toolbox (TSAT) is a MATLAB/Simulink based tool for modeling and analysis
of dynamic and steady-state thermal systems involving heat transfer. TSAT consists of a Simulink library
that will appear in your Simulink library browser when installed. Each block has a help file that can
be reached by right clicking the block and clicking ""help"". The help files will provide general modeling 
information of what the block does; will described inputs, parameters, and outputs; and will provide 
references if appropriate. In addition to the help files, several examples are provided that illustrate
usage of several of the TSAT blocks. TSAT also provides several MATLAB functions that can be used in a
variety of ways. Descriptions of function inputs, outputs, and usage is provided by comments within the 
function files.

The ""Trunk"" folder contains all the tools including the Simulink libraries, the MATLAB tools, the help
files, and examples. The ""Resources"" folder contains the TSAT Quickstart Guide (TSATquicksart.pdf) and 
could include various other materials if future updates of the software package are made. The quickstart 
guide is recommended reading before attempting to install, uninstall, or use TSAT.

TSAT was created during the development of thermal models of aero-engines for the purpose of approximating
the thermal environment relevant for control system components when considering the application of 
distributed engine control. Also of interest was thermal modeling relevent to turbine tip clearance control.
A significant portion of the TSAT library blocks and MATLAB tools are a direct result of these efforts. At 
the core of the library are its 1D and 2D conduction blocks used to model conduction through solid structures.
The library also has various options of estabilishing boundary conditions and it contains various general-use 
tools as well. 

NOTE: TSAT and the examples it comes with were developed using MATLAB/Simulink R2015a. Although compatability
with newer versions is not thought to be an issue, the user should be aware of the potential for 
compatability issues."
252,nasa/cumulus-ecs-task-python,Python,"# cumulus-ecs-task-python

Use this Docker image to run a Python AWS Lambda function in AWS [ECS](https://aws.amazon.com/ecs/). It mimics [cumulus-ecs-tas](https://github.com/cumulus-nasa/cumulus-ecs-task) for Node.js AWS Lambda functions.

## About

cumulus-ecs-task-python is a Docker image that can run Lambda functions as ECS services.

When included in a Cumulus workflow and deployed to AWS, it will download a specified Lambda function, and act as an activity in a Step Functions workflow.

## Usage

[See documentation in cumulus-ecs-task](https://github.com/cumulus-nasa/cumulus-ecs-task/blob/master/README.md#usage).

## Building

```
export VERSION=0.0.2
docker build -t cumuluss/cumulus-ecs-task-python:$VERSION .
```

## Pushing to docker

```
# docker login
docker tag cumuluss/cumulus-ecs-task-python:$VERSION cumuluss/cumulus-ecs-task-python:$VERSION
docker push cumuluss/cumulus-ecs-task-python:$VERSION
```

## Testing

```
export AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>
export AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>

docker run -it -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
 -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  cumuluss/cumulus-ecs-task-python:$VERSION \
  cumulus-ecs-task \
  --activityArn arn:aws:states:us-east-1:433612427488:activity:cce-DownloadTiles-Activity \
  --lambdaArn arn:aws:lambda:us-east-1:433612427488:function:cce-ViirsProcessing
```
"
253,nasa/cumulus-process-py-seed,Python,"# cumulus-process-py-seed

[Cumulus](https://github.com/cumulus-nasa/cumulus) is a cloud-based framework for distributing and processing data, used by NASA's Distributed Active Archive Center's (DAAC's). A Cumulus workflow is made up of tasks, such as finding new data at an FTP site, downloading, publishing metadata, etc. 

[cumulus-process-py](https://github.com/cumulus-nasa/cumulus-process-py) is a Python library that makes it easy to create Cumulus tasks in Python that are primarily for processing data. The cumulus-process-py library includes convenience functions that tend to be common across processing tasks. It also provides the ability to run processing at the command line in addition to the within the Cumulus framework, which can be very valuable for development.

This repository, cumulus-process-py-seed, is a project template for creating a new Cumulus task in Python. Python is a good choice for processing data with existing code libraries because Python's subprocess library for spawning commands. This allows legacy code to be integrated into Cumulus while taking advantage of the convenience of Python for wrapping the legacy code.

### The Process class
Before delving into setting up a new task it's important to understand the basics of the cumulus_process library. The main part of the library is made up of the *Process* class which is meant to be subclassed in other projects. The processing unique to this new Process child class is defined in the *process* member function.

The Process class provides a host of functionality through the use of member functions and attributes. Entrypoints (AWS Lambda handler, Activity handler, and CLI) are also provided through the use of member functions, so that any child of the Process class will automatically inherit the functions and the entrypoints. The main advantage of using a class is that it minimizes the need for passing around a lot of variables between functions, while remaining very flexible as any of the existing functions can be overridden.

### GDAL and Geolambda
This template project utilizes [Cumulus Geolambda](https://github.com/cumulus-nasa/geolambda) to provide common geospatial libraries (e.g., proj.4, GDAL). If these libraries are not required then some of the files here can be simplified further. These are noted in the steps below.

## Creating a new cumulus-process based task

Follow the steps below to update the template files in this project in the creation of a new task. This will create a new pip-installable Python project, as well as a command line utility.

#### 1. Update setup.py and rename project folder

The setup.py file needs to be updated with the name of the project. Replace ${PROJECT} with what the name of the Python package will be. It is suggested to include preface the project name with 'cumulus_' to avoid any potential conflicts with other packages and for clarity (e.g., cumulus_modis, cumulus_aster, cumulus_mynewdatasource).

The *entry_points* field in setup allows the name of the command line interface (CLI) to be specified.
```
    entry_points={
        'console_scripts': ['${PROJECT}=${PROJECT}.main:Process.cli']
    },
```
The first part of the string, *${PROJECT}=* specifies the name of the CLI program, while the second half *${PROJECT}.main:Process.cli* specifies the path to the CLI entrypoint in your main module. For example:
```
    entry_points={
        'console_scripts': ['cumulus_mynewtask=cumulus_mynewtask.main:Process.cli']
    },
```
This will install a CLI program that can be called as **cumulus_mynewtask** and will point to the cli function in your Process class.

At this time also rename the *project* folder to match the same name as your project.

#### 2.  Write Process subclass (project/main.py)

The Process subclass will be a class specific to the Processing of a new type of data. While any of the Process members can be overridden in this subclass, most users will only need provide two functions: the *default_keys* property function, and the *process* member function.

The *default_keys* property is a way to hard-code a default set of keys and file patterns to identify which files are what so they can be referenced in the *process* function. It is used as a fallback when the parent class *input_keys* property fails to retrieve keys from the cumulus message config because none exist. For example:
```
    @property
    def default_keys(self):
        return {
            'hdf': r""^.*hdf$"",
            'thumbnail': r""^BROWSE.*hdf$"",
            'meta': r""^.*met$""
        }
```
We now can reference the keys *hdf, thumbnail, and meta* in the process function to get filenames rather than have to parse the input filenames manually.

The process function is where the actual processing code goes, and is a member function of your Process subclass. An instance of your Process subclass is created for a set of input files (if the CLI is used) or message (if used within the Cumulus framework). The Process subclass is instantiated with the the input file names and/or the config information from a Cumulus message, which is stored and accessible from within the Process function.

A complete list of member functions that you can use within the process class is given in the section below, but the most commonly used will be the *fetch* and *upload* functions. There are just a few requirements when writing a new Process subclass.

- **Clean up files**: This process may be deployed as a Lambda function or as a StepFunction activity running on ECS. In this type of deployment the containers can be reused between processes so it is important that any temporary files created are deleted. Files downloaded with the self.fetch() are automatically deleted when processing is over, the developer does not need to delete these.
- **Add output files to self.output**: Any output files that will be uploaded should be added to the self.output list. All files in self.output can be uploaded at once with the self.upload_output_files() function, and they will also be deleted when processing is over. Alternately, files can be uploaded with the self.upload(filename) function without adding them to self.output, in which case they need to be manuallly deleted.
- **Return S3 URLs from process function**: The process function must return the S3 URLs for all output files that were uploaded to S3.

#### 3. Update requirements files and MANIFEST.in

The requirements.txt file already includes the reference to cumulus-process. The requirements-dev.txt file contains requirements needed for development and testing, for now it just includes the *coverage* package for testing test code coverage. Update the requirements files to include your own Python requirements needed.

The MANIFEST.in file specifies files, other than Python source files, in the project directory that should be included in the deploy. Python files are included automatically, but other scripts, executables, small data or config files, etc. should be specified in the MANIFEST file. For more info see the [Python packaging tutorial](https://packaging.python.org/tutorials/distributing-packages/#manifest-in).

#### 4. Write test payload

The included test_main.py file shoud not need modification and is configured to read in a test payload and run the process function. The example payload here is based on MODIS.
```
{
  ""config"": {
    ""buckets"": {
      ""internal"": ""cumulus"",
      ""private"": ""cumulus"",
      ""public"": ""cumulus"",
      ""protected"": ""cumulus""
    },
    ""granuleIdExtraction"": ""(.*)"",
    ""url_path"": ""testing/cumulus-modis"",
    ""files_config"": [
      {
        ""regex"": ""^M.*\\.hdf$""
      },
      {
        ""regex"": ""^M.*\\.hdf.met$""
      },
      {
        ""regex"": ""^M.*\\.jpg$""
      },
      {
        ""regex"": ""^M.*\\.meta\\.xml$""
      }
    ]
  },
  ""input"": [
    ""s3://cumulus/testdata/modis/MOD09GQ.A2017001.h11v04.006.2017014010124.hdf"",
    ""s3://cumulus/testdata/modis/MOD09GQ.A2017001.h11v04.006.2017014010124.hdf.met""
  ]
}
```

There are several important pieces in the payload. The first is the list of input files. These could be local files, but it would make it difficult to test unless the data was included in the repository. For large data files this is not recommended. Instead, these can be put on an S3 bucket and the test process will fetch and process them.

The config section of the payload specifies the buckets to be used, as well as the files_config section. These fields are used in conjunction to determine where files should be published to. The Process.get_publish_info(filename) function will iterate through the regexes in files_config to find a matching file, then will use the *bucket* and *url_path* fields to generate the complete s3 URL. If this information is not provided (such as when calling from the command line with only a list of input files and no config info), then files will not be uploaded.

#### 5. Write Dockerfile and docker-compose.yml

The basic included Dockerfile simply installs the requirements files and the Python package. In this template it is using a Geolambda image which allows one to easily deploy to AWS Lambda (while including the common geospatial libraries). If geolambda is not used, then the developer will need to take care of packaging for Lambda themselves.

If the new Process subclass requires separate compiled code, then the Dockerfile is where that should be compiled and installed. This can be done in a variety of ways, but the recommended way is to install any compiled binaries with the PROJECT directory (alongside the Python files). Then, include these in the MANIFEST.in file so they are included in deployment, then call the executable from the Python code by getting the path dynamically:
```
exepath = os.path.dirname(__file__)
myexe = os.path.join(exepath, 'my.exe')
```

At the end of the Dockerfile, set the entrypoint to your CLI program specified in setup.py.
```
ENTRYPOINT ${CLI_NAME}
```

The included docker-compose provides several services for running and testing your process, as well as several services for deploying and testing to AWS Lambda. See the [docker-compose reference page](https://docs.docker.com/compose/compose-file/compose-file-v2/) for more info on docker-compose services. The image is built with
```
$ docker-compose build
```
and a specific service can be run with
```
$ docker-compose run servicename
```
The services in the included docker-compose.yml are:

- **base**: This simply builds the image, and if run will drop you into an interactive bash shell in the container
- **cumulus**: This will run the CLI program specified in your setup.py file (assuming ENTRYPOINT has been set in the Dockerfile)
- **test**: This runs the tests, with code coverage included
- **package**: This creates a zipfile of needed system libraries and Python dependencies suitable for deploying to AWS Lambda.
- **testpackage**: This runs the tests using the packaged files and a plain Amazon Linux image (see bin/ files below)
- **deploy**: This calls a deploy script to push the Lambda zip file to S3 (see bin/ files below)

#### 6. Update package/deploy scripts if using Lambda (optional)

If deploying to Lambda there are two scripts in the bin/ directory that are used for packaging and deploying, although in most cases they will not need to be edited.

- **deploy-to-s3.sh**: This script runs lambda-package.sh to create a zip file then uploads the file to s3, renamed with the version # of the package.
- **lambda-package.sh**: This file is a placeholder that just calls lambda-package-base.sh which is a script provided by Geolambda that packages the geospatial system libraries and Python packages on the system. This lambda-package.sh script can be used for performing additional steps beyond what is included in lambda-package-base.sh

#### 7. Update lambda_handler if using Lambda (optional)
The last step, if using AWS Lambda, is to update the lambda/lambda_handler.py file to import your new Process subclass from your Python package. Update the import line to import the correct package name that was set in setup.py.
```
from ${PROJECT} import MyProcess
```

## Process Command Line Interface
In addition to Lambda and Activity handlers, the Process class will automatically supply a CLI to your new Process subclass. Call it on the command line with ""-h"" to see online help. The CLI supports three different commands:

- **process**: This allows you to pass in a list of remote or local input files and generate local output. Useful for development.
- **payload**: This takes in a simple payload message, not a Cumulus message. Useful for testing.
- **activity**: Given an AWS SF Activity ARN, this runs a Cumulus activity (which expects Cumulus messages). Useful for production.

## Process Member functions

Below are descriptions of the Process class member functions (included properties and class methods). These can be caleld from within your *MyProcess.process()* function. They can also be overridden if needed.

### Properties

##### Process.input_keys
As described above, input_keys provides keys and file patterns so that files can be identified in the process function. These will be retrieved from the Cumulus message, or default to default_keys when none exist there. This allows a generalised process to dynamically handle a wide range of file patterns, or simply use default_keys when this functionality is not needed.
Note: In Cumulus workflows regex patterns for the input files are usually included, which are not the same as input keys. These regex expressions included in the Cumulus message are specific and used for validation. The regex patterns used in the process() function are simply for indentifying types of files and as such can be more general than the detailed regexes defined in the Cumulus workflow. An example of the difference is input file patterns for the collection, versus input_keys for a whole product family.

##### Process.default_keys
As described above, default_keys is meant to supply hard-coded keys and file patterns so that files can be identified in the process function. It is used as a fallback when no keys are specified in the Cumulus message config.

##### Process.gid
The gid property returns the ""Granule ID"", based upon the input files. The default property tries to generate the GID using the following 3 methods, in order of preference:

- 1. If *granuleIdExtraction* is provided in the config part the message it will be used against the 1st file passed in to generate the GID.
- 2. Otherwise, it will try to determine the GID by finding a common prefix among all input files.
- 3. If there is no common prefix the GID is the filename of the first input file.

If other behavior is desired for determining GID, this property should be overridden.

### Member Functions
##### Process.__init__(input, path=None, config={})
The __init__ function is what is called to initialize a Process instance. It takes in:

- **input**: a list of input files, these can be local files or S3 URLs and are stored in self.input
- **path**: A local path to store output files and temp files. The valus is stored in self.path Defaults to a tmp directory
- **config**: The config part of a Cumulus message, which is stored in self.config

##### Process.fetch(key, remote=False)
The fetch function is used to download remote input files and store them in *self.path*. Any and all input files matching the provided key (see input_keys) are downloaded and returned as a list. If *remote=True* then the original input filenames (remote or not) will be returned, and not downloaded. This can be useful to retrieve the remote filename when local processing is not required.

##### Process.fetch_all(remote=False)
This convenience function will run Process.fetch for all keys provided in Process.input_keys and return the resulting filenames as a dictionary.

##### Process.upload_file(filename)
For the provided filename, the Process class will try and retrieve publication info about it from the Process.config dictionary. If publishing info is not available upload_file will jsut return the local filename. If publication info is available then the filename will be uploaded to the proper bucket and the S3 url will be returned. See *Process.get_publish_info(filename)* for more info.

##### Process.upload_output_files()
This function will upload all files (using Process.upload_file) for all files in the self.output list.

##### Process.clean_downloads(), Process.clean_output(), Process.clean_all()
Process.clean_downloads will delete all downloaded files (these are stored in self.downloads by the Process.fetch function).
Process.clean_output will delete all local output files.
Process.clean_all will call the above two functions.

##### Process.get_publish_info(filename)
Using the passed in filename and the Process.config information, this will generate the S3 URL the file should be uploaded to, as well as the http URL that can be used to access it. The URL may to a publicly available bucket, or may be a private bucket accessible only through the URL.
```
{
    ""s3"": ""s3://bucket/path/myfile.dat"",
    ""http"": ""http://mydomain.com/path/myfile.dat""
}
```

### Class Methods

##### Process.dicttoxml(meta, pretty=False, root='Granule')
This function takes in a Python dictionary and converts it to XML. This was written to create XML files suitable for posting to the NAS Common Metadata Repository (CMR). If pretty=True then indents will be used when writing it.
Note: CMR requires data in a specific order, but Python dictionaries are unordered. This function will accept a Python ordered dictionary as well:
```
from collections import OrderedDict as od
mydict = od([('key1', 'val1'), ('key2', 'val2')])
myxml = self.dicttoxml(mydict)
```
This will create an ordered dictionary and then create the XML, ensuring that key1 in the XML comes before key2.

The related convenience function *Process.write_metadata(meta, fout, pretty=False)* will convert a metadata dictionary to XML and write it to a file named *fout*

##### Process.run_command(cmd)
This takes in a string command and spawns it using subprocess. Output will be logged and if an error occurs it will throw a RuntimeError.

##### Process.gunzip(filename, remove=False)
This function takes in a gzipped file and unzips it, creating a new file. If remove=True the original file will be deleted after.

##### Process.basename(filename)
This returns the basename of the file, without the path and without the extension.

### Handlers
These are handlers (aka Entrypoints) that can be used to perform a complete run. They are all class methods.

##### Process.run(*args, **kwargs, noclean=False)
The run function combines the initialization of a Process class, running the *process()* function, and cleaning up the input and output files afterward (if noclean=False). All of the other handlers end up calling this function.

##### Process.handler(event, context=None, path=None, noclean=False)
#### Process.cumulus_handler(event, context=None)
The handler function takes in a simple payload (as seen in test/payload.json), not a Cumulus message and calls Process.run().
The cumulus_handler function takes is the same as *Process.handler()* except it takes in a Cumulus message. It automatically sets noclean=False and uses a tmp directory for path. This is the entrypoint that would be used by a Lambda function.

##### Process.cli()
This is the entrypoint called by the Command Line Interface

##### Process.activity(arn=os.getenv('ACTIVITY_ARN'))
##### Process.cumulus_activity(arn=os.getenv('ACTIVITY_ARN'))
This is the handler that is called to run an AWS Step Function activity. Pass in the ARN for the Activity and tasks will be consumed, instantiating a new Myprocess instance for each message.

"
254,nasa/geolambda,Python,"# geolambda: geospatial Docker image and packaging for Amazon Linux

The geolambda project aims to make it easier to develop and deploy code to AWS Lambda functions, however the Docker images available also provide a ready to go Docker image based on Amazon Linux that contains common geospatial libaries and packages for other purposes.

## Usage

Unless interested in modifying the geolambda images themselves, most users will find most useful the product of this repository: a series of images available on Dockerhub. This repository contains a series of Dockerfiles that build upon one another to provide different versions of a geospatial Docker image for different appications.

### Available image tags

The developmentseed/geolambda image in Docker Hub has several available tags:

- **base**: The base image consists of an Amazon base image with python added, along with the boto3 and nose libraries. The purpose of the main image is to replicate the base system in Lambda with the nose testing framework added so that a deployable package can be tested on an image like what is used in the Lambda container.
- **core**: The core image contains system dependencies that are used by other other images. It also defines version numbers for packages that are installed in other images.
- **min**: GDAL2, with a minimized set of packages/drivers is installed with Python support, along with proj.4
- **hdf**: GDAL2, with HDF4 and 5 support added (including szip and proj.4)
- **cloud**: GDAL2, with libraries common for cloud access: GeoTiff and Jpeg2000 (using OpenJPEG)
- **full**: GDAL2 with all libraries compiled in other images

Pull whichever one is most appropriate with the docker command:

	$ docker pull developmentseed/geolambda:<tag>

### Creating a new geolambda based project

The geolambda image will most often be used an image used in the creation of a package suitable for deploying to an AWS Lambda function. There are two main use cases:

- No additional libraries are required, and the client application is a simpler handler function that draws upon libraries and packages that are already included in the geolambda image. In this case the only files really needed are the docker-compose.yml file and the handler.py function.
- Additional libraries, either installed via PyPi or Git, and custom modules are needed to run the Lambda function. In this case a new Dockerfile is needed to create a new image that will be used (along with docker-compose.yml and the handler.py function)

In either case, the files in the geolambda-seed directory in this repository can be used as a template to create your new Lambda function.

### Deploying to Lambda

The geolambda imgaes contain two scripts for collecting and packaging all the files needed to deploy to a Lambda function (the zip file can either be uploaded directly to a Lambda function or added to S3).


### geolambda Development

Contributions to the geolambda project are encouraged. The goal is to provide a turnkey method for developing and deploying geospatial Python based projects to Amazon Web Services. The 'master' branch in this repository contains the current state as deployed to the developmentseed/geolambda image on Dockerhub. The 'develop' branch is the development version and is not deployed to Dockerhub. To use the develop branch the images must be locally built first.
"
255,nasa/CCSDS-MAL-Http-Binding-Xml-Encoding,Java,"# CCSDS Message Abstraction Layer (MAL) Prototype using Http Binding & XML Encoding
* Visit CCSDS at [Homepage](https://public.ccsds.org/default.aspx)
* Visit MAL at [Wiki](https://en.wikipedia.org/wiki/Message_Abstraction_Layer)
* Visit Http Binding & XML Encoding at [specification](https://public.ccsds.org/Lists/CCSDS%205243R1/524x3r1.pdfgi)
## About
In order to verify the clarity and validity of the specification documents, two independent prototypes are implemented. 
After both prototypes are completed, they are verified by interoperability tests using a [test bed](https://github.com/esa/CCSDS_MO_TESTBEDS).
Four different types of tests were conducted and passed. 
1. Prototype-A Server to Prototype-B Client
2. Prototype-B Server to Prototype-A Client
3. Prototype-A Server to Prototype-A Client
4. Prototype-B Server to Prototype-B Client
## License
Copyright ¬© 2017-2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
## Installations
### Prerequisites
* Java-1.8
* Maven
### Modules
1. **CCSDS_MAL_Encode_HTTP**: XML Encoder converting `MAL java objects` to `XML` or an `XML` document to `MAL java objects`
2. **CCSDS_MAL_HEADER_HTTP**: Header Encoder converting HTTP headers into MAL header and vice versa
3. **CCSDS_MAL_TRANSPORT_HTTP**: Http Server to send and receive CCSDS messages
4. **CCSDS_MAL_IP_TEST**: Integration tests to verify Http Server is working as intended. 
### Instructions
1. Clone the repo
2. In `root` directory, run `mvn package`
   * Compiling java files
   * Running unit tests
   * Running integration tests
   * Creating a fat jar by combining all the libraries
   

"
256,nasa/FEI,Java,"# FEI5 Client Repository
Visit FEI5 at [FEI Overview](https://www-mipl.jpl.nasa.gov/mdms/Fei/feiOverview.html)
## About
The File Exchange Interface (FEI5) service offers secure file transaction, store, transport, and management services. FEI5 is the science data product management and distribution service used by most major space missions. The service offers a transaction-oriented approach in file management. That is, all concurrent updates to the same data product are prohibited. All uncommitted file transactions are automatically rolled back. The latest distribution, FEI5 software code name Komodo, is a complete redesign from its predecessors, which adopts the latest computing technologies and standards.
## License
Copyright ¬© 2002-2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
## Installations
### Prerequisites 
* Java-1.7 or later
* [Apache Ant](https://ant.apache.org/)
## Instructions
1. Clone this repository to a new directory. 
2. build FEI5 Client by running `ant clean package`
   * alternatively, `ant clean all` can be used to generate `javadoc`, and `checkstyle` tasks. 
   * `java` class files are saved in `build/classes/`
  
3. 3 jar files and 2 distribution packages are created in `dist/`
   * mdms.jar
   * mdms-komodo-client.jar
   * mdms-komodo-lib.jar
   * mdms-fei5.zip
   * mdms-fei5.tar.gz
4. Use distribution packages to create an FEI5-Client instance. The instruction is included inside the distribution packages. 




"
257,nasa/PLEXIL5,Java,"PLEXIL5 
========
The Plan Execution Interchange Language ([PLEXIL](http://plexil.sourceforge.net)) is an open source
synchronous language developed by [NASA](https://www.nasa.gov) for commanding and monitoring
autonomous systems. PLEXIL Formal Interactive Verification Environment
(PLEXIL5) is a tool that implements a formal executable semantics of
PLEXIL. PLEXIL5 provides a graphical interface that enable access to
formal verification techniques such as model-checking, symbolic
execution, theorem proving, and static analysis of plans. The
graphical environment supports formula editing and visualization of
counterexamples, interactive simulation of plans at different
granularity levels, and random initialization of external environment
variables.


PLEXIL‚Äôs operational semantics has been formally specified and key
meta-theoretical properties of the language, such as determinism and
compositionality, have been
[formally verified](https://shemesh.larc.nasa.gov/fm/PLEXIL)  in the [Prototype
Verification System](http://pvs.csl.sri.com/) (PVS). This formalization yields a formal
executable semantics of the language that serves as an efficient
formal interpreter and reference implementation of PLEXIL. This formal
semantics is at the core of the verification capabilities of 
PLEXIL5. The formal analysis capabilities offered by PLEXIL5 are based
on PLEXIL‚Äôs rewriting logic semantics written in the [Maude](http://maude.cs.illinois.edu/) system and verified in
PVS.

The graphical user interface has been developed in Java using
the model-view-controller pattern. The object oriented model
represents the hierarchical structure of plans, their execution
behavior, and the external environment. The view consists of several
classes that present the user with views of the tree-like-structure of
plans. The controller consists of a custom controller-facade class and
listener classes using and extending the Java framework.  PLEXIL5
supports a number of input formats defining plans. For this purpose,
the tool links a series of parsers and translators that internally

1.  generate the format supported by the rewriting logic semantics of the
language implemented in Maude and

2. construct an object oriented plan model from Maude‚Äôs output.

 Java and Maude communicate as processes at the operating system‚Äôs
level with help of the Java/Maude Integration API, developed as part
of the PLEXIL5 framework.

### Current Release

PLEXIL5 v0.0 (May-31-2018)

### Requirements

* [Apache Ant](https://ant.apache.org/)

* [Maude Interpreter Binaries](http://maude.cs.uiuc.edu/download/). Copy maude.linux64, maude.linux, maude.intelDarwin to `plexilite` folder.

## Compiling and running

In `build` directory:

```bash
$ ant run
```

### License

The code in this repository is released under NASA's Open Source
Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. 

### Authors

* [C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center, USA. 
* Hector Cadavid, Escuela Colombiana de Ingenier&iacute;a, Colombia.
* Camilo Rocha, Pontificia Universidad Javeriana de Cali, Colombia.
* Marco Feli&uacute;, National Institute of Aerospace, USA. 

### Copyright Notice

Copyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
258,nasa/dictionaries,HTML,"This repository contains a collection of NASA thesauri, dictionaries, taxonomies, and related documents, as well as a Python script to load concepts from a SKOS file.

**What You'll Find Here**
* Dictionaries
  * Acronyms
    * *Acronym Dictionary* from the NASA Center for AeroSpace Information
    * Human Space Flight acronym finder
    * Johnson Space Center (JSC) ""Acronym Central""
    * Kennedy Space Center (KSC) list of acronyms
  * *Concepts of Mathematics for Students of Physics and Engineering: A Dictionary* by Joseph C. Kolecki
  * Planetary Data System Data Dictionary
* Taxonomies
  * NASA Taxonomy 2.0
* Thesauri
  * NASA Thesaurus
* Python script
  * bin/*load_terms_from_skos.py*
  * *requirements.txt*


## Installation of Python Environment

There are any number of ways to set up your python environment to
use this code. My preferred one (described below) is using
[virtualenv](https://pypi.python.org/pypi/virtualenv).

```bash
# clone this repository to your local machine
git clone https://github.com/nasa/dictionaries.git

# switch to the local repository dir
cd dictionaries

# install virtualenv environment for python3
virtualenv -p <python_3_exe> ./env

# activate your environment
source lib/bin/activate.sh

# install requirements
pip install -r requirements.txt

#add aggregate dictionary code to python path
# there are many ways to do this, but for example
# for the bash shell on MacOS/Linux the command is:
export PYTHONPATH=`pwd`

```
"
259,nasa/SingleDop,Jupyter Notebook,"SingleDop README
----------------
SingleDop is a software module, written in the Python programming language, that will retrieve two-dimensional low-level winds from either real or simulated Doppler radar data. It mimics the functionality of the algorithm described in the following reference:
- Xu et al., 2006: Background error covariance functions for vector wind analyses using Doppler-radar radial-velocity observations. Q. J. R. Meteorol. Soc., 132, 2887-2904.  

The interface is simplified to a single line of code in the end user's Python scripts, making implementation of the algorithm in their research analyses very easy. The software package also interfaces well with other open source radar packages, such as the [Python ARM Radar Toolkit (Py-ART)](https://github.com/ARM-DOE/pyart). Simple visualization (including vector and contour plots) and save/load routines (to preserve analysis results) are also provided.

SingleDop Installation
----------------------
SingleDop works under Python 2.7 and 3.4 on most Mac/Linux setups. Windows installation and other Python versions are currently untested.

To install:  
`python setup.py install`

The following dependencies need to be installed first:

- A robust version of Python 2.7 or 3.4 w/ most standard scientific packages (e.g., numpy, matplotlib, scipy, etc.) - Get one for free here: https://store.continuum.io/cshop/anaconda/
- [The Python Atmospheric Radiation Measurement (ARM) Radar Toolkit (Py-ART)](https://github.com/ARM-DOE/pyart)
- [Python Turbulence Detection Algorithm (PyTDA)](https://github.com/nasa/PyTDA)
- [xarray - optional](http://xarray.pydata.org/en/stable/)

Specific import calls in the SingleDop source code:
```
from __future__ import division, print_function
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
import sys
import scipy
import math
import time
import warnings
import pyart
from pytda import get_sweep_data, get_sweep_azimuths, get_sweep_elevations, \
                  flatten_and_reduce_data_array
from .common import radar_coords_to_cart
from .cmap_map import lighten_cmap
try:
    import xarray
except ImportError:
    warnings.warn(
        'xarray not installed, cannot load or save SingleDop datasets')  
```

Using SingleDop
---------------
To access everything:
```
import singledop
```

A demonstration notebook is in the notebooks directory.
"
260,nasa/glm_ql,Python,"# glm_ql
quick-look imagery (PNG and GeoTIFF) when provided Level2 Operational GLM (Geostationary Lightning Mapper) data files
"
261,nasa/CFS_CI,C,"# Command Ingest

NASA core Flight System Command Ingest Application

## Description

Command Ingest (CI) application is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at http://cfs.gsfc.nasa.gov

The Command Ingest (CI) Application is responsible for receiving commands from an external source (such as a ground station) over a transport channel, and to forward the command to the appropriate application over the cFE Software Bus (SB).

## License

This software is licensed under the NASA Open Source Agreement. 
http://ti.arc.nasa.gov/opensource/nosa
"
262,nasa/CFS_TO,C,"# Telemetry Output

NASA core Flight System Telemetry Output Application

## Description

Telemetry Output (TO) application is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at http://cfs.gsfc.nasa.gov

The Telemetry Output (TO) Application is responsible for transmitting telemetry to external destination(s) (such as a ground station) over transport devices(s).

## License

This software is licensed under the NASA Open Source Agreement. 
http://ti.arc.nasa.gov/opensource/nosa
"
263,nasa/TLNS3D,Fortran,"The TLNS3D code was developed to solve Reynolds-averaged Navier-Stokes
Equations to simulate turbulent, viscous flows over three-dimensional
configurations. A general multiblock grid approach is used to model
complex configurations.  A multi-stage Runge-Kutta pseudo-time stepping
scheme is coupled with residual smoothing and multigrid acceleration
techniques to form an efficient algorithm for solving transonic viscous
flows over aerodynamic configurations of practical interest.

The TLNS3D framework is licensed under the Apache License, Version 2.0
(the ""License""); you may not use this application except in compliance
with the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0. 

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"
264,nasa/MISR-View,IDL,"0123456789012345678901234567890123456789012345678901234567890123456789012
==============
README
==============
March 8, 2018

This document is intended to be a brief guide to setting up and running
misr_view.  For more detailed information, please consult the misr_view
user's guide, located in the ""ug5.3_html"" directory under the ""external""
directory or online at https://nasa.github.io/MISR-View/ .
The FAQ below offers additional information.

Once misr_view is unpacked, there should be two top-level directories,
""external"" and ""src"".  The ""src"" directory contains all of the source
code for all of the modules that comprise misr_view.  The ""external""
directory contains all files that are needed to operate misr_view.
Generally speaking, the user does not need to recompile the source
code.

For information regarding the history of changes to misr_view, please
refer to the file ""CHANGELOG"".

PLEASE READ THE FAQ AT THE END OF THIS DOCUMENT FOR ANSWERS TO COMMON
QUESTIONS REGARDING THE INFORMATION CONTAINED HEREIN.

=========================================================================

OPTION #1
OPTION #1
OPTION #1
OPTION #1
OPTION #1
OPTION #1


STARTING UP MISR_VIEW WITHOUT A LICENSED VERSION OF IDL

=========================================================================
A ""licensed version of IDL"" means that you have purchased a copy of IDL
from Harris Geospatial Solutions, Inc. (http://www.harrisgeospatial.com).
If you have never dealt with this company, you probably do not have a 
valid IDL license.

In order to run misr_view without an IDL license, the IDL Virtual Machine
(IDL-VM) must be downloaded from the HARRIS website. Due to export 
control policies, download and installation of IDL (and subsequently the
IDL Virtual Machine) requires that you complete the registration and 
verification process via the HARRIS website.  Once done, click on 
'My Account' in the top right-hand corner, navigate to ""Downloads"", and
then ""All Downloads"". 
Select the appropriate platform to begin the download and installation 
process.
Installation of the Virtual Machine will not be covered in this document.

---------------
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
---------------
(1) Start the IDL-VM by double-clicking the IDL-VM icon or selecting the
IDL-VM via the Start button.

(2) Dismiss the IDL-VM window that appears by clicking the 
""click to continue"" button.

(3) When the selection dialog box appears, find and select the
""misr_view.sav"" file and click the ""OK"" button.  ""misr_view.sav"" will
be located in the ""external"" directory.

The data selection interface and main console should appear on the
screen.

-------------
Macintosh OSX
Macintosh OSX
Macintosh OSX
Macintosh OSX
Macintosh OSX
-------------
There are two ways to invoke the IDL-VM on a Mac, either via a terminal
window, or by clicking on the IDL-VM icon.  The latter will be covered 
here, the former below.

(1) Start the IDL-VM by double-clicking the IDL-VM icon.

(2) Dismiss the IDL-VM window that appears by clicking the 
""click to continue"" button.

(3) When the selection dialog box appears, find and select the
""misr_view.sav"" file and click the ""OK"" button.  ""misr_view.sav"" will
be located in the ""external"" directory.

The data selection interface and main console should appear on the
screen.

-----------------------------
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
-----------------------------
Please note that it is assumed that the IDL-VM has been configured
on your system.  If the steps below do not work, most likely the
IDL-VM is not set up properly on your system.  Consult with a system
administrator if you have one, or look at the online documentation
for the IDL-VM (http://www.rsinc.com/idlvm/idlvm.pdf).

(1) In a terminal window, type:

idl -vm

(2) Dismiss the IDL-VM window that appears by clicking the 
""click to continue"" button.

(3) When the selection dialog box appears, find and select the
""misr_view.sav"" file and click the ""OK"" button.  ""misr_view.sav"" will
be located in the ""external"" directory.

The data selection interface and main console should appear on the
screen.


=========================================================================

OPTION #2
OPTION #2
OPTION #2
OPTION #2
OPTION #2
OPTION #2


STARTING UP MISR_VIEW WITH A LICENSED VERSION OF IDL (PRE-COMPILED CODE)

=========================================================================
A ""licensed version of IDL"" means that you have purchased a copy of IDL
from Harris Geospatial Solutions, Inc. (http://www.harrisgeospatial.com).
If you have never dealt with this company, you probably do not have a 
valid IDL license.
Please see how to run misr_view without an IDL license (above)... Please
note that misr_view users who have a valid IDL license can run misr_view
as unlicensed users do...

If you *do* have a valid IDL license, follow the instructions below for
your specific operating system if you want to invoke misr_view from
pre-compiled code:

---------------
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
---------------
(1) Start up IDL.

(2) Make the ""external"" directory the current active directory.  For
example, if the desired current directory is 
C:\misr_stuff\misr_view\external\, the following command
would be entered at the IDL command line:

CD,'C:\misr_stuff\misr_view\external\'

(3) At the IDL command line, type the following:

RESTORE,'misr_view.sav'

(4) Then, simply type:

misr_view

The data selection interface and main console should appear on the
screen.

------------------------------
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
------------------------------
(1) Start up IDL in either command-line mode or the IDL Development 
Interface, ""idlde"".
(2) Make the ""external"" directory the current active directory.  For
example, if the desired current directory is 
/home/misr/misr_view/external/, the following command
would be entered at the IDL command line:

CD,'/home/misr/misr_view/external/'

(3) At the IDL command line, type the following:

RESTORE,'misr_view.sav'

(4) Then, simply type:

misr_view

The data selection interface and main console should appear on the
screen.


=========================================================================

OPTION #3
OPTION #3
OPTION #3
OPTION #3
OPTION #3
OPTION #3


STARTING UP MISR_VIEW WITH A LICENSED VERSION OF IDL (SOURCE CODE)

=========================================================================
A ""licensed version of IDL"" means that you have purchased a copy of IDL
from Harris Geospatial Solutions, Inc. (http://www.harrisgeospatial.com).
If you have never dealt with this company, you probably do not have a 
valid IDL license.
Please see how to run misr_view without an IDL license (above)... Please
note that misr_view users who have a valid IDL license can run misr_view
as unlicensed users do...

If you *do* have a valid IDL license, follow the instructions below for
your specific operating system if you want to compile and run misr_view
from source code:

---------------
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
Windows XP/2000/7/Vista
---------------
(1) Start up IDL.

(2) Make the ""src"" directory the current active directory.  For
example, if the desired current directory is 
C:\misr_stuff\misr_view\src\, the following command
would be entered at the IDL command line:

CD,'C:\misr_stuff\misr_view\src\'

(3) At the IDL command line, type the following:

.compile compile_all
RESOLVE_ALL

(4) Make the ""external"" directory the current active directory.  For
example, if the desired current directory is 
C:\misr_stuff\misr_view\external\, the following command
would be entered at the IDL command line:

CD,'C:\misr_stuff\misr_view\external\'

(5) Then, simply type:

misr_view

The data selection interface and main console should appear on the
screen.

-----------------------------
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
UNIX/Linux/Macintosh OSX/IRIX
-----------------------------
(1) Start up IDL in either command-line mode or the IDL Development 
Interface, ""idlde"".
(2) Make the ""src"" directory the current active directory.  For
example, if the desired current directory is 
/home/misr/misr_view/src/, the following command
would be entered at the IDL command line:

CD,'/home/misr/misr_view/src/'

(3) At the IDL command line, type the following:

.compile compile_all
RESOLVE_ALL

(4) Make the ""external"" directory the current active directory.  For
example, if the desired current directory is 
/home/misr/misr_view/external/, the following command
would be entered at the IDL command line:

CD,'/home/misr/misr_view/external/'


(5) Then, simply type:

misr_view

The data selection interface and main console should appear on the
screen.


-------------------------------------------------------------------------
   FFFFFFFFFF     AAAAAAAAAAA     QQQQQQQQQQ
   FFFFFFFFFF     AAAAAAAAAAA     QQQQQQQQQQ
   FF             AA       AA     QQ      QQ
   FF             AA       AA     QQ      QQ
   FFFFFF         AAAAAAAAAAA     QQ      QQ
   FFFFFF         AAAAAAAAAAA     QQ      QQ
   FF             AA       AA     QQ      QQ
   FF             AA       AA     QQ      QQ
   FF             AA       AA     QQQQQQQQQQQQ
   FF             AA       AA     QQQQQQQQQQQQQ
                                             QQ
-------------------------------------------------------------------------


=========================================================================
>>> QUESTION: How do I open and access the misr_view user's guide?

>>> ANSWER: First, you must have a common browser available, such as
Netscape, Firefox, Internet Explorer, Mozilla, or Safari.  The assumption
here is that the user knows how to start up the browser application.
Once the browser is active, use the ""Open File..."" or ""Open..."" menu item
under the ""File"" menu to open the file ""misr_view_ug5.3.html"", which is
located in the ""ug5.3_html"" directory under the ""external"" directory.
=========================================================================

=========================================================================
>>> QUESTION: Are there any differences in functionailty when running
misr_view without a valid IDL license?

>>> ANSWER: Yes.  When running misr_view with the IDL-VM, users will not
be able to modify or create transform files.  Some transform files are
provided with misr_view, and these should be sufficient for most users.
The transform interface is described in detail in the misr_view user's
guide.
=========================================================================

=========================================================================
>>> QUESTION: How does one make a directory ""active"" on a PC so that a 
licensed IDL user can run misr_view?

>>> ANSWER: There are several ways to do this; look in the IDL documents
to see how to add directories to a search list.  Another way to do this
is by using the IDL command ""CD"".  For example, if the desired current
directory is C:\misr_stuff\misr_view\external\, the following command
would be entered at the IDL command line:

CD,'C:\misr_stuff\misr_view\external\'
=========================================================================
0123456789012345678901234567890123456789012345678901234567890123456789012

"
265,nasa/mplStyle,Python,".. _plot2d_styles:

Plotting Styles
===============

The style system allow you to customize the look of your plots easily
and to maintain a consistent style on a large number of plots.  Styles
are used to control how plot elements look (color, marker style, line
style, font, etc.) and not how the plot is structured (line
vs. marker, which tick formatter to use, etc.).

They can be combined and chained together hierarchically.  They can be
edited programmaticaly via the python shell, or in a text editor.

All of these examples assume that you have imported the plot and style
manager into your script:

.. code-block:: python

   import pylab
   from mplStyle import mgr as smgr


Background and Problems
=======================

This style system was extracted from a much larger Python project and
so the current naming conventions, coding style, and package structure
is not ideal.  The conventions were those of the larger project and
have not been updated yet to be PEP8/MPL compliant.  The various
internal dependencies were extracted from the larger system as well so
the current package structure could be made a lot simpler.  This
overview document was also extracted from a larger custom Sphinx
documentation project and needs to be updated to work properly as a
standalone RST document.

If there is sufficient interest in this kind of style system, the hope
is that it can modified into a standard MPL package and delivered with
MPL.  At that point these style, documentation, and test conventions
can be addressed.

------------------------------------------------------------------------

.. _plot2d_styles_overview:

Overview
--------

The style system has two primary user interfaces: the style manager class,
and the style class.  The style manager is responsible for creating styles,
applying styles to plots, saving and loading styles, and remembering
which styles were applied to which plots so they can be automatically
updated.

The style class is responsible for storing the set of properties to
change in the plot.  Styles have a variety of display parameters,
applying to different parts of the plot.  Each parameter can be set or
unset.  When the style is applied to a plot, those parameters which
are set will be implemented; those parameters which are unset will be
ignored.

The structure of a style reflects the structure of a figure: in a figure,
there are usually  several contained objects (for example, an Axes object, 
a Line object, etc.).  Likewise, in a style the parameters are organized by
the type of object they affect: there are portions that affect only Axes
objects, only Line objects, etc.

The main style structure has some default attributes (bgColor,
fgColor, line, patch, text) which apply to any appropriate plot
element.  If you set a property in the text attribute of a style, it
will apply to any text in the plot (which is a nice way to control
fonts, colors, etc).  The figure and axes attributes are used to
control specific parts of the plot (e.g. axes.xAxes applies just to
the X axis and axes.yAxes applies just to the Y axes).

The basic MplStyle structure looks like this:

      +--------------+-----------------------------+-------------------------------------+
      | **Property** | **Type**                    | **Description**                     |
      +==============+=============================+=====================================+
      | axes         | `Axes <doc/axes.rst>`__     | Controls how to style an Axes and   |
      |              |                             | all it's components.                |
      +--------------+-----------------------------+-------------------------------------+
      | bgColor      | `Color <doc/color.rst>`__   | The default background color to use |
      |              |                             | for a sub-style if none is          |
      |              |                             | is specified (i.e. the default      |
      |              |                             | background color).                  |
      +--------------+-----------------------------+-------------------------------------+
      | fgColor      | `Color <doc/color.rst>`__   | The default foreground color to use |
      |              |                             | for a sub-style if none is          |
      |              |                             | is specified (i.e. the default      |
      |              |                             | foreground color).                  |
      +--------------+-----------------------------+-------------------------------------+
      | figure       | `Figure <doc/figure.rst>`__ | Controls how to style a Figure.     |
      +--------------+-----------------------------+-------------------------------------+
      | line         | `Line <doc/line.rst>`__     | This specifies the style properties |
      |              |                             | for line elements plotted on an     |
      |              |                             | Axes.                               |
      +--------------+-----------------------------+-------------------------------------+
      | patch        | `Patch <doc/patch.rst>`__   | This specifies the style properties |
      |              |                             | for patch elements plotted on an    |
      |              |                             | Axes.                               |
      +--------------+-----------------------------+-------------------------------------+
      | text         | `Text <doc/text.rst>`__     | The default text style to use.      |
      |              |                             | Sub-style elements may override for |
      |              |                             | specific pieces of text.            |
      +--------------+-----------------------------+-------------------------------------+

.. note::

   The full list of available parameters can be found here:
   `Full List <doc/style_all.rst>`__ 

------------------------------------------------------------------------

.. _plot2d_styles_create:

Style Creation
--------------

Styles are created by calling the manager create method with the name
of the style.  Once the style has been created, set the parameters on
the style using the names of the style attributes (see the top level
style structure above for the possible names)

.. code-block:: python

   # Create the style
   style = smgr.create( ""Big Title"" )

   # Change the axes title font.
   style.axes.title.font.family = ""sans-serif""
   style.axes.title.font.size = 24

You can also pass a dictionary of style attributes to the create
method to create and initialize the style in one call.  The keys of
the dictionary are the string form of the variable path: so the
variable style.this.parameter.path becomes the dictionary key
'this.parameter.path'.  The value associated with each key needs to be
of the proper type for the parameter; some require floating point
values, integers, booleans, or other strings.

.. code-block:: python

   # Create the style
   style = smgr.create( ""Big Title"", {
                        ""axes.title.font.family"" : ""sans-serif"",
                        ""axes.title.font.size"" : 24,
                        } )


When defining a style, you can optionally name a parent style.  When the 
style is applied, the parent style is automatically applied first.  This
means that a child style will overwrite the settings contained in the
parent style, if and when those styles conflict.

.. code-block:: python

   # Create the style to make fonts larger and easier to read.
   s1 = smgr.create( ""Big Text"" )
   s1.text.font.scale = 1.5

   # Create a new style, with a parent style   
   s2 = smgr.create( 'Presentation', parent='Big Text' )

   # Set something particular to the child style
   s2.figure.width = 800
   s2.figure.height = 600

   # Big Text will be applied before the other parts of Presentation
   smgr.apply( fig, 'Presentation' )

It should be noted that in the above example the 'Big Text' style is
*not* overwritten when we create the 'Presentation' style.  This is
because when we create styles in this manner, they are automatically
registered and stored in a style manager class.  They can then be
access later by name.


Setting Attributes
------------------

Each Style object has a set of parameters affecting how plots are
displayed.  The parameters are unset by default; they will not affect
the display of a plot unless they are set.  In an existing style
object, you can use Python's dot syntax to access and set parameters.

To access an already defined style, use the find() method on the manager

.. code-block:: python

   # Find a previous define dstyle
   style = smgr.find( ""Big Text"" )

   # Change some of the style attributes
   style.text.font.size = 16


Applying Styles to a Plot
-------------------------

Styles can be applied to any matplotlib plot element (figures, axes,
axis, lines, etc).  Applying the style to the figure is most common
use case.  When you apply a style to a figure, it will search the
figure for various plot elements and axes and recursively apply the
same style to them (the same is true when calling apply only on the
axes).

The style attributes dictate which matplotlib elements are modified.
So the attribute axes.bgColor will only change the color of the axes
while the attribute figure.bgColor will change the background for the
figure.

Style are applied using the apply method on the manager.  You can pass
in the style object or the name of the style to apply.

.. code-block:: python

   fig, ax = pylab.subplots()
   lines = ax.plot( [1, 2, 3, 4, 5], [2, 1, 0, 1, 2] )
   ax.set_title( ""This is the Title"" )
   
   # Create the style and set some fields
   style = smgr.create( ""Big Title"" )
   style.axes.title.font.family = ""sans-serif""
   style.axes.title.font.size = 24

   # Apply the style to the figure.
   smgr.apply( fig, style )

   # Apply a list of styles to just the lines.
   smgr.apply( lines, [ ""Dashed Lines"", ""Blue Lines"", ""Bold Lines"" ] )

The style manager will recursively walk down through the input plot
element and apply the style.  So if a plot contains four subplots, the
axes style will be applied to each of the four subplots and the text
style will apply to all the text in the plot.  If you want to apply
the style only the input object (say an input figure), pass
recurse=False to the apply method.

.. code-block:: python

   # Apply the style only to the figure
   smgr.apply( fig, 'Figure Style', recurse=False )


Updating and Re-applying Styles
-------------------------------

When the manager applyes a style to the figure (or to any other other
plotting element), the manager will remember what style was applied to
which element, so if you later modify any styles, the changes can be
automatically applied to the plot elements by calling the reapply
method.

.. code-block:: python

   # Modify the style
   style.axes.title.font.size = 16

   # Apply the update to everything that is currently using it.
   smgr.reapply()

This will change the fonts from size 24 (the original ""Big Title""
size) to the new size of 16 and update the plot.  The reapply() method
will update any and all plots that have styes applied to them.


Saving & Loading
----------------

The style manager can be used to save and load styles to a persistent
form.  Each style is saved into a file with the form
'Style_Name.mplstyle'.  Style files are human readable, Python files
and may be edited by the user.  Styles are NOT automatically saved and
loaded by the manager (though that could change based on user
feedback).

.. note::

   Style names including a space ' ' will be changed to use an
   underscore '_' when saved as a .mplstyle file.  For Example, 
   a style named ""DSS 16"" will be saved as ""DSS_16.mplstyle"".

To save the current set of styles, use the manager save method.  To
load all the available styles, use the load method.

.. code-block:: python

   # Save the current styles to $HOME/.masar/styles
   smgr.save()

   # Save the current styles to the local directory.
   smgr.save( ""."" )

   # Load all available styles.
   smgr.load()

When loading styles, the manager will use a search path that looks for
styles in the following order (high priority to low priority):

#. The current directory.
#. The user's home directory: $HOME/.matplotlib/styles/

Styles that are defined in more than one of these locations will use
the first definition.  This way, each user can override and customize
certain Monte styles to their liking; they can also use different
directories to try out different style options in parallel.  You can
change the list of directories to look in by modifying your STYLEPATH
environment variable.

You can also manipulate the loading and saving of styles in your
Python script directly.  The ""path"" variable on the style manager is a
simple Python list of directory names.  By changing the path, you can
change what styles are loaded:

.. code-block:: python

   # Add a search path and load the styles.
   smgr.path.append( ""/proj/scripts/styles"" )
   smgr.load()


Tagging Plot Elements
---------------------

.. _plot2d_styles_tags:

Tagging or style tags are way to filter which plot elements (figure,
axes, lines, etc) a style is applied to by setting a tag (string name)
to a plot element.  The script that creates the plot tags each element
with a name.  When a style is applied to an element, the tag input can
be specified to limit which elements get changed.

Let's say you have a plot that shows two lines for each DSN complex
(Goldstone, Canberra, and Madrid).  The plotting script has access to
those lines and knows which complex they are a part of but the lines
are hard to get to after the plotting script is finished.  If the
plotting script tags the lines with the complex name like this:

.. code-block:: python

   def createPlot():
      fig, ax = pylab.subplots()
      # create data to plot, layout plot, etc.

      l = ax.plot( gldX, gldY )
      smgr.tag( l, ""Goldstone"" )

      l = ax.plot( madX, madY )
      smgr.tag( l, ""Madrid"" )

      l = ax.plot( canX, canY )
      smgr.tag( l, ""Canberra"" )

      return fig

The calling script can use those tags to apply styles to the
individual lines without having direct access to them.  Both the
apply() and set() functions can use the tag keyword to filter which
elements are used.

.. code-block:: python

   fig = createPlot()

   # Apply the 'Goldstone Style' to elements tagged Goldstone
   smgr.apply( fig, ""Goldstone Style"", tag=""Goldstone"" )

   # Change every line tagged Canberra to be blue.
   smgr.set( fig, { 'line.color' : 'blue' }, tag=""Canberra"" )

Tags are a powerful tool that allows you to write complicated plotting
scripts and then control individual elements in those plots using
styles from outside the plotting script.


.. _plot2d_styles_unmanaged:

Setting Attributes and Unmanaged Styles
---------------------------------------

The style system can also be used to quickly set plot attributes
without creating a style by calling the manager set() method.  This
method can accept either a single style attribute or a dictionary of
style attributes and can use the tag system to filter which plot
elements are set.

.. code-block:: python

   # Change the background color to black.
   smgr.set( fig, ""bgColor"", ""black"" )

   # Change the multiple attributes.
   smgr.set( fig, { ""bgColor"" : ""black"",
                    ""fgColor"" : ""white"",
                    ""text.font.scale"" : 1.25 } )

   # Change lines tagged 'DSS 14' to gold.
   smgr.set( fig, ""line.color"", ""gold"", tag=""DSS 14"" )

An ""unmanaged"" style can be created using the style constructor and
applied directly to a plot.  The style manager will have no knowlege
of this style and so reapply will not work, and the style will not be
saved.

.. code-block:: python

   import mpy.plot.style as S

   # Unmanaged style - won't be saved.
   style = S.MplStyle( 'dummy' )

   # Must use style.apply(), smgr.apply() won't work.
   style.apply( fig )


------------------------------------------------------------------------

.. _plot2d_styles_example:

An Example
----------

Following is a more complete example on how to make the plot at the top of
this page:

.. code-block:: python

   # import some modules
   import pylab
   from mplStyle import mgr as smgr

   # create the plot
   fig, ax = pylab.subplots()

   xdata = [ 1, 1.5,  2, 2.5,  3, 3.5,  4, 4.5,  4.75, 5 ]
   ydata = [ 1, 1.75, 2, 2.75, 3, 2.75, 2, 2.25, 2.75, 3 ]
   line = ax.plot( xdata, ydata )

   rect = mpylab.Rectangle( (2.8, 1.0), 0.4, 1.2 )
   ax.add_patch( rect )

   figTitle = fig.suptitle( ""Figure Title"" )
   axTitle = ax.set_title( ""Axes Title"" )
   xLabel = ax.set_xlabel( ""X-Axis Label"" )
   yLabel = ax.set_ylabel( ""Y-Axis Label"" )

   figText = fig.text( 0.02, 0.02, ""FigureText"" )
   txt = ax.text( 4.2, 1.1, ""Text"" )

   # Create the style
   style = smgr.create( ""My Style"" )
   style.bgColor = 'white'
   style.fgColor = 'black'
   # Figure
   style.figure.width = 10
   style.figure.height = 10
   # Axes
   style.axes.axisBelow = True
   style.axes.leftEdge.color = 'magenta'
   style.axes.leftEdge.width = 5
   style.axes.leftEdge.style = '--'
   style.axes.bottomEdge.color = 'magenta'
   style.axes.bottomEdge.width = 5
   style.axes.bottomEdge.style = 'dashed'
   style.axes.topEdge.visible = False
   style.axes.rightEdge.visible = False
   style.axes.title.font.scale = 2.0
   style.axes.title.font.family = 'sans-serif'
   # X-Axis
   style.axes.xAxis.autoscale = True
   style.axes.xAxis.dataMargin = 0.1
   style.axes.xAxis.label.font.scale = 1.2
   style.axes.xAxis.majorTicks.labels.font.scale = 0.75
   style.axes.xAxis.majorTicks.marks.visible = True
   style.axes.xAxis.majorTicks.grid.visible = True
   style.axes.xAxis.majorTicks.grid.color = '#B0B0B0'
   style.axes.xAxis.majorTicks.grid.width = 1.5
   style.axes.xAxis.majorTicks.grid.style = ':'
   style.axes.xAxis.majorTicks.length = 15.0
   style.axes.xAxis.majorTicks.width = 1.5
   style.axes.xAxis.minorTicks.marks.visible = True
   style.axes.xAxis.minorTicks.grid.visible = True
   style.axes.xAxis.minorTicks.grid.color = '#B0B0B0'
   style.axes.xAxis.minorTicks.grid.width = 0.5
   style.axes.xAxis.minorTicks.grid.style = ':'
   style.axes.xAxis.minorTicks.length = 5.0
   style.axes.xAxis.minorTicks.width = 0.5
   # Y-Axis
   style.axes.yAxis = style.axes.xAxis.copy()
   # Lines
   style.line.color = ""blue""
   style.line.style = 'dash-dot'
   style.line.width = 1.5
   style.line.marker.color = 'red'
   style.line.marker.edgeColor = 'green'
   style.line.marker.edgeWidth = 3
   style.line.marker.size = 20
   style.line.marker.style = 'circle'
   style.line.marker.fill = 'bottom'
   # Patches
   style.patch.color = 'gold'
   style.patch.filled = True
   style.patch.edgeColor = 'purple'
   style.patch.edgeWidth = 5
   # Text
   style.text.lineSpacing = 1.0
   style.text.font.size = 12
   style.text.font.family = 'monospace'

   # apply the style
   smgr.apply( fig, style )
"
266,nasa/EADINLite,C++,"# EADINLite
EADIN_Lite Network Protocol 
 
Authored by Eliot Aretskin-Hariton: eliot.d.aretskin-hariton@nasa.gov, [Intelligent Control and Autonomy Branch](http://www.grc.nasa.gov/WWW/cdtb/index.html),
NASA Glenn Research Center,


**Network Protocol Summary Stats:**
* Half-Duplex & Hard Real-Time, works with Wired or Wireless networks
* Command / Response protocol using 1 Master / Multiple Slave Architecture
* 8 Byte payload
* RTT Performance (See Table and Note below)  
      
|  Speed       |  TYPICAL        | WORSE CASE      |
|:------------:|:---------------:|:---------------:|
| 4000000 baud |     943 +/- 13  |   981 +/- 13    |
|  921600 baud |   1,197 +/- 15  |  1,280 +/-  7   |
|  115200 baud |   4,467 +/- 12  |  4,907 +/-  7   |
|    9600 baud |  45,798 +/- 12  | 50,750 +/- 20   |
|     units    | (micros 1-sigma)|(micros 1-sigma) |

Note: Performance based on message Round Trip Time (RTT), which includes
formulation of the message by the master, receipt of message by slave
and recept of respons from slave by master. master -> slave -> master. 
Time is expressed in microseconds.

* Memory Requirements

| Memeory Req. | Program Storage (Bytes) | Dynamic Memory (Bytes) |
|:------------:|:-----------------------:|:----------------------:|
| Default      | 9,496                   |    900                 |
| Minimum      | 8,604                   | 388                    |

* Successfull Deployment
	* Arduino Yun
	* Arduino Mega 2560 (the $10 version)
* Expected Compatibility
	* Arduino Pro Micro
	* Any Arduino with Hardware Serial ports

**Overview:**
This code was created to support the Distributed Engine Control task
as part of the Fixed Wing Aeronautics project. The purpose of this research 
was to enable multiple microcontrollers to speak with eacho ther per the
protocol specified in the preliminary release of EADIN BUS. EADIN BUS is a 
candidate for distributed control systems on aircraft engines and is being
worked on by the Distributed Engine Control Working Group (DECWG) 
http://www.decwg.org/. The primary use of this code was to assist in the 
modelling of small local networks which contain 16 or fewer nodes. 
Ultimately, we expect this network to be implemented in an FPGA or ASIC 
as opposed to it's current implementation on a microcontroller. 

This communication protocol uses a master node which distributes 
information between nodes through a call and response system. The RS-485 
network is simplex and thus does not allow multiple nodes to talk at 
the same time. No time synchronization between nodes is required for 
this network. These factors enable the master to request information 
from sensors and command actuators, one at a time. In the current 
implementation, no information is passed from individual nodes without 
first going through the master node. 

While other communication protocols do exist like ModbusMaster and simple-modbus,
the speed of these communication protocols on the RS-485 network was not 
sufficient for our needs which required message send to reply receipt times
of 1 millisecond. Additionally, the other protocols did not implement the 
same message system as specified by the preliminary documents regarding
the EADIN protocol.

**Details:**
The EADIN protocal as implemented by this code has the following structure:
Total Size: 18 bytes
* Preamble: 3 bytes
	* 2 bytes start of message (0x00, 0x01)
	* 1 byte a sync  byte (0x55)
* Headder: 5 bytes 
	* 1 byte request type (ENQ/ACK/NAK = 0x05/0x06/0x15)
	* 1 byte node_ID of desination
	* 1 byte node_ID of sender
	* 1 byte unique message number
	* 1 byte extra space to be used in future development
* Data: X bytes (8 bytes Default)
	* 8 bytes DATA_L (can be modified)
* Footer: 2 bytes 
	* 2 bytes CRCFast (a 16 bit CRC, Default)


**Updates:**
Version 2 is incompatible with previous versions as it is constructed with 
different function calls using an object oriented programming approach for
easier use. The code now contains built in timing functions which should
enable the user to simply call OBJ.read() OBJ.write() functions without worrying
about inserting delays between the write and read operations. These delays
should scale with network speed selected from 9600 - 4000000 baud. 

**References:**
[EADIN Lite Communication Network](http://www.techbriefs.com/component/content/article/ntb/tech-briefs/electronics-and-computers/23450)

Eliot Aretskin-Hariton, [Benchmarking Variants of a Hardware-in-the-Loop Simulation System](http://arc.aiaa.org/doi/abs/10.2514/6.2016-1425)

Eliot Aretskin-Hariton, [A Modular Framework for Modeling Hardware Elements in Distributed Engine Control Systems](http://arc.aiaa.org/doi/abs/10.2514/6.2014-3530)

Dennis Culley, [Developing an Integration Infrastructure for Distributed Engine Control Technologies](http://arc.aiaa.org/doi/abs/10.2514/6.2014-3532)

Ross N. Williams, [A painless guide to CRC Error Detection Algorithms](http://www.ross.net/crc/download/crc_v3.txt)"
267,nasa/GFR,Fortran,"# GFR
GFR (Glenn Flux Reconstruction) is a high-order computational fluid dynamics
(CFD) Fortran code for large-eddy simulations. It is based on the simple and
efficient flux reconstruction method and accurate to arbitrary order through a
user-supplied input parameter. It is currently capable of using unstructured
grids containing quadrilateral and hexahedra elements.
"
268,nasa/mct,Java,"
### _The desktop client is no longer under active development, as our development efforts are now focused on [Open MCT](https://nasa.github.io/openmct) for the web and mobile devices._

Open MCT Desktop
--

The [MCT](https://sites.google.com/site/openmct/) project was developed at the NASA Ames Research Center for use in spaceflight mission operations, but is equally applicable to any other data monitoring and control application.

Getting Started
--
1. MCT is built using Maven (Java SE6), so start by downloading [maven 2.2.1 or greater](http://maven.apache.org/download.html)
2. Clone the git repository `git clone https://github.com/nasa/mct.git` into a local folder (referred to as `MCT_HOME`).
3. Run `mvn -N install` from the `MCT_HOME/superpom` directory.
4. Run `mvn clean install -Dmaven.test.skip=true -Ddistribution` from the `MCT_HOME/platform-assembly` directory.
   1. If Maven complains about missing dependencies org.eclipse:equinox-osgi:jar:3.5.1 or org.eclipse:equinox-osgi-services:jar:3.2.0, download the JARs for the two plugins from http://archive.eclipse.org/equinox/drops/R-3.5.1-200909170800/index.php.  Then follow the instructions Maven provides for installing the JARs.
5. The platform distribution archive can be found in the `MCT_HOME/platform-assembly/target` directory.
6. Extract the distribution archive, i.e. `mct-platform-1.8b4-dist.tar.gz` to the directory you wish to install MCT.
   The subdirectory `mct-platform-1.8b4` will be created from the archive (referred to as `MCT_DIST`).
7. Run `MCT.jar` from the extracted MCT directory. On most systems, this can be done with a double-click from a file browser; from the command line, `java -jar MCT.jar`

Working on MCT
--
[Work on MCT in Eclipse](https://github.com/nasa/mct/wiki/How-to-build-and-run-MCT-in-Eclipse)

[Building a MySQL database](https://github.com/nasa/mct/wiki/Creating-a-MySQL-database-for-MCT)

[Using a Derby database](https://github.com/nasa/mct/wiki/Using-Derby-in-MCT)

[Contributing to MCT](https://github.com/nasa/mct/wiki/Contributing-to-MCT)
"
269,nasa/openmct-heatmap,JavaScript,"# Open MCT Heatmap

A plugin for [Open MCT](https://nasa.github.io/openmct)
adding heat map style visualizations of telemetry data.

## Build

```bash
$ git clone https://github.com/VWoeltjen/openmct-heatmap.git
$ cd openmct-heatmap
$ npm install
```

A UMD module with associated source maps will be written to the
`dist` folder. When installed as a global, the plugin will be
available as `HeatmapPlugin`.

## Usage

See [`index.html`](index.html) for an example of use.

## Developer Environment

Follow build instructions, then trigger a build of `openmct`:

```bash
cd node_modules/openmct
npm install
cd ../..
```

To serve the application, use `webpack-dev-loader`:

```bash
npm install -g webpack webpack-dev-loader
webpack-dev-loader
```

There is an example `index.html` included which provides
a basic instance of Open MCT with this plugin installed for development
purposes.
"
270,nasa/SC,C,"# Stored Command

NASA core Flight System Stored Command Application

## Description

The Stored Command application (SC) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

The SC application allows a system to be autonomously commanded 24 hours a day using sequences of commands that are loaded to SC. Each command has a time tag associated with it, permitting the command to be released for distribution at predetermined times. SC supports both Absolute Time tagged command Sequences (ATSs) as well as multiple Relative Time tagged command Sequences (RTSs).

## License

This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa
"
271,nasa/MM,C,"# Memory Manager

NASA core Flight System Memory Manager Application

## Description

The Memory Manager application (MM) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

The MM application is used for the loading and dumping system memory. MM provides an operator interface to the memory manipulation functions contained in the PSP (Platform Support Package) and OSAL (Operating System Abstraction Layer) components of the cFS. MM provides the ability to load and dump memory via command parameters, as well as, from files. Supports symbolic addressing.

MM requires use of the [cFS application library](https://github.com/nasa/cfs_lib).

## License

This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa
"
272,nasa/cfs_lib,C,"# cfs_lib

NASA core Flight System Application Library

## Description

The cFS Application Library (cfs_lib) is a core Flight System (cFS) library provides functions used by several cFS applictions.

The cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)

## License

This software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa
"
273,nasa/LH2Sim,Matlab,"# LH2Sim

Matlab simulation of cryogenic tanks with temperature stratification.

## Quickstart

In Matlab, run `testSim.m`. This script sets up the model parameters, runs the
simulation, and plots the results.

## Citation

For details on system being simulated and the physics models, see:

M. Daigle, J. Boschee, M. Foygel, and V. Smelyanskiy, ""Temperature Stratification in a Cryogenic Fuel Tank,"""" AIAA Journal of Thermophysics and Heat Transfer, vol. 27, no. 1, pp. 116-126, January 2013.

To cite this software, please use:

M. Daigle, M. Foygel, V. Smelyanskiy, J. Boschee; LH2Sim [Computer software]. (2017). Retrieved from https://github.com/nasa/LH2Sim.

## Contributions

Contributions are welcome. Please submit pull requests to the `develop` branch. By contributing to this project, you are promising that the work contributed is your own and you have the rights to contribute it.

## Notices

Copyright ¬©2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
274,nasa/OpenVSP3Plugin,Java,"= OpenVSP3Plugin
A plugin to run OpenVSP in a MDAO frameworks.

== What is OpenVSP?

Open Vehicle Sketch Pad (OpenVSP) is a parametric aircraft geometry tool.
OpenVSP allows the user to create a 3D model of an aircraft defined by common engineering parameters.
This model can be processed into formats suitable for engineering analysis.footnoteref:[openvsporg, from www.openvsp.org/learn.shtml]

The predecessors to OpenVSP have been developed by J.R. Gloudemans and others for NASA since the early 1990's.
On January 10 2012, OpenVSP was released as an open source project under the NASA Open Source Agreement (NOSA) version 1.3.footnoteref:[openvsporg]

== What is the OpenVPS3Plugin?

The OpenVSP3Plugin is a JAVA software tool that allows for the interface between OpenVSP and an analysis framework such as *Phoenix Integration's ModelCenter* or the NASA developed *OpenMDAO* software.
The OpenVSP3Plugin is designed to work on any computer desktop platform (Mac, Linux, and Windows) that has a local installation of JAVA.

.[big]#The OpenVSP3Plugin performs three main functions:#
1. Parse an OpenVSP v. 3 file
2. Auto-generate an OpenVSP script and a design (.des) file
3. Execute OpenVSP with the auto-generated script and apply user selected design variables in the design file. (ModelCenter only)

The user selects the design variables written in the design file through the interactive graphical user interface.

== Build

To build the OpenVSP3Plugin, edit the build.bat file and run it.
Edit the build.bat file and set *JDK_HOME* to the path of your JAVA installation.
If you have *ModelCenter* installed set *MC_INSTALL* to the path of your *ModelCenter* installation.


[source,options-""nowrap""]
----
:: JDK_HOME is required and must point to an installed JDK 
set ""JDK_HOME=C:/Program Files/Java/jdk1.8.0_151""

:: MC_INSTALL is optional but required to use the plugin with ModelCenter
:: Comment this line if you don't have ModelCenter
::set ""MC_INSTALL=C:/Program Files/Phoenix Integration/ModelCenter 12.0""
----

== Prebuilt releases (OpenVSP3Plugin.jar)

Prebuilt releases will be built using the latest source code and JAVA 8 JDK and ModelCenter 12 interfaces

== Installation

There is no custom installer for the OpenVSP3Plugin, it must be done by hand. +
The OpenVSP3Plugin uses 2 environment variables:

`*OpenVSP_EXE* (__Required__)`:: This needs to be set to the path of the OpenVSP executable (vsp.exe) that the plugin will use.
`*TMP* (__ModelCenter only__)`:: (probably already set on your machine) Is used to create temporary folders in which OpenVSP is executed, and the output files are parsed.

== OpenMDAO

To use the OpenVSP3Plugin in an OpenMDAO environment simply execute the OpenVSP3Plugin.jar file.
This can be accomplished by running the following command: +
`java -jar OpenVSP3Plugin.jar`

In this mode all temporary files are written to the current working directory.

== ModelCenter

See ModelCenter help on installing plugins and ""registrationless plugins"".

OpenVSP3Plugin logging can be viewed in the ModelCenter JAVA Console.
Open the ""Preferences"" dialog, `Tools->Preferences...`, select the `Java Plug-Ins` tab,
and check the `Show Java Console` checkbox.

== Documentation

A presentation on how to use the OpenVSP3Plugin is located here:
link:presentations/welstead-openvsp3plugin.pdf[OpenVSP Workshop 2017 OpenVSP3Plugin]

"
275,nasa/SAFE-DART,C++,"# Software Architecture Framework for Extensibility (SAFE-DART)

## What is SAFE-DART?
SAFE-DART is a simple framework for projects written in the C++ programming language and using the Qt framework. SAFE-DART simplifies the modularization of object-oriented software, allowing software implementations of the components of a software system to be provided at run-time by modules (shared libraries).

SAFE-DART is split into two parts: the SAFE-DART library (libsafedart) and the SAFE-DART executable (safedart). The library is linked to any projects which use SAFE-DART, and provides the necessary functionality for creating and using modules via SAFE-DART. The executable provides a way to start applications which are built as SAFE-DART modules, and its use is optional.

## Building SAFE-DART
In order to make use of SAFE-DART, the libsafedart (library) project must first be built. This will create a shared library which may be linked into any applications which are to use SAFE-DART. This can be done in two ways on Linux:

1. Via the GUI: Open libsafedart/libsafedart.pro in Qt Creator and build.
2. Via the terminal: `cd libsafedart && qmake libsafedart.pro && make`

This will create a libsafedart shared library in the bin directory located in the same directory as this README.

The safedart (executable) project may be built similarly:

1. Via the GUI: Open safedart/safedart.pro in Qt Creator and build.
2. Via the terminal: `cd safedart && qmake safedart.pro && make`

The safedart executable will also be placed in the bin directory.

## Using SAFE-DART
A sample project setup, which can be run via the SAFE-DART executable, can be found in the `examples/greet` directory of the repository.

### Project Setup
A project can be linked to libsafedart by changing the following in its project file:

1. Add the libsafedart directory to its INCLUDEPATH (`INCLUDEPATH += <path to SAFE-DART repository>/libsafedart`).
2. Add the library directory and name to LIBS (`LIBS += -L<path to SAFE-DART repository>/bin -lsafedart`).

The project may then include SAFE-DART headers and make use of its functionality.

### Getting a Builder
Depending, the Builder may be acquired in one of two ways:

1. Provided to user code via the SAFE-DART executable. This approach has two main advantages:
    1. The SAFE-DART configuration file can be changed via the command-line.
    2. The application code can easily be tested all the way up to its main entry point.
2. Created by the user code. This has the advantage of allowing the application more control of SAFE-DART's behavior.

### Defining an Interface
An interface to a class is shared by any binary which makes use of the type. It should be written in a header which is shared by all modules that make use of the component it represents. In general, interfaces should contain only pure virtual methods to reduce the possibility of different implementations within different modules, and should also have a virtual destructor. An abstract class that extends `QObject` (a Qt class) may also be used. 

The interface should also be declared as an interface via `Q_DECLARE_INTERFACE` after the class definition. An example is shown below.

```
#include <QObject>
class Greeter
{
public:
    virtual ~Greeter() {}
    virtual void greet() = 0;
};
Q_DECLARE_INTERFACE(Greeter, ""Greeter"")
```

### Defining an Implementation
An implementation of an interface works mostly as normal, with a few additions:

1. If the interface does not extend `QObject`, then the implementation must.
2. The implementation should extend `Reflectable<T>`, where `T` is the implementation type.
3. The interface must be specified via `Q_INTERFACES`.
4. The implementation must have one of the following explicit constructors, shown in order of preference:
    1. `Q_INVOKABLE T(Builder *)`
    2. `Q_INVOKABLE T()`

An example of an implementation of the `Greeter` interface above is shown below:

```
#include <iostream>
#include <builder.h>
#include <greeter.h>
#include <reflectable.h>
class EnglishGreeter : public QObject, public Greeter, public Reflectable<EnglishGreeter>
{
    Q_OBJECT
    Q_INTERFACES(Greeter)
public:
    Q_INVOKABLE EnglishGreeter() {}
    void greet() override
    {
        std::cout << ""Hello world!"" << std::endl;
    }
};
```

### Creating an Application
Creating an application that may be used via the SAFE-DART executable is done by defining an implementation of an interface provided by SAFE-DART. This interface is called `Application`, and it contains only a single method (with the same signature as the standard `main` function).

The application can use the `T(Builder *)` constructor to receive an instance of the `Builder` class. The `Builder` instance can then be used to get an implementation of an interface (in this case, `Greeter`).

Note that the `Builder` returns a `QSharedPointer` to the implementation, and will return the same pointer as long as it continues to exist. The `Builder` itself holds a weak reference, so the object will be freed automatically once the application no longer holds a pointer to it.

An example implementation of `Application` that does this is as follows:

```
#include <application.h>
#include <builder.h>
class GreetApplication : public QObject, public Application, public Reflectable<GreetApplication>
{
    Q_OBJECT
    Q_INTERFACES(Application)
public:
    Q_INVOKABLE GreetApplication(Builder *builder) : _builder(builder) {}
    int main(int argc, char **argv) override
    {
        QSharedPointer<Greeter> greeter = _builder->get<Greeter>();
        greeter->greet();
        return 0;
    }

private:
    Builder *_builder;
};
```

### Configuring SAFE-DART
SAFE-DART uses a configuration file for two things:

1. Locations to load modules from. Only applicable when using the SAFE-DART executable.
    1. The `@module_dirs` key lists directories from which to load all modules. 
    2. The `@module_files` key lists specific module files to load.
2. Implementations to use for interfaces. The key is the interface name, the value is the implementation name. Always used by the `Builder`.

For the application above, the configuration file should contain the following:

```
[safedart]
@module_dirs=<path to directory containing modules>
Greeter=EnglishGreeter
ModuleLoader=LibraryModuleLoader
```

The config file is set up by the SAFE-DART executable normally. By default, the SAFE-DART executable will use `safedart.ini` as the path and `safedart` as the section.The path to the config file, as well as the section within the file to use, can be changed using the `-f` and `-s` arguments respectively.

Programs not using the SAFE-DART executable may set up their own source of configuration data. SAFE-DART using the `Configuration` interface.

### Running the Application
In general, a SAFE-DART application can be run by invoking the SAFE-DART executable as follows:

`<path to SAFE-DART executable> [-f <path to config file>] [-s <config section to use>] <name of implementation of Application to run>`.

For the example above, this could be as simple as:

`safedart GreetApplication`
"
276,nasa/RVLib,C++,"# RandomVariable

## Description
The RandomVariable Library is an C++ open source library for representing statistical uncertainty in a precise, readable, and usable manner. Value uncertainty can be represented with a parametric distribution such as Gaussian or a nonparametric distribution such as a weighted sample set. The library enables sampling from a parametric or fitting to a parametric distribution. Calculations may also be performed on multiple RandomVariable objects by sampling/overlaying techniques.

* **RandomVariable**:

  The RandomVariable class holds and represents the uncertainty data. It is the superclass of the two types of distributions and their respective subclasses. The Hierarchical structure allows for polymorphism and late binding on distribution objects.

* **Translation**:

  Translation is a namespace with functions handling Parametric and NonParametric types. By using an external namespace that can access the definitions of all Parametric and NonParametric distributions, we can decrease coupling between distribution objects and avoid circular dependencies. For instance, if a developer wanted to call a sampling function on a Gaussian distribution and receive and unweighted sample object, the Gaussian object would have to know the definition of an unweighted sample object whereas with Translation, the namespace can provide information about other classes and free distributions of that burden

* **Statistics**:

  Statistics is a small struct that allows the packaging of information that we can use to instantiate a distribution or detail a distribution in common terms (parameters for each distribution will have different meaning, but the Statistics struct will be ""universal"")
  
* **[UNDER CONSTRUCTION] RandomVariableContainer**:

  The RandomVariableContainer object is a container for RandomVariable objects which allows the user to perform calculations on multiple RandomVariable objects. By passing a function pointer and the correct amount of RandomVariable object pointers.

## Structure
![RV Hierarchy](images/Hierarchy.png)
  The abstract RandomVariable class is inherited by the Parametric and NonParametric classes (also abstract). Each distribution type is inherited by named distributions (Lognormal, Exponential, etc.) and data set representations (Weighted, Unweighted, etc.). The statistics struct is used by the RandomVariable object (and it's descendants). The Translation namespace uses the RandomVariable and it's derivatives.

## Using

### REQUIREMENTS
* Cmake (https://cmake.org)
* Compiler capable of building C++11

### BUILD
To start build properly enter RandomVariableProject directory then change into the build directory.

```
$ cd RandomVariable Project
$ cd build
```

execute cmake and specify path to source as one directory higher

`$ cmake ..`

If cmake is not able to locate your gcc/g++ compiler, you can use the command

`$ cmake -D CMAKE_C_COMPILER=[location of gcc] -D CMAKE_CXX_COMPILER=[location of gcc] ..`

Next run make from the build directory or make -C build from RandomVariableProject.

### USE
Random Variable Library is built into a library in the lib directory. This library can then be linked with other software products to be used.

## Examples
There are a number of examples in the Examples directory.

## Semantics
* INCLUDES
Includes are placed in each file dependent on them, unless an included file already contains the necessary information.
If a library was included in two subclasses of a superclass, that include was not moved to the superclass to reduce the number of includes so an unnecessary include is avoided if the superclass were to be included on a different project.
* SYNTAX
Accessor function names follow the protocol:
Access entails a calculation - solely the name of the attribute
  Examples: mean()
Access does not entail calculation - ‚Äòget‚Äô + name of attribute
  Example: getMu()
* EXCEPTIONS:
std::invalid_argument instances are thrown in functions with input range limits (due to user input) such as:
pdf(), cdf(), caldIcdf(), setSigma(), etc.
These exceptions are not caught in the function, but are left to be caught by users calling these functions directly. If another function is calling a function containing a possible throw statement, it will contain a try/catch block.

## Applications
Uncertainty representation can be used in an almost infinite number of fields from applied mathematics to telecommunications. Most notably it can be utilized when handling data from entities susceptible to inaccuracies such as machinery, telemetry, manual calculation. A primary application is the monitoring system for a fuel cell or structural component prognostics.

## Contact
If you have questions, please open an issue on github

## Contributing
All contributions are welcome! If you are having problems with the plugin, please open an issue on GitHub. If you would like to contribute directly, please feel free to open a pull request against the ""dev"" branch. Pull requests will be evaluated and integrated into the next official release.

## Next Steps
Software developers expanding this library should improve the coupling between a select few distribution classes. Some shortcuts were taken where privacy and was not a concern and checking was unnecessary. If future contributors aim to make the software more secure and abstracted, they should decrease further direct access to the underlying data structure (overload index operator) and class private variables. Adding more distributions and data set representations should be trivial if they follow a similar format as the current objects. The process for setting up a calculation involving multiple RandomVariables could be make simpler.

## Notices

Copyright ¬©2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF
ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED
TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY
IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR
FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR
FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE
SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN
ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,
RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS
RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY
DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF
PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE
UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY
PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY
LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,
INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE
OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED
STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR
RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH
MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
277,nasa/VirtualADAPT,Matlab,"# Motivation

The Advanced Diagnostic and Prognostic Testbed (ADAPT), developed at NASA Ames Research Center, is functionally representative of an electrical power system (EPS) on an exploration vehicle, and has been developed to:
* Serve as a technology-neutral basis for testing and evaluating software and hardware diagnostic systems,
* Allow accelerated testing of diagnostic algorithms by manually or algorithmically inserting faults,
* Provide a real-world physical system such that issues that might be disregarded in smaller-scale experiments and simulations are exposed,
* Act as a stepping stone between pure research and deployment in aerospace systems, thus creating a concrete path to maturing diagnostic technologies, and
* Develop analytical methods and software architectures in support of the above goals.

The ADAPT hardware includes components that can generate, store, distribute, and monitor electrical power. The EPS can deliver AC (Alternating Current) and DC (Direct Current) power to loads. A data acquisition and control system sends commands to and receives data from the EPS. The testbed operator stations are integrated into a software architecture that allows for nominal and faulty operations of the EPS, and includes a system for logging all relevant data to assess the performance of the health management applications.

# The ADAPT Hardware

The major system components of ADAPT include power generation, storage, and distribution components. Two power generation sources are connected to three sets of batteries, which in turn supply two load banks. Each load bank has provision for 6 AC loads and 2 DC loads. To be more specific, ADAPT consists of the following three classes of components ‚Äì power generation, power storage, and power distribution.
* **Power Generation:** The two sources of power generation include two battery chargers. The battery chargers are connected to appropriate wall outlets through relays. The two power generation sources can be interchangeably connected to the three batteries. Hardware relay logic prevents connecting one charge source to more than one battery at the same time, and from connecting one charging circuit to another charging circuit.
* **Power Storage:** Three sets of batteries are used to store energy for operation of the loads. Each ‚Äúbattery‚Äù consists of two 12-volt sealed lead acid batteries connected in series to produce a 24-volt output. Two battery sets are rated at 100 Amp-hrs and the third set is rated at 50 Amp-hrs. The batteries and the main circuit breakers are placed in a ventilated cabinet that is physically separated from the equipment racks; however, the switches for connecting the batteries to the upstream chargers or downstream loads are located in the equipment racks.
* **Power Distribution:** Electromechanical relays are used to route the power from the sources to the batteries; and from the batteries to the AC and DC loads. All relays are of the normally-open type. An inverter converts the 24-volt DC battery input to a 120-volt r.m.s. AC output. Circuit breakers are located at various points in the distribution network to prevent overcurrents from causing unintended damage to the system components.


# Virtual ADAPT

VirtualADAPT is a high-fidelity, Matlab¬Æ Simulink¬Æ-based simulation testbed that emulates the ADAPT hardware for running offline health management experiments. This simulation testbed models all components of the ADAPT hardware within the power storage and power distribution subsystems. The physical components of the testbed, i.e., the batteries, relays, and the loads, are replaced by simulation modules that generate the same dynamic behaviors as the hardware test bed. 



## Installation

For the model to execute, the fault interface GUI functions must be on the Matlab path. To do this, run the script 
[installVirtualADAPT.m](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/installVirtualADAPT.m).

The script [uninstallVirtualADAPT.m](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/uninstallVirtualADAPT.m) will remove the directories from the Matlab path.


## Quick Start Guide

To generate data, load [VirtualADAPT.mdl](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/VirtualADAPT.mdl) in Simulink and hit the start button. The simulation is configured to run indefinitely and write data to the MATLAB workspace as a matrix of floating-point values. The 'Sensors' section of the file [Sensors And Actuators.txt](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/Sensors%20and%20Actuators.txt) contains the sensor names which correspond to the columns of this matrix.

To command a relay or circuit breaker, find the simulation input port for the desired actuator (eg. 'EY144_CL') and 
double-click the switch connected to it. Because the simulation is 'solved' as quickly as possible rather than running 
in real-time, it is easier to configure relays before starting the simulation.

To inject ADAPT faults, use the fault injection GUI which is opened (and closed) automatically when the Simulink model 
is opened (and closed). Fault modes are specific to components but are of the following general modes when referring to
parameter values:

* Nominal - Magnitude (M) unused. Nominal value (N) unaltered.
* Incipient - N + M\*T, where T is the number of seconds since injection.
* Abrupt - (N + 1)\*M
* Bias - N + M
* StuckAt - M

Faults are added/removed to/from the Simulink model when added/removed using the GUI.

A second method for injecting faults is to open the model in simulink, navigate to the desired block, and change the values 
manually. Alternately, the MATLAB functions get_param and set_param can be used to set these values from a script or the 
command line.


## Extended Documentation

The document [VirtualADAPT.pdf](https://github.com/nasa/VirtualADAPT/blob/master/docs/VirtualADAPT.pdf) describes the Virtual ADAPT simulation model in more detail. Please refer to this document for instructions on running the simulation.

## License

This software is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/VirtualADAPT/blob/master/License.pdf).

## Notices

Copyright ¬© 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

### Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
278,nasa/ominas_maps,,"# ominas_maps
Repository for OMINAS map library
"
279,nasa/SGNDI,,"[![Build Status](https://travis-ci.org/thearn/SGNDI.svg?branch=master)](https://travis-ci.org/thearn/SGNDI)
[![Coverage Status](https://coveralls.io/repos/github/thearn/SGNDI/badge.svg?branch=master)](https://coveralls.io/github/thearn/SGNDI?branch=master)

Separable Grid N-Dimensional Interpolator (SGDNI)
-------------------------------------------------

![alt text](example.png ""Example interpolation"")

A multi-dimensional interpolating class with first-order gradients.
This module provides a class `SeparableGridNDInterpolator` similair in
interface to the interpolators provided by [`scipy.interpolate`](https://docs.scipy.org/doc/scipy/reference/interpolate.html).

This class provides interpolation on a regular grid in arbitrary dimensions, by applying
a selected 1D interpolation class on each grid axis sequentially. These
1D interpolation classes are the ones provided by [`scipy.interpolate`](https://docs.scipy.org/doc/scipy/reference/interpolate.html), such
as [`CubicSpline`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html#scipy.interpolate.CubicSpline), [`UnivariateSpline`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html#scipy.interpolate.UnivariateSpline), or [`Akima1DInterpolator`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Akima1DInterpolator.html#scipy.interpolate.Akima1DInterpolator). By default, `CubicSpline` is used if no interpolator is specified.

This method can be considered a generalization of the class of multidimensional interpolators that operate on each dimension sequentially, such as [bilinear](https://en.wikipedia.org/wiki/Bilinear_interpolation), [bicubic](https://en.wikipedia.org/wiki/Bicubic_interpolation), [trilinear](https://en.wikipedia.org/wiki/Trilinear_interpolation), [tricubic](https://en.wikipedia.org/wiki/Tricubic_interpolation), etc. In other words, is provides n-linear, n-cubic, etc. interpolation capabilities within a single class.

If derivatives are provided by the chosen 1D interpolation method, then
a gradient vector of the multidimensional interpolation may be computed
and and cached when the interpolation is performed. This can then be accessed by the `derivative` method. At the moment, only
first-order derivatives are supported.

Examples
---------
The shortest example might be fitting the XOR function:

```python
import numpy as np
from sgndi import SeparableGridNDInterpolator

a = np.array([0, 1])
b = np.array([0, 1])

c = np.array([[0, 1], [1, 0]])

cs = SeparableGridNDInterpolator([a, b], c)

x = [0.8,0.1]

value = cs(x)
gradient = cs.derivative(x)

print(value, gradient)
```

which prints the interpolated value, and gradient vector

```0.7400000000000001, [ 0.8 -0.6]```

-------------------

A more ambitious 4D paraboloid example, using `np.meshgrid` to create the structured (regular) grid:


```python
import numpy as np
from sgndi import SeparableGridNDInterpolator

# Let's define a 4D function to test with:

def F(u,v,z,w):
	return (u-5)**2 + (v-2)**2 + (z-5)**2 + (w-0.5)**2

# Now create 1D arrays for each of the function parameters for sampling.

U = np.linspace(0, 10, 10)
V = np.linspace(0, 4, 6)
Z = np.linspace(0, 10, 7)
W = np.linspace(0, 1, 8)
points = [U, V, Z, W]

# Create coordinate mesh

u, v, z, w = np.meshgrid(*points, indexing='ij')

# Now create the 4D value array

values = F(u, v, z, w)

# Define a random point to interpolate at

x = [5.26434, 2.121235, 2.7352, 0.5213345]

# Create the interpolation class instance

interp = SeparableGridNDInterpolator(points, values)

# Call the interpolation at the point above, which by default also
# computes the gradient of the interpolant at this point

def dF(u,v,z,w):
	# the actual gradient
	return 2*(u-5), 2*(v-2), 2*(z-5), 2*(w-0.5)

f = interp(x)
dfdx = interp.derivative(x)

print(""actual value"", F(*x))
print(""computed value"", f)
print(""actual gradient:"", dF(*x))
print(""computed gradient:"", dfdx)
```

This produces the interpolated output:

```
actual value 5.21434776171525
computed value 5.214347761715252
actual gradient: (0.5286799999999996, 0.24246999999999996, -4.5296, 0.04266900000000007)
computed gradient: [ 0.52868   0.24247  -4.5296    0.042669]
```

---------------------------------

A 2D example can show the performance visually. A very course sampling is used to product a 2D interpolation, making use of the 1D `UnivariateSpline` from scipy.interpolate with degree `k=5`. This is then used to produce a finer-scale approximation of the original function.

```python
import numpy as np
from sgndi import SeparableGridNDInterpolator
from scipy.interpolate import UnivariateSpline
import matplotlib.pyplot as plt

def F(u,v):
    return u*np.cos(u*v) + v*np.sin(u*v)

def dF(u,v):
    return -u*v*np.sin(u*v) + v**2*np.cos(u*v) + np.cos(u*v), -u**2*np.sin(u*v) + u*v*np.cos(u*v) + np.sin(u*v)

# Plot exact function, with high sampling, n = 200
U = np.linspace(0, 3, 200)
V = np.linspace(0, 3, 200)

points = [U, V]

u, v = np.meshgrid(*points, indexing='ij')

values = F(u, v)
plt.subplot(131)
plt.imshow(values[::-1], extent=(U[0], U[-1], V[0], V[-1]))

#--------------------------------
# Now gather & plot a very course sample for creating the interpolant, n = 10

U = np.linspace(0, 3, 10)
V = np.linspace(0, 3, 10)

points = [U, V]

u, v = np.meshgrid(*points, indexing='ij')

values = F(u, v)
plt.subplot(132)
plt.imshow(values[::-1], extent=(U[0], U[-1], V[0], V[-1]))

interp = SeparableGridNDInterpolator(points, values,
				interpolator = UnivariateSpline, interp_kwargs = {'k' : 5})

#--------------------------------
# Test that the created interpolator can actually approximate a fine level
# of detail, n = 50

U = np.linspace(0, 3, 50)
V = np.linspace(0, 3, 50)

points = [U, V]

u, v = np.meshgrid(*points, indexing='ij')

vals = []
for x in np.array([u.ravel(), v.ravel()]).T:
    f = interp(x)
    vals.append(f)

A = np.array(vals).reshape(50, 50)

plt.subplot(133)
plt.imshow(A[::-1], extent=(U[0], U[-1], V[0], V[-1]))

plt.show()
```

Which gives the plot shown at the top of this readme.

Example use in a numerical optimization
------------------------------
```python
import numpy as np
from scipy.optimize import fmin_bfgs
from sgndi import SeparableGridNDInterpolator

def F(u, v, z, w):
    # min at u=5.234 v=2.128 z=5.531 w=0.574
    return (u - 5.234)**2 + (v - 2.128)**2 + (z - 5.531)**2 + (w - 0.574)**2

U = np.linspace(0, 10, 10)
V = np.linspace(0, 10, 6)
Z = np.linspace(0, 10, 7)
W = np.linspace(0, 10, 8)

points = [U, V, Z, W]

u, v, z, w = np.meshgrid(*points, indexing='ij')

values = F(u, v, z, w)

interp = SeparableGridNDInterpolator(points, values)

x = np.zeros(4)
```

Without gradients:
```
print(fmin_bfgs(interp, x))
```

```
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 3
         Function evaluations: 24
         Gradient evaluations: 4
[ 5.23399953  2.12799974  5.53100043  0.57400106]
```

With gradients:
```
print(fmin_bfgs(interp, x, fprime=interp.derivative))
```

```
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 3
         Function evaluations: 4
         Gradient evaluations: 4
[ 5.234  2.128  5.531  0.574]
```

Limitations
------------
- The interpolation can only be called one evaluation point at a time. I am working
on vectorizing this to allow for a collection of points to be interpolated at once.
- Only first-order derivatives (gradients) are computed.


"
280,nasa/knife,C,"
knife

Boolean Subtraction Library for Polyhedra

The knife library calculates the boolean subtraction of arbitrary
watertight triangular polyhedra. The result of this subtraction is
also watertight triangular polyhedra. The triangular faces of the
resultant polyhedra are created with a Delaunay triangle mesher.
These polyhedra are suitable for performing cut cell partial
differential equation solutions (i.e., computational fluid flow
simulations). Tetrahedra as well as the median dual of a tetrahedral
mesh are standard inputs. The knife library is implemented with an
object-oriented flavor in the C language.

The knife library can be called by FUN3D <http://fun3d.larc.nasa.gov/>
to perform dual tetrahedra cut cell flow and adjoint solutions.

See the INSTALL file for build instructions.

Mike Park


"
281,nasa/DON-Federate-HLA2MPC,HTML,"# DON-Federate-HLA2MPC

Distributed Observer Network 3 (DON3) combines NASA simulation technologies, NASA information technologies and commercial video game technology to provide a free immersive viewer for complex simulation information.  A key component is a standardized interface for simulation related information that is coupled with custom software integrated into the game environment.  The DON Federate is an HLA / IEEE 1516 federate that subscribes to simulation state information, formats that data to comply with the Model Process Control specification and then sends the information to DON clients for display.
"
282,nasa/PyBlock,Jupyter Notebook,"PyBlock README
--------------
PyBlock is a Python 2 or 3 module that enables the end user to estimate partial beam blockage using polarimetric radar data. The methodologies it uses depend on the self-consistency of polarimetric radar variables - reflectivity (Zh), differential reflectivity (Zdr), and specific differential phase (Kdp) - in pure rain. There are two methodologies currently available to the end user, both described in Lang et al. (2009): The KDP method, and the Fully Self-Consistent (FSC) method. Briefly, the KDP method will check the behavior of Zh and Zdr for a given range of Kdp both inside and outside of blocked azimuths, and use that to suggest corrections to these measurands. This is effectively a relative calibration of Z and Zdr. The FSC method uses a derived or specified self-consistency relationship to do an absolute calibration of Zh within the blocked regions. PyBlock implements these methodologies within an object-oriented Python framework. 

PyBlock Installation
-------------------
The following dependencies need to be installed first:

- A robust version of Python 2.7 or 3.4-3.6 (other versions untested) w/ most standard scientific packages (e.g., numpy, matplotlib, pandas, etc.) - Get one for free [here.](https://store.continuum.io/cshop/anaconda/)
- The `h5py` module (available via most package managers; e.g., conda)
- [The Python Atmospheric Radiation Measurement (ARM) Radar Toolkit (Py-ART)] (https://github.com/ARM-DOE/pyart)
- [CSU_RadarTools](https://github.com/CSU-Radarmet/CSU_RadarTools)
- [SkewT](https://pypi.python.org/pypi/SkewT) - Python 3 version can be found [here.](https://github.com/tjlang/SkewT)
- [DualPol] (https://github.com/nasa/DualPol)

Specific import calls in the PyBlock source code:

```
from __future__ import division
from __future__ import print_function
import numpy as np
import matplotlib.pyplot as plt
from warnings import warn
import statsmodels.api as sm
import os
import h5py
import pyart
import dualpol
from csu_radartools import csu_misc
import six
```

To install PyBlock, in the main directory for the package:

```
python setup.py install
```

Using PyBlock
-------------
To access everything:
```
import pyblock
```

To see PyBlock in action, check out the IPython notebook provided in this distribution.
"
283,nasa/MMM-Py,Jupyter Notebook,"MMM-Py README
-------------

The National Oceanic and Atmospheric Administration (NOAA) regularly produces
national 3D radar reflectivity mosaics via its Multi-Radar/Multi-Sensor (MRMS)
system. These mosaics are wonderful for storm and precipitation analysis and research,
but they are distributed in odd formats that NOAA is ever changing. Sometimes you
just want to read a file and make a plot! 

This is what MMM-Py is for. With it, you can read any version of the MRMS radar mosaics,
past or present, and you can analyze, plot, subsection, and output custom mosaics of your 
own, which MMM-Py can ingest later. MMM-Py is free and open source. It is capable of 
producing publication-ready figures and analyses, but it also can do quicklook plots so 
you can check out the cool storm that just happened.

For more info about the latest version of MRMS, see [here.](https://docs.google.com/document/d/1LeVcn_taIXZgzZb5JgWqaVr0xVs7GmA6RpHcb8ZGiwk/edit)


MMM-Py Installation
-------------------

MMM-Py works under Python 2.7 and 3.4-3.6 on most Mac/Linux setups. Windows installation is currently untested.

Put `mmmpy.py` in your `PYTHONPATH`.

You'll need the following Python packages. Most are easily obtained or already installed
with common Python frameworks such as [Anaconda](http://continuum.io/downloads):
`numpy`, `matplotlib`, `six`, `netCDF4`, `os`, `Basemap`, `struct`, `time`, `calendar`, `gzip`, `datetime`

You may also want to install `pygrib` from [here](https://pypi.python.org/pypi/pygrib). This is an optional dependency.

Get MRMS-modified wgrib2 package and installation info from ftp://ftp.nssl.noaa.gov/projects/MRMS/GRIB2_DECODERS/MRMS_modified_wgrib2_v2.0.1-selectfiles.tgz

Install `wgrib2` and note the path to it. Modify the `BASE_PATH`, `TMPDIR`, `WGRIB2_PATH`, and `WGRIB2_NAME` 
global variables in `mmmpy.py` as necessary. `TMPDIR` is where intermediate netCDFs created by `wgrib2`
will go.

Without `wgrib2` or `pygrib`, MMM-Py can still read legacy MRMS binaries and netCDFs. The `pygrib` module will obviate the need to install `wgrib2`, as it enables direct ingest of the grib2 files without converting to netCDF.


Using MMM-Py
------------

To access everything:
```
import mmmpy
```
To see MMM-Py in action, check out the IPython notebooks provided in this distribution.

This [conference presentation](https://ams.confex.com/ams/95Annual/webprogram/Paper262779.html) discusses MMM-Py (among other modules).

MMM-Py was developed at the NASA Marshall Space Flight Center by Timothy Lang (timothy.j.lang@nasa.gov)

See LICENSE file for NASA open source license information.
"
284,nasa/KeplerPORTs,Python,"# KeplerPORTs
KeplerPORTs.py - Illustrate making use of numerous Kepler Planet Occurrence Rate Data Products for Data Release 25 and SOC 9.3 Kepler Pipeline version.  This code generates a detection contour according to the documentation

Burke, C.J. & Catanzarite, J. 2017, ""Planet Detection Metrics: Per-Target Detection Contours for Data Release 25"", KSCI-19111-001

Additional recommended background reading

- Earlier Data Release 24 version of detection contour
    * Burke et al. 2015, ApJ, 809, 8
- Transit injection and recovery tests for the Kepler pipeline
    * Christiansen et al. 2013, ApJS, 207, 35
    * Christiansen et al. 2015, ApJ, 810, 95   (One Year Kepler data)
    * Christiansen et al. 2016, ApJ, 828, 99   (Data Release 24)
    * Christiansen, J. L. 2017, Planet Detection Metrics: Pixel-Level Transit Injection Tests of Pipeline Detection Efficiency for Data Release 25 (KSCI-19110-001)
    * Burke & Catanzarite 2017, Planet Detection Metrics: Per-Target Flux-Level Transit Injection Tests of TPS for Data Release 25 (KSCI-19109-001)
- Kepler Target Noise and Data Quality metrics
    * Burke & Catanzarite 2016, Planet Detection Metrics: Window and One-Sigma Depth Functions for Data Release 25 (KSCI-19101-002)
    
**Assumptions** - python packages numpy, scipy, matplotlib, astropy, and h5py are available and files

- detectEffData_alpha12_02272017.h5
- detectEffData_alpha12_SlopeLongShort_02272017.txt
- detectEffData_alpha_base_02272017.txt
- kplr003429335_dr25_onesigdepth.fits
- kplr003429335_dr25_window.fits
are available in the same directory as KeplerPORTs.py

**Running**: python KeplerPORTs.py

**Output**: Displays a series of figures and generates hardcopy

Notices:

Copyright ¬© 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.

NASA acknowledges the SETI Institute‚Äôs primary role in authoring and producing the KeplerPORTs (Kepler Planet Occurrence Rate Tools) under Cooperative Agreement Number NNX13AD01A.


Disclaimers

No Warranty: THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT ""AS IS.""

Waiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.
"
285,nasa/ESPRESSO,,"# Project ESPRESSO

This repository will contain code products generated by the efforts of Project ESPRESSO, a NASA SSERVI node.

https://www.espresso.institute
"
286,nasa/visionworkbench,C++,"************************************************************************
1. INTRODUCTION

The NASA Vision Workbench is a modular, extensible, cross-platform
computer vision software framework written in C++.  It was designed to
support a variety of space exploration tasks, including automated
science and engineering analysis, robot perception, and 2D/3D
environment reconstruction, though it can also serve as a
general-purpose image processing and machine vision framework in other
contexts as well.

This package is composed of several modules each of which provides a
separate C++ library.  The core library provides the basic image and
pixel data types as well as a range of fundamental image processing
operations.  The other modules provided in this release are:

 * Math: geometric, numeric, and other mathematical types and functions
 * GPU: accelerated image processing using commodity graphics hardware
 * HDR: creating, processing, and compressing high dynamic range images
 * InterestPoint: Detecting, tracking, and matching interest points
 * Mosaic: compositing, blending, and manipulating 2D image mosaics
 * Camera: camera models and related types and functions
 * Cartography: tools for manipulating geospatially-referenced images

Each of these modules and their dependencies are discussed in greater
detail in section 3, ""LIBRARY STRUCTURE"".

************************************************************************
2. LICENSE (see COPYING for the full text)

A. Copyright and License Summary

Copyright (C) 2009 United States Government as represented by the
Administrator of the National Aeronautics and Space Administration
(NASA).  All Rights Reserved.

This software is distributed under the NASA Open Source Agreement
(NOSA), version 1.3.  The NOSA has been approved by the Open Source
Initiative.  See the file ""COPYING"" at the top of the distribution
directory tree for the complete NOSA document.

THE SUBJECT SOFTWARE IS PROVIDED ""AS IS"" WITHOUT ANY WARRANTY OF ANY
KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT
LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO
SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR
A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT
THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT
DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE.

B. Third-Party Libraries

This distribution includes some bundled third-party software as a
convenience to the user.  This software, located in the ""thirdparty/""
directory, is not covered by the above-mentioned distribution
agreement or copyright.  See the included documentation for detailed
copyright and license information for any third-party software.  In
addition, various pieces of the Vision Workbench depend on additional
third-party libraries that the user is expected to have installed.
The specific dependencies of each component of the Vision Workbench
are discussed section 3, ""LIBRARY STRUCTURE"", and information of where
to obtain non-bundled third-party libraries is provided in section 4,
""INSTALLATION"".

************************************************************************
3. LIBRARY STRUCTURE

The Vision Workbench software is located in the directory ""src/vw/""
and consists of a core library and several optional libraries or
""modules"".  Each module is contained in a subdirectory with the name
name as the module.

A. The Main Vision Workbench Library

At the center of the Vision Workbench are three modules that are
in fact linked together to form a single fundamental library.

i. The ""Core/"" module provides fundamental services that are not
specific to image processing, such as C++ exception types and type
computation classes.

ii. The ""Image/"" module provides the pixel and image types and
functions that form the heart of the Vision Workbench, including
support for various color spaces, filtering operations, and other
image processing primitives.

iii. The ""FileIO/"" module contains routines to support reading and
writing images from and to disk in a variety of file formats.

The only required dependency of the Vision Workbench core is the Boost
C++ libraries, which provide a variety of low-level C++ library
servies.  (These are actually many libraries by different authors
released under a common license, but they are generally distributed as
a package and we will treat them that way for simplicity.)  The file
I/O module has a number of optional dependencies, each providing
support for one or more image file formats.  Specifically these are:
libpng for PNG files, libjpeg for JPEG/JFIF files, libtiff for TIFF
files, NetPBM for PBM/PGM/etc. files, and OpenEXR for EXR files.

B. Math Module

This module provides a variety of mathematical data types and
algorithms.  It is centered around fundamental vector, matrix, and
quaternion data types which support the usual range of mathematical
operations.  On top of that foundation there are higher-level types
and algorithms for geometric computation, linear algebra,
optimization, and statistics, and so forth.

The linear algebra numerical algorithms rely on the standard low-level
routines provided by LAPACK (Linear Algebra Package) and BLAS (Basic
Linear Algebra Subprograms).  Many computers (e.g. those running OS X)
come with optimized implementations of LAPACK and BLAS, in which case
you can (and probably should) just use those.  To support other
platforms we provide public-domain implementations automatically
translated from the original Fortran and taken from the online Netlib
repository.

C. GPU Module

Most modern graphics hardware includes a general-purpose graphics
processing unit (GPU), and this module provides an interface to take
advantage of that high-performance hardware for a variety of basic
image-processing operations.  Under Linux it requires that you have
installed the OpenGL interface library GLEW.  It can optionally also
take advantage of the NVIDIA CG library, which provides an alternative
low-level framework which may be preferable on some graphics hardware.

D. HDR Module

While the core Vision Workbench library supports working with high
dynamic range (HDR) image data directly, most input and output devices
(e.g. digital cameras and displays) only support a limited dynamic range.
This module provides tools for interfacing between the two worlds, by
generating HDR images from collections of ordinary images as well as
providing several methods to compress HDR images for display on ordinary
output devices.

E. InterestPoint Module

Interest points are points in an image that can be reliably detected and
tracked, such as corners or peaks.  This module provides tools for
locating interest points in images in a variety of ways.  It can also
generate local descriptors for those points using several methods which
can then be used to locate corresponding points in sets of images.

F. Mosaic Module

The Mosaic module provides tools for working with large images
assembled from many smaller images, such as panoramas or aerial maps.
It includes support for assembling source images into a mosaic,
blending the images in the mosaic to create a single large image, and
dicing the extremely large images that result into better formats for
visualization.

G. Camera Module

The Camera module provides a variety of data types and tools for
working with camera models and camera data.  Most notably it includes
support for a variety of standard camera geometries, the ability to
transform images between camera models, and the ability to interpret
camera data stored in the EXIF headers of image files generated by
digital cameras.

H. Cartography Module

The Cartography module provides a variety of 2D and 3D mapping
capabilities.  It allows you to georeference an image, specifying its
projection, position within the projected space, and altitude
reference (or ""datum""), and it supports reprojecting images into the
desired projection.  It requires the PROJ.4 cartographic projection
library, and also optionally depends on the GDAL library which
provides support for a variety of GIS file formats.

I. Testing Frameworks

Each module includes a collection of tests located in that module's
""tests/"" subdirectory.  You can use these tests to confirm that the
library is installed and working properly on your system.  To do this,
simply run ""make check"" after building the library.  Please report any
errors you encounter, using the contact information at the bottom of
this file.  Note that while the tests do currently exercise a
significant portion of the library, they are not yet fully exhaustive.

************************************************************************
4. INSTALLATION AND USE

A. Obtaining the Software

If you are reading this text then presumably you have a copy of
the software.  However, you can obtain the most recent version from

  http://ti.arc.nasa.gov/visionworkbench

Before attempting to configure, build or install the Vision Workbench
you should obtain and install any prerequisite libraries that you
need.  The only absolute requirement is the Boost.  The others are
either required to build a specific module, or will enable a
particular feature if available.  A complete list of dependencies
is shown in the table below, where each library is noted as being
either a required or optional dependency of one or modules.  All of
these libraries are distributed under some variation on the themes
of the MIT and BSD licenses.  See each individual library's
documentation for details.

+---------+--------------------+------------------------------------+
| Library | Relevant Modules   | Source Website                     |
+---------+--------------------+------------------------------------+
| Boost   | Core, etc. (req.)  | http://www.boost.org/              |
| PROJ.4  | Cartography (req.) | http://www.remotesensing.org/proj/ |
| GDAL    | Cartography (opt.) | http://www.remotesensing.org/gdal/ |
| GLEW    | GPU (req.)         | http://glew.sourceforge.net/       |
| CG      | GPU (opt.)         | http://developer.nvidia.com/       |
| PNG     | FileIO (opt.)      | http://www.libpng.org/             |
| JPEG    | FileIO (opt.)      | http://www.ijg.org/                |
| TIFF    | FileIO (opt.)      | http://www.libtiff.org/            |
| OpenEXR | FileIO (opt.)      | http://www.openexr.com/            |
+---------+--------------------+------------------------------------+

In addition, the some Vision Workbench modules require other, lower
level modules to be built.  The internal Vision Workbench dependency
table appears below.

+------+--------+-------------+
| HDR  | Mosaic | Cartography |   Application-specific Toolkits
+------+--------+-------------+
+---------------+-------------+
|    FileIO     |    Camera   |   High-level Primatives
+---------------+-------------+
+---------------+-------------+
|    Image      |    Math     |   Low-level image processing/Linear Algrebra
+---------------+-------------+
+-----------------------------+
|             Core            |   Basic Programming Infrastructure
+-----------------------------+


B. Building and Installing

Once you have obtained and installed all of the prerequisite software
the process of building the Vision Workbench itself is generally
straightforward.  There are four steps:

i. Configure the library.  This is usually as simple as running the
""./configure"" script from within the root Vision Workbench package
directory.

ii. Build the library by running ""make"".

iii. Run the tests by running ""make check"".

iv. Install the library by running ""make install"".

While this simple sequence will suffice for most users, the configure
script has many options that you can use to adjust various properties,
such as compiler optimization flags or the search paths used to find
required libraries.  See the ""INSTALL"" file in this directory for more
detailed information.

C. Using the Library

When you install the library it will place files into three
subdirectories in the installation location.  The header files which
you will need to develop software using the library are located in the
""include/"" subdirectory.  The compiled libraries, which you will need
to link your software against, are located in the ""lib/"" subdirectory.
You will need to configure your software development environment as
appropriate to locate these files.  Finally, a number of simple
command-line tools are provided in the ""bin/"" directory.  These are
intended primarily as demo applications, but many of them are in fact
useful in their own right.  See the documentation for a complete list
of the tools.

************************************************************************
5. DOCUMENTATION

The primary source of documentation is the Vision Workbook, which is
provided in source form along with this distribution.  It includes a
gentle introduction to using the core image processing routines, as
well as documentation for each of the high level Vision Workbench
modules.  A copy of this document in PDF format should be available
from wherever you obtained this package.  The original source for this
document can be found in ""docs/workbook"" and it can be built by
running ""make workbook"".  This operation requires the latex typesetting
package.

************************************************************************
6. CONTACTS & CREDITS

A. Mailing List

All bugs, feature requests, and general discussion should be sent to
the Vision Workbench user mailing list:

  vision-workbench@lists.nasa.gov

To subscribe to this list, send an empty email message with the subject
'subscribe' (without the quotes) to

  vision-workbench-request@lists.nasa.gov

To contact the lead developers and project manager directly, send mail
to:

  vision-workbench-owner@lists.nasa.gov

Please do NOT use this second list for technical inquiries, which
should all be sent to the main vision-workbench list above.

B. Credits

The Vision Workbench was developed within the Autonomous Systems and
Robotics area of the Inteligent Systems Division at NASA's Ames
Research Center.  It leverages the Intelligent Robotics Group's (IRG)
extensive experience developing surface reconstruction and tools for
planetary exploration---e.g. the Mars Pathfinder and Mars Exploration
Rover missions---and rover autonomy.  It has also been developed in
collaboration with the Adaptive Control and Evolvable Systems (ACES)
group, and draws on their experience developing computer vision
techniques for autonomous vehicle control systems.

See the AUTHORS file for a complete list of developers.
"
287,nasa/MAV,Limbo,"# MAV

This consists of modeling, analysis, and visualization of ATM concepts. 
"
288,nasa/NASA-Space-Weather-Media-Viewer,,
289,nasa/RtRetrievalFrameworkDoc,Python,"====================================
RT Retrieval Framework Documentation
====================================

Jet Propulsion Laboratory, California Institute of Technology. 
Copyright 2016 California Institute of Technology. 
U.S. Government sponsorship acknowledged.

This is the Sphinx source for the RT Retrieval Framework documentation.

The software is located at:
https://github.com/nasa/RtRetrievalFramework

This documentation rendered into HTML for is available at:
http://nasa.github.io/RtRetrievalFrameworkDoc/
"
290,nasa/NASTRAN-95,FORTRAN,"# NASTRAN-95

NASTRAN has been released under the  
[NASA Open Source Agreement version 1.3](https://github.com/nasa/NASTRAN-95/raw/master/NASA%20Open%20Source%20Agreement-NASTRAN%2095.doc).


NASTRAN is the NASA Structural Analysis System, a finite element analysis program (FEA) completed in the early 1970's. It was the first of its kind and opened the door to computer-aided engineering. Subsections of a design can be modeled and then larger groupings of these elements can again be modeled. NASTRAN can handle elastic stability analysis, complex eigenvalues for vibration and dynamic stability analysis, dynamic response for transient and steady state loads, and random excitation, and static response to concentrated and distributed loads, thermal expansion, and enforced deformations.

NOTE: There is no technical support available for this software.
"
