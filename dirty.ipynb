{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Acquire NASA Github Readme's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import acquire as a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the actual url to variable\n",
    "url = ('https://github.com/nasa')\n",
    "# make the http request and turn the response into a beautiful soup object\n",
    "response = get(url)\n",
    "#raw html\n",
    "html = response.text\n",
    "#Turn the html string into a BeautifulSoup object\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- soup.find to find one thing\n",
    "- soup.find_all to find all the matching things\n",
    "- soup.select to find all the matching things (as a list of tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('h3')[0].text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(soup.select('h3')[0].text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS = []\n",
    "\n",
    "#def get_repo_titles():\n",
    "for n in range(0,30):\n",
    "    REPOS.append(soup.select('h3')[n].text.split())\n",
    "#return repo_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get_repo_titles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list = list(itertools.chain(*REPOS))\n",
    "flatten_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_links = get_repo_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flatten_list = list(itertools.chain(*repo_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repos = pd.Series((v[0] for v in get_repo_titles()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(repos)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS = []\n",
    "i = 11\n",
    "while i < 12:\n",
    "    url = 'https://github.com/nasa?page=9'\n",
    "    response = get(url)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    for n in range(0,30):\n",
    "        REPOS.append(soup.select('h3')[n].text.split())\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['rtap'],\n",
       " ['meshNetwork'],\n",
       " ['GTM_DesignSim'],\n",
       " ['MLMCPy'],\n",
       " ['astrobee_gds'],\n",
       " ['conduit_cms'],\n",
       " ['PyAMPR'],\n",
       " ['xasgo'],\n",
       " ['abaverify'],\n",
       " ['gen_sch_tbl'],\n",
       " ['MINX'],\n",
       " ['GPU_SDR'],\n",
       " ['DdsJs'],\n",
       " ['CF'],\n",
       " ['CFS-101'],\n",
       " ['MCMCPy'],\n",
       " ['russ-open-source'],\n",
       " ['perfutils-java'],\n",
       " ['multipath-tcp-tools'],\n",
       " ['rjnieves-open-repo'],\n",
       " ['ARC-SGE-interns-19'],\n",
       " ['RHEAS'],\n",
       " ['georef_imageregistration'],\n",
       " ['TrickFMI'],\n",
       " ['Kepler-PyKE'],\n",
       " ['cratous'],\n",
       " ['Stratway'],\n",
       " ['CPB'],\n",
       " ['ACCoRD'],\n",
       " ['FPRoCK']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_list = [['cumulus'],\n",
    " ['openmct'],\n",
    " ['astrobee_android'],\n",
    " ['astrobee'],\n",
    " ['earthdata-search'],\n",
    " ['daa-displays'],\n",
    " ['code-nasa-gov'],\n",
    " ['cumulus-orca'],\n",
    " ['dorado-scheduling'],\n",
    " ['cFS'],\n",
    " ['ow_simulator'],\n",
    " ['NASA-Acronyms'],\n",
    " ['vscode-pvs'],\n",
    " ['osal'],\n",
    " ['cmr-stac'],\n",
    " ['fprime'],\n",
    " ['cumulus-dashboard'],\n",
    " ['gunns'],\n",
    " ['mmt'],\n",
    " ['edsc-echoforms'],\n",
    " ['cumulus-distribution-api'],\n",
    " ['Open-Source-Catalog'],\n",
    " ['delta'],\n",
    " ['isle'],\n",
    " ['openmct-map'],\n",
    " ['nasapress'],\n",
    " ['cumulus-message-adapter-js'],\n",
    " ['pvslib'],\n",
    " ['skeleton_app'],\n",
    " ['ow_autonomy'],\n",
    " ['nos3'],\n",
    " ['harmony-py'],\n",
    " ['zarr-eosdis-store'],\n",
    " ['Common-Metadata-Repository'],\n",
    " ['cumulus-api'],\n",
    " ['cFE'],\n",
    " ['harmony'],\n",
    " ['CARA_Analysis_Tools'],\n",
    " ['NPSS-Power-System-Library'],\n",
    " ['prog_models'],\n",
    " ['harmony-netcdf-to-zarr'],\n",
    " ['cmr-csw'],\n",
    " ['harmony-service-lib-py'],\n",
    " ['koviz'],\n",
    " ['icarous'],\n",
    " ['eo-metadata-tools'],\n",
    " ['nasa.github.io'],\n",
    " ['trick'],\n",
    " ['ow_europa'],\n",
    " ['cmr-opensearch'],\n",
    " ['harmony-service-example'],\n",
    " ['apod-api'],\n",
    " ['GlennOPT'],\n",
    " ['cmr-metadata-review'],\n",
    " ['concept-tagging-training'],\n",
    " ['pigans-material-ID'],\n",
    " ['openmct-hello'],\n",
    " ['cumulus-template-deploy'],\n",
    " ['EMTAT'],\n",
    " ['XPlaneConnect'],\n",
    " ['HyperInSPACE'],\n",
    " ['ominas'],\n",
    " ['JHU-PIV-data'],\n",
    " ['sample_lib'],\n",
    " ['CFL3D'],\n",
    " ['edsc-timeline'],\n",
    " ['ipv6_python'],\n",
    " ['cFS-GroundSystem'],\n",
    " ['prog_algs'],\n",
    " ['exoscene'],\n",
    " ['MFISPy'],\n",
    " ['TrickHLA'],\n",
    " ['dorado-sensitivity'],\n",
    " ['Mixed-Reality-Exploration-Toolkit'],\n",
    " ['refine'],\n",
    " ['SMCPy'],\n",
    " ['Kamodo'],\n",
    " ['CCDD'],\n",
    " ['SROMPy'],\n",
    " ['IDF'],\n",
    " ['PSP'],\n",
    " ['radbelt'],\n",
    " ['GMSEC_API'],\n",
    " ['bingo'],\n",
    " ['uam-apis'],\n",
    " ['swSim'],\n",
    " ['api-docs'],\n",
    " ['harmony-regression-tests'], \n",
    " ['sch_lab'],\n",
    " ['ci_lab'],\n",
    " ['to_lab'],\n",
    " ['sample_app'],\n",
    " ['svs'],\n",
    " ['Lightkurve'],\n",
    " ['DTNME'],\n",
    " ['eefs'],\n",
    " ['GeneLab_Data_Processing'],\n",
    " ['cumulus-process-py'],\n",
    " ['GSAP'],\n",
    " ['MOSAIC'],\n",
    " ['concept-tagging-api'],\n",
    " ['CompDam_DGD'],\n",
    " ['astrobot'],\n",
    " ['irg_open'],\n",
    " ['tblCRCTool'],\n",
    " ['elf2cfetbl'],\n",
    " ['nasa'],\n",
    " ['digital-strategy'],\n",
    " ['GeneLab-sampleProcessing'],\n",
    " ['RtRetrievalFramework'],\n",
    " ['PanNDE'],\n",
    " ['harmony-qgis'],\n",
    " ['T-MATS'],\n",
    " ['bplib'], #page5\n",
    " ['simupy-flight'],\n",
    " ['europa'],\n",
    " ['SBN'],\n",
    " ['atd2-fuser'],\n",
    " ['utm-apis'],\n",
    " ['aladynpi'],\n",
    " ['astrobee_media'],\n",
    " ['nasa-latex-docs'],\n",
    " ['cumulus-message-adapter-java'],\n",
    " ['webgs'],\n",
    " ['NASAaccess'],\n",
    " ['daidalus'],\n",
    " ['EMTG'],\n",
    " ['HDTN-BPCodec'],\n",
    " ['LHASA'],\n",
    " ['System_Monitor_for_Radiation_Testing'],\n",
    " ['NASA-3D-Resources'],\n",
    " ['CTF'],\n",
    " ['cumulus-ecs-task'],\n",
    " ['dawn-grand-toolbox'],\n",
    " ['PRECiSA'],\n",
    " ['cpr'],\n",
    " ['PolyCARP'],\n",
    " ['LC'],\n",
    " ['CS'],\n",
    " ['MD'], #page6\n",
    " ['WellClear'],\n",
    " ['AGTF30'],\n",
    " ['PyMKAD'],\n",
    " ['polyfit'],\n",
    " ['EdsLib'],\n",
    " ['cFS-EDS-GroundStation'],\n",
    " ['Kodiak'],\n",
    " ['gFTL'],\n",
    " ['HK'],\n",
    " ['HS'],\n",
    " ['DS'],\n",
    " ['MXMCPy'],\n",
    " ['PrognosticsAlgorithmLibrary'],\n",
    " ['PrognosticsModelLibrary'],\n",
    " ['spaceapps-phenomena_detection'],\n",
    " ['World-Wind-Java'],\n",
    " ['icc'],\n",
    " ['bingocpp'],\n",
    " ['stol-mode'],\n",
    " ['common-mapping-client'],\n",
    " ['ccmc-swpc-cat-web'],\n",
    " ['cumulus-message-adapter-python'],\n",
    " ['ECI'],\n",
    " ['PrognosticsMetricsLibrary'],\n",
    " ['ISS_Camera_Geolocate'],\n",
    " ['data-nasa-gov-frontpage'],\n",
    " ['K2CE'], #page7\n",
    " ['FM'],\n",
    " ['georef_deploy'],\n",
    " ['Three-Dimensional-Nozzle-Design-Code'],\n",
    " ['simplegrid'],\n",
    " ['S4P'],\n",
    " ['S4PA'],\n",
    " ['aos-dr'],\n",
    " ['MISR-Toolkit'],\n",
    " ['Rapid-Model-Import-Tool'],\n",
    " ['MultiDop'],\n",
    " ['openmct-demo'],\n",
    " ['SCH'],\n",
    " ['MiniWall'],\n",
    " ['kepler-pipeline'],\n",
    " ['cumulus-message-adapter'],\n",
    " ['resample_GLISTIN_DEMs'],\n",
    " ['CrisisMappingToolkit'],\n",
    " ['Coordinate-systems-class-library'],\n",
    " ['rbf'],\n",
    " ['Kepler-FLTI'],\n",
    " ['podaac_tools_and_services'], #page8\n",
    " ['podaacpy'], \n",
    " ['SIL'],\n",
    " ['MCT-Plugins'],\n",
    " ['ADOPT'],\n",
    " ['utm-docs'],\n",
    " ['PointCloudsVR'],\n",
    " ['BrightComets'],\n",
    " ['OpenSPIFe'],\n",
    " ['openmct-tutorial'],\n",
    " ['georef'],\n",
    " ['PyTDA'],\n",
    " ['UQ-Kernel-Mini-App'],\n",
    " ['Reinforcement-Learning-Benchmarking'],\n",
    " ['vsm'],\n",
    " ['Giovanni'],\n",
    " ['api-docs-stage'],\n",
    " ['gen_msgids'],  #page9\n",
    " ['meshNetwork'],\n",
    " ['GTM_DesignSim'],\n",
    " ['MLMCPy'],\n",
    " ['astrobee_gds'],\n",
    " ['conduit_cms'],\n",
    " ['PyAMPR'],\n",
    " ['xasgo'],\n",
    " ['abaverify'],\n",
    " ['gen_sch_tbl'],\n",
    " ['MINX'],\n",
    " ['GPU_SDR'],\n",
    " ['DdsJs'],\n",
    " ['CF'],\n",
    " ['CFS-101'],\n",
    " ['MCMCPy'],\n",
    " ['perfutils-java'],\n",
    " ['multipath-tcp-tools'],\n",
    " ['ARC-SGE-interns-19'],\n",
    " ['RHEAS'],\n",
    " ['georef_imageregistration'],\n",
    " ['TrickFMI'],\n",
    " ['Kepler-PyKE'],\n",
    " ['cratous'],\n",
    " ['FPRoCK'],\n",
    " ['kepler-robovetter'],\n",
    " ['DualPol'],\n",
    " ['SCA'],\n",
    " ['lager'],\n",
    " ['DERT'],\n",
    " ['AprilNav'],\n",
    " ['sitepod'],\n",
    " ['CFS_IO_LIB'],\n",
    " ['georef_geocamutilweb'],\n",
    " ['georef_geocamtiepoint'],\n",
    " ['pyCMR'],\n",
    " ['Computational-Materials-Miniapp1'],\n",
    " ['nasapress-companion'],\n",
    " ['content-guide'],\n",
    " ['scifen-solver'],\n",
    " ['cumulus-circleci-image'],\n",
    " ['TCML'],\n",
    " ['TSAT'],\n",
    " ['cumulus-ecs-task-python'],\n",
    " ['cumulus-process-py-seed'],\n",
    " ['geolambda'],\n",
    " ['CCSDS-MAL-Http-Binding-Xml-Encoding'],\n",
    " ['FEI'],\n",
    " ['PLEXIL5'],\n",
    " ['dictionaries'],\n",
    " ['SingleDop'],\n",
    " ['glm_ql'],\n",
    " ['CFS_CI'],\n",
    " ['CFS_TO'],\n",
    " ['TLNS3D'],\n",
    " ['MISR-View'],\n",
    " ['mplStyle'],\n",
    " ['EADINLite'],\n",
    " ['GFR'],\n",
    " ['mct'],\n",
    " ['openmct-heatmap'],\n",
    " ['SC'],\n",
    " ['MM'],\n",
    " ['cfs_lib'],\n",
    " ['LH2Sim'],\n",
    " ['OpenVSP3Plugin'],\n",
    " ['SAFE-DART'],\n",
    " ['RVLib'],\n",
    " ['VirtualADAPT'],\n",
    " ['ominas_maps'],\n",
    " ['SGNDI'],\n",
    " ['knife'],\n",
    " ['DON-Federate-HLA2MPC'],\n",
    " ['PyBlock'],\n",
    " ['MMM-Py'],\n",
    " ['KeplerPORTs'],\n",
    " ['ESPRESSO'],\n",
    " ['visionworkbench'],\n",
    " ['MAV'],\n",
    " ['NASA-Space-Weather-Media-Viewer'],\n",
    " ['RtRetrievalFrameworkDoc'],\n",
    " ['NASTRAN-95']] \n",
    "             \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPOS = []\n",
    "#for i in range(0,12):\n",
    "   # url = 'https://github.com/nasa?page=' + str(i)\n",
    "    #response = get(url)\n",
    "    #html = response.text\n",
    "    #soup = BeautifulSoup(html)\n",
    "    \n",
    "    #for n in range(0,30):\n",
    "    #REPOS.append(soup.select('h3')[n].text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flatten_list = list(itertools.chain(*repo_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cumulus',\n",
       " 'openmct',\n",
       " 'astrobee_android',\n",
       " 'astrobee',\n",
       " 'earthdata-search',\n",
       " 'daa-displays',\n",
       " 'code-nasa-gov',\n",
       " 'cumulus-orca',\n",
       " 'dorado-scheduling',\n",
       " 'cFS',\n",
       " 'ow_simulator',\n",
       " 'NASA-Acronyms',\n",
       " 'vscode-pvs',\n",
       " 'osal',\n",
       " 'cmr-stac',\n",
       " 'fprime',\n",
       " 'cumulus-dashboard',\n",
       " 'gunns',\n",
       " 'mmt',\n",
       " 'edsc-echoforms',\n",
       " 'cumulus-distribution-api',\n",
       " 'Open-Source-Catalog',\n",
       " 'delta',\n",
       " 'isle',\n",
       " 'openmct-map',\n",
       " 'nasapress',\n",
       " 'cumulus-message-adapter-js',\n",
       " 'pvslib',\n",
       " 'skeleton_app',\n",
       " 'ow_autonomy',\n",
       " 'nos3',\n",
       " 'harmony-py',\n",
       " 'zarr-eosdis-store',\n",
       " 'Common-Metadata-Repository',\n",
       " 'cumulus-api',\n",
       " 'cFE',\n",
       " 'harmony',\n",
       " 'CARA_Analysis_Tools',\n",
       " 'NPSS-Power-System-Library',\n",
       " 'prog_models',\n",
       " 'harmony-netcdf-to-zarr',\n",
       " 'cmr-csw',\n",
       " 'harmony-service-lib-py',\n",
       " 'koviz',\n",
       " 'icarous',\n",
       " 'eo-metadata-tools',\n",
       " 'nasa.github.io',\n",
       " 'trick',\n",
       " 'ow_europa',\n",
       " 'cmr-opensearch',\n",
       " 'harmony-service-example',\n",
       " 'apod-api',\n",
       " 'GlennOPT',\n",
       " 'cmr-metadata-review',\n",
       " 'concept-tagging-training',\n",
       " 'pigans-material-ID',\n",
       " 'openmct-hello',\n",
       " 'cumulus-template-deploy',\n",
       " 'EMTAT',\n",
       " 'XPlaneConnect',\n",
       " 'HyperInSPACE',\n",
       " 'ominas',\n",
       " 'JHU-PIV-data',\n",
       " 'sample_lib',\n",
       " 'CFL3D',\n",
       " 'edsc-timeline',\n",
       " 'ipv6_python',\n",
       " 'cFS-GroundSystem',\n",
       " 'prog_algs',\n",
       " 'exoscene',\n",
       " 'MFISPy',\n",
       " 'TrickHLA',\n",
       " 'dorado-sensitivity',\n",
       " 'Mixed-Reality-Exploration-Toolkit',\n",
       " 'refine',\n",
       " 'SMCPy',\n",
       " 'Kamodo',\n",
       " 'CCDD',\n",
       " 'SROMPy',\n",
       " 'IDF',\n",
       " 'PSP',\n",
       " 'radbelt',\n",
       " 'GMSEC_API',\n",
       " 'bingo',\n",
       " 'uam-apis',\n",
       " 'swSim',\n",
       " 'api-docs',\n",
       " 'harmony-regression-tests',\n",
       " 'sch_lab',\n",
       " 'ci_lab',\n",
       " 'to_lab',\n",
       " 'sample_app',\n",
       " 'svs',\n",
       " 'Lightkurve',\n",
       " 'DTNME',\n",
       " 'eefs',\n",
       " 'GeneLab_Data_Processing',\n",
       " 'cumulus-process-py',\n",
       " 'GSAP',\n",
       " 'MOSAIC',\n",
       " 'concept-tagging-api',\n",
       " 'CompDam_DGD',\n",
       " 'astrobot',\n",
       " 'irg_open',\n",
       " 'tblCRCTool',\n",
       " 'elf2cfetbl',\n",
       " 'nasa',\n",
       " 'digital-strategy',\n",
       " 'GeneLab-sampleProcessing',\n",
       " 'RtRetrievalFramework',\n",
       " 'PanNDE',\n",
       " 'harmony-qgis',\n",
       " 'T-MATS',\n",
       " 'bplib',\n",
       " 'simupy-flight',\n",
       " 'europa',\n",
       " 'SBN',\n",
       " 'atd2-fuser',\n",
       " 'utm-apis',\n",
       " 'aladynpi',\n",
       " 'astrobee_media',\n",
       " 'nasa-latex-docs',\n",
       " 'cumulus-message-adapter-java',\n",
       " 'webgs',\n",
       " 'NASAaccess',\n",
       " 'daidalus',\n",
       " 'EMTG',\n",
       " 'HDTN-BPCodec',\n",
       " 'LHASA',\n",
       " 'System_Monitor_for_Radiation_Testing',\n",
       " 'NASA-3D-Resources',\n",
       " 'CTF',\n",
       " 'cumulus-ecs-task',\n",
       " 'dawn-grand-toolbox',\n",
       " 'PRECiSA',\n",
       " 'cpr',\n",
       " 'PolyCARP',\n",
       " 'LC',\n",
       " 'CS',\n",
       " 'MD',\n",
       " 'WellClear',\n",
       " 'AGTF30',\n",
       " 'PyMKAD',\n",
       " 'polyfit',\n",
       " 'EdsLib',\n",
       " 'cFS-EDS-GroundStation',\n",
       " 'Kodiak',\n",
       " 'gFTL',\n",
       " 'HK',\n",
       " 'HS',\n",
       " 'DS',\n",
       " 'MXMCPy',\n",
       " 'PrognosticsAlgorithmLibrary',\n",
       " 'PrognosticsModelLibrary',\n",
       " 'spaceapps-phenomena_detection',\n",
       " 'World-Wind-Java',\n",
       " 'icc',\n",
       " 'bingocpp',\n",
       " 'stol-mode',\n",
       " 'common-mapping-client',\n",
       " 'ccmc-swpc-cat-web',\n",
       " 'cumulus-message-adapter-python',\n",
       " 'ECI',\n",
       " 'PrognosticsMetricsLibrary',\n",
       " 'ISS_Camera_Geolocate',\n",
       " 'data-nasa-gov-frontpage',\n",
       " 'K2CE',\n",
       " 'FM',\n",
       " 'georef_deploy',\n",
       " 'Three-Dimensional-Nozzle-Design-Code',\n",
       " 'simplegrid',\n",
       " 'S4P',\n",
       " 'S4PA',\n",
       " 'aos-dr',\n",
       " 'MISR-Toolkit',\n",
       " 'Rapid-Model-Import-Tool',\n",
       " 'MultiDop',\n",
       " 'openmct-demo',\n",
       " 'SCH',\n",
       " 'MiniWall',\n",
       " 'kepler-pipeline',\n",
       " 'cumulus-message-adapter',\n",
       " 'resample_GLISTIN_DEMs',\n",
       " 'CrisisMappingToolkit',\n",
       " 'Coordinate-systems-class-library',\n",
       " 'rbf',\n",
       " 'Kepler-FLTI',\n",
       " 'podaac_tools_and_services',\n",
       " 'podaacpy',\n",
       " 'terraform-aws-cumulus-thin-egress-app',\n",
       " 'VIIRS-demo',\n",
       " 'SIL',\n",
       " 'QuIP',\n",
       " 'MCT-Plugins',\n",
       " 'ADOPT',\n",
       " 'utm-docs',\n",
       " 'PointCloudsVR',\n",
       " 'Analysis_SDK',\n",
       " 'BrightComets',\n",
       " 'PyGNSS',\n",
       " 'OpenSPIFe',\n",
       " 'openmct-tutorial',\n",
       " 'georef',\n",
       " 'PyTDA',\n",
       " 'UQ-Kernel-Mini-App',\n",
       " 'Reinforcement-Learning-Benchmarking',\n",
       " 'vsm',\n",
       " 'Giovanni',\n",
       " 'api-docs-stage',\n",
       " 'gen_msgids',\n",
       " 'meshNetwork',\n",
       " 'GTM_DesignSim',\n",
       " 'MLMCPy',\n",
       " 'astrobee_gds',\n",
       " 'conduit_cms',\n",
       " 'PyAMPR',\n",
       " 'xasgo',\n",
       " 'abaverify',\n",
       " 'gen_sch_tbl',\n",
       " 'MINX',\n",
       " 'GPU_SDR',\n",
       " 'DdsJs',\n",
       " 'CF',\n",
       " 'CFS-101',\n",
       " 'MCMCPy',\n",
       " 'perfutils-java',\n",
       " 'multipath-tcp-tools',\n",
       " 'ARC-SGE-interns-19',\n",
       " 'RHEAS',\n",
       " 'georef_imageregistration',\n",
       " 'TrickFMI',\n",
       " 'Kepler-PyKE',\n",
       " 'cratous',\n",
       " 'FPRoCK',\n",
       " 'kepler-robovetter',\n",
       " 'DualPol',\n",
       " 'SCA',\n",
       " 'lager',\n",
       " 'DERT',\n",
       " 'AprilNav',\n",
       " 'sitepod',\n",
       " 'CFS_IO_LIB',\n",
       " 'georef_geocamutilweb',\n",
       " 'georef_geocamtiepoint',\n",
       " 'pyCMR',\n",
       " 'Computational-Materials-Miniapp1',\n",
       " 'nasapress-companion',\n",
       " 'content-guide',\n",
       " 'scifen-solver',\n",
       " 'cumulus-circleci-image',\n",
       " 'TCML',\n",
       " 'TSAT',\n",
       " 'cumulus-ecs-task-python',\n",
       " 'cumulus-process-py-seed',\n",
       " 'geolambda',\n",
       " 'CCSDS-MAL-Http-Binding-Xml-Encoding',\n",
       " 'FEI',\n",
       " 'PLEXIL5',\n",
       " 'dictionaries',\n",
       " 'SingleDop',\n",
       " 'glm_ql',\n",
       " 'CFS_CI',\n",
       " 'CFS_TO',\n",
       " 'TLNS3D',\n",
       " 'MISR-View',\n",
       " 'mplStyle',\n",
       " 'EADINLite',\n",
       " 'GFR',\n",
       " 'mct',\n",
       " 'openmct-heatmap',\n",
       " 'SC',\n",
       " 'MM',\n",
       " 'cfs_lib',\n",
       " 'LH2Sim',\n",
       " 'OpenVSP3Plugin',\n",
       " 'SAFE-DART',\n",
       " 'RVLib',\n",
       " 'VirtualADAPT',\n",
       " 'ominas_maps',\n",
       " 'SGNDI',\n",
       " 'knife',\n",
       " 'DON-Federate-HLA2MPC',\n",
       " 'PyBlock',\n",
       " 'MMM-Py',\n",
       " 'KeplerPORTs',\n",
       " 'ESPRESSO',\n",
       " 'visionworkbench',\n",
       " 'MAV',\n",
       " 'NASA-Space-Weather-Media-Viewer',\n",
       " 'RtRetrievalFrameworkDoc',\n",
       " 'NASTRAN-95']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "Repos = pd.Series(flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              cumulus\n",
       "1                              openmct\n",
       "2                     astrobee_android\n",
       "3                             astrobee\n",
       "4                     earthdata-search\n",
       "                    ...               \n",
       "286                    visionworkbench\n",
       "287                                MAV\n",
       "288    NASA-Space-Weather-Media-Viewer\n",
       "289            RtRetrievalFrameworkDoc\n",
       "290                         NASTRAN-95\n",
       "Length: 291, dtype: object"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS = list('nasa/' + Repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nasa/cumulus',\n",
       " 'nasa/openmct',\n",
       " 'nasa/astrobee_android',\n",
       " 'nasa/astrobee',\n",
       " 'nasa/earthdata-search',\n",
       " 'nasa/daa-displays',\n",
       " 'nasa/code-nasa-gov',\n",
       " 'nasa/cumulus-orca',\n",
       " 'nasa/dorado-scheduling',\n",
       " 'nasa/cFS',\n",
       " 'nasa/ow_simulator',\n",
       " 'nasa/NASA-Acronyms',\n",
       " 'nasa/vscode-pvs',\n",
       " 'nasa/osal',\n",
       " 'nasa/cmr-stac',\n",
       " 'nasa/fprime',\n",
       " 'nasa/cumulus-dashboard',\n",
       " 'nasa/gunns',\n",
       " 'nasa/mmt',\n",
       " 'nasa/edsc-echoforms',\n",
       " 'nasa/cumulus-distribution-api',\n",
       " 'nasa/Open-Source-Catalog',\n",
       " 'nasa/delta',\n",
       " 'nasa/isle',\n",
       " 'nasa/openmct-map',\n",
       " 'nasa/nasapress',\n",
       " 'nasa/cumulus-message-adapter-js',\n",
       " 'nasa/pvslib',\n",
       " 'nasa/skeleton_app',\n",
       " 'nasa/ow_autonomy',\n",
       " 'nasa/nos3',\n",
       " 'nasa/harmony-py',\n",
       " 'nasa/zarr-eosdis-store',\n",
       " 'nasa/Common-Metadata-Repository',\n",
       " 'nasa/cumulus-api',\n",
       " 'nasa/cFE',\n",
       " 'nasa/harmony',\n",
       " 'nasa/CARA_Analysis_Tools',\n",
       " 'nasa/NPSS-Power-System-Library',\n",
       " 'nasa/prog_models',\n",
       " 'nasa/harmony-netcdf-to-zarr',\n",
       " 'nasa/cmr-csw',\n",
       " 'nasa/harmony-service-lib-py',\n",
       " 'nasa/koviz',\n",
       " 'nasa/icarous',\n",
       " 'nasa/eo-metadata-tools',\n",
       " 'nasa/nasa.github.io',\n",
       " 'nasa/trick',\n",
       " 'nasa/ow_europa',\n",
       " 'nasa/cmr-opensearch',\n",
       " 'nasa/harmony-service-example',\n",
       " 'nasa/apod-api',\n",
       " 'nasa/GlennOPT',\n",
       " 'nasa/cmr-metadata-review',\n",
       " 'nasa/concept-tagging-training',\n",
       " 'nasa/pigans-material-ID',\n",
       " 'nasa/openmct-hello',\n",
       " 'nasa/cumulus-template-deploy',\n",
       " 'nasa/EMTAT',\n",
       " 'nasa/XPlaneConnect',\n",
       " 'nasa/HyperInSPACE',\n",
       " 'nasa/ominas',\n",
       " 'nasa/JHU-PIV-data',\n",
       " 'nasa/sample_lib',\n",
       " 'nasa/CFL3D',\n",
       " 'nasa/edsc-timeline',\n",
       " 'nasa/ipv6_python',\n",
       " 'nasa/cFS-GroundSystem',\n",
       " 'nasa/prog_algs',\n",
       " 'nasa/exoscene',\n",
       " 'nasa/MFISPy',\n",
       " 'nasa/TrickHLA',\n",
       " 'nasa/dorado-sensitivity',\n",
       " 'nasa/Mixed-Reality-Exploration-Toolkit',\n",
       " 'nasa/refine',\n",
       " 'nasa/SMCPy',\n",
       " 'nasa/Kamodo',\n",
       " 'nasa/CCDD',\n",
       " 'nasa/SROMPy',\n",
       " 'nasa/IDF',\n",
       " 'nasa/PSP',\n",
       " 'nasa/radbelt',\n",
       " 'nasa/GMSEC_API',\n",
       " 'nasa/bingo',\n",
       " 'nasa/uam-apis',\n",
       " 'nasa/swSim',\n",
       " 'nasa/api-docs',\n",
       " 'nasa/harmony-regression-tests',\n",
       " 'nasa/sch_lab',\n",
       " 'nasa/ci_lab',\n",
       " 'nasa/to_lab',\n",
       " 'nasa/sample_app',\n",
       " 'nasa/svs',\n",
       " 'nasa/Lightkurve',\n",
       " 'nasa/DTNME',\n",
       " 'nasa/eefs',\n",
       " 'nasa/GeneLab_Data_Processing',\n",
       " 'nasa/cumulus-process-py',\n",
       " 'nasa/GSAP',\n",
       " 'nasa/MOSAIC',\n",
       " 'nasa/concept-tagging-api',\n",
       " 'nasa/CompDam_DGD',\n",
       " 'nasa/astrobot',\n",
       " 'nasa/irg_open',\n",
       " 'nasa/tblCRCTool',\n",
       " 'nasa/elf2cfetbl',\n",
       " 'nasa/nasa',\n",
       " 'nasa/digital-strategy',\n",
       " 'nasa/GeneLab-sampleProcessing',\n",
       " 'nasa/RtRetrievalFramework',\n",
       " 'nasa/PanNDE',\n",
       " 'nasa/harmony-qgis',\n",
       " 'nasa/T-MATS',\n",
       " 'nasa/bplib',\n",
       " 'nasa/simupy-flight',\n",
       " 'nasa/europa',\n",
       " 'nasa/SBN',\n",
       " 'nasa/atd2-fuser',\n",
       " 'nasa/utm-apis',\n",
       " 'nasa/aladynpi',\n",
       " 'nasa/astrobee_media',\n",
       " 'nasa/nasa-latex-docs',\n",
       " 'nasa/cumulus-message-adapter-java',\n",
       " 'nasa/webgs',\n",
       " 'nasa/NASAaccess',\n",
       " 'nasa/daidalus',\n",
       " 'nasa/EMTG',\n",
       " 'nasa/HDTN-BPCodec',\n",
       " 'nasa/LHASA',\n",
       " 'nasa/System_Monitor_for_Radiation_Testing',\n",
       " 'nasa/NASA-3D-Resources',\n",
       " 'nasa/CTF',\n",
       " 'nasa/cumulus-ecs-task',\n",
       " 'nasa/dawn-grand-toolbox',\n",
       " 'nasa/PRECiSA',\n",
       " 'nasa/cpr',\n",
       " 'nasa/PolyCARP',\n",
       " 'nasa/LC',\n",
       " 'nasa/CS',\n",
       " 'nasa/MD',\n",
       " 'nasa/WellClear',\n",
       " 'nasa/AGTF30',\n",
       " 'nasa/PyMKAD',\n",
       " 'nasa/polyfit',\n",
       " 'nasa/EdsLib',\n",
       " 'nasa/cFS-EDS-GroundStation',\n",
       " 'nasa/Kodiak',\n",
       " 'nasa/gFTL',\n",
       " 'nasa/HK',\n",
       " 'nasa/HS',\n",
       " 'nasa/DS',\n",
       " 'nasa/MXMCPy',\n",
       " 'nasa/PrognosticsAlgorithmLibrary',\n",
       " 'nasa/PrognosticsModelLibrary',\n",
       " 'nasa/spaceapps-phenomena_detection',\n",
       " 'nasa/World-Wind-Java',\n",
       " 'nasa/icc',\n",
       " 'nasa/bingocpp',\n",
       " 'nasa/stol-mode',\n",
       " 'nasa/common-mapping-client',\n",
       " 'nasa/ccmc-swpc-cat-web',\n",
       " 'nasa/cumulus-message-adapter-python',\n",
       " 'nasa/ECI',\n",
       " 'nasa/PrognosticsMetricsLibrary',\n",
       " 'nasa/ISS_Camera_Geolocate',\n",
       " 'nasa/data-nasa-gov-frontpage',\n",
       " 'nasa/K2CE',\n",
       " 'nasa/FM',\n",
       " 'nasa/georef_deploy',\n",
       " 'nasa/Three-Dimensional-Nozzle-Design-Code',\n",
       " 'nasa/simplegrid',\n",
       " 'nasa/S4P',\n",
       " 'nasa/S4PA',\n",
       " 'nasa/aos-dr',\n",
       " 'nasa/MISR-Toolkit',\n",
       " 'nasa/Rapid-Model-Import-Tool',\n",
       " 'nasa/MultiDop',\n",
       " 'nasa/openmct-demo',\n",
       " 'nasa/SCH',\n",
       " 'nasa/MiniWall',\n",
       " 'nasa/kepler-pipeline',\n",
       " 'nasa/cumulus-message-adapter',\n",
       " 'nasa/resample_GLISTIN_DEMs',\n",
       " 'nasa/CrisisMappingToolkit',\n",
       " 'nasa/Coordinate-systems-class-library',\n",
       " 'nasa/rbf',\n",
       " 'nasa/Kepler-FLTI',\n",
       " 'nasa/podaac_tools_and_services',\n",
       " 'nasa/podaacpy',\n",
       " 'nasa/terraform-aws-cumulus-thin-egress-app',\n",
       " 'nasa/VIIRS-demo',\n",
       " 'nasa/SIL',\n",
       " 'nasa/QuIP',\n",
       " 'nasa/MCT-Plugins',\n",
       " 'nasa/ADOPT',\n",
       " 'nasa/utm-docs',\n",
       " 'nasa/PointCloudsVR',\n",
       " 'nasa/Analysis_SDK',\n",
       " 'nasa/BrightComets',\n",
       " 'nasa/PyGNSS',\n",
       " 'nasa/OpenSPIFe',\n",
       " 'nasa/openmct-tutorial',\n",
       " 'nasa/georef',\n",
       " 'nasa/PyTDA',\n",
       " 'nasa/UQ-Kernel-Mini-App',\n",
       " 'nasa/Reinforcement-Learning-Benchmarking',\n",
       " 'nasa/vsm',\n",
       " 'nasa/Giovanni',\n",
       " 'nasa/api-docs-stage',\n",
       " 'nasa/gen_msgids',\n",
       " 'nasa/meshNetwork',\n",
       " 'nasa/GTM_DesignSim',\n",
       " 'nasa/MLMCPy',\n",
       " 'nasa/astrobee_gds',\n",
       " 'nasa/conduit_cms',\n",
       " 'nasa/PyAMPR',\n",
       " 'nasa/xasgo',\n",
       " 'nasa/abaverify',\n",
       " 'nasa/gen_sch_tbl',\n",
       " 'nasa/MINX',\n",
       " 'nasa/GPU_SDR',\n",
       " 'nasa/DdsJs',\n",
       " 'nasa/CF',\n",
       " 'nasa/CFS-101',\n",
       " 'nasa/MCMCPy',\n",
       " 'nasa/perfutils-java',\n",
       " 'nasa/multipath-tcp-tools',\n",
       " 'nasa/ARC-SGE-interns-19',\n",
       " 'nasa/RHEAS',\n",
       " 'nasa/georef_imageregistration',\n",
       " 'nasa/TrickFMI',\n",
       " 'nasa/Kepler-PyKE',\n",
       " 'nasa/cratous',\n",
       " 'nasa/FPRoCK',\n",
       " 'nasa/kepler-robovetter',\n",
       " 'nasa/DualPol',\n",
       " 'nasa/SCA',\n",
       " 'nasa/lager',\n",
       " 'nasa/DERT',\n",
       " 'nasa/AprilNav',\n",
       " 'nasa/sitepod',\n",
       " 'nasa/CFS_IO_LIB',\n",
       " 'nasa/georef_geocamutilweb',\n",
       " 'nasa/georef_geocamtiepoint',\n",
       " 'nasa/pyCMR',\n",
       " 'nasa/Computational-Materials-Miniapp1',\n",
       " 'nasa/nasapress-companion',\n",
       " 'nasa/content-guide',\n",
       " 'nasa/scifen-solver',\n",
       " 'nasa/cumulus-circleci-image',\n",
       " 'nasa/TCML',\n",
       " 'nasa/TSAT',\n",
       " 'nasa/cumulus-ecs-task-python',\n",
       " 'nasa/cumulus-process-py-seed',\n",
       " 'nasa/geolambda',\n",
       " 'nasa/CCSDS-MAL-Http-Binding-Xml-Encoding',\n",
       " 'nasa/FEI',\n",
       " 'nasa/PLEXIL5',\n",
       " 'nasa/dictionaries',\n",
       " 'nasa/SingleDop',\n",
       " 'nasa/glm_ql',\n",
       " 'nasa/CFS_CI',\n",
       " 'nasa/CFS_TO',\n",
       " 'nasa/TLNS3D',\n",
       " 'nasa/MISR-View',\n",
       " 'nasa/mplStyle',\n",
       " 'nasa/EADINLite',\n",
       " 'nasa/GFR',\n",
       " 'nasa/mct',\n",
       " 'nasa/openmct-heatmap',\n",
       " 'nasa/SC',\n",
       " 'nasa/MM',\n",
       " 'nasa/cfs_lib',\n",
       " 'nasa/LH2Sim',\n",
       " 'nasa/OpenVSP3Plugin',\n",
       " 'nasa/SAFE-DART',\n",
       " 'nasa/RVLib',\n",
       " 'nasa/VirtualADAPT',\n",
       " 'nasa/ominas_maps',\n",
       " 'nasa/SGNDI',\n",
       " 'nasa/knife',\n",
       " 'nasa/DON-Federate-HLA2MPC',\n",
       " 'nasa/PyBlock',\n",
       " 'nasa/MMM-Py',\n",
       " 'nasa/KeplerPORTs',\n",
       " 'nasa/ESPRESSO',\n",
       " 'nasa/visionworkbench',\n",
       " 'nasa/MAV',\n",
       " 'nasa/NASA-Space-Weather-Media-Viewer',\n",
       " 'nasa/RtRetrievalFrameworkDoc',\n",
       " 'nasa/NASTRAN-95']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(REPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module for obtaining repo readme and language data from the github API.\n",
    "Before using this module, read through it, and follow the instructions marked\n",
    "TODO.\n",
    "After doing so, run it like this:\n",
    "    python acquire.py\n",
    "To create the `data.json` file that contains the data.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "\n",
    "from env import github_token, github_username\n",
    "\n",
    "# TODO: Make a github personal access token.\n",
    "#     1. Go here and generate a personal access token https://github.com/settings/tokens\n",
    "#        You do _not_ need select any scopes, i.e. leave all the checkboxes unchecked\n",
    "#     2. Save it in your env.py file under the variable `github_token`\n",
    "# TODO: Add your github username to your env.py file under the variable `github_username`\n",
    "# TODO: Add more repositories to the `REPOS` list below.\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "    raise Exception(\n",
    "        \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "    )\n",
    "\n",
    "\n",
    "def github_api_request(url: str) -> Union[List, Dict]:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Error response from github api! status code: {response.status_code}, \"\n",
    "            f\"response: {json.dumps(response_data)}\"\n",
    "        )\n",
    "    return response_data\n",
    "\n",
    "\n",
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    repo_info = github_api_request(url)\n",
    "    if type(repo_info) is dict:\n",
    "        repo_info = cast(Dict, repo_info)\n",
    "        if \"language\" not in repo_info:\n",
    "            raise Exception(\n",
    "                \"'language' key not round in response\\n{}\".format(json.dumps(repo_info))\n",
    "            )\n",
    "        return repo_info[\"language\"]\n",
    "    raise Exception(\n",
    "        f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "    contents = github_api_request(url)\n",
    "    if type(contents) is list:\n",
    "        contents = cast(List, contents)\n",
    "        return contents\n",
    "    raise Exception(\n",
    "        f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a response from the github api that lists the files in a repo and\n",
    "    returns the url that can be used to download the repo's README file.\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        if file[\"name\"].lower().startswith(\"readme\"):\n",
    "            return file[\"download_url\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def process_repo(repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "    dictionary with the language of the repo and the readme contents.\n",
    "    \"\"\"\n",
    "    contents = get_repo_contents(repo)\n",
    "    readme_download_url = get_readme_download_url(contents)\n",
    "    if readme_download_url == \"\":\n",
    "        readme_contents = \"\"\n",
    "    else:\n",
    "        readme_contents = requests.get(readme_download_url).text\n",
    "    return {\n",
    "        \"repo\": repo,\n",
    "        \"language\": get_repo_language(repo),\n",
    "        \"readme_contents\": readme_contents,\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_github_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loop through all of the repos and process them. Returns the processed data.\n",
    "    \"\"\"\n",
    "    return [process_repo(repo) for repo in REPOS]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = scrape_github_data()\n",
    "    json.dump(data, open(\"data.json\", \"w\"), indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'repo': 'nasa/cumulus',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Cumulus Framework\\n\\n[![npm version](https://badge.fury.io/js/%40cumulus%2Fapi.svg)](https://badge.fury.io/js/%40cumulus%2Fapi)\\n[![Coverage Status](https://coveralls.io/repos/github/nasa/cumulus/badge.svg?branch=master)](https://coveralls.io/github/nasa/cumulus?branch=master)\\n\\n## 📖 Documentation\\n\\n- Documentation for the latest [released version](https://nasa.github.io/cumulus).\\n- Documentation for the [unreleased work](https://nasa.github.io/cumulus/docs/next/cumulus-docs-readme).\\n\\n## More Information\\n\\nFor more information about this project of more about NASA's Earth Observing System Data and Information System (EOSDIS) and its cloud work, please contact [Katie Baynes](mailto:katie.baynes@nasa.gov) or visit us at https://earthdata.nasa.gov.\\n\\n# 🔨 Development\\n\\nThe Cumulus core repo is a [monorepo](https://en.wikipedia.org/wiki/Monorepo)\\nmanaged by [Lerna](https://lerna.js.org/). Lerna is responsible for installing\\nthe dependencies of the packages and tasks that belong in this repo. In general,\\nCumulus's npm packages can be found in the [packages](./packages) directory, and\\nworkflow tasks can be found in the [tasks](./tasks) directory.\\n\\nTo help cut down on the time and disk space required to install the dependencies\\nof the packages in this monorepo, all `devDependencies` are defined in the\\ntop-level [package.json](./package.json). The\\n[Node module resolution algorithm](https://nodejs.org/api/modules.html#modules_loading_from_node_modules_folders)\\nallows all of the packages and tasks to find their dev dependencies in that\\ntop-level `node_modules` directory.\\n\\nTL;DR - If you need to add a `devDependency` to a package, add it to the\\ntop-level [package.json](./package.json) file, not the `package.json` associated\\nwith an individual package.\\n\\n## Installation\\n\\nThis is for installation for Cumulus development.  See the [Cumulus deployment instructions](https://nasa.github.io/cumulus/docs/deployment/deployment-readme) for instructions on deploying the released Cumulus packages.\\n\\n### Prerequisites\\n\\n- [NVM](https://github.com/creationix/nvm) and node version 12.18\\n- [AWS CLI](http://docs.aws.amazon.com/cli/latest/userguide/installing.html)\\n- BASH\\n- Docker (only required for testing)\\n- docker-compose (only required for testing `pip install docker-compose`)\\n- Python 3.6+\\n- [pipenv](https://pypi.org/project/pipenv/)\\n\\nInstall the correct node version:\\n\\n```bash\\nnvm install\\nnvm use\\n```\\n\\n### Install Lerna\\n\\nWe use Lerna to manage multiple Cumulus packages in the same repo. You need to install lerna as a global module first:\\n\\n    $ npm install -g lerna\\n\\n### Install Local Dependencies\\n\\nWe use npm for local package management\\n\\n    $ npm install\\n    $ npm run bootstrap\\n\\nBuilding All packages:\\n\\n    $ npm run build\\n\\nBuild and watch packages:\\n\\n    $ npm run watch\\n\\n## Running the Cumulus APIs locally\\n\\nStart the API:\\n\\n    $ npm run serve\\n\\nOr start the distribution API:\\n\\n    $ npm run serve-dist\\n\\nSee the [API package documentation](packages/api/README.md#running-the-api-locally) for more options.\\n\\n## 📝 Tests\\n\\n### Unit Tests\\n\\n#### LocalStack\\n\\n[LocalStack](https://github.com/localstack/localstack) provides local versions of most AWS services for testing.\\n\\nThe LocalStack repository has [installation instructions](https://github.com/localstack/localstack#installing).\\n\\nLocalstack is included in the docker-compose file. You only need to run the docker-compose command in the next section in order to use it with your tests.\\n\\n#### Docker containers\\n\\nTurn on the docker containers first:\\n\\n    $ npm run start-unit-test-stack\\n\\nStop localstack/unit test services:\\n\\n    $ npm run stop-unit-test-stack\\n\\n#### Run database migrations\\n\\n```\\n$ npm run db:local:migrate\\n```\\n\\n#### Run tests\\n\\nRun the test commands next\\n```\\n    $ export LOCAL_ES_HOST=127.0.0.1\\n    $ export LOCALSTACK_HOST=127.0.0.1\\n    $ npm test\\n```\\n\\n### Integration Tests\\n\\nFor more information please [read this](docs/development/integration-tests.md).\\n\\n### Running tests via VS Code debugger\\n\\nCopy the `.vscode.example` directory to `.vscode` to create your debugger launch configuration. Refer to the [VS Code documentation on how to use the debugger](https://code.visualstudio.com/docs/editor/debugging).\\n\\n## 🔦 Code Coverage and Quality\\n\\nFor more information please [read this](docs/development/quality-and-coverage.md).\\n\\n## 📦 Adding New Packages\\n\\nCreate a new folder under `packages` if it is a common library or create folder under `cumulus/tasks` if it is a lambda task. `cd` to the folder and run `npm init`.\\n\\nMake sure to name the package as `@cumulus/package-name`.\\n\\n## Running command in all package folders\\n\\n    $ lerna exec -- rm -rf ./package-lock.json\\n\\n## Cleaning Up all the repos\\n\\n    $ npm run clean\\n\\n## Contribution\\n\\nPlease refer to: https://github.com/nasa/cumulus/blob/master/CONTRIBUTING.md for more information.\\n\\n## 🛒 Release\\n\\nTo release a new version of cumulus [read this](docs/development/release.md).\\n\"},\n",
       " {'repo': 'nasa/openmct',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# Open MCT [![license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)\\n\\nOpen MCT (Open Mission Control Technologies) is a next-generation mission control framework for visualization of data on desktop and mobile devices. It is developed at NASA's Ames Research Center, and is being used by NASA for data analysis of spacecraft missions, as well as planning and operation of experimental rover systems. As a generalizable and open source framework, Open MCT could be used as the basis for building applications for planning, operation, and analysis of any systems producing telemetry data.\\n\\nPlease visit our [Official Site](https://nasa.github.io/openmct/) and [Getting Started Guide](https://nasa.github.io/openmct/getting-started/)\\n\\n## See Open MCT in Action\\n\\nTry Open MCT now with our [live demo](https://openmct-demo.herokuapp.com/).\\n![Demo](https://nasa.github.io/openmct/static/res/images/Open-MCT.Browse.Layout.Mars-Weather-1.jpg)\\n\\n## Building and Running Open MCT Locally\\n\\nBuilding and running Open MCT in your local dev environment is very easy. Be sure you have [Git](https://git-scm.com/downloads) and [Node.js](https://nodejs.org/) installed, then follow the directions below. Need additional information? Check out the [Getting Started](https://nasa.github.io/openmct/getting-started/) page on our website.\\n(These instructions assume you are installing as a non-root user; developers have [reported issues](https://github.com/nasa/openmct/issues/1151) running these steps with root privileges.)\\n\\n1. Clone the source code\\n\\n `git clone https://github.com/nasa/openmct.git`\\n\\n2. Install development dependencies\\n\\n `npm install`\\n\\n3. Run a local development server\\n\\n `npm start`\\n\\nOpen MCT is now running, and can be accessed by pointing a web browser at [http://localhost:8080/](http://localhost:8080/)\\n\\n## Open MCT v1.0.0\\nThis represents a major overhaul of Open MCT with significant changes under the hood. We aim to maintain backward compatibility but if you do find compatibility issues, please let us know by filing an issue in this repository. If you are having major issues with v1.0.0 please check-out the v0.14.0 tag until we can resolve them for you.\\n\\nIf you are migrating an application built with Open MCT as a dependency to v1.0.0 from an earlier version, please refer to [our migration guide](https://nasa.github.io/openmct/documentation/migration-guide).\\n\\n## Documentation\\n\\nDocumentation is available on the [Open MCT website](https://nasa.github.io/openmct/documentation/).\\n\\n### Examples\\n\\nThe clearest examples for developing Open MCT plugins are in the\\n[tutorials](https://github.com/nasa/openmct-tutorial) provided in\\nour documentation.\\n\\nWe want Open MCT to be as easy to use, install, run, and develop for as\\npossible, and your feedback will help us get there! Feedback can be provided via [GitHub issues](https://github.com/nasa/openmct/issues), or by emailing us at [arc-dl-openmct@mail.nasa.gov](mailto:arc-dl-openmct@mail.nasa.gov).\\n\\n## Building Applications With Open MCT\\n\\nOpen MCT is built using [`npm`](http://npmjs.com/) and [`webpack`](https://webpack.js.org/).\\n\\nSee our documentation for a guide on [building Applications with Open MCT](https://github.com/nasa/openmct/blob/master/API.md#starting-an-open-mct-application).\\n\\n## Plugins\\n\\nOpen MCT can be extended via plugins that make calls to the Open MCT API. A plugin is a group \\nof software components (including source code and resources such as images and HTML templates)\\nthat is intended to be added or removed as a single unit.\\n\\nAs well as providing an extension mechanism, most of the core Open MCT codebase is also \\nwritten as plugins.\\n\\nFor information on writing plugins, please see [our API documentation](https://github.com/nasa/openmct/blob/master/API.md#plugins).\\n\\n## Tests\\n\\nTests are written for [Jasmine 3](https://jasmine.github.io/api/3.1/global)\\nand run by [Karma](http://karma-runner.github.io). To run:\\n\\n`npm test`\\n\\nThe test suite is configured to load any scripts ending with `Spec.js` found\\nin the `src` hierarchy. Full configuration details are found in\\n`karma.conf.js`. By convention, unit test scripts should be located\\nalongside the units that they test; for example, `src/foo/Bar.js` would be\\ntested by `src/foo/BarSpec.js`. (For legacy reasons, some existing tests may\\nbe located in separate `test` folders near the units they test, but the\\nnaming convention is otherwise the same.)\\n\\n### Test Reporting\\n\\nWhen `npm test` is run, test results will be written as HTML to\\n`dist/reports/tests/`. Code coverage information is written to `dist/reports/coverage`.\\n\\n# Glossary\\n\\nCertain terms are used throughout Open MCT with consistent meanings\\nor conventions. Any deviations from the below are issues and should be\\naddressed (either by updating this glossary or changing code to reflect\\ncorrect usage.) Other developer documentation, particularly in-line\\ndocumentation, may presume an understanding of these terms.\\n\\n* _plugin_: A plugin is a removable, reusable grouping of software elements.\\n  The application is composed of plugins.\\n* _composition_: In the context of a domain object, this refers to the set of\\n  other domain objects that compose or are contained by that object. A domain\\n  object's composition is the set of domain objects that should appear\\n  immediately beneath it in a tree hierarchy. A domain object's composition is\\n  described in its model as an array of id's; its composition capability\\n  provides a means to retrieve the actual domain object instances associated\\n  with these identifiers asynchronously.\\n* _description_: When used as an object property, this refers to the human-readable\\n  description of a thing; usually a single sentence or short paragraph.\\n  (Most often used in the context of extensions, domain\\n  object models, or other similar application-specific objects.)\\n* _domain object_: A meaningful object to the user; a distinct thing in\\n  the work support by Open MCT. Anything that appears in the left-hand\\n  tree is a domain object.\\n* _identifier_: A tuple consisting of a namespace and a key, which together uniquely\\n  identifies a domain object.\\n* _model_: The persistent state associated with a domain object. A domain\\n  object's model is a JavaScript object which can be converted to JSON\\n  without losing information (that is, it contains no methods.)\\n* _name_: When used as an object property, this refers to the human-readable\\n  name for a thing. (Most often used in the context of extensions, domain\\n  object models, or other similar application-specific objects.)\\n* _navigation_: Refers to the current state of the application with respect\\n  to the user's expressed interest in a specific domain object; e.g. when\\n  a user clicks on a domain object in the tree, they are _navigating_ to\\n  it, and it is thereafter considered the _navigated_ object (until the\\n  user makes another such choice.)\\n* _namespace_: A name used to identify a persistence store. A running open MCT \\napplication could potentially use multiple persistence stores, with the \\n\"},\n",
       " {'repo': 'nasa/astrobee_android',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Astrobee Robot Software - Android submodule\\n\\n## About\\n\\nAstrobee is a free-flying robot that is designed to operate as a payload inside\\nthe International Space Station (ISS). The Astrobee Robot Software consists of\\nembedded (on-board) software, supporting tools and a simulator. The Astrobee\\nRobot Software operates on Astrobee\\'s three internal single board computers and\\nuses the open-source Robot Operating System (ROS) framework as message-passing\\nmiddleware. The Astrobee Robot Software performs vision-based localization,\\nprovides autonomous navigation, docking and perching, manages various sensors\\nand actuators, and supports user interaction via screen-based displays, light\\nsignaling, and sound. The Astrobee Robot Software enables Astrobee to be\\noperated in multiple modes: plan-based task execution (command sequencing),\\nteleoperation, or autonomously through execution of hosted code uploaded by\\nproject partners (guest science). The software simulator enables Astrobee Robot\\nSoftware to be evaluated without the need for robot hardware.\\n\\nThis repository contains the libraries and API to support Guest Science\\napplication running on the Astrobee High Level Processor (HLP). The HLP runs the\\n[Android Nougat](https://www.android.com/versions/nougat-7-0/)\\nOperating System (7.1.1). The Astrobee Robot Software exposes a Java\\nAPI that can be used either in pure Java land or Android land to interact with\\nrobot internal messaging system based on ROS.\\n\\nA distinct repository, [`astrobee`](https://github.com/nasa/astrobee), contains\\nthe core flight software for Astrobee. Note that `astrobee` repository is\\nrequired to used `astrobee_android` (it contains all the message definitions).\\n\\nAlso note that the Astrobee Robot Software is in beta stage. This means that\\nsome features are missing and some functionalities are incomplete. Please\\nconsult [RELEASE.md](https://github.com/nasa/astrobee/blob/HEAD/RELEASE.md) for\\nthe current list of features and limitations.\\n\\nPlease see the [Guest Science Readme](guest_science_readme.md) for a description\\nof how guest science should interact with the Astrobee flight software. This\\ndocumentation also contains examples to help guest science interface with\\nAstrobee.\\n\\nPlease see the\\n[Guest Science Resources](https://www.nasa.gov/content/guest-science-resources)\\npage for information on guest science capabilities on Astrobee.\\n\\n## License\\n\\nCopyright (c) 2017, United States Government, as represented by the\\nAdministrator of the National Aeronautics and Space Administration.\\nAll rights reserved.\\n\\nThe Astrobee platform is licensed under the Apache License, Version 2.0 (the\\n\"License\"); you may not use this file except in compliance with the License. You\\nmay obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\\n\\nUnless required by applicable law or agreed to in writing, software distributed\\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n'},\n",
       " {'repo': 'nasa/astrobee',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Astrobee Robot Software\\n\\n### About\\n\\n<p>\\n<img src=\"doc/images/astrobee.png\" srcset=\"../images/astrobee.png 1x\" \\n  title=\"Astrobee\" align=\"right\" style=\"display: inline\"/>\\nAstrobee is a free-flying robot designed to operate as a payload inside\\nthe International Space Station (ISS). The Astrobee Robot Software consists of\\nembedded (on-board) software, supporting tools and a simulator. The Astrobee\\nRobot Software operates on Astrobee\\'s three internal single board computers and\\nuses the open-source Robot Operating System (ROS) framework as message-passing\\nmiddleware. The Astrobee Robot Software performs vision-based localization,\\nprovides autonomous navigation, docking and perching, manages various sensors\\nand actuators, and supports user interaction via screen-based displays, light\\nsignaling, and sound. The Astrobee Robot Software enables Astrobee to be\\noperated in multiple modes: plan-based task execution (command sequencing),\\nteleoperation, or autonomously through execution of hosted code uploaded by\\nproject partners (guest science). The software simulator enables Astrobee Robot\\nSoftware to be evaluated without the need for robot hardware.\\n</p>\\n\\nThis repository provides flight software and a simulator, both primarily written\\nin C++. The repository also provides several other utilities, including a tool\\nfor creating maps for localization. A separate repository,\\n[`astrobee_android`](https://github.com/nasa/astrobee_android), contains the\\nJava API, which uses the ROS messaging system to communicate with flight\\nsoftware.\\n\\nThe Astrobee Robot Software is in a beta stage. This means that some\\nfeatures are incomplete, and extensive changes can be expected. Please consult the\\n[release](https://nasa.github.io/astrobee/html/md_RELEASE.html) for the current list of features and limitations.\\n\\n### Usage instructions\\n\\nTo install and use astrobee, please see the\\n[usage instructions](https://nasa.github.io/astrobee/html/md_INSTALL.html).\\n\\n### Contributors\\n\\nThe Astrobee Robot Software is open source, and we welcome contributions\\nfrom the public. Please submit pull requests to the develop branch.\\nFor us to merge any pull requests, we must request that contributors sign and submit a\\n[Contributor License Agreement](https://www.nasa.gov/sites/default/files/atoms/files/astrobee_individual_contributor_license_agreement.pdf)\\ndue to NASA legal requirements. Thank you for your understanding.\\n\\n### Documentation\\n\\nTo view all the Astrobee documentation, please visit [documentation](https://nasa.github.io/astrobee/documentation.html).\\n\\nIf you want to perform research using the astrobee platform, a good tutorial guide is [\"A Brief Guide to Astrobee’s Flight Software\"](https://github.com/albee/a-brief-guide-to-astrobee/raw/master/a_brief_guide_to_astrobee_v1.0.pdf). This will teach you what Astrobee is, how the robot works, how to make your own package, and much more!\\n\\nFor more information, read the Astrobee [publications](https://www.nasa.gov/content/research-publications-0).\\nLearning about the Astrobee [platform](https://www.nasa.gov/sites/default/files/atoms/files/bualat_spaceops_2018_paper.pdf),\\n[software](https://www.nasa.gov/sites/default/files/atoms/files/fluckiger2018astrobee.pdf),\\nand [localization](https://www.nasa.gov/sites/default/files/atoms/files/coltin2016localization.pdf)\\nare good starting points.\\n\\n### Guest Science\\n\\nIf you are interested in guest science, please checkout the astrobee_android nasa github\\nproject (if you followed the usage instructions, you should have checked this\\nout already). Once that is checked out, please see\\n[`astrobee_android/README.md`](https://github.com/nasa/astrobee_android/blob/master/README.md)\\nlocated in the `astrobee_android/` folder.\\n\\n### License\\n\\nCopyright (c) 2017, United States Government, as represented by the\\nAdministrator of the National Aeronautics and Space Administration.\\nAll rights reserved.\\n\\nThe Astrobee platform is licensed under the Apache License, Version 2.0 (the\\n\"License\"); you may not use this file except in compliance with the License. You\\nmay obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\\n\\nUnless required by applicable law or agreed to in writing, software distributed\\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\\nCONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n'},\n",
       " {'repo': 'nasa/earthdata-search',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# [Earthdata Search](https://search.earthdata.nasa.gov)\\n\\n[![serverless](http://public.serverless.com/badges/v3.svg)](http://www.serverless.com)\\n![Build Status](https://github.com/nasa/earthdata-search/workflows/CI/badge.svg?branch=master)\\n[![codecov](https://codecov.io/gh/nasa/earthdata-search/branch/master/graph/badge.svg?token=kIkZQ0NrqK)](https://codecov.io/gh/nasa/earthdata-search)\\n[![Known Vulnerabilities](https://snyk.io/test/github/nasa/earthdata-search/badge.svg)](https://snyk.io/test/github/nasa/earthdata-search)\\n\\n## About\\nEarthdata Search is a web application developed by [NASA](http://nasa.gov) [EOSDIS](https://earthdata.nasa.gov)\\nto enable data discovery, search, comparison, visualization, and access across EOSDIS\\' Earth Science data holdings.\\nIt builds upon several public-facing services provided by EOSDIS, including\\nthe [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/) for data discovery and access,\\nEOSDIS [User Registration System (URS)](https://urs.earthdata.nasa.gov) authentication,\\nthe [Global Imagery Browse Services (GIBS)](https://earthdata.nasa.gov/gibs) for visualization,\\nand a number of OPeNDAP services hosted by data providers.\\n\\n## License\\n\\n> Copyright © 2007-2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n>\\n> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\n> You may obtain a copy of the License at\\n>\\n>    http://www.apache.org/licenses/LICENSE-2.0\\n>\\n>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\n>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n\\n## Application Installation and Usage\\n\\nThe Earthdata Search application uses Node v12.16.3 and Webpack 4.39.3 to generate static assets. The serverless application utilizes the following AWS services (important to note if deploying to an AWS environment):\\n- S3\\n  - We highly recommend using CloudFront in front of S3.\\n- SQS\\n- API Gateway\\n- Lambda\\n- Cloudwatch (Events)\\n\\n### Prerequisites\\n\\n##### Node\\nEarthdata Search runs on Node.js, in order to run the application you\\'ll need to [install it](https://nodejs.org/en/download/).\\n\\n**Recommended:** Use Homebrew\\n\\n\\tbrew install node\\n\\n##### NPM\\nnpm is a separate project from Node.js, and tends to update more frequently. As a result, even if you’ve just downloaded Node.js (and therefore npm), you’ll probably need to update your npm. Luckily, npm knows how to update itself! To update your npm, type this into your terminal:\\n\\n    npm install -g npm@latest\\n\\n##### NVM\\nTo ensure that you\\'re using the correct version of Node it is recommended that you use Node Version Manager. Installation instructions can be found on [the repository](https://github.com/nvm-sh/nvm#install--update-script). The version used is defined in .nvmrc and will be used automatically if NVM is configured correctly.\\n\\n\\n##### Serverless Framework\\nEarthdata Search utilizes the [Serverless Framework](https://serverless.com/) for managing AWS resources. In order to fully run and manage the application you\\'ll need to install it:\\n\\n    npm install -g serverless@latest\\n\\n##### PostgreSQL\\nEarthdata Search uses PostgreSQL in production on AWS RDS. If you don\\'t already have it installed, [download](https://www.postgresql.org/download/) and install it to your development environment.\\n\\n**Recommended:** Use Homebrew\\n\\n    brew install postgresql\\n\\nIf you decide to install via Homebrew you\\'ll need to create the default user.\\n\\n    /usr/local/opt/postgres/bin/createuser -s postgres\\n\\n### Initial Setup\\n\\n##### Package Installation\\n\\nOnce npm is installed locally, you need to download the dependencies by executing the command below in the project root directory:\\n\\n    npm install\\n\\n##### Configuration\\n\\n###### Secrets\\n\\nFor local development Earthdata Search uses a json configuration file to store secure files, an example is provided and should be copied and completed before attempting to go any further.\\n\\n\\tcp secret.config.json.example secret.config.json\\n\\nIn order to operate against a local database this file will need `dbUsername` and `dbPassword` values set (you may need to update `dbHost`, `dbName` or `dbPort` in `static.config.json` if you have custom configuration locally)\\n\\n###### Public (Non-Secure)\\nNon-secure values are stored in `static.config.json`. In order to prevent conflicts amongst developers you copy the static config into `overrideStatic.config.json` and change the config values there. Do not commit changes to `static.config.json`.\\n\\n    cp static.config.json overrideStatic.config.json\\n\\n##### Database Migration\\n\\nEnsure that you have a database created:\\n\\n\\tcreatedb edsc_dev\\n\\nOur database migrations run within Lambda due to the fact that in non-development environments our resources are not publicly accessible. To run the migrations you\\'ll need to invoke the Lambda:\\n\\n\\tserverless invoke local --function migrateDatabase\\n\\n\\n### Building the Application\\n\\nThe production build of the application will be output in the `/static/dist/` directory:\\n\\n    npm run build\\n\\n\\n### Run the Application Locally\\n\\nThe local development environment for the static assets can be started by executing the command below in the project root directory:\\n\\n    npm run start\\n\\nThis will run the React application at [http://localhost:8080](http://localhost:8080) -- please see `Serverless Framework` below for enabling the \\'server\\' side functionality.\\n\\n\\n### Serverless Framework\\n\\nThe [serverless framework](https://serverless.com/framework/docs/providers/aws/) offers many plugins which allow for local development utilizing many of the services AWS offers. For the most part we only need API Gateway and Lambda for this application but there are plugins for many more services (a list of known exceptions will be maintained below).\\n\\n##### Exceptions\\n- SQS\\n\\n\\tWhile there is an sqs-offline plugin for serverless it still requires an actual queue be running, we may investigate this in the future but for now sqs functionality isn\\'t available while developing locally which means the following pieces of functionality will not operate locally:\\n\\t- Generating Colormaps\\n\\n#### Running API Gateway and Lambda Locally\\n\\nRunning the following command will spin up API Gateway and Lambda locally which will open up a vast majority of the functionality the backend offers.\\n\\n\\tserverless offline\\n\\nThis will provide access to API Gateway at [http://localhost:3001](http://localhost:3001)\\n\\nAdditionally, this ties in with the `serverless webpack` plugin which will ensure that your lambdas are re-built when changes are detected.\\n\\n\\n### Run the Automated [Jest](https://jestjs.io/) tests\\n\\nOnce the project is built, you must ensure that the automated unit tests pass:\\n\\n    npm run test\\n\\n### Run the Automated [Cypress](https://www.cypress.io/) tests\\n\\nYou must also ensure that the automated integration tests pass:\\n\\n    npm run cypress:run\\n\\nYou can also use the Cypress GUI with:\\n\\n    npm run cypress:open\\n\\n\\n##### Configuration\\n\\n###### Cypress Secrets\\n\\nWhen adding new Cypress tests, you will need to modify the secrets.config.json file. You will need to edit the \"cypress\" object to include data from your local database:\\n\\n    \"cypress\": {\\n        \"user\": {\\n        \"id\": 1, // This should match the ID of your user in the \\'users\\' database table\\n        \"username\": \"your username here\" // Replace with the urs_id field of your user in the \\'users\\' database table\\n        }\\n    }\\n\\n### Deployment\\n\\nWhen the time comes to deploy the application, first ensure that you have the required ENV vars set:\\n\\n- AWS_ACCESS_KEY_ID\\n- AWS_SECRET_ACCESS_KEY\\n\\nThis application runs in a VPC for NASA security purposes, therefore the following values are expected when a deployment occurs:\\n\\n- VPC_ID\\n- SUBNET_ID_A\\n- SUBNET_ID_B\\n\\nFor production use, this application uses Scatter Swap to obfuscate some IDs -- the library does not require a value be provided but if you\\'d like to control it you can set the following ENV vars:\\n\\n- OBFUSCATION_SPIN\\n- OBFUSCATION_SPIN_SHAPEFILES\\n\\nTo deploy the full application use the following:\\n\\n\\tNODE_ENV=production serverless deploy --stage UNIQUE_STAGE\\n'},\n",
       " {'repo': 'nasa/daa-displays',\n",
       "  'language': 'TypeScript',\n",
       "  'readme_contents': '# DAA-Displays: A Toolkit for the Analysis of Detect-And-Avoid Functions in Cockpit Displays\\nDAA-Displays is a toolkit for model-based design and analysis of software functions in cockpit\\ndisplays. It includes a library of interactive graphical display elements (widgets)\\nfor cockpit systems, and simulations tools supporting comparative analysis of cockpit displays.\\n\\n![](src/images/danti.gif \"\")\\n\\n## Latest version\\ndaa-displays-1.0.10\\n\\n## Functionalities\\n- Library of interactive graphical display elements (called widgets) for cockpit systems: Compass, Interactive Map, Airspeed Tape, Altitude Tape, Vertical Speed, Virtual Horizon \\n- Simulation tools supporting comparative analysis of cockpit displays\\n\\n## Requirements\\nThe following software is necessary to compile and execute DAA-Displays\\n- NodeJS (v14.16.1 or greater) https://nodejs.org/en/download\\n- Java Open JDK (1.9 or greater) https://openjdk.java.net/install\\n- C++ compiler (gcc version 7.4.0 for Linux, Apple clang 11.0.0 for MacOS)\\n- Google Chrome (80.0.x or greater) https://www.google.com/chrome\\n\\n## Installation instructions\\n1. Download the latest release of DAA-Displays from the github repository.\\n2. Open a terminal window and change directory to the `daa-displays` folder.\\n3. Run `make` in the terminal window.\\n   This command will download the dependencies and create a folder `dist/` with the distribution.\\n\\n## Use instructions\\n1. Open a terminal in the `daa-displays` folder, and run the bash script `./restart.sh` in the terminal window. The script will launch the daa-server on port 8082. *(Please keep the terminal window open otherwise the execution of the server will be terminated.)*\\n2. Open Google Chrome at http://localhost:8082\\n3. Launch one of the DAA Apps (`DANTi`, `single-view`, etc.) by clicking on the corresponding icon.\\n4. Select a flight scenario, a configuration, and a DAA specification using the drop-down menus provided by the App. Click `Load Selected Scenario and Configuration` to initialize the simulation.\\n5. Click `Play` to simulate the DAA specification in the selected flight scenario. Use the other simulation controls to jump to specific time instants, change the simulation speed, and generate plot diagrams.\\n\\n## Tips for developers\\nThe `./restart.sh` script supports the following options:\\n- `-help`                (Shows the available options)\\n- `-pvsio`               (Enables the pvsio process; pvsio must be in the execution path; requires nasalib)\\n- `-pvsio <path>`        (Enables the pvsio process; the given pvsio path is used for executing the pvsio environment; requires PVS and NASALib)\\n- `-fast`                (Enables optimizations, including caching of simulation results)\\n- `-port <port number>`  (The server will use the given port)\\n\\n## Publications\\nPaolo Masci and César Muñoz, [A Graphical Toolkit for the Validation of Requirements for Detect and Avoid Systems](https://doi.org/10.1007/978-3-030-50995-8_9), Proceedings of the 14th International Conference on Tests and Proofs (TAP 2020), Lecture Notes in Computer Science, Vol. 12165, pp. 155-166, 2020 [[PDF](https://shemesh.larc.nasa.gov/fm/papers/TAP2020-MM.pdf)][[BibTex](https://shemesh.larc.nasa.gov/fm/papers/TAP2020-MM.bib)]\\n\\n## Use Cases\\nThe following examples illustrate concrete applications of the daa-displays toolkit for the analysis of DAA functions in cockpit displays.\\n\\n### **Example 1**: Demonstration of a DAA algorithm\\nThe rapid prototyping functionalities of the toolkit allow developers to create realistic simulations of cockpit displays suitable to demonstrate DAA specifications and implementations on concrete encounters.\\nThe following example is for the analysis of maneuver guidance provided by a DAA algorithm\\nto help pilots resolve route conflicts in a flight scenario. \\nManeuver guidance has the form of *bands*, i.e., ranges\\nof heading, horizontal speed, vertical speed, and altitude maneuvers that\\nare predicted to be conflict free (the prediction is based on a mathematical \\nformula that uses distance and time separation thresholds). Bands are color-coded: \\nyellow denotes a corrective maneuver, red denotes a warning maneuver, and green\\ndenotes a recovery maneuver.\\n\\nThe simulation shown in the figure below, illustrate a scenario where a traffic \\naircraft is overtaking the ownship.\\nBands on the flight display indicate maneuvers that can be performed to avoid\\nthe conflict. For example, yellow and red bands on the right side of the compass \\nindicate that the ownship should avoid right turns.\\n\\n![](screenshots/scenario_6_danti.gif \"\")\\n\\nTo reproduce the demonstration shown in the figure:\\n1. Launch the `DANTi` app (see *Use instructions* above).\\n2. Select flight scenario `scenario_6` and click `Load Selected Scenario and Configuration`.\\n3. Click `Play` to watch the behavior of the DAA specification for the selected flight scenario.\\n\\n### **Example 2**: Comparison of different DAA configurations\\nSplit-view simulations facilitate the comparative analysis of \\ntwo DAA implementations and formal specifications on the same encounter.\\nIn the following example, a newer version of a DAA reference implementation\\n(on the left-hand side of the split-view) is compared with an older version. \\nThe newer version introduces additional maneuver guidance in the form of \\nspeed/heading/altitude bugs rendered on the flight display. \\n\\n![](screenshots/s_1_turn_L_wind_0_50.gif \"\")\\n\\nTo reproduce the demonstration shown in the figure:\\n1. Launch the `split-view` app (see *Use instructions* above).\\n2. Select flight scenario `s_1_turn_L_wind_0_50`, then select `WellClear-2.0.f.jar` on the left player and `WellClear-1.0.1.jar` in the right player. Click `Load Selected Scenario and Configuration`.\\n3. Click `Play` to watch the behavior of the two versions in the same flight scenario.\\n\\n### **Example 3**: 3D Simulation\\n3D simulations move the focus of the analysis from a cockpit-centric \\nview to a scenario-centric view that includes the wider airspace \\naround the ownship. The viewport can be adjusted by tilting, \\npanning, and zooming the view.  This capability\\ncan be used to gain a better understanding of spatial information \\non the trajectories followed by the ownship and\\ntraffic aircraft in a given scenario. This is useful, e.g., when\\nassessing DAA algorithms with computer-generated flight scenarios, as\\nthis view provides a tangible idea of what the scenario is about.\\n\\nThe following example 3D simulation is used to examine \\nthe same flight scenario of Example 1, where a traffic aircraft overtakes the ownship.\\n\\n![](screenshots/scenario_6_3d.gif \"\")\\n\\nTo reproduce the demonstration shown in the figure:\\n1. Launch the `3D view` app (see *Use instructions* above).\\n2. Select flight scenario `scenario_6` and click `Load Selected Scenario and Configuration`.\\n3. Click `Play` to watch the behavior of the DAA specification for the selected flight scenario. Position the mouse in the view and use mouse wheel to zoom in/out, use the central mouse button to tilt/pan the view.\\n\\n\\n## Structure\\n```\\n.\\n├── src\\n│   ├── daa-displays                     // DAA-Displays widgets library\\n│   │     ├── daa-airspeed-tape.ts       // Airspeed Tape widget\\n│   │     ├── daa-altitude-tape.ts       // Altitude Tape widget\\n│   │     ├── daa-compass.ts             // Compass Display widget\\n│   │     ├── daa-interactive-map.ts     // Interactive Map widget\\n│   │     ├── daa-vertical-speed-tape.ts // Vertical Speed Tape widget\\n│   │     ├── daa-virtual-horizon.ts     // Virtual Horizon widget\\n│   │     ├── daa-player.ts              // Single-view player\\n│   │     ├── daa-split-view.ts          // Split-view player\\n│   │     └── daa-spectrogram.ts         // Spectrogram renderer\\n│   │\\n│   ├── daa-config                       // Well-Clear configuration files\\n│   ├── daa-logic                        // Well-Clear logic\\n│   ├── daa-output                       // Output files generated by simulation runs\\n│   ├── daa-scenarios                    // Scenario files for running simulation runs\\n│   ├── daa-server                       // Server-side component of DAA-Displays\\n│   │\\n│   ├── LICENSES                         // NASA Open Source License Agreement\\n│   └── index.html                       // Client entry-point\\n│\\n├── restart.sh                           // Script for launching the daa-server\\n├── Makefile                             // Compilation targets\\n└── package.json                         // Manifest file\\n\\n```\\n\\n\\n## Notices\\n### Copyright \\nCopyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n \\n### Disclaimers\\n**No Warranty**: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY\\n  WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,\\n  INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE\\n  WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF\\n  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\n  INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\n  FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO\\n  THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER,\\n  CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT\\n  OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY\\n  OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\n  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\n  REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\n  AND DISTRIBUTES IT \"AS IS.\"\\n \\n**Waiver and Indemnity**: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS\\n  AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND\\n  SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF\\n  THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,\\n  EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM\\n  PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT\\n  SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\n  STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\n  PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE\\n  REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL\\n  TERMINATION OF THIS AGREEMENT.\\n\\n\\n## Contacts\\n* Paolo Masci (NIA) (paolo.masci@nianet.org)\\n* Cesar Munoz (NASA LaRC) (cesar.a.munoz@nasa.gov)\\n'},\n",
       " {'repo': 'nasa/code-nasa-gov',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': '# CODE.NASA.GOV\\n\\n[![Build Status](https://travis-ci.org/nasa/code-nasa-gov.svg?branch=master)](https://travis-ci.org/nasa/code-nasa-gov)\\n\\nCatalog of Open Source Software from NASA. Built using [Polymer](https://www.polymer-project.org).\\n\\n## Do You have a Open-Source Code Project For This Site?\\n\\n#### Instructions\\nInstructions for releasing a NASA open-source project can be found on <a href=\"https://code.nasa.gov/#/guide\">https://code.nasa.gov/#/guide</a>.\\n\\n#### Code.json vs Category.json\\nNewly approved code projects for release are added to code.json. You can add your approved open-source NASA project to <a href=\"https://raw.githubusercontent.com/nasa/Open-Source-Catalog/master/code.json\"><b>code.json</b></a>, <a href=\"https://github.com/nasa/Open-Source-Catalog\">here</a>.\\n\\nAll federal agencies are mandated to have a code.json that is then harvested by the General Services Adminstration (GSA) and aggregated into code.gov. \\n\\nCode.json is reformatted by a script run by NASA\\'s open-innovation team into <a href=\"https://raw.githubusercontent.com/nasa/code-nasa-gov/master/data/catalog.json\">category.json</a>. Category.json has some attributes not in code.json and is used to build the project page on code.nasa.gov.\\n\\nAdditionally, at this time, only category.json has the A.I.-generated keyword tags in addition to the human-generated tags. This may change in the future. \\n\\n#### Why code.json is bigger than category.json\\nSome of the code projects in code.json have open-source licenses. Other projects in code.json have government-source only licenses, meaning sharing is constrainted to government agencies. All of the code projects listed in category.json have open-source licenses. \\n\\n### Making your own data visualization with the JSONs that drive code.nasa.gov:\\n- https://observablehq.com/@justingosses/finding-recent-additions-to-code-nasa-gov\\n- https://observablehq.com/@briantoliveira/untitled\\n\\nIf you make your own visualization, please add it as an issue. We would love to see it!\\n\\n## Running The Code In This Repository\\n\\n### Setup\\n\\ntest\\n\\n### Prerequisites\\n\\nInstall bower and [polymer-cli](https://github.com/Polymer/polymer-cli):\\n\\n    npm install -g bower polymer-cli\\n\\nCheck that you are using Node v8+\\n\\n    node -v\\n    \\n### Install dependencies\\n\\n    bower i\\n\\n### Start the development server\\n\\nThis command serves the app at `http://localhost:8080` and provides basic URL\\nrouting for the app:\\n\\n    polymer serve --open\\n\\n\\n### Build\\n\\nThis command performs HTML, CSS, and JS minification on the application\\ndependencies and generates a service-worker.js file with code to pre-cache the\\ndependencies based on the entrypoint and fragments specified in `polymer.json`.\\nThe minified files are output to the `build/unbundled` folder, and are suitable\\nfor serving from a HTTP/2+Push compatible server.\\n\\nIn addition the command also creates a fallback `build/bundled` folder,\\ngenerated using fragment bundling, suitable for serving from non\\nH2/push-compatible servers or to clients that do not support H2/Push.\\n\\n    polymer build\\n\\n### Preview the build\\n\\nThis command serves the minified version of the app at `http://localhost:8080`\\nin an unbundled state, as it would be served by a push-compatible server:\\n\\n    polymer serve build/unbundled\\n\\nThis command serves the minified version of the app at `http://localhost:8080`\\ngenerated using fragment bundling:\\n\\n    polymer serve build/bundled\\n    \\n### Deploying\\n\\nWhen deploying to a static web server (with no HTTP/2+Push), be sure to copy only\\nthe files from `build/bundled` directory (**NOT** the project directory) which\\ncontains a functional service worker and minified files. Put them in a top level part of the directory, not within another build/bundled directory within the production directory.\\n\\n### Adding a new view\\n\\nYou can extend the app by adding more views that will be demand-loaded\\ne.g. based on the route, or to progressively render non-critical sections\\nof the application.  Each new demand-loaded fragment should be added to the\\nlist of `fragments` in the included `polymer.json` file.  This will ensure\\nthose components and their dependencies are added to the list of pre-cached\\ncomponents (and will have bundles created in the fallback `bundled` build).\\n'},\n",
       " {'repo': 'nasa/cumulus-orca',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '## Clone and build Operational Recovery Cloud Archive (ORCA)\\n\\nClone the `dr-podaac-swot` repo from https://github.com/ghrcdaac/operational-recovery-cloud-archive\\n\\n```\\ngit clone https://github.com/ghrcdaac/operational-recovery-cloud-archive\\n```\\n## Build lambdas\\nBefore you can deploy this infrastructure, you must download the release zip or the build the lambda function source-code locally.\\n\\n`./bin/build_tasks.sh` will crawl the `tasks` directory and build a `.zip` file (currently by just `zipping` all python files and dependencies) in each of it\\'s sub-directories. That `.zip` is then referenced in the `modules/lambdas/main.tf` lamdba definitions.\\n\\n```\\n./bin/build_tasks.sh\\n```\\n\\n# ORCA Deployment\\n\\nThe ORCA deployment is done with [Terraform root module](https://www.terraform.io/docs/configuration/modules.html),\\n`main.tf`.\\n\\nThe following instructions will walk you through installing Terraform,\\nconfiguring Terraform, and deploying the root module.\\n\\n## Install Terraform\\n\\nIf you are using a Mac and [Homebrew](https://brew.sh), installing Terraform is\\nas simple as:\\n\\n```shell\\nbrew update\\nbrew install terraform\\n```\\n\\nFor other cases,\\nVisit the [Terraform documentation](https://learn.hashicorp.com/terraform/getting-started/install.html) for installation instructions.\\n\\nVerify that the version of Terraform installed is at least v0.12.0.\\n\\n```shell\\n$ terraform --version\\nTerraform v0.12.2\\n```\\n\\n## Configure the Terraform backend\\n\\nThe state of the Terraform deployment is stored in S3. In the following\\nexamples, it will be assumed that state is being stored in a bucket called\\n`dr-tf-state`. You can also use an existing bucket, if desired.\\n\\nCreate the state bucket:\\n\\n```shell\\naws s3api create-bucket --bucket dr-tf-state --create-bucket-configuration LocationConstraint=us-west-2\\n```\\n**Note:** The `--create-bucket-configuration` line is only necessary if you are creating your bucket outside of `us-east-1`.\\n\\nIn order to help prevent loss of state information, it is recommended that\\nversioning be enabled on the state bucket:\\n\\n```shell\\n$ aws s3api put-bucket-versioning \\\\\\n    --bucket dr-tf-state \\\\\\n    --versioning-configuration Status=Enabled\\n```\\n\\nTerraform uses a lock stored in DynamoDB in order to prevent multiple\\nsimultaneous updates. In the following examples, that table will be called\\n`dr-tf-locks`.\\n\\nCreate the locks table:\\n\\n⚠️ **Note:** The `--billing-mode` option was recently added to the AWS CLI. You\\nmay need to upgrade your version of the AWS CLI if you get an error about\\nprovisioned throughput when creating the table.\\n\\n```shell\\n$ aws dynamodb create-table \\\\\\n    --table-name dr-tf-locks \\\\\\n    --attribute-definitions AttributeName=LockID,AttributeType=S \\\\\\n    --key-schema AttributeName=LockID,KeyType=HASH \\\\\\n    --billing-mode PAY_PER_REQUEST\\n```\\n\\n## Configure and deploy the `main` root module\\n\\nThese steps should be executed in the root directory of the repo.\\n\\nCreate a `terraform.tf` file, substituting the appropriate values for `bucket`\\nand `dynamodb_table`. This tells Terraform where to store its\\nremote state.\\n\\n**terraform.tf**\\n\\n```\\nterraform {\\n  backend \"s3\" {\\n    region         = \"us-west-2\"\\n    bucket         = \"dr-tf-state\"\\n    key            = \"terraform.tfstate\"\\n    dynamodb_table = \"dr-tf-locks\"\\n  }\\n}\\n```\\n\\n## Variables\\nFirst, run a `mv terraform.tfvars.example terraform.tfvars` to get a template `terraform.tfvars` in your working directory. This is where you will place input variables to Terraform.\\n\\n**Necessary:**\\n* `ngap_subnets` - NGAP Subnets (array)\\n* `vpc_id` - ID of VPC to place resources in - recommended that this be a private VPC (or at least one with restricted access).\\n* `glacier_bucket` - Bucket with Glacier policy\\n* `public_bucket` - Bucket with public permissions (Cumulus public bucket)\\n* `private_bucket` - Bucket with private permissions (Cumulus private bucket)\\n* `internal_bucket` - Analogous to the Cumulus internal bucket \\n* `protected_bucket` - Analogous to the Cumulus protected bucket\\n* `permissions_boundary_arn` - Permission Boundary Arn (Policy) for NGAP compliance\\n* `postgres_user_pw` - password for the postgres user\\n* `database_name` - disaster_recovery\\n* `database_app_user` - druser \\n* `database_app_user_pw` - the password for the application user\\n\\n**Optional:**\\n* `prefix` - Prefix that will be pre-pended to resource names created by terraform. \\n  Defaults to `dr`.\\n* `profile` - AWS CLI Profile (configured via `aws configure`) to use. \\n  Defaults to `default`.\\n* `region` - Your AWS region. \\n  Defaults to `us-west-2`.\\n* `restore_expire_days` - How many days to restore a file for. \\n  Defaults to 5.\\n* `restore_request_retries` - How many times to retry a restore request to Glacier. \\n  Defaults to 3.\\n* `restore_retry_sleep_secs` - How many seconds to wait between retry calls to `restore_object`. \\n  Defaults to 3.\\n* `restore_retrieval_type` -  the Tier for the restore request. Valid values are \\'Standard\\'|\\'Bulk\\'|\\'Expedited\\'. \\n  Defaults to `Standard`. Understand the costs associated with the tiers before modifying.\\n* `copy_retries` - How many times to retry a copy request from the restore location to the archive location. \\n  Defaults to 3.\\n* `copy_retry_sleep_secs` - How many seconds to wait between retry calls to `copy_object`. \\n  Defaults to 0.\\n* `ddl_dir` - the location of the ddl dir that contains the sql to create the application database. \\n  Defaults to \\'ddl/\\'.\\n* `drop_database` - Whether or not to drop the database if it exists (True), or keep it (False). \\n  Defaults to False.\\n* `database_port` - the port for the postgres database. \\n  Defaults to \\'5432\\'.\\n* `platform` - indicates if running locally (onprem) or in AWS (AWS). \\n  Defaults to \\'AWS\\'.\\n\\n## Deploying with Terraform\\nRun `terraform init`.\\nRun `terraform plan` #optional, but allows you to preview the deploy.\\nRun `terraform apply`.\\n\\nThis will deploy ORCA.\\n\\n## Delete Terraform stack\\nIf you want to remove it:\\n```\\nterraform destroy\\n```\\n\\n## Integrating with Cumulus\\nIntegrate ORCA with Cumulus to be able to recover a granule from the Cumulus Dashboard.\\n\\n### Define the ORCA workflow (Cumulus < v1.15)\\n\\nCopy the workflow from `workflows/workflows.yml.dr` into your Cumulus workflow.\\n\\nSet the values of these environment variables to the ARN for the \\n{prefix}-extract-filepaths-for-granule and {prefix}-request-files lambdas,\\nrespectively:\\n```\\nDR_EXTRACT_LAMBDA_ARN=arn:aws:lambda:us-west-2:012345678912:function:dr_extract_filepaths_for_granule\\n\\nDR_REQUEST_LAMBDA_ARN=arn:aws:lambda:us-west-2:012345678912:function:dr_request_files\\n```\\n\\nAdd an `aws` provider to `main.tf`:\\n\\n```\\nprovider \"aws\" {\\n  version = \"~> 2.13\"\\n  region  = var.region\\n  profile = var.profile\\n}\\n```\\n\\n### Integrating ORCA With Cumulus >= v1.15\\n\\n#### Adding a ORCA module to the Cumulus deployment\\n\\nNavigate to `cumulus-tf/main.tf` within your Cumulus deployment directory and add the following module:\\n```\\nmodule \"orca\" {\\n  source = \"https://github.com/ghrcdaac/operational-recovery-cloud-archive/releases/download/1.0.2/orca-1.0.2.zip\"\\n\\n  prefix = var.prefix\\n  subnet_ids = module.ngap.ngap_subnets_ids\\n  database_port = \"5432\"\\n  database_user_pw = var.database_user_pw\\n  database_name = var.database_name\\n  database_app_user = var.database_app_user\\n  database_app_user_pw = var.database_app_user_pw\\n  ddl_dir = \"ddl/\"\\n  drop_database = \"False\"\\n  platform = \"AWS\"\\n  lambda_timeout = 300\\n  restore_complete_filter_prefix = \"\"\\n  vpc_id = module.ngap.ngap_vpc.id\\n  copy_retry_sleep_secs = 2\\n  permissions_boundary_arn = var.permissions_boundary_arn\\n  buckets = var.buckets\\n  workflow_config = module.cumulus.workflow_config\\n  region = var.region\\n}\\n```\\n\\n*Note*: This above snippet assumes that you\\'ve configured your Cumulus deployment. More information on that process can be found in their [documentation](https://nasa.github.io/cumulus/docs/deployment/deployment-readme#configure-and-deploy-the-cumulus-tf-root-module)\\n\\n#### Add necessary variables (unique to ORCA) to the Cumulus TF configuration\\n\\nTo support this module, you\\'ll have to add the following values to your `cumulus-tf/variables.tf` file:\\n```\\n# Variables specific to ORCA\\nvariable \"database_user_pw\" {\\n  type = string\\n}\\n\\nvariable \"database_name\" {\\n  type = string\\n}\\n\\nvariable \"database_app_user\" {\\n  type = string\\n}\\n\\nvariable \"database_app_user_pw\" {\\n  type = string\\n}\\n```\\n\\nThe values corresponding to these variables must be set in your `cumulus-tf/terraform.tfvars` file, but note that many of these variables are actually hardcoded at the time of updating this README\\n\\n#### Adding the Copy To Glacier Step to the Ingest Workflow\\nNavigate to `cumulus-tf/ingest_granule_workflow.tf` then add the following step after the PostToCMR step being sure to change the PostToCMR\\'s \"Next\" paramter equal to \"CopyToGlacier\"\\n```\\n\"CopyToGlacier\":{\\n         \"Parameters\":{\\n            \"cma\":{\\n               \"event.$\":\"$\",\\n               \"task_config\":{\\n                  \"bucket\":\"{$.meta.buckets.internal.name}\",\\n                  \"buckets\":\"{$.meta.buckets}\",\\n                  \"distribution_endpoint\":\"{$.meta.distribution_endpoint}\",\\n                  \"files_config\":\"{$.meta.collection.files}\",\\n                  \"fileStagingDir\":\"{$.meta.collection.url_path}\",\\n                  \"granuleIdExtraction\":\"{$.meta.collection.granuleIdExtraction}\",\\n                  \"collection\":\"{$.meta.collection}\",\\n                  \"cumulus_message\":{\\n                     \"input\":\"{[$.payload.granules[*].files[*].filename]}\",\\n                     \"outputs\":[\\n                        {\\n                           \"source\":\"{$}\",\\n                           \"destination\":\"{$.payload}\"\\n                        }\\n                     ]\\n                  }\\n               }\\n            }\\n         },\\n         \"Type\":\"Task\",\\n         \"Resource\":\"${module.orca.copy_to_glacier_lambda_arn}\",\\n         \"Catch\":[\\n            {\\n               \"ErrorEquals\":[\\n                  \"States.ALL\"\\n               ],\\n               \"ResultPath\":\"$.exception\",\\n               \"Next\":\"WorkflowFailed\"\\n            }\\n         ],\\n         \"Retry\":[\\n            {\\n               \"ErrorEquals\":[\\n                  \"States.ALL\"\\n               ],\\n               \"IntervalSeconds\":2,\\n               \"MaxAttempts\":3\\n            }\\n         ],\\n         \"Next\":\"WorkflowSucceeded\"\\n      },\\n```\\n\\n### Collection configuration\\nTo configure a collection to enable ORCA, add the line\\n`\"granuleRecoveryWorkflow\": \"DrRecoveryWorkflow\"` to the collection configuration:\\n```\\n{\\n  \"queriedAt\": \"2019-11-07T22:49:46.842Z\",\\n  \"name\": \"L0A_HR_RAW\",\\n  \"version\": \"1\",\\n  \"sampleFileName\": \"L0A_HR_RAW_product_0001-of-0420.h5\",\\n  \"dataType\": \"L0A_HR_RAW\",\\n  \"granuleIdExtraction\": \"^(.*)((\\\\\\\\.cmr\\\\\\\\.json)|(\\\\\\\\.iso\\\\\\\\.xml)|(\\\\\\\\.tar\\\\\\\\.gz)|(\\\\\\\\.h5)|(\\\\\\\\.h5\\\\\\\\.mp))$\",\\n  \"reportToEms\": true,\\n  \"createdAt\": 1561749178492,\\n  \"granuleId\": \"^.*$\",\\n  \"provider_path\": \"L0A_HR_RAW/\",\\n  \"meta\": {\\n    \"response-endpoint\": \"arn:aws:sns:us-west-2:012345678912:providerResponseSNS\",\\n    \"granuleRecoveryWorkflow\": \"DrRecoveryWorkflow\"\\n  },\\n  \"files\": [\\n    {\\n```\\n### Enable `Recover Granule` button\\n\\nTo enable the `Recover Granule` button on the Cumulus Dashboard (available at github.com/nasa/cumulus-dashboard), \\nset the environment variable `ENABLE_RECOVERY=true`.\\n\\nHere is an example command to run the Cumulus Dashboard locally:\\n```\\n  APIROOT=https://uttm5y1jcj.execute-api.us-west-2.amazonaws.com:8000/dev ENABLE_RECOVERY=true npm run serve\\n```\\n\\n## Release Documentation:\\n\\nInformation about how to create an ORCA release can be found [here](docs/release.md).\\n\\n\\n## ORCA Static Documentation\\n\\nNake sure you are using the following node.js versions to view the documentation.\\n- npm 6.14.10\\n- node 12.15.0\\n\\nORCA documentation can be read locally by performing the following:\\n```\\ncd website\\nnpm install\\nnpm run start\\n```\\n\\nOnce the server is running, documentation should be available on `http://localhost:3000`.\\n'},\n",
       " {'repo': 'nasa/dorado-scheduling',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Dorado observation planning and scheduling simulations\\n\\n[![Build Status](https://github.com/nasa/dorado-scheduling/actions/workflows/build-and-test.yml/badge.svg)](https://github.com/nasa/dorado-scheduling/actions)\\n[![Documentation Status](https://readthedocs.org/projects/dorado-scheduling/badge/?version=latest)](https://dorado-scheduling.readthedocs.io/en/latest/?badge=latest)\\n[![Codecov](https://img.shields.io/codecov/c/github/nasa/dorado-scheduling)](https://app.codecov.io/gh/nasa/dorado-scheduling)\\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/dorado-scheduling)](https://pypi.org/project/dorado-scheduling/)\\n\\nDorado is a proposed space mission for ultraviolet follow-up of gravitational\\nwave events. This repository contains a simple target of opportunity\\nobservation planner for Dorado.\\n\\nThis project is free and open source, but it calls commercial software: it uses\\n[IBM ILOG CPLEX Optimization Studio (\"CPLEX\")][CPLEX] for mathematical\\noptimization. CPLEX is free for students, faculty, and staff at accredited\\neducational institutions through the [IBM Academic Initiative].\\n\\n**To get started with dorado-scheduling, see the [quick start instructions] in\\nthe [manual].**\\n\\n![Example Dorado observing plan](examples/6.gif)\\n\\n## Features\\n\\n*   **Global**: jointly and globally solves the problems of tiling (the set of\\n    telescope boresight orientations and roll angles) and the scheduling (which\\n    tile is observed at what time), rather than solving each sub-problem one at\\n    a time\\n*   **Optimal**: generally solves all the way to optimality, rather than\\n    finding merely a \"good enough\" solution\\n*   **Fast**: solve an entire orbit in about 5 minutes\\n*   **General**: does not depend on heuristics of any kind\\n*   **Flexible**: problem is formulated in the versatile framework of\\n    [mixed integer programming]\\n\\n## Dependencies\\n\\n*   [Astropy]\\n*   [Astroplan] for calculating the field of regard\\n*   [HEALPix], [cdshealpix], and [astropy-healpix] for observation footprints\\n*   [sgp4] for orbit propagation\\n*   [CPLEX] (via [docplex] Python interface) for constrained optimization\\n\\n[quick start instructions]: https://dorado-scheduling.readthedocs.io/en/latest/quickstart.html\\n[manual]: https://dorado-scheduling.readthedocs.io/\\n[mixed integer programming]: https://en.wikipedia.org/wiki/Integer_programming\\n[Astropy]: https://www.astropy.org\\n[Astroplan]: https://github.com/astropy/astroplan\\n[HEALPix]: https://healpix.jpl.nasa.gov\\n[astropy-healpix]: https://github.com/astropy/astropy-healpix\\n[cdshealpix]: https://github.com/cds-astro/cds-healpix-python\\n[sgp4]: https://pypi.org/project/sgp4/\\n[CPLEX]: https://www.ibm.com/products/ilog-cplex-optimization-studio\\n[IBM Academic Initiative]: https://www.ibm.com/academic/technology/data-science\\n'},\n",
       " {'repo': 'nasa/cFS',\n",
       "  'language': 'CMake',\n",
       "  'readme_contents': '[![Build Status](https://travis-ci.com/nasa/cFS.svg)](https://travis-ci.com/nasa/cFS)\\n[![LGTM Alerts](https://img.shields.io/lgtm/alerts/github/nasa/cFS)](https://lgtm.com/projects/g/nasa/cFS/alerts/?mode=list)\\n[![LGTM Grade](https://img.shields.io/lgtm/grade/python/github/nasa/cFS)](https://lgtm.com/projects/g/nasa/cFS/alerts/?mode=list)\\n[![LGTM Grade](https://img.shields.io/lgtm/grade/cpp/github/nasa/cFS)](https://lgtm.com/projects/g/nasa/cFS/alerts/?mode=list)\\n\\n# Core Flight System - BUNDLE\\n\\nThe Core Flight System (cFS) is a generic flight software architecture framework used on flagship spacecraft, human spacecraft, cubesats, and Raspberry Pi.  This repository is a bundle of submodules that make up the cFS framework.  Note the \"lab\" apps are intended as examples only, and enable this bundle to build, execute, receive commands, and send telemetry.  This is not a flight distribution, which is typically made up of the cFE, OSAL, PSP, and a selection of flight apps that correspond to specific mission requirements.\\n\\nThis bundle has not been fully verified as an operational system, and is provided as a starting point vs an end product.  Testing of this bundle consists of building, executing, sending setup commands and verifying receipt of telemetry.  Unit testing is also run, but extensive analysis is not performed.  All verification and validation per mission requirements is the responsibility of the mission (although attempts are made in the cFS Framework to provide a testing framework to facilitate the process).\\n\\nThe cFS Framework is a core subset of cFS.  There are additional OSALs, PSPs, and tools as listed below available from a variety of sources.\\n\\n## References Documentation\\n  - cFE User\\'s Guide: https://github.com/nasa/cFS/blob/gh-pages/cFE_Users_Guide.pdf\\n  - OSAL User\\'s Guide: https://github.com/nasa/cFS/blob/gh-pages/OSAL_Users_Guide.pdf\\n  - cFE App Developer\\'s Guide: https://github.com/nasa/cFE/blob/main/docs/cFE%20Application%20Developers%20Guide.md\\n  - Training documentation: https://ntrs.nasa.gov/citations/20205000691\\n  - cFS Overview: https://cfs.gsfc.nasa.gov/cFS-OviewBGSlideDeck-ExportControl-Final.pdf\\n\\n## Release Notes\\n\\nSee [releases](https://github.com/nasa/cFS/releases) for release history and associated artifacts related to the cFS BUNDLE.\\n\\n**Aquila: OFFICIAL RELEASE**:\\n  - Released under Apache 2.0\\n  - Includes cFE 6.7.0 (cFE, PSP, framework apps, and framework tools as marked) and OSAL 5.0.0\\n\\n**cFS 6.6.0a Suite: OFFICIAL RELEASE**:\\n  - cFE 6.6.0a is released under Apache 2.0 license, see [LICENSE](https://github.com/nasa/cFE/blob/v6.6.0a/LICENSE-18128-Apache-2_0.pdf)\\n  - OSAL 4.2.1a is released under the NOSA license, see [LICENSE](https://github.com/nasa/osal/blob/osal-4.2.1a/LICENSE)\\n  - [Release notes](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_release_notes.md)\\n  - [Version description document](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_6_6_0_version_description.md)\\n  - [Test results](https://github.com/nasa/cFE/tree/v6.6.0a/test-and-ground/test-review-packages/Results)\\n\\nOther elements listed below are released under a variety of licenses as detailed in their respective repositories.\\n\\n## Known issues\\n\\nHistorical version description documents contain references to internal repositories and sourceforge, which is no longer in use.  Not all markdown documents have been updated for GitHub.\\n\\nSee related repositories for current open issues.\\n\\n## Major future work\\n\\n### Caelum (aka 7.0) Major release development plans (Targeting end of 2020 for release candidate)\\n\\n  - Certification framework with automated build verification tests of framework requirements\\n    - Executable on real/emulated/simulated/ or dockerized targets\\n    - Add PSP coverage testing framework (nasa/psp#184, nasa/psp#174)\\n    - Add PSP and cFE functional testing framework for APIs  (nasa/cfe#779)\\n    - Scrub OSAL coverage and functional tests\\n    - Scrub cFE coverage tests\\n    - Add cFE API functional tests\\n    - NOTE: Command verification pending tool open source release\\n  - Documentation (updated traceability, APIs/ICDs, general update)\\n  - Framework for mission customization of core services\\n    - Header customization support (nasa/cFE#726)\\n  - Remove deprecated code\\n  - Cmd/Tlm structure scrub for alignment/padding/consistency\\n  - Library query and reporting and ES resource management (nasa/cFE#28, nasa/cFE#797)\\n\\n### Other (may not make 7.0)\\n  - Open source automated build verification execution framework for emulated targets (likely docker based)\\n  - Deployment quality of life improvements (configuration, transition to CMake source selection vs compiler directives)\\n  - Update OS support (VxWorks 7, RTEMS 5)\\n  - Time services refactor\\n  - Symmetric multi-processing APIs\\n  - Electronic Data Sheet integration option and improvements to packet layouts for portability/consistency\\n  - Toolchain updates\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests to this repo.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n\\nCommunity email list subscription request: https://lists.nasa.gov/mailman/listinfo/cfs-community\\n\\n## Setup\\n\\nEnsure the following software are installed: Make, CMake, GCC, and Git.  To setup the cFS BUNDLE directly from the latest set of interoperable repositories:\\n\\n    git clone https://github.com/nasa/cFS.git\\n    cd cFS\\n    git submodule init\\n    git submodule update\\n\\nCopy in the default makefile and definitions:\\n\\n    cp cfe/cmake/Makefile.sample Makefile\\n    cp -r cfe/cmake/sample_defs sample_defs\\n\\n## Build and Run\\n\\nThe cFS Framework including sample applications will build and run on the pc-linux platform support package (should run on most Linux distributions), via the steps described in https://github.com/nasa/cFE/tree/master/cmake/README.md.  Quick-start is below:\\n\\nTo prep, compile, and run on the host (from cFS directory above) as a normal user (best effort message queue depth and task priorities):\\n\\n    make SIMULATION=native prep\\n    make\\n    make install\\n    cd build/exe/cpu1/\\n    ./core-cpu1\\n\\nShould see startup messages, and CFE_ES_Main entering OPERATIONAL state.  Note the code must be executed from the build/exe/cpu1 directory to find the startup script and shared objects.\\n\\nNote: The steps above are for a debug, permissive mode build and includes deprecated elements.  For a release build, recommendation is `make BUILDTYPE=release OMIT_DEPRECATED=true prep`.  Unit tests can be added with `ENABLE_UNIT_TESTS=true`, run with `make test`, and coverage reported with `make lcov`.\\n\\n## Send commands, receive telemetry\\n\\nThe cFS-GroundSystem tool can be used to send commands and receive telemetry.  For details on using and setting up the Ground System, see the [Guide-GroundSystem](https://github.com/nasa/cFS-GroundSystem/blob/main/Guide-GroundSystem.md).  Note it depends on PyQt5 and PyZMQ:\\n\\n1. Install PyQt5 and PyZMQ on your system.  Some systems may also require installing libcanberra-gtk-module.\\n       \\n2. Compile cmdUtil and start the ground system executable\\n\\n       cd tools/cFS-GroundSystem/Subsystems/cmdUtil\\n       make\\n       cd ../..\\n       python3 GroundSystem.py\\n\\n3. Select \"Start Command System\"\\n4. Select \"Enable Tlm\"\\n5. Enter IP address of system executing cFS, 127.0.0.1 if running locally\\n\\nShould see telemetry, can send noops and see command counters increment.\\n\\n## Compatible list of cFS apps\\n\\nThe following applications have been tested against this release:\\n  - TBD\\n\\n## Other cFS related elements/tools/apps/distributions\\n\\nThe following list is user submitted, and not CCB controlled.  They are released by various organizations, under various licenses.\\n\\n  - Distributions\\n    - cFS-101: Virtual machine distribution at https://github.com/nasa/CFS-101\\n    - OpenSatKit: Open source kit for satellite software at https://github.com/OpenSatKit/OpenSatKit\\n  - Other Ground station software\\n    - TBD\\n  - Other Apps\\n    - CS: Checksum application at https://github.com/nasa/CS\\n    - CF: CFDP application at https://github.com/nasa/CF\\n    - CI: Command Ingest application at https://github.com/nasa/CFS_CI\\n    - DS: Data Store application at https://github.com/nasa/DS\\n    - FM: File Manager application at https://github.com/nasa/FM\\n    - HK: Housekeeping application at https://github.com/nasa/HK\\n    - HS: Health and Safety application at https://github.com/nasa/HS\\n    - LC: Limit Checker application at https://github.com/nasa/LC\\n    - MD: Memory Dwell application at https://github.com/nasa/MD\\n    - MM: Memory Manager application at https://github.com/nasa/MM\\n    - SC: Stored Commands application at https://github.com/nasa/SC\\n    - SCA: Stored Command Absolute application at https://github.com/nasa/SCA\\n    - SCH: Scheduler application at https://github.com/nasa/SCH\\n    - TO: Telemetry Output application at https://github.com/nasa/CFS_TO\\n    - Skeleton App: A bare-bones application to which you can add your business logic at https://github.com/nasa/skeleton_app \\n  - Other Interfaces\\n    - SIL: Simulink Interface Layer at https://github.com/nasa/SIL\\n    - ECI: External Code Interface at https://github.com/nasa/ECI\\n  - Other Libraries\\n    - cFS_IO_LIB: IO library at https://github.com/nasa/CFS_IO_LIB\\n    - cFS_LIB: at https://github.com/nasa/cfs_lib\\n  - Other Tools\\n    - CCDD: Command and Data Dictionary Tool at https://github.com/nasa/CCDD\\n    - Perfutils-java: Java based performance analyzer for cFS at https://github.com/nasa/perfutils-java\\n    - gen_sch_tbl: Tool to generated SCH app tables\\n  - Other OSALs\\n    - TBD\\n  - Other PSPs\\n    - TBD\\n  \\n'},\n",
       " {'repo': 'nasa/ow_simulator',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '\\n# Ocean Worlds Autonomy Testbed for Exploration Research & Simulation (OceanWATERS)\\n[Overview](#overview) |\\n[Code Organization](#code-Organization) |\\n[Getting Started](#getting-started) |\\n[Contributing](#contributing) |\\n[License](#license)\\n\\n## Overview\\nOceanWATERS is a physical and visual simulation of a lander on Europa. It is intended as a\\ntestbed to aid in producing software that could fly on lander missions to ocean\\nworlds, such as Europa and Enceladus.\\n\\n\\n<p align=\"center\"><img width=\"80%\" src=\"oceanwaters/doc/lander_europa.jpg\" /></p>\\n\\n\\n## Code Organization\\n\\nThe [ow_simulator](https://github.com/nasa/ow_simulator) is the top level repository\\nfor OceanWATERS. It primarily contains ROS/Gazebo packages related to visual and\\nphysical simulation for OceanWATERS. It also contains workspace files for\\nsetting up the rest of the OceanWATERS repositories:\\n- [ow_autonomy](https://github.com/nasa/ow_autonomy)\\n- [ow_europa](https://github.com/nasa/ow_europa)\\n- [irg_open](https://github.com/nasa/irg_open)\\n\\n## Getting Started\\n- [Install software prerequisites](oceanwaters/doc/setup_dev_env.md)\\n- [Download and build OceanWATERS](oceanwaters/doc/setup_oceanwaters.md)\\n- [User Guide](https://github.com/nasa/ow_simulator/wiki/Using-OceanWATERS)\\n\\n## Contributing\\nPlease review [current bugs and features requests](https://github.com/nasa/ow_simulator/issues)\\nbefore submitting a new one. If we are unable to accomodate your request and you\\nwant to contribute to this project yourself, follow these instructions:\\n\\nContributions must be the original work of the contributor with no conflicting\\nlicense or copyright restrictions. See our [license](LICENSE.txt) for more\\ndetails.\\n\\nIf you wish to contribute code or a bug fix please:\\n- Create your own fork of this repository. In the upper-right corner of the\\n[ow_simulator front page](https://github.com/nasa/ow_simulator) click `Fork`.\\nYour fork will be called `<your_username>/ow_simulator`.\\n- In your newly forked repository, create a branch with an appropriate name for\\nyour feature or bug fix.\\n- Make changes to your new branch.\\n- Create a pull request against the `master` branch of `nasa/ow_simulator`.\\n\\n## Citation\\nComing soon\\n\\n## License\\nOceanWATERS is open source software licensed under the\\n[NASA Open Source Agreement version 1.3](LICENSE.txt).\\n\\n## Notices\\nCopyright © 2020 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration.  All Rights Reserved.\\n\\n## Disclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/NASA-Acronyms',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'This expands over 25,000 NASA acronyms.\\n\\nSearch engine: https://nasaacronyms.com or https://nasaacronyms.com?[insert acronym]\\n\\nChrome: https://chrome.google.com/webstore/detail/nasa-acronyms/anpbkdhjbebjjkgdbglbcfaenjldbinf\\n\\nFirefox: https://addons.mozilla.org/addon/nasa-acronyms\\n\\nAuthors: Joel Malissa & Logan Stafman\\n'},\n",
       " {'repo': 'nasa/vscode-pvs',\n",
       "  'language': 'TypeScript',\n",
       "  'readme_contents': '# VSCode-PVS: An Integrated Development Environment for the Prototype Verification System\\n\\n[![version](https://vsmarketplacebadge.apphb.com/version/paolomasci.vscode-pvs.svg)](https://marketplace.visualstudio.com/items?itemName=paolomasci.vscode-pvs)\\n[![installs](https://vsmarketplacebadge.apphb.com/installs-short/paolomasci.vscode-pvs.svg)](https://marketplace.visualstudio.com/items?itemName=paolomasci.vscode-pvs)\\n[![license](https://img.shields.io/badge/license-NASA-blue.svg)](https://opensource.org/licenses/NASA-1.3)\\n[![chat](https://img.shields.io/badge/Chat%20on-PVS%20Google%20Group-blue.svg)](https://groups.google.com/g/pvs-group)\\n\\nVSCode-PVS is a new integrated development environment for creating, evaluating and verifying PVS specifications.\\nThe environment redefines the way developers interact with PVS, and better aligns the PVS front-end to the functionalities provided by development environments used for programming languages such as C++ and Java.\\n\\n<img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-screenshot.png\" width=\"800\">\\n\\n## Latest version\\n[vscode-pvs-1.0.47](releases/vscode-pvs-1.0.47.vsix)\\n\\n\\n## Documentation\\n- [Quick reference guide](https://github.com/nasa/vscode-pvs/blob/master/vscode-pvs/docs/USER-INTERFACE.md) for the VSCode-PVS User Interface\\n- [FAQs](https://github.com/nasa/vscode-pvs/blob/master/vscode-pvs/docs/FAQ.md) on VSCode-PVS, including troubleshooting techniques for basic problems.\\n- Are you new to PVS? Try out our [tutorial](https://github.com/nasa/vscode-pvs/blob/master/vscode-pvs/docs/TUTORIAL.md)!\\n- Join the new [PVS group on Google](https://groups.google.com/g/pvs-group)\\n\\n\\n## Requirements\\n- Linux or MacOS operating system\\n- NodeJS (v12.16.1 or greater) https://nodejs.org/en/download\\n- Visual Studio Code (v1.49.0 or greater) https://code.visualstudio.com\\n\\n## Installation instructions\\nVSCode-PVS can be installed from the Visual Studio Code Marketplace or from GitHub.\\n\\n\\n**Automatic installation from Visual Studio Code Marketplace**\\n- Search `pvs` in https://marketplace.visualstudio.com and select `install`\\n<br><br><img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/how-to-install-vscode-pvs-from-marketplace.gif\" width=\"600\">\\n\\n**Manual installation from GitHub**\\n1. Download the .vsix file of VSCode-PVS from [github](https://github.com/nasa/vscode-pvs/raw/master/releases).\\n2. Click on the Extensions icon in the Activity Bar \\n3. Click on the `...` menu in the title bar, and use `Install from VSIX` to select the downloaded .vsix file\\n<br><br><img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/how-to-install-vscode-pvs.gif\" width=\"600\">\\n\\n>Note: When installing VSCode-PVS for the first time, it will check if PVS Allegro v7.1.0 is present on your system. If not, VSCode-PVS will show a dialog that allows you to download PVS.\\n\\n>Note: If you have used earlier releases of VSCode-PVS (`< 1.0.26`), you will need to download and install the latest version of PVS and NASALib. You can do this in VSCode-PVS, with the commands `M-x install-pvs` and `M-x install-nasalib`. \\n\\n**Manual installation of PVS**\\n\\nShould VSCode-PVS fail to install PVS, please install PVS manually: Download PVS Allegro v7.1.0 for [MacOS](https://pvs.csl.sri.com/license.html?tgzfile=pvs7.1.0-ix86_64-Linux-allegro.tgz) or [Linux](https://pvs.csl.sri.com/license.html?tgzfile=pvs-6.0-ix86_64-Linux-allegro.tgz) and follow the installation instructions reported in the `INSTALL` file included in the downloaded package. Once PVS is installed on your machine, you can link up PVS and VSCode-PVS by indicating the location of the PVS executables in the VSCode-PVS settings.\\n\\n\\n## Updating VSCode-PVS\\nVSCode-PVS will be automatically updated every time we publish a new release in the marketplace.\\n\\nAll settings and preferences will be maintained when installing a new release.\\n\\nIf you would like to perform manual updates or try out nightly builds, you can download the .vsix file from [github](https://github.com/nasa/vscode-pvs/raw/master/releases).\\n\\n\\n## Publications\\nPaolo Masci and César Muñoz, [An Integrated Development Environment for the Prototype Verification System](https://dx.doi.org/10.4204/EPTCS.310.5), Electronic Proceedings in Theoretical Computer Science (EPTCS), Vol. 310, pp. 35-49, 2019 [[PDF](https://arxiv.org/pdf/1912.10632v1)]\\n\\n\\n## Functionalities\\nThe main functionalities provided by the environment are as follows:\\n- **Syntax highlighting**: PVS keywords and library functions are automatically highlighted.\\n- **Autocompletion and code snippets**: Tooltips suggesting function names and language keywords are automatically presented in the editor when placing the mouse over a symbol name. Code snippets are provided for frequent modeling blocks, e.g., if-then-else. \\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-autocompletion.gif\" width=\"600\">\\n\\n- **Hover information for symbol definitions**: Hover boxes providing information about identifiers are automatically displayed when the user places the cursor over an identifier.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-hover.gif\" width=\"600\">\\n\\n- **Go-to definition**: Click on the name of the identifier while holding down the Ctrl key to jump to the location where the identifier is declared.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-goto-definition.gif\" width=\"600\">\\n\\n- **Live diagnostics**: Parsing is automatically performed in the background, and errors are reported in-line in the editor. Problematic expressions are underlined with red wavy lines. Tooltips presenting the error details are shown when the user places the cursor over the wavy lines.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-live-diagnostics.gif\" width=\"600\">\\n\\n- **Workspace Explorer**: Interactive tree view showing all theories in the current workspace, name and status of theorems and typecheck conditions.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-theory-explorer.gif\" width=\"600\">\\n\\n- **Proof Explorer + Prover Console**: Interactive tree view for viewing and editing the current proof. A prover console allows interaction with the theorem prover. Auto-completion is provided (using the TAB key) for prover commands, as well as access to the commands history.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-proof-explorer.gif\" width=\"600\">\\n\\n- **Plot Executable Functions**: Executable functions that return a list of real numbers can be rendered in a plot diagram.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-plot-expression.gif\" width=\"600\">\\n\\n- **Search NASALib**: Search definitions and theorems in [NASALib](https://github.com/nasa/pvslib), an extensive PVS library developed and maintained by the NASA Langley Formal Methods Team.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-search-nasalib.gif\" width=\"600\">\\n\\n- **Prototype Builder**: Build interactive visual prototypes to demonstrate the behavior of executable PVS specifications.\\n<br><br> <img src=\"https://github.com/nasa/vscode-pvs/raw/master/vscode-pvs/screenshots/vscode-pvs-rapid-prototyping.gif\" width=\"600\">\\n\\n<br>\\n\\n## Structure\\n```\\n.\\n├── client                       // PVS Language Client (VSCode entry point)\\n│   └── src\\n│       ├── providers            // Client-side service providers\\n│       ├── views                // User interface components\\n│       ├── common               // Utility functions \\n│       └── pvsLanguageClient.ts // PVS Language Client implementation\\n├── icons                        // PVS icons theme\\n├── package.json                 // The extension manifest\\n├── syntax                       // Syntax highlighting\\n├── LICENSES                     // NASA Open Source License Agreement\\n├── Makefile                     // Makefile for building a .vsix image from the source code\\n└── server                       // PVS Language Server\\n    └── src\\n        ├── providers                          // Server-side service providers\\n        │     ├── pvsCodeLensProvider.ts       // In-line actionable commands\\n        │     ├── pvsCompletionProvider.ts     // Auto-completion\\n        │     ├── pvsDefinitionProvider.ts     // Find definitions\\n        │     ├── pvsHoverProvider.ts          // Hover information \\n        │     ├── pvsProofExplorer.ts          // Proof tree editor\\n        │     └── pvsPackageManager.ts         // Installation manager \\n        ├── parser               // Parser grammar and scripts      \\n        ├── common               // Utility functions           \\n        ├── pvsCli.ts            // PVS Command Line Interface\\n        ├── pvsProcess.ts        // PVS process wrapper\\n        ├── pvsLisp.ts           // Lisp reader for parsing PVS responses\\n        └── pvsLanguageServer.ts // PVS Language Server implementation\\n```\\n\\n<br>\\n\\n## Notices\\n### Copyright \\nCopyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n \\n### Disclaimers\\n**No Warranty**: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY\\n  WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,\\n  INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE\\n  WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF\\n  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\n  INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\n  FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO\\n  THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER,\\n  CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT\\n  OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY\\n  OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\n  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\n  REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\n  AND DISTRIBUTES IT \"AS IS.\"\\n \\n**Waiver and Indemnity**: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS\\n  AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND\\n  SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF\\n  THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,\\n  EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM\\n  PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT\\n  SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\n  STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\n  PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE\\n  REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL\\n  TERMINATION OF THIS AGREEMENT.\\n\\n\\n## Contacts\\n* Paolo Masci (NIA) (paolo.masci@nianet.org)\\n* Cesar Munoz (NASA LaRC) (cesar.a.munoz@nasa.gov)\\n\\n'},\n",
       " {'repo': 'nasa/osal',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/osal/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/osal/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : Operating System Abstraction Layer\\n\\nThis repository contains NASA\\'s Operating System Abstraction Layer (OSAL), which is a framework component of the Core Flight System.\\n\\nThis is a collection of abstractio APIs and associated framework to be located in the `osal` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes build and execution instructions.\\n\\nThe autogenerated OSAL user\\'s guide can be viewed at <https://github.com/nasa/cFS/blob/gh-pages/OSAL_Users_Guide.pdf>.\\n\\n## Version History\\n\\n### Development Build: v5.1.0-rc1+dev417\\n\\n- Fixes infinite loop  in `UtPrintx()`. Adds the data\\'s memory address to output. Note, UtPrintf displays the the file/line of the `UtPrintx` function, **not the actual test location**; it is better to call `UT_BSP_DoText` directly.\\n- Adds `OS_SocketShutdown()` API wrapper around BSD\\'s socket shutdown() API. This allows a data transfer of a stream socket to be gracefully shut down prior to socket closure.\\n- See osal <https://github.com/nasa/osal/pull/979> and <https://github.com/nasa/cFS/pull/252>\\n\\n### Development Build: v5.1.0-rc1+dev411\\n\\n- [docs] Clarifies that that zero will be returned on EOF condition in the API documentation for OS_read/write/TimedRead/TimedWrite. In the case of the timed API calls, the `OS_ERR_TIMEOUT` status code will be returned if the timeout expired without the handle becoming readable/writable during that time.\\n- Addresses a shortcomings in the UT Assert hook functions. Namely the assumed return type of int32 which is not always the case.\\n- Adds the concept of a \"handler\" function to UT assert to replace hard-coded custom logic in UT assert. A handler is the custom logic that exists between the hook function and the return to the stub caller. The handler is directly responsible for setting all outputs.\\n- Adds a script to auto-generate stub functions that match this pattern. Given an API header file, the script extracts\\nthe declarations, and generates a source file with stub definitions that rely on a separate handler to deal with the needed outputs.\\n- Refactors `os-shared-printf.h`) into two parts to improve the compatibility with the script method.\\n- Updates all existing stubs in OSAL to use the auto-generated stub logic from the script, created directly from the C header. This ensures that stubs will match the FSW implementation.\\n- [continuous-integration] Adds a local osal-specific makefile to help build unit tests. Adds a new github workflow that runs the unit tests in both the context of the bundle configuration and the local OSAL config. Verifies 100% line coverage.\\n- Fixes incorrect token use in `OS_SocketAccept`. Enables the `network-api-test` to handle multiple connections that re-use the same acceptor socket between them.\\n- Promotes the `OS_CONFIG_CONSOLE_ASYNC` option into the shared layer to remove duplicate implementation code and add more coverage testing.\\n- Adds an osconfig option to allow the user to elect this mode at configuration time.\\n\\n\\n### Development Build: v5.1.0-rc1+dev393\\n\\n- Changes parameter names to avoid collisions. Renames `access` as `access_mode` in `osapi-file.h`. Renames `time` as `TimeSp` in `os-impl-posix-gettime.c`.\\n- Deletes the broken RTEMS `os-impl-shell.c` file so so OSAL builds with `OSAL_CONFIG_INCLUDE_SHELL=true`. RTEMS will always report `OS_ERR_NOT_IMPLEMENTED`.\\n- See <https://github.com/nasa/osal/pull/967> and <https://github.com/nasa/cFS/pull/248>\\n\\n### Development Build: v5.1.0-rc1+dev387\\n\\n- Replaces the separate \"Initialized\" and \"Shutdown\" flags with a single state flag. Creates a global single source of truth for the OSAL state. This enables users to run tests and OS_API_Init() multiple times without a reboot in the middle to reset the state.\\n  - Multiple invocations of OS_API_Init() are allowed - subsequent calls can be ignored\\n  - Deleting of any internal objects that did get created if OS_API_Init() fails (this leaves system in same state as when it started)\\n  - Allows Re-initialization of OSAL after OS_ApplicationShutdown() - may be relevant when running unit tests several times without rebooting.\\n- Adds OS_API_Teardown to complement OS_API_Init. This cleans up all OSAL resources ideally leaving the system in a state where `OS_API_Init()` may be invoked again.\\n- Reworks the shell unit test which was probably not working. Note this requires modifying the osal config to enable shell, otherwise test is skipped.\\n- See <https://github.com/nasa/osal/pull/956> and <https://github.com/nasa/cFS/pull/242>\\n\\n### Development Build: v5.1.0-rc1+dev378\\n\\n- Replaces nonstandard header file block comments and include guards. No behavior changes\\n- Removes `CLOCK_MONOTONIC` as the osal colck source since PSP no longer needs it. `OS_GetLocalTime()` and `OS_SetLocalTime()` will work as described.\\n- Replaces `shellName` with a specific `localShellName` that can be polled safely and changes its type to a char of `OS_MAX_API_NAME` length for safety.\\n- See <https://github.com/nasa/osal/pull/951> and <https://github.com/nasa/cFS/pull/238>\\n\\n### Development Build: v5.1.0-rc1+dev367\\n\\n- Removes `SOFTWARE_BIG_BIT_ORDER` and `SOFTWARE_LITTLE_BIT_ORDER` macros from `common_types.h`. These are not needed by OSAL and cannot handle all cases.  Application code with endianness dependency that was relying on these symbols may break. Users should leverage code in cFE: `cfe_endian.h`. See <https://github.com/nasa/cFE/pull/1218> for more details.\\n- Applies minor code and documentation cleanup: white space, typos, etc.\\n- Adds test to get full coverage of vxworks in `os-impl-bsd-socket.c` resulting in full line coverage for OSAL\\n- Adds more descriptive return codes if `OS_SymbolTableDump_Impl` does not do what is expected. Adds a new error `OS_ERR_OUTPUT_TOO_LARGE` if the size limit was insufficient. Return `OS_ERROR` if an empty file was written - this likely indicates some fundamental issue with the VxWorks symbol table. Returns `OS_ERR_NAME_TOO_LONG` if one of the symbol names was too long. Improves unit test to check for/verify these responses.\\n- Removes the unneeded `OS_TaskRegister()` and all references to it in code, tests, and documentation. No impact to behavior, but does affect API and has depenedencies\\n- Removes unused `-SCRIPT_MODE` flag in cmake\\n- Remove comparison between `osal_id_t` and `integers` to use the provided comparison function, `OS_ObjectIdDefined()`. System builds and runs again when using a type-safe/non-integer osal_id_t type.\\n- See <https://github.com/nasa/osal/pull/927>\\n\\n### Development Build: v5.1.0-rc1+dev350\\n\\n- Moves copyblock size to a macro and add comments. Defines `OS_CP_BLOCK_SIZE` and adds clear documentation that it could be adjusted for page size, performance, etc.\\n- Removes while loop\\n- Replaces all #includes of <os and <OSC_ matches with \" to match coding standard.\\n- Consolidates the duplicated switch in `OS_SocketOpen_Impl`\\n- Add const to input pointers for `OS_FdSet_ConvertIn_Impl` and `OS_ObjectIdTransactionFinish`\\n- Removes network prototypes defined in `osapi_sockets.h` that are also in `osapi_network.h`\\n- Removes `NULL` redefine from `common_types.h`\\n- Adds `Contributing.md` that points to bundle-level contribution guide\\n- Reports test cases that \"fail\" as \"not implemented\" with new `UtAssert_NA` macro instead of `UtPrintf`\\n- Calls to `OS_SelectSingle` and `OS_SelectMultiple` will fail if an FD within the set is outside the range of the underlying `FD_SETSIZE` from the C library.\\n- Fixes calculation used for the relative time interval in the `select()` call. Also adds a UT case that specifically exercises the carryover described. Fixes delay when this carry condition is hit\\n- Documents algorithm that provides application-controlled timeout on the connection initiation. Also adds a debug statement if the connect fails for a reason other than `EINPROGRESS`. No impact to normal behavior.\\n- Adds check for `EAGAIN` if the system fails to allocate kernel-internal resources.\\n- Adds a `CompileTimeAssert` to confirm that the size of the abstract buffer for socket addresses is large enough to store any of the enabled address types thereby removing the need for runtime tests.\\n- With this change, if `OS_SOCKADDR_MAX_LENis` not large enough for the address type, osal will fail to compile. This enforces that the abstract size is large enough for any/all enabled address types, regardless of what is actually used.\\n- Adds missing functional test for `OS_ShellOutputToFile`\\n- Add test for `fcntl()` error return of -1 and report errno. If setting `O_NONBLOCK` fails, then debug message is printed and blocking mode is used and timeouts will not work as a result.\\n- Improves error codes when attempting to seek on a pipe/socket. Translates the `OS_ERR_OPERATION_NOT_SUPPORTED` error rather than \"not implemented\". The `ESPIPE` errno means that seeking is not supported on the given file handle.\\n- Renames `OS_U32ValueWrapper_t` as `OS_VoidPtrValueWrapper_t` to better indicate its purpose. The point is to pass a value through a `void*`. Adds a compile-time assert to check that this is only used to directly pass values which have a size of less than or equal to sizeof(void*).\\n- Refactors the return statement for `OS_FileSys_FindVirtMountPoint()` so it is easier to read and adds some informational comments.\\n- Reports an error if calling `timer_gettime` after `timer_settime` fails.\\n- Returns `OS_ERROR` status to caller after an error on moduleInfoGet()\\n- Removes an extraneous/unreachable OS_ObjectIdDefined check and its accompanying debug statement. The only way this check could have been reached would be if the normal unlock process was bypassed such that the underlying OS mutex was unlocked but OSAL state still had it owned by a task. This condition never happens at runtime.\\n- Updates documentation for `OS_MAX_MODULE`\\n- See <https://github.com/nasa/osal/pull/917>\\n\\n### Development Build: v5.1.0-rc1+dev297\\n\\n- Fix #836, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/osal/pull/838>\\n\\n### Development Build: v5.1.0-rc1+dev293\\n\\n- Avoids various \"possible uninitialized variable\" warnings for routines that utilize this API.\\n- Renames `sockaddr*` structures to `sa*` to deconflict from structure name in `os-impl-bsd-sockets.c`. Adds `OS_NETWORK_SUPPORTS_IPV6` to `os-impl-bsd-sockets.c` compilation. Renames `bsd-select-stubs.c` to `sys-select-stubs.c`. Coverage now includes all currently possible files in VxWorks build\\n- Resolves CodeQL security warning by restricting permissions on file create.\\n- Changes comments using \"cpp\" comment style to \"c\" style\\n- Adds _new_ accessor functions APIs to get version strings and return the values of string macros defined in `osapi-version.h`.\\n  - The \"simple\" version currently `OS_VERSION` macro - this is the semantic version without any extra detail.  This is returned by `OS_GetVersion()`.\\n  - The \"descriptive\" version in `OS_VERSION_STRING` macro - this has extra detail like the most recent official release.  This is returned by `OS_GetVersionDescription()`.\\n  - The release code name, now returned by `OS_GetVersionDescription()`.  \\n- These accessor functions are the preferred way to get the OSAL version string, from now on users should avoid using the macro definitions as it is evaluated at OSAL library compile time, rather than application compile time, and thus will remain correct in the event that OSAL is relinked without recompiling the application.\\nAdds `osapi-version.c` to implement these 3 calls and associated coverage test. This allows the version.c file to be auto-generated in the future.\\n- See <https://github.com/nasa/osal/pull/835>\\n\\n### Development Build: v5.1.0-rc1+dev280\\n\\n- Makes tests skip after getting their first not implemented error.\\n- Updates stub helpers to match the behavior of calling the default implementation stub macro (NULL VA list)\\n- Removes redundant logic and assignment to fix static analysis warnings\\n- Truncates at the end of the logic flow for socket name as opposed to possibly 3 different locations. Fixes static analysis warning.\\n- Renames `timer_id` in unit tests to `local_timer_id` to avoid conflicts\\n- Removes all deprecated elements\\n- No behavior change. Renames `UT_Stub_CheckForceFail` to `UT_Stub_CheckDefaultReturnValue`, also only sets `Value` if not `NULL` (can pass in NULL value doesn\\'t matter)\\n- See <https://github.com/nasa/osal/pull/830>\\n\\n### Development Build: v5.1.0-rc1+dev262\\n\\n- Adds test cases for `OS_ObjectIdFinalizeDelete`, `OS_DeleteAllObjects`, and others to get 100% line and function coverage on VxWorks and shared/portable layers.\\n- Ensures APIs check for `NULL` inputs or have documentation stating that a null value is allowed.\\n- Adds timeout to static analysis check and adds format check. Removes old .travis.yml and updates badges in readme.\\n- Adds Code QL analysis on push to main and pull requests (to main)\\n- Cleans commented-out code in tests to address static analysis warnings\\n- Initializes local variables to avoid returning uninitialized values from stubs and address static-analysis findings\\n- Replaces two local return codes defined as `uint32` with `int32` to resolve static-analysis warnings\\n- Simplifies switch statements based on previous checks. Removes unreachable, dead code to resolve static-analysis warnings\\n- Terminates  unit test macros variadic lists with `NULL` to address CWE-121 CodeQL warning\\n- Adds a check to send the semaphore to avoid unreachable code\\n- Adds a status return to `OS_ConsoleAPI_Init` so debug warnings will get reported correctly on errors.\\n- Declares `DummyVec` as static to avoid warning and returning stack allocated memory when returning `VecTbl` in `OSC_INUM_TO_IVEC` stub\\n- Updates types in `os-impl-no-symtab.c` to match latest APIs\\n- Updates types in `os-impl-no-symtab.c` to match latest APIs\\n- Fixes missing `NULL` terminations and applies the standard \"sizeof\" pattern where appropriate. No longer truncates filename in `OS_ModuleInfo`.\\n- Fixes `%u` conversion in RTEMS so to address build warning\\n- Create a wrapper around `memchr()` that mimics the non-C99 function `strnlen()` defined in POSIX-2008. Use this instead of `strlen()` whenever the string being checked either originates in or will be copied into a fixed-length array buffer. No behavior changes except if a bug causes strings to be unterminated.\\n- No behavior change, applies the standard formatting using `clang-format`\\n- See <https://github.com/nasa/osal/pull/774>\\n\\n### Development Build: v5.1.0-rc1+dev221\\n\\n- Fixes `printf` format to correctly build in RTEMS-5.\\n- **Deprecates `OS_fsBlocksFree()` and `OS_fsBytesFree()`** in favor of `OS_FileSysStatVolume()`.\\n- Adds `Security.md` with instructions to report vulnerabilities.\\n- Add `UtDebug` in `OS_printf` stub. Output the `OS_printf` input as a debug message from stub.\\n- Documentation: Add note on `UtTest_Add` API. Nesting `UtTest_Add` from within an added test fails without error.\\n- Unit Test: No more garbage characters written to test report log\\n- Fix typo in `osapi.h` affecting C++ build. No other functional change\\n- Unit Test: Rename `UT_ClearForceFail` as `UT_ClearDefaultValue`. Update the comments of `UT_SetDefaultReturnValue` to match the more general function.\\n- Unit Test: Add test teardown failures to the test summary and changed the printout to use the same style as startup failures.\\n- Unit Test: Removes no longer applicable `UT_CheckForOpenSockets` since the UT framework resets the state for each unit test.\\n- Changes the file-create operation to read-write permissions to work on RTEMS\\n- Unit Test: Fixes incorrect assertions in `network-api-test` to correctly check return values.\\n- Unit Test: Generalizes queue timeout test to also test message queue functionality to validate settings and permissions to work with mqueues.\\n- Implements `OS_time_t` with a single 64-bit tick counter rather than a split 32 bit seconds + 32 bit microseconds counter.\\n- Unit Test: Installs the modules used in unit testing and adds removal of post-test, left-over files.\\n- See <https://github.com/nasa/osal/pulls/767>\\n\\n### Development Build: v5.1.0-rc1+dev184\\n\\n- Address issues with OSAL global table management:\\n  - use iterators whenever possible\\n  - use an unlock key rather than task ID so OS_TaskExit() doesn\\'t trigger a warning\\n  - general cleanup of lock/unlock impl and remove redundant logic\\n  - unlock global tables during create/delete\\n  - keep threads \"attached\" in POSIX, so they can be joined when deleted.\\n- No longer triggers warning with OS_TaskExit() on VxWorks (see #645)\\n- `OS_TaskDelete()` on POSIX does not return until the task has actually exited (see #642)\\n- The chmod test is now skipped on VxWorks rather than failing. The `OS_FileChmod_Impl()` function now returns `OS_ERR_NOT_IMPLEMENTED` when run on a file system that does not have permissions, which in turn causes the unit test to be skipped rather than fail.   \\n- Corrects a file handle leak.\\n-  Add parameter check to `OS_SocketSendTo` and adjust coverage test to validate.\\n- Replace `OS_fsBytesFree` and `OS_fsBlocksFree` with `OS_FileSysStatVolume`. This new API for getting stats on file system. Uses existing `OS_FileSysStatVolume_Impl` call and exposes it in the public API.\\n- When cleaning up for shutdown, delete resources that have a task/thread first, followed by other resource types. This helps avoid possible dependencies as running threads might be using the other resources. No detectable external impact; internally, the tasks are deleted first during shutdown, which only has an impact if/when tasks are actively using other OSAL resources.\\n- The mount/unmount *VxWorks* implementation was not adequately checking for and handling the `FS_BASED` pass -through mapping type - which should be mostly a no-op. Create a mount point directory if it does not already exist when using this mapping type for consistency with POSIX.\\n- Adds a documentation note to `OS_FileSysAddFixedMap()`: The virtual mount point cannot be empty - so `OS_FileSysAddFixedMap(.., \"/\", \"/\")` does not work but `OS_FileSysAddFixedMap(.., \"/\", \"/root\")` does work and allows one to open files in the root as `\"/root/\"` from OSAL applications. Mount-point directories do not need to previously exist when using OS_FileSysAddFixedMap\\n- store `taskTCB` return in a `void *`, then cast to `OS_impl_task_internal_record_t *` to avoid a strict alignment compiler error\\n- Removes the non-portable `OS_PACK` and `OS_ALIGNED` macros.\\n- Uses the POSIX dir implementation on VxWorks 6.9. The only incompatibility is the prototype for `mkdir()`which is missing the second argument; this is worked around with a compatibility macro for VxWorks 6.x builds.\\n- Translate and convert the VxWorks coverage test cases to the portable dir implementation, which benefits VxWorks7, RTEMS, and POSIX.\\n- Fixes prototypes so they run on RTEMS by replacing uint32 with size_t\\n- Adds` OS_CHECK_POINTER` macros to `OS_ConvertToArrayIndex` and `OS_TimeBaseGetFreeRun` so they can handle NULL pointers and return the correct error.\\n- Adds access functions to convert/extract different units from an OS_time_t value - so that other code in CFE/PSP/Apps can be updated to use the access functions and thereby not break when the internal time definition changes. Replaces the `int32` with `OS_time_t` in the \"stat\" structure used by the file module. Updates the pointer argument to `OS_SetLocalTime()` to be `const`. Prototype change of `OS_SetLocalTime()` should be backward compatible.\\n- See <https://github.com/nasa/osal/pulls/750>\\n\\n### Development Build: v5.1.0-rc1+dev149\\n\\n- Document UtAssert_Message parameters, also adds \"see also\" note for helper macros.\\n- Fix doxygen typo\\n- Replace `OS_BinSemFlush` with `OS_BinSemGive` to prevent a rare race condition. Change the port numbers to be different from network test for when tests are run in parallel.\\n- Fix doxygen format errors. Usersguide now builds without warnings.\\n- Suppress invalid cppcheck warning in `OS_WaitForStateChange`\\n- Add cppcheck static analysis workflow to osal CI\\n- See <https://github.com/nasa/osal/pull/744>\\n\\n### Development Build: v5.1.0-rc1+dev132\\n\\n- Convert the OSAL Configuration Guide from docx and pdf to a markdown file.\\n- Test Tasks do not run at 100%. Move all definitions and instantiations out of the core-test header file and reuse the already-existing single task definition.\\n- Break up `osapi-os-*.h` files into units that correspond to the implementation units. Kept old header file names for compatibility.\\n- Reworks the POSIX global lock implementation. Does not change the POSIX signal mask when locking/unlocking the global.\\n  - Fixes a race condition.\\n  - Adds a condition variable to the global lock structure. improves handling of tasks competing for access to the same object.\\n  - No longer changing signal masks repeatedly/unexpectedly. May be relevant to some BSP/driver developers.\\n- Checks return of sysconf for error and reports them. Only sets PageSize on success. If sysconf fails it provides a mechanism to avoid error propagation.\\n- Uses `errno` instead of status return from `clock_getres` with `strerror` reporting.\\n- Adds support for VxWorks 7\\n- See <https://github.com/nasa/osal/pull/690>\\n\\n### Development Build: v5.1.0-rc1+dev109\\n\\n- Add support for RTEMS 5.1 in the OSAL and provides defines and necessary ifdefs so RTEMS 4.11 can continue to be supported.\\n- Adds functional test for OS_chmod\\n- Refactor the table array access across OSAL. Use a token concept in combination with a macro to obtain the table entry instead of indexing arrays directly. All access is then done through this table pointer. Use the full object ID in the timer call back list. Update the timer sync callback prototype. Pass the entire OSAL ID to the sync function, not just the index. This is technically an API change.\\n- Replaces condition on forever loops to end on shutdown. Loops now exit on shutdown.\\n- Removes obsolete printf tests that didn\\'t work\\n- See <https://github.com/nasa/osal/pull/680>\\n\\n\\n### Development Build: v5.1.0-rc1+dev91\\n\\n- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing\\n- Add a 5th timer to TimerTest functional to test the one shot (zero-length time interval) case.\\n- Ensure all APIs use the proper type. Sizes are now size_t; these will now be 64 bits on a 64 bit platform.\\n- Fix build issue on VxWorks 6.9 by using the 3 argument form of `open()`. Passing `0` as the mode solves the build issue. This parameter is ignored when not creating a file.\\n-  The address calculations now use `unsigned long` instead of `long` to ensure that all rounding and base address adjustments behave the same way in the event that the addresses lie in the upper half of memory (i.e. start with a 1 bit) which would put it in the negative range of a long type.\\n- See <https://github.com/nasa/osal/pull/662>\\n\\n\\n### Development Build: v5.1.0-rc1+dev75\\n\\n- Ensure that the handle is not NULL before invoking dlclose(). In particular the handle will be NULL for static modules. Shutdown after CTRL+C occurs normally (no segfault).\\n- Add a \"flags\" parameter to OS_ModuleLoad() to indicate the desired symbol visibility:\\n    - GLOBAL (0, the default, and matches current behavior)\\n    - LOCAL which hides from other modules and prevents other modules from binding to symbols in this module, thereby ensuring/preserving the ability to unload in the future\\n  - CFE should use LOCAL flag for apps, and GLOBAL flags for libraries.\\n- See <https://github.com/nasa/osal/pull/652>\\n\\n### Development Build: v5.1.0-rc1+dev68\\n\\n- When `OS_DEBUG` is enabled, this adds a message if mutex give/take actions occur outside the expected sequence. This informs the user (via the debug console) if a lock is taken more than once or if a lock is given by a different task than the one that originally took it:\\n```\\nOS_MutSemTake():216:WARNING: Task 65547 taking mutex 327685 while owned by task 65547\\n```\\n- Removes all FIXME comments\\n- Resolves security/filename race issue by opening file and acting on descriptor by adding fstat stub\\n- Squashed the minor recommended bugs\\n- UtAssert macros now accept variable string arguments.The `UtAssert_True` wrapper around call is no longer needed to accommodate dynamic string output, thus removing the double assert. UtAssert macros will now be able to offer more information by themselves.\\n- See <https://github.com/nasa/osal/pull/639>\\n\\n### Development Build: v5.1.0-rc1+dev60\\n\\n- Appliy standard formating, whitespace-only changes\\n- See <https://github.com/nasa/osal/pull/627>\\n\\n### Development Build: v5.1.0-rc1+dev55\\n\\n- Deprecate `OS_open` and `OS_creat` to and replaced them with by `OS_OpenCreate`, which implements both functions via flags, and follows the correct OSAL API patterns.\\n- Change use of uint32 for ID to the correct typedef. Also use ObjectIdFromInteger/ObjectIdToInteger where it is intended to convert these values to integers e.g. for the \"name\" fields in RTEMS.\\n- See <https://github.com/nasa/osal/pull/621>\\n\\n### Development Build: v5.1.0-rc1+dev49\\n\\n- Adds an event callback mechanism to certain state changes in OSAL. This allows the CFE PSP to be notified at these points, and therefore it can add platform-specific functionality.\\n- Correct issues involving recent OS_Milli2Ticks change.\\n- See <https://github.com/nasa/osal/pull/612>\\n\\n### Development Build: v5.1.0-rc1+dev44\\n\\n- Removes OS_Tick2Micros and internalize OS_Milli2Ticks.\\n- Adds ut_assert address equal macro.\\n- See <https://github.com/nasa/osal/pull/607>\\n\\n### Development Build: v5.1.0-rc1+dev38\\n\\n- Sets Revision to 99 for development builds\\n- See <https://github.com/nasa/osal/pull/600>\\n\\n### Development Build: v5.1.0-rc1+dev34\\n\\n- Move this existing function into the public API, as it is performs more verification than the OS_ConvertToArrayIndex function.\\n- The C library type is signed, and this makes the result check work as intended.\\n- See <https://github.com/nasa/osal/pull/596>\\n\\n\\n### Development Build: v5.1.0-rc1+dev16\\n\\n - In the next major OSAL release, this code will be no longer supported at all. It should be removed early in the cycle to avoid needing to maintain this compatibility code. This code was already conditional on the OSAL_OMIT_DEPRECATED flag and as such the CCB has already tested/verified running the code in this configuration as part of CI scripts. After this change, the build should be equivalent to the result of building with OMIT_DEPRECATED=true.\\n- See <https://github.com/nasa/osal/pull/582>\\n\\n### Development Build: v5.1.0-rc1+dev12\\n\\n- Removes internal functions that are no longer used or defined but whose prototypes and stubs were still present in OS_ObjectIdMap\\n- Removes repetitive clearing of the global ID and unlocking global table and replaces these with common implementation in the idmap source file. This moves deleting tables to be similar to creating tables and provides\\na common location for additional table-deletion-related logic.\\n- Propagates return code from OS_TaskRegister_Impl(). If this routine fails then return the error to the caller, which also prevents the task from starting.\\n- See <https://github.com/nasa/osal/pull/576>\\n\\n### Development Build: v5.1.0-rc1+dev5\\n\\n- Adds OSAL network APIs missing functional tests as well as tests for OS_TimedRead and OS_TimedWrite\\n- Allows separate, dynamic registration of test setup and teardown routines which are executed before and after the normal test routine, which can create and delete any global/common test prerequisites.\\n- Adds FileSysAddFixedMap missing functional API test\\n- See <https://github.com/nasa/osal/pull/563>\\n\\n\\n### Development Build: 5.0.0+dev247\\n\\n- `OS_SocketOpen()` sets `sock_id` and returns a status when successful.\\n- Changed timer-test to be able to use OS_MAX_TIMERS value on top of the hard-coded NUMBER_OF_TIMERS value. This will allow the test to be functional even if the OS_MAX_TIMERS value is reconfigured.\\n- Ensures that\\n  - All stub routines register their arguments in the context, so that the values will be available to hook functions.\\n  - The argument names used in stubs match the name in the prototype/documentation so the value can be retrieved by name.\\n- Adds back rounding up to PTHREAD_STACK_MIN and also adds rounding up to a system page size. Keeps check for zero stack at the shared level; attempts to create a task with zero stack will fail. Allows internal helper threads to be created with a default minimum stack size.\\n- Avoids a possible truncation in snprintf call. No buffer size/truncation warning when building with optimization enabled.\\n- Added new macros to `osapi-version` to report baseline and build number\\n- The coverage binaries are now correctly installed for CPU1 and CPU2 as opposed to installed twice to CPU2 but not at all for CPU1.\\n- Fixes a typo in ut_assert README and clarifies stub documentation.\\n- See <https://github.com/nasa/osal/pull/529>\\n\\n### Development Build: 5.0.21\\n\\n- Command line options in Linux are no longer ignored/dropped.\\n- No impact to current unit testing which runs UT assert as a standalone app. Add a position independent code (PIC) variant of the ut_assert library, which can be dynamically loaded into other applications rather than running as a standalone OSAL application. This enables loading\\nUT assert as a CFE library.\\n- Unit tests pass on RTEMS.\\n- Resolve inconsistency in how the stack size is treated across different OS implemntations. With this change the user-requested size is passed through to the underlying OS without an enforced minimum. An additional sanity check is added at the shared layer to ensure that the stack size is never passed as 0.\\n- Update Licenses for Apache 2.0\\n- See <https://github.com/nasa/osal/pull/521>\\n\\n### Development Build: 5.0.20\\n-  Add \"non-zero\" to the out variable description for OS_Create (and related) API\\'s.\\n- Increases the buffer for context info from 128 to 256 bytes and the total report buffer to 320 bytes.\\n- Add stub functions for `OS_TaskFindIdBySystemData()`, `OS_FileSysAddFixedMap()`, `OS_TimedRead()`, `OS_TimedWrite()`, and `OS_FileSysAddFixedMap()`\\n- Added the following wrappers macros around `UtAssert_True` for commonly-used asserts:\\n  - `UtAssert_INT32_EQ` - check equality as 32 bit signed int\\n  - `UtAssert_UINT32_EQ` - check equality as 32 bit unsigned int\\n  - `UtAssert_NOT_NULL` - check pointer not null\\n  - `UtAssert_NULL` - check pointer is null\\n  - `UtAssert_NONZERO` - check integer is nonzero\\n  - `UtAssert_ZERO` - check integer is zero\\n  - `UtAssert_STUB_COUNT` - check stub count\\n-  Using `unsigned long` instead of `uintmax_t` to fix support for VxWorks\\n\\n- See <https://github.com/nasa/osal/pull/511> and <https://github.com/nasa/osal/pull/513>\\n\\n### Development Build: 5.0.19\\n\\n- Rename BSPs that can be used on multiple platforms.\\n`mcp750-vxworks` becomes `generic-vxworks`\\n`pc-linux` becomes `generic-linux`\\n- New features only, does not change existing behavior.\\nUT Hook functions now have the capability to get argument values by name, which is more future proof than assuming a numeric index.\\n- Add functional test for `OS_TimerAdd`\\n- Added functional tests for `OS_TimeBase Api` on `OS_TimeBaseCreate`, `OS_TimeBaseSet`, `OS_TimeBaseDelete`, `OS_TimeBaseGetIdByName`, `OS_TimeBaseGetInfo`, `OS_TimeBaseGetFreeRun`\\n- See <https://github.com/nasa/osal/pull/487> for details\\n\\n\\n### Development Build: 5.0.18\\n\\n- Add functional tests for `OS_IdentifyObject`, `OS_ConvertToArrayIndex` and `OS_ForEachObject` functions.\\n- Fix doxygen warnings\\n- Unit test cases which use `OS_statfs` and run on an `RTEMS IMFS` volume will be skipped and categorized as \"NA\" due to `OS_ERR_NOT_IMPLEMENTED` response, rather than a failure.\\n- The device_name field was using the wrong length, it should be of `OS_FS_DEV_NAME_LEN` Also correct another length check on the local path name.\\n- For RTEMS, will not shutdown the kernel if test abort occurs.\\n- Unit tests work on RTEMS without BSP preallocating ramdisks\\n- If `OSAL_EXT_SOURCE_DIR` cache variable is set, this location will be checked first for a BSP/OS implementation layer.\\n- Implement `OS_GetResourceName()` and `OS_ForEachObjectOfType()`, which are new functions that allow for additional query capabilities. No impact to current behavior as the FSW does not currently use any of these new APIs.\\n- A functional test enhancement to `bin-sem-test` which replicates the specific conditions for the observed bug to occur. Deletes the task calling `OS_BinSemTake()` and then attempts to use the semaphore after this.\\n- Employ a `pthread` \"cleanup handler\" to handle the situation where a task is canceled during the `pthread_cond_wait()` call. This ensures that the `mutex` is unlocked as part of the cleanup, so other tasks may continue using the semaphore.    \\n- Change all initial `mutex` locking to be a finite \"timed\" wait rather than an infinite wait. In all cases, the condition variable is only held for brief periods of time and should be readily available. If a task blocks for a long time, this considers the mutex \"broken\" and aborts, thereby avoiding deadlock. This is a \"contingency\" fix in that if an exception or signal or other unknown/unhandled async event occurs that leaves the mutex permanently locked.\\n- Adds the mutex to protect the timer callback `timecb` resource table.\\n- See <https://github.com/nasa/osal/pull/482>\\n\\n### Development Build: 5.0.17\\n\\n-  `OS_QueueCreate()` will return an error code if the depth parameter is larger than the configured `OS_MAX_QUEUE_DEPTH`.\\n- See <https://github.com/nasa/osal/pull/477>\\n\\n### Development Build: 5.0.16\\n\\n- Resized buffers and added explicit termination to string copies. No warnings on GCC9 with strict settings and optimization enabled.\\n- New API to reverse lookup an OS-provided thread/task identifier back to an OSAL ID. Any use of existing OStask_id field within the task property structure is now deprecated.\\n- See <https://github.com/nasa/osal/pull/458>\\n\\n### Development Build: 5.0.15\\n\\n- Changes the build system.\\n- No more user-maintained osconfig.h file, this is now replaced by a cmake configuration file.\\n- Breaks up low-level implementation into small, separate subsystem units, with a separate header file for each one.\\n- See <https://github.com/nasa/osal/pull/444>\\n\\n### Development Build: 5.0.14\\n\\n- Adds library build, functional, and coverage test to CI\\n- Deprecates `OS_FS_SUCCESS, OS_FS_ERROR , OS_FS_ERR_INVALID_POINTER, OS_FS_ERR_NO_FREE_FDS , OS_FS_ERR_INVALID_FD, and OS_FS_UNIMPLEMENTED` from from `osapi-os-filesys.h`\\n- Individual directory names now limited to OS_MAX_FILE_NAME\\n- Fix tautology, local_idx1 is now compared with local_idx2\\n- Module files are generated when the `osal_loader_UT` test is built and run\\n- Consistent osal-core-test execution status\\n- See <https://github.com/nasa/osal/pull/440> for more details\\n\\n### Development Build: 5.0.13\\n\\n- Added coverage test to `OS_TimerCreate` for `OS_ERR_NAME_TOO_LONG`.\\n- Externalize enum for `SelectSingle`, ensures that pointers passed to `SelectFd...()` APIs are not null, ensures that pointer to `SelectSingle` is not null.\\n- Command to run in shell and output to fill will fail with default (not implemented) setting.\\n- Builds successfully using the inferred OS when only `OSAL_SYSTEM_BSPTYPE` is set. Generates a warning when `OSAL_SYSTEM_BSPTYPE` and `OSAL_SYSTEM_OSTYPE` are both set but are mismatched.\\n- See <https://github.com/nasa/osal/pull/433> for more details\\n\\n### Development Build: 5.0.12\\n\\n- Use the target_include_directories and target_compile_definitions functions from CMake to manage the build flags per target.\\n- Build implementation components using a separate CMakeLists.txt file rather than aux_source_directory.\\n- Provide sufficient framework for combining the OSAL BSP, UT BSP, and the CFE PSP and eliminating the duplication/overlap between these items.\\n- Minor updates (see <https://github.com/nasa/osal/pull/417>)\\n\\n### Development Build: 5.0.11\\n\\n- The more descriptive return value OS_ERR_NAME_NOT_FOUND (instead of OS_FS_ERROR) will now be returned from the following functions (): OS_rmfs, OS_mount, OS_unmount, OS_FS_GetPhysDriveName\\n- Wraps OS_ShMem* prototype and unit test wrapper additions in OSAL_OMIT_DEPRECATED\\n- Minor updates (see <https://github.com/nasa/osal/pull/408>)\\n\\n### Development Build: 5.0.10\\n\\n- Minor updates (see <https://github.com/nasa/osal/pull/401>)\\n\\n  - 5.0.9: DEVELOPMENT\\n\\n- Documentation updates (see <https://github.com/nasa/osal/pull/375>)\\n\\n### Development Build: 5.0.8\\n\\n- Minor updates (see <https://github.com/nasa/osal/pull/369>)\\n\\n### Development Build: 5.0.7\\n\\n- Fixes memset bug\\n- Minor updates (see <https://github.com/nasa/osal/pull/361>)\\n\\n### Development Build: 5.0.6\\n\\n- Minor updates (see <https://github.com/nasa/osal/pull/355>)\\n\\n### Development Build: 5.0.5\\n\\n- Fixed osal_timer_UT test failure case\\n- Minor updates (see <https://github.com/nasa/osal/pull/350>)\\n\\n### Development Build: 5.0.4\\n\\n- Minor updates (see <https://github.com/nasa/osal/pull/334>)\\n\\n### Development Build: 5.0.3\\n\\n- Minor updates (see <https://github.com/nasa/osal/pull/292>)\\n\\n### Development Build: 5.0.2\\n\\n- Bug fixes and minor updates (see <https://github.com/nasa/osal/pull/281>)\\n\\n### Development Build: 5.0.1\\n\\n- Minor updates (see <https://github.com/nasa/osal/pull/264>)\\n\\n### **_OFFICIAL RELEASE: 5.0.0 - Aquila_**\\n\\n- Changes are detailed in [cFS repo](https://github.com/nasa/cFS) release 6.7.0 documentation\\n- Released under the Apache 2.0 license\\n\\n### **_OFFICIAL RELEASE: 4.2.1a_**\\n\\n- Released under the [NOSA license](https://github.com/nasa/osal/blob/osal-4.2.1a/LICENSE)\\n- See [version description document](https://github.com/nasa/osal/blob/osal-4.2.1a/OSAL%204.2.1.0%20Version%20Description%20Document.pdf)\\n- This is a point release from an internal repository\\n\\n# Quick Start:\\n\\nTypically OSAL is built and tested as part of cFS as detailed in: [cFS repo](https://github.com/nasa/cFS)\\n\\nOSAL library build pc-linux example (from the base osal directory):\\n```\\nmkdir build_osal\\ncd build_osal\\ncmake -DOSAL_SYSTEM_BSPTYPE=generic-linux ..\\nmake\\n```\\n\\nOSAL permissive build with tests example (see also [CI](https://github.com/nasa/osal/blob/master/.travis.yml))\\n```\\nmkdir build_osal_test\\ncd build_osal_test\\ncmake -DENABLE_UNIT_TESTS=true -DOSAL_SYSTEM_BSPTYPE=generic-linux -DOSAL_CONFIG_DEBUG_PERMISSIVE_MODE=TRUE ..\\nmake\\nmake test\\n```\\n\\nSee the [Configuration Guide](https://github.com/nasa/osal/blob/master/doc/OSAL-Configuration-guide.pdf) for more information.\\n\\nSee also the autogenerated user\\'s guide: <https://github.com/nasa/cFS/blob/gh-pages/OSAL_Users_Guide.pdf>\\n\\n## Known issues\\n\\nSee all open issues and closed to milestones later than this version.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/cmr-stac',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': \"# CMR STAC API Proxy\\n\\nAn implementation of the [SpatioTemporal Asset Catalog API](https://github.com/radiantearth/stac-spec) on top of NASA's [Common Metadata Repository](https://cmr.earthdata.nasa.gov/search/).\\n\\nDeployed at [https://cmr.earthdata.nasa.gov/stac/](https://cmr.earthdata.nasa.gov/stac/)\\n\\nThere is more detailed documentation in the [docs](docs/readme.md) folder of this repository.\\n\\n## Development Quick Start\\n\\n### Prerequisites\\n\\n* `node.js`\\n* `nvm`: node version manager. Installing via nvm is the best way, see the appropriate version of node.js in `.nvmrc`.\\n* AWS CLI\\n\\n### Setup\\n\\nThis application is a service that supports the STAC proxy. It is organized as an NPM module and will install all dependencies if you run the following command:\\n\\n```bash\\n# checks for and installs if missiong the necessary version of node.js\\nnvm install\\nnpm install\\n```\\n\\n### Running locally\\n\\n```bash\\nnpm start\\n```\\n\\nThis will run the process in the current terminal session. Via browser or another terminal session you can find the entrypoint to the STAC collections:\\n\\n```\\nopen http://localhost:3000/dev/stac\\n```\\n\\n### Deploying\\n\\n- cd `search`\\n- `npm run deploy -- --stage <sit|uat|prod> --cmr-search-host <cmr-search-host> --cmr-search-protocol <http|https>`\\n\\n## License\\n\\nThe full license can be found [here](./LICENSE.txt)\\n\"},\n",
       " {'repo': 'nasa/fprime',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# F´: A Flight-Proven, Multi-Platform, Open-Source Flight Software Framework\\n\\n[![Language grade: C++](https://img.shields.io/lgtm/grade/cpp/g/nasa/fprime.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/fprime/context:cpp)\\n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/nasa/fprime.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/fprime/context:python)\\n[![Language grade: JavaScript](https://img.shields.io/lgtm/grade/javascript/g/nasa/fprime.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/fprime/context:javascript)\\n\\n**F´ (F Prime)** is a component-driven framework that enables rapid development and deployment of spaceflight and other embedded software applications. Originally developed at the [Jet Propulsion Laboratory](https://www.jpl.nasa.gov/), F´ has been successfully deployed on several space applications. It is tailored but not limited to small-scale spaceflight systems such as CubeSats, SmallSats, and instruments.\\n\\n**Please Visit the F´ Website:** [https://nasa.github.io/fprime/](https://nasa.github.io/fprime/).  This website contains project information, user guides, documentation, tutorials, and more!\\n\\nF´ comprises several elements: \\n\\n* An architecture that decomposes flight software into discrete components with well-defined interfaces\\n* A C++ framework that provides core capabilities such as message queues and threads\\n* Modeling tools for specifying components and connections and automatically generating code\\n* A growing collection of ready-to-use components\\n* Testing tools for testing flight software at the unit and integration levels.\\n\\n## Quick Installation Guide\\n\\nThe following utilities are prerequisites to installing F´:\\n\\n- [cmake](https://cmake.org/)\\n- [git](https://git-scm.com/)\\n- [Python](https://www.python.org/) 3.6+ with pip\\n\\nOnce these utilities are installed, you can install F´ Python dependencies. Installing dependencies in a Python virtual environment prevents issues at the system level, but installing in a virtual environment is not required. \\n\\nTo install F´ quickly, enter:\\n\\n```\\ngit clone https://github.com/nasa/fprime.git\\ncd fprime\\npip install --upgrade wheel setuptools pip\\npip install Fw/Python Gds/\\n```\\n\\nFor full installation instructions, including virtual environment creation and installation verification, see [INSTALL.md](./docs/INSTALL.md). \\n\\n## Example Deployments\\n\\nF´ comes with two example deployments. The deployments represent working F´ applications to help you understand F´. You can use these examples for reference, or clone them to start a new project. \\n\\nThe next section links to more step-by-step tutorials, but it\\'s a good idea to build and run at least the first example deployment to ensure that F´ is installed correctly.\\n\\n**Example one:** [Ref](./Ref/README.md)\\n\\n   The standard reference application demonstrates how most of the system components should be wired together. The reference application can build on Linux or macOS, allowing you to get started immediately without the need for embedded hardware.\\n\\n**Example two:** [RPI](./RPI/README.md)\\n\\n\\nThis Raspberry PI application shows how to run F´ in an embedded context by running on the Raspberry PI (a $35 embedded Linux computer). This application shows you how to get started on embedded projects with cross-compiling, drivers, and more. The Raspberry Pi was chosen because it is commercially available for a low price and runs Linux.\\n\\n## Tutorials\\n\\nF´ provides several tutorials in order to help understand and develop within the framework. These tutorials cover basic component creation, system and topology design, tooling, and more. These tutorials are available at [docs/Tutorials/README.md](./docs/Tutorials/README.md).\\n\\n## Getting Help with F´\\n\\nAs F´ becomes a community centered product line, there are more items available from the community at large. \\n\\nYou can join the mailing list at [https://groups.google.com/d/forum/fprime-community](https://groups.google.com/g/fprime-community).\\n\\nThe F´ community GitHub Organization contains third party contributions, more documentation of flight software development, and more! [https://github.com/fprime-community](https://github.com/fprime-community).\\n\\nYou can open issues with this repository at: [https://github.com/nasa/fprime/issues](https://github.com/nasa/fprime/issues)\\n\\n## F´ Features\\n\\nF´ has the following key features that enable robust embedded system design.\\n\\n### Reusability\\n\\nF´\\'s component-based architecture enables a high degree of modularity and software reusability.\\n\\n### Rapid Deployment\\n\\nF´ provides a complete development ecosystem, including modeling tools, testing tools, and a ground data system. Developers use the modeling tools to write high-level specifications, automatically generate implementations in C++, and fill in the implementations with domain-specific code. The framework and the code generators provide all the boilerplate code required in an F´ deployment, including code for thread management, code for communication between components, and code for handling commands, telemetry, and parameters. The testing tools and the ground data system simplify software testing, both on workstations and on flight hardware in the lab.\\n\\n### Portability\\n\\nF´ runs on a wide range of processors, from microcontrollers to multicore computers, and on several operating systems. Porting F´ to new operating systems is straightforward.\\n\\n### High Performance\\n\\nF´ utilizes a point-to-point architecture. The architecture minimizes the use of computational resources and is well suited for smaller processors.\\n\\t\\n### Adaptability\\n\\nF´ is tailored to the level of complexity required for small missions. This makes F´ accessible and easy to use while still supporting a wide variety of missions.\\n\\n### Analyzability\\n\\nThe typed port connections provide strong compile-time guarantees of correctness.\\n\\n## F´ Release Notes\\n\\n#### Release 1.0: \\n\\n * This is the initial release of the software to open source. See the license file for terms of use.\\n\\n#### Release 1.01\\n\\n * Updated contributor list. No code changes. \\n\\n#### Release 1.1\\n\\n * Created a Raspberry Pi demo. Read about it [here](RPI/README.md)\\n * Added a tutorial [here](docs/Tutorials/README.md)\\n * Updated Svc/BufferManager with bug fix\\n * Fixed a bunch of shell permissions\\n \\n#### Release 1.2\\n\\n* Better MagicDraw Plugin\\n* Prototype CMake build system. See: [CMake Documentation](./docs/UsersGuide/cmake/cmake-intro.md)\\n* Mars Helicopter Project fixes migrated in\\n* Python 3 support added\\n* Gse refactored and renamed to Gds\\n* Wx frontend to Gds\\n* UdpSender and UdpReceiver components added\\n* Purged inaccurate ITAR and Copyright notices\\n* Misc. bug fixes\\n\\n#### Release 1.3\\n\\n* New prototype HTML GUI\\n* Python packages Fw/Python and Gds\\n* Refined CMake and fprime-util helper script\\n* Better ground interface component\\n* Integration test API\\n* Baremetal components\\n\\n#### Release 1.4\\n\\n* Ref app no longer hangs on Linux exit\\n* GDS improvements:\\n  * File Uplink and Downlink implemented\\n  * GDS supports multiple active windows\\n  * Usability improvements for EVRs and commands\\n* CMake improvements:\\n  * Baremetal compilation supported\\n  * Random rebuilding fixed\\n  * Missing Cheetah templates properly rebuild\\n  * Separate projects supported without additional tweaks\\n* Updated MemAllocator to have:\\n  * \"recoverable\" flag to indicate if memory was recoverable across boots\\n  * size variable is now modifiable by allocator to indicate actual size\\n  * This will break existing code that uses MemAllocator\\n* Updated CmdSequencer\\n  * Uses new MemAllocator interface  \\n\\n#### Release 1.5\\n\\n* Documentation improvements\\n  * New user\\'s guide containing considerable content: [https://nasa.github.io/fprime/UsersGuide/guide.html](https://nasa.github.io/fprime/UsersGuide/guide.html)\\n  * Auto-generated API documentation\\n  * Rewrites, edits, improvements across the board\\n* F´ Project restructuring\\n  * Projects may now link to F´ and F´ library packages, without needing to keep the framework code in the same source tree\\n  * Usage of framework can be out-of-source\\n  * `settings.ini` Introduced\\n  * Example: [https://github.com/fprime-community/fprime-arduino](https://github.com/fprime-community/fprime-arduino)\\n* Refactored `fprim-util`\\n  * Replaced redundant targets with flags e.g. build-ut is now build --ut\\n  * Added `info` command\\n  * Bug and usability fixes\\n* GDS Improvements\\n  * Prototype GDS CLI tool\\n  * Project custom dashboard support\\n* Array, Enum type support and examples\\n* Code linting and bug fixes\\n\\n'},\n",
       " {'repo': 'nasa/cumulus-dashboard',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Cumulus Dashboard\\n\\nCode to generate and deploy the dashboard for the Cumulus API.\\n\\n## Documentation\\n\\n- [Configuration](#configuration)\\n- [Quick start](#quick-start)\\n- [Dashboard development](#dashboard-development)\\n- [Run the dashboard](#run-the-dashboard)\\n- [Deployment](#deployment)\\n- [Testing](#testing)\\n- [Create a Dashboard Release](#create-a-dashboard-release)\\n\\n\\nOther pages:\\n- [Usage](https://github.com/nasa/cumulus-dashboard/blob/master/USAGE.md)\\n- [Development Guide](https://github.com/nasa/cumulus-dashboard/blob/master/DEVELOPMENT.md)\\n- [Technical documentation on tables](https://github.com/nasa/cumulus-dashboard/blob/master/TABLES.md)\\n\\n## Configuration\\n\\nThe dashboard is populated from data retrieved from the Cumulus API. The environment for the Cumulus API must be predetermined and set before the dashboard can be built and deployed. The information needed to configure the dashboard is found in `app/src/js/config/config.js`, but it is generally preferred to set environmental variables overriding the default values during the build process.\\n\\nThe following environment variables override the default values.\\n\\n| Env Name | Description | Default |\\n| -------- | ----------- | -------- |\\n| APIROOT | The API URL. This must be set by the user. | *example.com* |\\n| AUTH_METHOD | The type of authorization method protecting the Cumulus API. [launchpad or earthdata] | *earthdata*  |\\n| AWS\\\\_REGION | Region in which Cumulus API is running. | *us-west-2*  |\\n| DAAC\\\\_NAME | An identifier: e.g. LPDAAC, | *Local* |\\n| ENABLE\\\\_RECOVERY | If true, adds recovery options to the granule and collection pages. | *false* |\\n| ESROOT | \\\\<optional\\\\> Should point to an Elasticsearch endpoint. Must be set for distribution metrics to be displayed. | |\\n| ES\\\\_PASSWORD | \\\\<optional\\\\> Elasticsearch password,needed when protected by basic authorization | |\\n| ES\\\\_USER | \\\\<optional\\\\> Elasticsearch username, needed when protected by basic authorization | |\\n| HIDE\\\\_PDR | Whether to hide (or show) the PDR menu. | *true* |\\n| KIBANAROOT | \\\\<optional\\\\> Should point to a Kibana endpoint. Must be set to examine distribution metrics details. | |\\n| LABELS | Choose `gitc` or `daac` localization. | *daac* |\\n| SHOW\\\\_DISTRIBUTION\\\\_API\\\\_METRICS | \\\\<optional\\\\> Display metrics from Cumulus Distribution API.| *false* |\\n| SHOW\\\\_TEA\\\\_METRICS | \\\\<optional\\\\> Display metrics from Thin Egress Application (TEA). | *true* |\\n| STAGE | Identifier displayed at top of dashboard page: e.g. PROD, UAT | *development* |\\n\\n## Quick start\\n\\n### Get dashboard source code\\nThe dashboard source is available on github and can be cloned with git.\\n\\n```bash\\n  $ git clone https://github.com/nasa/cumulus-dashboard\\n```\\nThe cloned directory `./cumulus-dashboard` will be refered as the root directory of the project and commands that are referenced in this document, should start from that directory.\\n\\n### Build the dashboard using Docker and Docker Compose\\n\\nIt is easy to build a producution-ready, deployable version of the Cumulus dashboard without having to learn the complicated build process details.  A single script, `./bin/build_dashboard_via_docker.sh`, when combined with your dashboard\\'s environment customizations, allows you to run the entire build process within a Docker container.\\n\\nAll of the environment variables in the [configuration](#configuration) section are available to override with custom values for your dashboard.  A recommended method is to store your variables in a sourceable environment file for each dashboard you are going to build and deploy.\\n\\nIf you are using bash, export the values for each configuration option. An example `production.env` could look like:\\n```sh\\n# production.env\\nexport APIROOT=https://afakeidentifier.cloudfront.net\\nexport DAAC_NAME=MY-DAAC\\nexport STAGE=production\\nexport HIDE_PDR=false\\n```\\n\\nAll values are optional except `APIROOT` which must point to the Cumulus API that the dashboard will connect to.\\n\\nSet the environment and build the dashboard with these commands:\\n```sh\\n  $ source production.env && ./bin/build_dashboard_via_docker.sh\\n```\\n\\nThis script uses Docker Compose to build and copy the a compiled dashboard into the `./dist` directory. You can now deploy this directory to AWS behind [CloudFront](https://aws.amazon.com/cloudfront/).  If you are in NGAP, follow the instructions for \"Request Public or Protected Access to the APIs and Dashboard\" on the earthdata wiki page [Using Cumulus with Private APIs](https://wiki.earthdata.nasa.gov/display/CUMULUS/Cumulus+Deployments+in+NGAP).\\n\\n\\n### Run the dashboard locally via Docker Image\\n\\nYou can also create a Docker container that will serve the dashboard behind a simple nginx configuration. Having a runnable Docker image is useful for testing a build before deployment or for NGAP Sandbox environments, where if you configure your computer to [access Cumulus APIs via SSM](https://wiki.earthdata.nasa.gov/display/CUMULUS/Accessing+Cumulus+APIs+via+SSM), you can run the dashboard container locally against the live Sandbox Cumulus API.\\n\\nThe script `./bin/build_dashboard_image.sh` will build a docker image containing the dashboard bundle served behind a basic [nginx](https://www.nginx.com/) configuration. The script takes one optional parameter, the tag to name the generated image which defaults to cumulus-dashboard:latest.  The same customizations as described in the [previous section](#build-the-dashboard-using-docker-and-docker-compose) are available to configure your dashboard.\\n\\nExample of building and running the project in Docker:\\n```bash\\n  $ source production.env && ./bin/build_dashboard_image.sh cumulus-dashboard:production-1\\n```\\n\\nThat command builds a Docker image with the name `cumulus-dashboard` and tag `production-1`. This image can be run in Docker to serve the Dashboard.\\n\\n```bash\\n  $ docker run --rm -it -p 3000:80 cumulus-dashboard:production-1\\n```\\nIn this example, the dashboard would be available at `http://localhost:3000/` in any browser.\\n\\n--------\\n\\n## Dashboard Development\\n\\n### Build the dashboard\\n\\nThe dashboard uses node v12.18.0. To build/run the dashboard on your local machine, install [nvm](https://github.com/creationix/nvm) and run `nvm install v12.18.0`.\\n\\n#### install requirements\\nWe use npm for local package management, to install the requirements:\\n```bash\\n  $ nvm use\\n  $ npm install\\n```\\n\\nTo build a dashboard bundle<sup>[1](#bundlefootnote)</sup>:\\n\\n```bash\\n  $ nvm use\\n  $ [SERVED_BY_CUMULUS_API=true] [DAAC_NAME=LPDAAC] [STAGE=production] [HIDE_PDR=false] [LABELS=daac] APIROOT=https://myapi.com npm run build\\n```\\n**NOTE**: Only the `APIROOT` environment variable is required and any of the environment varaibles currently set are passed to the build.\\n\\nThe compiled dashboard files (dashboard bundle) will be placed in the `./dist` directory.\\n\\n#### Build dashboard to be served by CloudFront\\n\\nIf you wish to serve the dashboard from behind [CloudFront](https://aws.amazon.com/cloudfront/).  Build a `dist` with your configuration including `APIROOT` and ensure the `SERVED_BY_CUMULUS_API` variable is unset. For NGAP uers, follow the documentation to *Request Public or Protected Access to the APIs and Dashboard*, Step 5 of [Cumulus Deployments in NGAP](https://wiki.earthdata.nasa.gov/display/CUMULUS/Cumulus+Deployments+in+NGAP).\\n\\n#### Build dashboard to be served by the Cumulus API.\\n\\nIt is possible to [serve the dashboard](https://nasa.github.io/cumulus-api/#serve-the-dashboard-from-a-bucket) with the Cumulus API. If you need to do this, you must build the dashboard with the environment variable `SERVED_BY_CUMULUS_API` set to `true`.  This configures the dashboard to work from the Cumulus `dashboard` endpoint.  This option should **only** be considered when you can\\'t serve the dashboard from behind CloudFront, for example in an NGAP Sandbox environment. *NOTE: Your dashboard bucket must be in the bucket definitions in your Cumulus `terraform.tfvars`, otherwise you will not be able to access the bucket from the API.*\\n\\n\\n#### Build a specific dashboard version\\n\\n`cumulus-dashboard` versions are distributed using tags in GitHub. You can select specific version in the following manner:\\n\\n```bash\\n  $ git clone https://github.com/nasa/cumulus-dashboard\\n  $ cd cumulus-dashboard\\n  $ git fetch origin ${tagNumber}:refs/tags/${tagNumber}\\n  $ git checkout ${tagNumber}\\n```\\n\\nThen follow the steps noted above to build the [dashboard locally](#build-the-dashboard) or [using Docker](#quick-start).\\n\\nIt is also possible to visit the repository at https://github.com/nasa/cumulus-dashboard/releases and download the source code bundle directly without cloning the repository.\\n\\n## Run the dashboard\\n\\n### Run the dashboard with hot reload\\nDuring development you can run the webpack development webserver to serve the dashboard while you are developing. When you run the dashboard this way, the compiled code in `./dist` is ignored, and the source code is served by the webpack-dev-server, which will watch for changes to the source and recompile as files are changed. Make sure you have [installed the requirements](#install-requirements) and then:\\n\\n```bash\\nAPIROOT=http://<myapi>.com npm run serve\\n```\\nThe dashboard should be available at http://localhost:3000\\n\\n### Run a built dashboard\\n\\nTo run a built dashboard, first [build the dashboard](#build-the-dashboard), then run:\\n\\n```bash\\n  $ npm run serve:prod\\n```\\nThis runs a node http-server in front of whatever exists in the `./dist` directory.  It\\'s fast, but will not pick up any changes as you are working.\\n\\n## Deployment\\n\\n### Using S3\\n\\nFirst, [build the dasbboard](#build-the-dashboard). Then deploy the `./dist` folder, the dashboard bundle, to an AWS bucket.\\n\\n```bash\\n  $ aws s3 sync dist s3://my-bucket-to-be-used\\n```\\nIf you are not in an NGAP environment, Look at the instructions for [Hosting a static website on Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html)\\nand [configuring a bucket as a static website](https://docs.aws.amazon.com/AmazonS3/latest/dev/HowDoIWebsiteConfiguration.html).\\n\\nOtherwise, follow the instructions for building and deploying the dashboard for [cloudfront](#build-dashboard-to-be-served-by-cloudfront) or [the Cumulus API](#build-dashboard-to-be-served-by-the-cumulus-api).\\n\\n\\n## Testing\\n\\n### Unit Tests\\n\\n```bash\\n  $ npm run test\\n```\\n\\n### Integration & Validation Tests\\n\\nFor the integration tests to work, you have to first run the localstack application, launch the localAPI and serve the dashboard first. Run the following commands in separate terminal sessions:\\n\\nRun background localstack application.\\n```bash\\n  $ npm run start-localstack\\n```\\n\\nServe the cumulus API (separate terminal)\\n```bash\\n  $ npm run serve-api\\n```\\n\\nServe the dashboard web application (another terminal)\\n```bash\\n  $ [HIDE_PDR=false SHOW_DISTRIBUTION_API_METRICS=true ENABLE_RECOVERY=true ESROOT=http://example.com APIROOT=http://localhost:5001] npm run serve\\n```\\n\\nIf you\\'re just testing dashboard code, you can generally run all of the above commands as a single docker-compose stack.\\n```bash\\n  $ npm run start-dashboard\\n```\\nThis brings up LocalStack, Elasticsearch, the Cumulus localAPI, and the dashboard.\\n\\nRun the test suite (yet another terminal window)\\n```bash\\n  $ npm run cypress\\n```\\n\\nWhen the cypress editor opens, click on `run all specs`.\\n\\n\\n### Local API server\\n\\nFor **development** and **testing** purposes only, you can run a Cumulus API locally. This requires `docker-compose` in order to stand up the Docker containers that serve Cumulus API.  There are a number of commands that will stand up different portions of the stack.  See the [Docker Service Diagram](#dockerdiagram) and examine the `docker-compose*.yml` file in the `/localAPI/` directory to see all of the possible combinations. Described below are each of the provided commands for running the dashboard and Cumulus API locally.\\n\\n*Important Note: These `docker-compose` commands do not build distributable containers, but are a provided as testing conveniences.  The docker-compose[-\\\\*].yml files show that they work by linking your local directories into the container.*\\n\\nIn order to run the Cumulus API locally you must first [build the dashboard](#buildlocally) and then run the containers that provide LocalStack and Elasticsearch services.\\n\\nThese are started and stopped with the commands:\\n```bash\\n  $ npm run start-localstack\\n  $ npm run stop-localstack\\n```\\n\\nAfter these containers are running, you can start a cumulus API locally in a terminal window `npm run serve-api`, the dashboard in another window. `[HIDE_PDR=false SHOW_DISTRIBUTION_API_METRICS=true ESROOT=http://example.com APIROOT=http://localhost:5001] npm run serve` and finally cypress in a third window. `npm run cypress`.\\n\\nOnce the Docker app is running, If you would like to see sample data you can seed the database. This will load the same sample data into the application that is used during cypress testing.\\n```bash\\n  $ npm run seed-database\\n```\\n\\nIf you prefer to stand up more of the stack in Docker containers, you can include the cumulus api in the docker-compose stack. To run the Cumulus API in a Docker container, (which still leaves running the dashboard and cypress up to you), just run the `cumulusapi` service.\\n\\nThe cumulusapi Docker service is started and stopped:\\n```bash\\n  $ npm run start-cumulusapi\\n  $ npm run stop-cumulusapi\\n```\\n\\nthe start command, will exit successfully long before the stack is actually ready to run.\\nThe output looks like this:\\n```bash\\n> cumulus-dashboard@2.0.0 start-cumulusapi /Users/savoie/projects/cumulus/cumulus-dashboard\\n> docker-compose -f ./localAPI/docker-compose.yml -f ./localAPI/docker-compose-serve-api.yml up -d\\n\\nCreating localapi_shim_1 ... done\\nCreating localapi_elasticsearch_1 ... done\\nCreating localapi_localstack_1    ... done\\nCreating localapi_serve_api_1     ... done\\n```\\nIn order to find out that the stack is fully up and ready to receive requests, you can run the command `npm run view-docker-logs` to follow the progress of the stack.  When the Docker logs have shown the following:\\n```bash\\nserve_api_1      | Starting server on port 5001\\n```\\nand\\n```bash\\nlocalstack_1     | Ready.\\n```\\nyou should be able to verify access to the local Cumulus API at http://localhost:5001/token\\n\\n\\nThen you can run the dashboard locally (without Docker) `[HIDE_PDR=false SHOW_DISTRIBUTION_API_METRICS=true ESROOT=http://example.com APIROOT=http://localhost:5001] npm run serve` and open cypress tests `npm run cypress`.\\n\\nThe Docker compose stack also includes a command to let a developer start all development containers with a single command.\\n\\nBring up and down the entire stack (the localAPI and the dashboard) with:\\n```bash\\n  $ npm run start-dashboard\\n  $ npm run stop-dashboard\\n```\\nThis runs everything, the backing Localstack and Elasticsearch containers, the local Cumulus API and dashboard.  Edits to your code will be reflected in the running dashboard.  You can run cypress tests still with `npm run cypress`.  As a warning, this command takes a very long time to start up because the containers come up in a specific order and generally this should be reserved for use by Earthdata Bamboo or some other continuous intergration service.  But, if you are using it locally, **be sure to wait until all containers are fully up** before trying to visit the dashboard which is exposed at http://localhost:3000\\nThe stack is ready when the `view-docker-logs` task shows:\\n```bash\\ndashboard_1      | > NODE_ENV=production http-server dist -p 3000 --proxy http://localhost:3000?\\ndashboard_1      |\\ndashboard_1      | Starting up http-server, serving dist\\ndashboard_1      | Available on:\\ndashboard_1      |   http://127.0.0.1:3000\\ndashboard_1      |   http://172.18.0.2:3000\\ndashboard_1      | Unhandled requests will be served from: http://localhost:3000?\\ndashboard_1      | Hit CTRL-C to stop the server\\n```\\n\\n\\n##### Troubleshooting Docker containers.\\n\\nIf something is not running correctly, or you\\'re just interested, you can view the logs with a helper script, this will print out logs from each of the running docker containers.\\n```bash\\n  $ npm run view-docker-logs\\n```\\nThis can be helpful in debugging problems with the docker application.\\n\\nA common error is running the dashboard containers when the cumulus core unit-test-stack is running on your machine.  Just stop that stack and restart the dashboard stack to resolve.\\n```sh\\nERROR: for localapi_shim_1  Cannot start service shim: driver failed programming external connectivity on endpoint localapi_shim_1 (7105603a4ff7fbb6f92211086f617bfab45d78cff47232793d152a244eb16feb): Bind for 0.0.0.0:9200 failed: port is already allocated\\n\\nERROR: for shim  Cannot start service shim: driver failed programming external connectivity on endpoint localapi_shim_1 (7105603a4ff7fbb6f92211086f617bfab45d78cff47232793d152a244eb16feb): Bind for 0.0.0.0:9200 failed: port is already allocated\\n```\\n\\n#### Fully contained cypress testing.\\n\\nYou can run all of the cypress tests locally that Earthdata Bamboo runs with a single command:\\n```bash\\n  $ npm run e2e-tests\\n```\\nThis stands up the entire stack as well as begins the e2e service that will run all cypress commands and report an exit code for their success or failure.  This is primarily used for CI, but can be useful to developers.\\n\\n\\n#### <a name=dockerdiagram></a> Docker Container Service Diagram.\\n![Docker Service Diagram](./ancillary/DashboardDockerServices.png)\\n\\n\\n## develop vs. master branches\\n\\nThe `master` branch is the branch where the source code of HEAD always reflects the latest product release. The `develop` branch is the branch where the source code of HEAD always reflects the latest merged development changes for the next release.  The `develop` branch is the branch where we should branch off.\\n\\nWhen the source code in the develop branch reaches a stable point and is ready to be released, all of the changes should be merged back into master and then tagged with a release number.\\n\\n## Create a Dashboard Release\\n\\n### 1. Checkout `develop` branch\\n\\nWe will make changes in the `develop` branch.\\n\\n### 2. Create a new branch for the release\\n\\nCreate a new branch off of the `develop` branch for the release named `release-vX.X.X` (e.g. `release-v1.3.0`).\\n\\n### 3. Update the version number\\n\\nWhen changes are ready to be released, the version number must be updated in `package.json`.\\n\\n### 4. Update the minimum version of Cumulus API if necessary\\n\\nSee the `minCompatibleApiVersion` value in `app/src/js/config/index.js`.\\n\\n### 5. Update CHANGELOG.md\\n\\nUpdate the CHANGELOG.md. Put a header under the \\'Unreleased\\' section with the new version number and the date.\\n\\nAdd a link reference for the GitHub \"compare\" view at the bottom of the CHANGELOG.md, following the existing pattern. This link reference should create a link in the CHANGELOG\\'s release header to changes in the corresponding release.\\n\\nCheck to make sure there are `Breaking Changes` and `All Changes` section for the release if there are breaking changes. e.g. a new version of Cumulus API is required.\\n\\n### 6. Update the version of the Cumulus API\\n\\nIf this release corresponds to a Cumulus Core package release, update the version of `@cumulus/api` to the latest package version so that the integration tests will run against that version.\\n\\n### 7. Manual testing\\n\\nTest the dashboard against a live API deployed with the latest Cumulus packages. The dashboard should be served from an S3 bucket through the [`/dashboard` API endpoint](https://nasa.github.io/cumulus-api/#serve-the-dashboard-from-a-bucket).\\n\\n### 8. Create a pull request against the develop branch\\n\\nCreate a PR for the `release-vX.X.X` branch against the `develop` branch. Verify that the Earthdata Bamboo CI build for the PR succeeds and then merge to `develop`.\\n\\n### 9. Create a pull request against the master branch\\n\\nCreate a PR for the `develop` branch against the `master` branch. Verify that the Earthdata Bamboo CI build for the PR succeeds and then merge to `master`.  Do not create a squash merge, but use a merge commit.\\n\\n### 10. Create a git tag for the release\\n\\nPush a new release tag to Github. The tag should be in the format `v1.2.3`, where `1.2.3` is the new version.\\n\\nCreate and push a new git tag:\\n\\n```bash\\n  $ git checkout master\\n  $ git tag -a v1.x.x -m \"Release 1.x.x\"\\n  $ git push origin v1.x.x\\n```\\n\\n### 11. Add the release to GitHub\\n\\nFollow the [Github documentation to create a new release](https://help.github.com/articles/creating-releases/) for the dashboard using the tag that you just pushed. Make sure to use the content from the CHANGELOG for this release as the description of the release on GitHub.\\n\\n### 12. Create PR of master back into develop\\n\\nCreate a PR for the `master` branch back into `develop` to bring the merge commit back into develop.\\n\\nIt is likely that no branch plan will exist for the `master` branch.\\n#### Create a bamboo branch plan for the release\\n - In the Cumulus Dashboard in Bamboo (<https://ci.earthdata.nasa.gov/browse/CUM-CDG>), click `Actions -> Configure Plan` in the top right.\\n - Next to `Plan branch` click the rightmost button that displays `Create Plan Branch` upon hover.\\n - Click `Create plan branch manually`.\\n - Choose Branch Name `master` and then click `create`.\\n - Verify that the build has started for this plan.\\n\\n\\n\\n\\n\\n<a name=\"bundlefootnote\">1</a>: A dashboard bundle is just a ready-to-deploy compiled version of the dashboard and environment.\\n'},\n",
       " {'repo': 'nasa/gunns',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '[![Unit Tests](https://github.com/nasa/gunns/actions/workflows/unit_test.yml/badge.svg)](https://github.com/nasa/gunns/actions/workflows/unit_test.yml) [![Test Trick Sim](https://github.com/nasa/gunns/actions/workflows/sim_test.yml/badge.svg)](https://github.com/nasa/gunns/actions/workflows/sim_test.yml)\\n\\n# General-Use Nodal Network Solver (GUNNS)\\n\\nGUNNS is a set of generic C++ math models that combines nodal circuit analysis algorithms with the hydraulic-thermal-electric analogy to model flow circuits (fluid, thermal, electrical).  The models include generic physical components like pumps, valves, resistors & capacitors.  These components, called \\'links\\', are connected to \\'nodes\\' and configured with their characteristic values, creating a \\'network\\' (circuit) of links and nodes.  The state of the network and its contained models are then propagated in the time domain.\\n\\n### GunnsDraw, the Network Design GUI\\n\\nNetworks can be drawn up with a [Draw.io](https://www.draw.io) drawing, from which run-time C++ code is auto-generated that can be built into simulations.  This is called \\'GunnsDraw\\'.  GunnsDraw includes custom Draw.io shapes for the generic physical components and Python scripts to auto-generate the run-time C++ code.\\n  \\n### Extensible\\n\\nGUNNS is designed to be extensible and flexible.  It is written with object-oriented polymorphic interfaces between the network solver, links, nodes, and other extensible object types.  This allows users to develop new classes to model things that the GUNNS baseline doesn\\'t cover.  Users can create custom GunnsDraw shapes by taking advantage of Draw.io shapes\\' extensible XML schema.  \\n  \\nGUNNS can also be extended to model other domains besides the above flow systems, as it can solve systems of the general form [A]{x} = {b} where [A] is a symmetric positive semi-definite matrix.  This is demonstrated in an included simple 6-DOF rigid body dynamics equations of motion model.\\n\\n### Simulation Environment Not Included\\n\\nGUNNS does not provide a complete simulation environment, as it is intended to be run in external environments.  GUNNS is optimized for the [NASA Trick](https://github.com/nasa/trick) simulation environment, but can be run in other environments with some additional work.\\n\\n# The Gunnsmiths\\n\\nGUNNS is managed by the [Simulation & Robotics Division, Simulation & Graphics Branch](https://www.nasa.gov/centers/johnson/engineering/robotics_simulation/index.html) at NASA Johnson Space Center.\\n\\nGUNNS is developed by [CACI International Inc.](https://www.caci.com) under contract to NASA.  Contact info for our team can be found in the wiki.\\n\\n<p align=left>\\n<img src=\"https://raw.github.com/nasa/gunns/master/ER7_logo.png\" alt=\"ER7 Logo\" height=75px>\\n<img src=\"https://raw.github.com/nasa/gunns/master/CACI_International_logo.png\" alt=\"CACI Logo\" height=75px>\\n</p>\\n\\n# Wiki\\n\\nSee the [wiki](https://github.com/nasa/gunns/wiki) for more information including tutorials, user & developer guides.\\n\\n# License\\n\\nThis software is released under the NASA Open Source Agreement Version 1.3 [license](https://github.com/nasa/gunns/blob/master/LICENSE).\\n\\n# Notices\\n\\nCopyright © 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\n# Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/mmt',\n",
       "  'language': 'Ruby',\n",
       "  'readme_contents': '# Metadata Management Tool Application\\nThe Metadata Management Tool (MMT) and Draft Metadata Management Tool (dMMT) are web applications designed to assist users in managing metadata and interfacing with the CMR. The user’s guide for MMT can be found [here](https://wiki.earthdata.nasa.gov/display/ED/Metadata+Management+Tool+%28MMT%29+User%27s+Guide \"MMT User Guide\") and the user’s guide for dMMT can be found [here](https://wiki.earthdata.nasa.gov/display/ED/Draft+MMT+%28dMMT%29+User%27s+Guide \"dMMT User Guide\"). Release notes for these applications can be found [here](https://wiki.earthdata.nasa.gov/display/ED/MMT+Release+Notes \"Release Notes\").\\n\\n## Getting Started\\n\\n### Requirements\\n - Ruby 2.7.2\\n\\n### Setup\\nClone the Metadata Management Tool Git project:\\n\\n    git clone https://github.com/nasa/mmt.git\\n\\nType the following command to install the necessary components:\\n\\n    bundle install\\n\\nDepending on your version of Ruby, you may need to install ruby rdoc/ri data:\\n\\n    <= 1.8.6 : unsupported\\n     = 1.8.7 : gem install rdoc-data; rdoc-data --install\\n     = 1.9.1 : gem install rdoc-data; rdoc-data --install\\n    >= 1.9.2 : you\\'re good to go!\\n\\n#### Additional Install Steps\\nSome operating systems may require additional steps.\\n\\nMac OS X 10.14.6 moved some required libraries around which has been known to cause nokogiri to not install, if you have errors with that gem, you may need to run the following:\\n\\n    open /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg\\n\\nDetails can be found on nokogiri\\'s [site](https://nokogiri.org/tutorials/installing_nokogiri.html#macos).\\n\\nThe libxml gem has also historically caused difficulty because it is a native library. If you are having issues installing libxml-ruby (cannot find libxml.h), you may need to configure it with the location of your libxml2 directory. You can do a:\\n\\n    find / -name xmlversion.h\\n\\nwhich may return something like the following:\\n\\n    /Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/libxml2/libxml/xmlversion.h\\n\\nthen you can run `bundle config` with the location of libxml2 returned from the find command as in:\\n\\n    bundle config build.libxml-ruby --with-xml2-include=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/libxml2\\n\\nYou should then be able to run `bundle install` as normal afterwards.\\n\\n#### Database\\n\\nCheck your `/config/` directory for a `database.yml` file (with no additional extensions). If you do not have one, duplicate* the `database.yml.example` file and then rename it to `database.yml`.\\n\\n*_Note: Do not simply rename the `database.yml.example` file as it is being tracked in Git and has its own history._\\n\\nNext, create your database by running the standard rails command:\\n\\n    rake db:create\\n\\nAnd then to migrate the database schema, run the standard rails command:\\n\\n    rake db:migrate\\n\\n#### Other Steps\\n\\nFinally, create an `application.yml` file in your `/config/` directory. The contents of this file will be supplied by an MMT developer\\n\\n### Usage\\n\\n*_Note: Before running this step, make sure you are **Running a local copy of CMR** as outlined below_\\n\\n*_Note: If you want to run on http://localhost:3000 and just use Earthdata Login, you may need to modify entries in the `application.yml` file. Replace the \\'urs...url\\' entries from \\'https://mmt.localtest.earthdata.nasa.gov\\' to \\'http://localhost:3000\\'_\\n\\n*_Note: With Launchpad Integration, you will need to set up MMT to run locally with HTTPS. Please see `/doc/local_https_setup.md` for options and instructions_\\n\\nTo start the project, just type the default rails command:\\n\\n    rails s\\n\\nIf you need to stop the server from running, hit `Ctrl + C` and the server will shutdown.\\n\\n### Running a local copy of CMR\\nIn order to use a local copy of the CMR you will need to download the latest file, set an environment variable, and run a rake task to set required permissions and ingest some data.\\n\\n#### 1. Downloading the CMR file\\nIf access to https://maven.earthdata.nasa.gov is possible, then the rake command `rake cmr:fetch` can be used to download the latest CMR jar. This task put the jar file in the `cmr` directory.\\nIf this task fails for some reason, such as the maven repository is down, you can follow the instructions below to download and install manually from Bamboo:\\n\\nGo to https://ci.earthdata.nasa.gov/browse/CN2-CSN2/latestSuccessful/artifact/, and download the `cmr-dev-system-uberjar.jar` file.\\n  * Note: It will rename itself to `cmr-dev-system-0.1.0-SNAPSHOT-standalone.jar`. This is the correct behavior. **DO NOT rename the file.**\\n\\nIn your root directory for MMT, create a folder named `cmr`. Place the `cmr-dev-system-0.1.0-SNAPSHOT-standalone.jar` file in the `cmr` folder.\\n\\n#### 2. Setting the environment variable needed by the local CMR\\nBefore running a local copy of the CMR, you will need to set a required environment variable. Add this line into your `.bash_profile`:\\n\\n    export CMR_URS_PASSWORD=mock-urs-password\\n\\nAfter adding the line and saving the file, don\\'t forget to source the file\\n\\n    source ~/.bash_profile\\n\\n#### 3. Setting up local redis for CMR\\nCMR comes with redis in the jar, but it is not compiled to run on Macs.  If you need to run the CMR on a Mac, download it from\\n\\n    https://redis.io/\\n\\nCMR does not appear to be making significant configuration changes to redis, so a positive response from executing these commands in redis\\'s root directory:\\n\\n    make\\n    make test\\n\\nshould be sufficient to run CMR locally.  Run this command before starting CMR each session:\\n\\n    path/to/redis/src/redis-server\\n\\nThe option \\'--daemonize yes\\' runs the server in the background.\\n\\nAlternatively, you can install Redis with homebrew\\n\\nThe basics are\\n\\n    brew update\\n    brew install redis\\n    brew services start redis\\n\\nFor more information, see one of these links\\n\\n    https://www.devglan.com/blog/install-redis-windows-and-mac\\n    https://gist.github.com/tomysmile/1b8a321e7c58499ef9f9441b2faa0aa8\\n\\n\\n\\n#### 4. Running the CMR rake tasks\\nTo start the local CMR and load data*:\\n\\n    rake cmr:start_and_load\\n\\nAfter you see \"Done!\", you can load the app in your browser and use the local CMR. After you have started CMR, to just reload the data:\\n\\n    rake cmr:load\\n\\nTo stop the locally running CMR, run this command:\\n\\n    rake cmr:stop\\n\\nYou will need to stop the CMR before upgrading to a new CMR version. Note: stopping the running CMR for any reason will delete all data from the CMR. You will have to load the data again when you start it.\\n\\n## Inserting Sample Drafts\\n\\nYou can insert sample drafts into your local database. These commands use the first user in the database (there should only be one), and add the drafts to your current provider, so make sure you login to the system and select a provider or the commands will fail.\\n\\nTo insert a sample draft that only has the required fields present:\\n\\n    rake drafts:load_required\\n\\nTo insert a sample draft with every field completed:\\n\\n    rake drafts:load_full\\n\\n## Troubleshooting\\n\\n### OpenSSL Issue\\n\\n* If you receive a error from running `rake cmr:start_and_load` like\\n\\n    Faraday::ConnectionFailed: SSL_connect returned=1 errno=0 state=SSLv3 read server certificate B: certificate verify failed\\n\\nTry the following steps:\\n\\n1. Ensure you are using RubyGems 2.0.3 or newer by typing `gem -v`. If it is older, type `gem update --system` to upgrade the RubyGems system.\\n\\n2. Update the SSL certificates by running the following commands\\n\\n    * `brew update`\\n    * `brew install openssl`\\n    * `brew link openssl --force`\\n\\n3. Restart your terminal to refresh the OpenSSL version.\\n\\n4. Check to ensure that OpenSSL version is 1.0.2 or newer with the command `openssl version`\\n\\n5. Try running `rake cmr:start` and `rake cmr:load` as instructed above. If you still have issues, continue with these instructions below:\\n\\n6. Uninstall Ruby 2.2.2. If you are using rvm, use the command `rvm remove 2.2.2`\\n\\n7. Find out where your OpenSSL directory is by typing `which openssl`. An example directory you might get would be `/usr/local/bin/openssl`\\n\\n8. Reinstall Ruby with the following command (if you are using rvm): `rvm install 2.2.2 --with-open-ssl-dir={DIRECTORY FROM STEP 7}`.\\n\\n    * Using the example directory from above, it would be `rvm install 2.2.2 --with-open-ssl-dir=/usr/local/bin/openssl`.\\n\\n9. Run `bundle install` to install any missing gems.\\n\\n    * If your terminal tells you that it does not recognize the `bundle` command, run `gem install bundler`\\n\\n9. Restart your terminal to refresh all settings.\\n\\n10. Navigate to MMT directory and check to make sure Ruby and OpenSSL version are correct.\\n\\n11. Run `rake cmr:start` and `rake cmr:load` again. If you still have issues, please reach out to a developer to help with troubleshooting.\\n\\n### Earthdata Login Issue\\n\\n* If you receive an error when logging into MMT using Earthdata Login such as\\n\\n    JSON::ParserError at /urs_login_callback\\n    784: unexpected token at \\'null\\'\\n\\nCheck your cmr.log file. It may show some errors and you need to restart your local copy of cmr.\\n\\n### UMM JSON-Schema\\n\\nYou can view/download the latest UMM JSON-Schema here, https://git.earthdata.nasa.gov/projects/CMR/repos/cmr/browse/umm-spec-lib/resources/json-schemas\\n\\n## Local Testing\\n\\n#### JavaScript\\nMMT uses PhantomJS which allows us to run our Capybara tests on a headless WebKit browser. Before you\\'re able to run tests locally you\\'ll need to install it. The easiest way to accomplish this would be to use [Homebrew](http://brew.sh/) or a similar packager manager. If you\\'re using Homebrew, run the following the command:\\n\\n    brew install phantomjs\\n\\n#### VCR\\nMMT uses [VCR](https://github.com/vcr/vcr) to record non-localhost HTTP interactions, it is configured in [spec/support/vcr.rb](spec/support/vcr.rb).\\n\\nAll calls to localhost are ignored by VCR and therefore will not be recorded.\\n\\nThis isn\\'t an issue normally but with MMT we run a number of services locally while developing that we would like to be recorded.\\n\\n#### CMR\\n\\nFor calls to CMR that are asynchronous, we do have a method of waiting for those to finish, synchronously. Within the [spec/helpers/cmr_helper.rb](spec/helpers/cmr_helper.rb) we have a method called `wait_for_cmr` that makes two calls to CMR and ElasticSearch to ensure all work is complete. This should ONLY be used within tests.\\n\\n## ACLs\\nAccess Control Lists (ACLs, aka Permissions) determine access to data and functionality in the CMR. See the [Access Control Documentation](https://cmr.earthdata.nasa.gov/access-control/site/docs/access-control/api.html) for technical information.\\n\\n### Testing against ACLs\\nWhen testing functionality in the browser that requires specific permissions you\\'ll need to ensure your environment is setup properly and you\\'re able to assign yourself the permissions necessary. This includes:\\n\\n1. Creating a Group\\n2. Add your URS account and the user \\'typical\\' as a member of the group\\n3. Ensuring the group created has appropriate Provider Context, Group, and Provider Object ACLs permissions.\\n\\nThis provides access to the Provider Object Permissions pages.\\n\\n4. Give your URS account and the user \\'typical\\' access to the Administrators_2 group\\n\\nThis gives you permission to view System Level Groups and the System Object Permissions pages.\\n\\nFrom here you\\'ll need to modify appropriate permissions of the group so that you can test functionality associated with any of the permissions via the group show page, or the Provider or System Object Permissions pages.\\n\\n##### Automating ACL Group Management\\nTo run the above steps automatically there is a provided rake task to do the heavy lifting.\\n\\n    rake acls:testing:prepare[URS_USERNAME]\\n\\nReplacing URS_USERNAME with your own username. An example:\\n\\n    $ rake acls:testing:prepare[username]\\n    [Success] Added username to MMT_2 Admin Group\\n    [Success] Added username to Administrators_2\\n\\nThen you can manage the Provider Level permissions by clicking on the group on the Provider Object Permissions page or by clicking on the Provider Object Permissions for MMT_2 link on the group show page. If System Level permissions are required, you can click on the Administrators_2 group from the System Object Permissions page or click on the System Object Permissions link from the group show page.\\n\\nAlternatively, if only one of provider level access or system level access is required, you can use the more specific rake task:\\n\\n    rake acls:groups:mmt_2_users[username]\\n\\nor\\n\\n    rake acls:groups:admins[username]\\n\\n### Draft MMT\\nThe Draft MMT is intended for Non-NASA Users to propose new metadata records or changes to existing records in the CMR.  There are several steps required to run a local version of Draft MMT.\\n\\n1. Enable https connections to the Draft MMT.     See the directions for configuring https [here](doc/local_https_setup.md)\\n\\n2. Configure MMT to use the https connection.  In your application.yml file, make sure that `urs_login_callback_url`   is set to `https://mmt.localtest.earthdata.nasa.gov/urs_login_callback_url`.  \\n\\n3. Create ACLs to give yourself permission to use Draft MMT. Access to the Draft MMT is controlled by the Non-NASA Draft User and Non-NASA Draft Approver ACLs. There is a rake task that will create the group and assign the ACL for you (make sure you use your own username):\\n\\n\\n    $ rake acls:proposal_mode:draft_user[URS_USERNAME]\\n\\nor\\n\\n    $ rake acls:proposal_mode:draft_approver[URS_USERNAME]\\n\\n  * make sure you use your own username\\n  ***NOTE: Make sure that `proposal_mode` is set to \\'false\\' in your `application.yml` file when you run this rake task. If you see `NotAllowedError: A requested action is not allowed in the current configuration.` when running this rake task, you missed this step.***\\n\\n4. Change the app to the Draft MMT (aka proposal mode) by changing the `proposal_mode` environment variable in your `application.yml` file. Set `proposal_mode` to `true`.\\n\\n\\n5. Start the MMT app as usual with `bin/rails server -p 3000`  \\n\\n6. Direct your browser to https://mmt.localtest.earthdata.nasa.gov .   Note that some browsers will give you a warning about the self-signed certificate that was created in step 1.  In that case,  use the browser controls to allow the certificate.\\n\\n7. To return to normal MMT mode,  set `proposal_mode` to `false` in the application.yml file and restart the app.\\n\\n### Replicating SIT Collections Locally\\nOften we need collections to exist in our local CMR that already exist in SIT for the purposes of sending collection ids (concept ids) as part of a payload to the ECHO API that doesn\\'t run locally, but instead on testbed. In order to do this the collection concept ids have to match those on SIT so we cannot simply download and ingest them. A rake task exists to replicate collections locally for this purpose.\\n\\n    $ rake collections:replicate\\n\\nThe task accepts two parameters\\n\\n- **provider:** The provider id to replicate collections for *default: MMT_2*\\n- **page_size:** The number of collections to request *default: 25*\\n\\n##### Examples\\n\\n    $ rake collections:replicate[MMT_1,10]\\n\\nWill download at most 10 collections from MMT_1.\\n\\n    $ rake collections:replicate[SEDAC]\\n\\nWill download at most 25 collections from SEDAC.\\n\\n**NOTE** Some providers have permissions set on their collections and make require a token to view/download collections. You can set an ENV variable named\\n\\n    CMR_SIT_TOKEN\\n\\nthat if set, will be provided to CMR when downloading collections. This variable is set by adding the following line to your **~/.bash_profile**\\n\\n    export CMR_SIT_TOKEN=\"\"\\n\\nAfter adding the line and saving the file, don\\'t forget to source the file.\\n\\n    source ~/.bash_profile\\n'},\n",
       " {'repo': 'nasa/edsc-echoforms',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Earthdata Search Components:  ECHO Forms\\n\\n[![npm version](https://badge.fury.io/js/%40edsc%2Fechoforms.svg)](https://badge.fury.io/js/%40edsc%2Fechoforms)\\n![Build Status](https://github.com/nasa/edsc-echoforms/workflows/CI/badge.svg?branch=master)\\n[![codecov](https://codecov.io/gh/nasa/edsc-echoforms/branch/master/graph/badge.svg?token=4d8wFDtAc0)](https://codecov.io/gh/nasa/edsc-echoforms)\\n\\nTry out the [online demo](http://nasa.github.io/edsc-echoforms/)\\n\\nA React component implementing the\\n[ECHO Forms](https://earthdata.nasa.gov/files/ECHO_Forms_Specification_0.pdf)\\nspecification. For a basic usage example and a testbed for changes,\\nsee `example/src`.\\n\\nThe ECHO Forms component was developed as a component of\\n[Earthdata Search](https://github.com/nasa/earthdata-search).\\n\\nFor the jQuery version of this plugin see [this branch](https://github.com/nasa/edsc-echoforms/tree/jquery-plugin).\\n\\n## Installation\\n\\n    npm install @edsc/echoforms\\n\\n## Usage\\n\\nAfter installing you can use the component in your code.\\n\\n```javascript\\nimport EDSCEchoform from \\'@edsc/echoforms\\'\\n\\nconst Component = () => {\\n  return (\\n    <EDSCEchoform\\n      form={formXml}\\n      onFormModelUpdated={onFormModelUpdated}\\n      onFormIsValidUpdated={onFormIsValidUpdated}\\n    />\\n  )\\n}\\n```\\n\\n### Props\\n\\n| Prop | Type | Required | Default Value | Description\\n| ---- |:----:|:--------:|:-------------:| -----------\\naddBootstrapClasses | Boolean | false | false | Adds Bootstrap class names to elements. Bootstrap is **not** included in this package.\\nform | String | true | | ECHO Forms XML string.\\nhasShapefile | Boolean | false | false | Is a shapefile included in the search parameters. This is used to display help text about shapefile processing to users on shapefile form fields.\\nprepopulateValues | Object | false | | Values used to prepopulate fields through the form\\'s `pre:prepopulate` extensions.\\nonFormModelUpdated | Function | true | | Callback function that returns `{ model, rawModel }`. `model` is the data model pruned of irrelevant fields. `rawModel` is the full data model.\\nonFormIsValidUpdated | Function | true | | Callback function that returns a Boolean value of the form\\'s isValid property.\\n\\n## Development\\n\\nTo compile:\\n\\n    npm install\\n\\nTo start the example project for local testing:\\n\\n    npm start\\n\\nTo run the tests:\\n\\n    npm test\\n\\n## Contributing\\n\\nSee CONTRIBUTING.md\\n\\n## License\\n\\n> Copyright © 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n>\\n> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\n> You may obtain a copy of the License at\\n>\\n>    http://www.apache.org/licenses/LICENSE-2.0\\n>\\n>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\n>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n'},\n",
       " {'repo': 'nasa/cumulus-distribution-api',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '## Cumulus Distribution API Documentation\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-distribution-api.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-distribution-api)\\n\\nCumulus Distribution API documentaion: https://nasa.github.io/cumulus-distribution-api\\n\\n### Installation\\n\\n     $ npm install\\n\\n### Build\\n\\n     $ npm run build\\n\\n### Serve\\n\\n     $ npm run serve\\n\\n### Deploy\\n\\n     $ npm run deploy\\n'},\n",
       " {'repo': 'nasa/Open-Source-Catalog',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Open-Source-Catalog\\n\\n[![Build Status](https://travis-ci.org/nasa/Open-Source-Catalog.svg?branch=master)](https://travis-ci.org/nasa/Open-Source-Catalog)\\n\\n## About\\n\\nThis GitHub repository is maintained by the [NASA OCIO Open Innovation Team](http://open.nasa.gov/about/) and contains a catalog of publicly available NASA open source projects that are published on [code.nasa.gov](http://code.nasa.gov). The catalog is persisted as a JSON file ```catalog.json``` and contains an array of projects.  As Code Sharing at NASA is a community effort, we encourage NASA developers to add a meta-record in to this catalog to publish their open source projects on [code.nasa.gov](http://code.nasa.gov/).\\n\\n## Requirements\\n* Open Source software project approved for open source release by your [NASA Field Center SRA](http://code.nasa.gov/#/guide)\\n* Code Project hosted in a code repository (preferably GitHub.com) and visible to Internet Users\\n* Meta record of your software project; instantiate ```required_fields_project_template.json```\\n\\n## Add/Edit your project\\n### OPTION 1\\nIf you are behind the NASA firewall, we recommend you use the online form located [here](https://developer.nasa.gov/pages/OpenInnovation/code-submission-app/)\\n### OPTION 2\\nCreate a project meta-record using the template from file required_fields_project_template.json:\\n  * Note that Category labels longer than 24 characters will be truncated.\\n```\\n{\\n    \"NASA Center\": \"Ames Research Center\",\\n    \"Contributors\": [\\n      \"jasonduley\"\\n    ],\\n    \"Software\": \"My Software Project\",\\n    \"External Link\": \"https://github.com/nasa/my-software-project/wiki\",\\n    \"Public Code Repo\": \"https://github.com/nasa/my-software-project\",\\n    \"Description\": \"This is a description of the software project.\",\\n    \"License\": [\\n      \"NASA Open Source\"\\n    ],\\n    \"Categories\": [\\n      \"Framework\",\\n      \"Toolkit\",\\n      \"Web\"\\n    ],\\n    \"Update_Date\": \"2014-09-23\",\\n    \"Labor_Hours\": 24\\n}\\n```\\n\\n* Add your instantiated meta-record to the array in the catalog.json file via a pull request\\n* Once the merge is complete, your project will be published on [code.nasa.gov](http://code.nasa.gov/)\\n\\n## Thanks\\nSpecial thanks goes out to [Chris Mattmann (NASA JPL)](https://github.com/chrismattmann), Sean Kelly (NASA JPL) and [Eric Whyne (DARPA)](https://github.com/ericwhyne) for their inspiration for this effort.\\n'},\n",
       " {'repo': 'nasa/delta',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"**DELTA** (Deep Earth Learning, Tools, and Analysis) is a framework for deep learning on satellite imagery,\\nbased on Tensorflow. DELTA classifies large satellite images with neural networks, automatically handling\\ntiling large imagery.\\n\\nDELTA is currently under active development by the\\n[NASA Ames Intelligent Robotics Group](https://ti.arc.nasa.gov/tech/asr/groups/intelligent-robotics/).\\nInitially, it is mapping floods for disaster response, in collaboration with the\\n[U.S. Geological Survey](http://www.usgs.gov), [National Geospatial Intelligence Agency](https://www.nga.mil/),\\n[National Center for Supercomputing Applications](http://www.ncsa.illinois.edu/), and\\n[University of Alabama](https://www.ua.edu/).\\n\\nInstallation\\n============\\n\\n1. Install [python3](https://www.python.org/downloads/), [GDAL](https://gdal.org/download.html),\\n   and the [GDAL python bindings](https://pypi.org/project/GDAL/). For Ubuntu Linux, you can run\\n   `scripts/setup.sh` from the DELTA repository to install these dependencies.\\n\\n2. Install Tensorflow following the [instructions](https://www.tensorflow.org/install). For\\n   GPU support in DELTA (highly recommended) follow the directions in the\\n   [GPU guide](https://www.tensorflow.org/install/gpu).\\n\\n3. Checkout the delta repository and install with pip:\\n\\n```bash\\ngit clone http://github.com/nasa/delta\\npython3 -m pip install delta\\n```\\n\\nDELTA is now installed and ready to use!\\n\\nDocumentation\\n=============\\nDELTA can be used either as a command line tool or as a python library.\\nSee the python documentation for the master branch [here](https://nasa.github.io/delta/),\\nor generate the documentation with `scripts/docs.sh`.\\n\\nExample\\n=======\\n\\nAs a simple example, consider training a neural network to map clouds with Landsat-8 images.\\nThe script `scripts/example/l8_cloud.sh` trains such a network using DELTA from the\\n[USGS SPARCS dataset](https://www.usgs.gov/core-science-systems/nli/landsat/spatial-procedures-automated-removal-cloud-and-shadow-sparcs),\\nand shows how DELTA can be used. The steps involved in this, and other, classification processes are:\\n\\n1. **Collect** training data. The SPARCS dataset contains Landsat-8 imagery with and without clouds.\\n\\n2. **Label** training data. The SPARCS labels classify each pixel according to cloud, land, water and other classes.\\n\\n3. **Train** the neural network. The script `scripts/example/l8_cloud.sh` invokes the command\\n\\n    ```\\n    delta train --config l8_cloud.yaml l8_clouds.h5\\n    ```\\n\\n    where `scripts/example/l8_cloud.yaml` is a configuration file specifying the labeled training data and\\n    training parameters (learn more about configuration files below). A neural network file\\n    `l8_clouds.h5` is output.\\n\\n4. **Classify** with the trained network. The script runs\\n\\n    ```\\n    delta classify --config l8_cloud.yaml --image-dir ./validate --overlap 32 l8_clouds.h5\\n    ```\\n\\n    to classify the images in the `validate` folder using the network `l8_clouds.h5` learned previously.\\n    The overlap tiles to ignore border regions when possible to make a more aesthetically pleasing classified\\n    image. The command outputs a predicted image and confusion matrix.\\n\\nThe results could be improved--- with more training, more data, an improved network, or more--- but this\\nexample shows the basic usage of DETLA.\\n\\nConfiguration and Extensions\\n============================\\n\\nDELTA provides many options for customizing data inputs and training. All options are configured via\\nYAML files. Some options can be overwritten with command line options (use\\n`delta --help` to see which). See the `delta.config` README to learn about available configuration\\noptions.\\n\\nDELTA can be extended to support custom neural network layers, image types, preprocessing operations, metrics, losses,\\nand training callbacks. Learn about DELTA extensions in the `delta.config.extensions` documentation.\\n\\nData Management\\n=============\\n\\nDELTA integrates with [MLFlow](http://mlflow.org) to track training. MLFlow options can\\nbe specified in the corresponding area of the configuration file. By default, training and\\nvalidation metrics are logged, along with all configuration parameters. The most recent neural\\nnetwork is saved to a file when the training program is interrupted or completes.\\n\\nView all the logged training information through mlflow by running::\\n\\n```\\n  delta mlflow_ui\\n```\\n\\nand navigating to the printed URL in a browser. This makes it easier to keep track when running\\nexperiments and adjusting parameters.\\n\\nContributors\\n============\\nWe welcome pull requests to contribute to DELTA. However, due to NASA legal restrictions, we must require\\nthat all contributors sign and submit a\\n[NASA Individual Contributor License Agreement](https://www.nasa.gov/sites/default/files/atoms/files/astrobee_individual_contributor_license_agreement.pdf).\\nYou can scan the document and submit via email. Thank you for your understanding.\\n\\nImportant notes for developers:\\n\\n * **Branching**: Active development occurs on `develop`. Releases are pushed to `master`.\\n\\n * **Code Style**: Code must pass our linter before merging. Run `scripts/linter/install_linter.sh` to install\\n   the linter as a git pre-commit hook.\\n\\n * **Unit Tests**: Code must pass unit tests before merging. Run `pytest` in the `tests` directory to run the tests.\\n   Please add new unit tests as appropriate.\\n\\n * **Development Setup**: You can install delta using pip's `-e` flag which installs in editable mode. Then you can\\n   run `delta` and it will use your latest changes made to the repo without reinstalling.\\n\\nLicensing\\n=========\\nDELTA is released under the Apache 2 license.\\n\\nCopyright (c) 2020, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.\\n\"},\n",
       " {'repo': 'nasa/isle',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': 'ISLE - Inventory System for Lab Equipment\\n==========\\n\\n![Demo](https://cloud.githubusercontent.com/assets/1322063/14293284/efe856d2-fb39-11e5-9765-7605555b06f8.gif)\\n\\n## Created By\\n[Brandon Ruffridge](https://github.com/bruffridge)  \\n[Brent Gardner](https://github.com/bggardner)\\n\\n## Brief Abstract\\nThis web application allows inventories of assets to be managed. Assets along with their specifications are added to the system and then used by users via a check-in/check-out function. The inventory can be browsed by category or using search. Users are given various roles within the system to allow them to perform certain functions such as view-only, check-in/check-out, modify, and full-control. Inventory managers can add and track detailed information on all inventory assets including manufacturer, storage location, custom attributes, and relationships with other assets. Assets can be found by browsing by category, search, location, or current user. Assets are checked out to specified locations by users.\\n\\n## Description of the Problem That Motivated ISLE\\'s Development\\nOne of our labs at the NASA Glenn Research Center wanted a way to track their inventory of over 350 pieces of equipment, who is using it, and where it is located. They also wanted to give lab users a way to see what equipment is available and see detailed specs on the equipment and check it out for use with their projects. This web based tool was developed to meet that objective.\\n\\n## Technical Description\\n\\nDeveloped using web standards and best practices such as Model-View-Controller architecture, Separation of Concerns, and Don\\'t Repeat Yourself.  \\nFast, intuitive UI featuring a custom application layout built using parts from Twitter Bootstrap, extensive AJAX and jQuery, and combined and minified Javascript and LESS CSS.  \\nSecure and 508 compliant.  \\nFeatures an innovative built-in bug reporting system to Pivotal Tracker.  \\nDeployed on the mature, open-source Linux, Apache, MySQL, and PHP (LAMP) technology stack.\\n\\n## Get Started\\n\\n* Have a Mac or Linux Box\\n  * Windows can run ISLE but can\\'t run the bash scripts for [building static files](#building-static-files) or [syncing database for multiple developer teams](#keeping-database-in-sync-for-multiple-developer-teams)\\n* Have a webserver running PHP and MySQL.\\n  * You can also use the included Vagrant LAMP Box to deploy and run the application locally by following the steps below.\\n* Download and Install [Vagrant][1]\\n* Download and Install [VirtualBox][2]\\n* Clone ISLE ```git clone https://github.com/nasa/isle.git```\\n* Run ``` vagrant up ```\\n* Access Your Project at  [http://192.168.33.10/myinstance][3] or [http://192.168.33.10/myinstance2][11]\\n\\n## Configuration\\n\\nSearch source code for ```config-todo:``` for things you may need to configure.  \\n**NOTE:** ISLE was modified to be easy to install and configure in a local development environment. Additional configuration steps would be needed to run ISLE in a secure production environment such as changing database credentials and moving them into a separate file and adding that file to .gitignore so the credentials don\\'t go into source control.\\n\\n## How to Contribute\\n\\n[Check out our backlog](https://www.pivotaltracker.com/n/projects/1569431) of the things we want to add/fix. Fork the project, make your changes, test ( we don\\'t have time to test for you ), then submit a pull request. Submit any new bugs or feature requests to the [issues page](https://github.com/nasa/isle/issues).\\n\\n## Multiple Inventories\\n\\nISLE supports multiple \"instances\" so multiple inventories can be managed separately. Each instance has a unique url, but accesses the same php files and database. Data is kept separate by using different tables.  \\nThe ```instances``` folder contains two example instances ```myinstance``` and ```myinstance2```.  \\n### It would be nice to have a bash script to automate creation of additional instances, however it is currently a manual process.  \\nTo create additional instances duplicate the ```instances/myinstance``` folder and rename to whatever you want to call your instance. Delete the .log files in ```logs```. Then replace ```myinstance``` with whatever instance name you chose in all files within the duplicated folder. Also rename ```isle.local.myinstance.conf``` to ```isle.local.INSERT_YOUR_INSTANCE_NAME.conf```.  \\nDuplicate ```webroot/myinstance```. Delete any files in ```uploads``` except the .htaccess files.  \\nEdit ```isle-init.sh``` and copy and paste the following lines for running sql and enabling conf files. Replace ```myinstance``` with whatever instance name you chose.\\n\\n```bash\\nmysql -uroot -p\\'root\\' -h localhost isle_dev < \"/var/www/instances/myinstance/init.sql\"\\nmysql -uroot -p\\'root\\' -h localhost isle_dev < \"/var/www/instances/myinstance/data.sql\"\\n\\ncp /var/www/instances/myinstance/isle.local.myinstance.conf /etc/apache2/sites-available/isle.local.myinstance.conf\\na2ensite isle.local.myinstance\\n\\ncat <<EOT >> /etc/logrotate.d/isle-myinstance\\n/var/www/instances/myinstance/logs/*.log {\\n        yearly\\n        maxsize 2M\\n        rotate 5\\n        notifempty\\n        missingok\\n        su vagrant vagrant\\n}\\nEOT\\n```\\n\\nThen run the following so the changes take effect.\\n\\n```bash\\nvagrant destroy\\nvagrant up\\n```  \\n\\n## Building Static Files\\n\\nWhen you want to make changes to CSS or JS the files you want to edit are located in:  \\n**JS:** ```webroot/isle/cdn/scripts-dev```  \\n**CSS:** ```webroot/isle/cdn/styles/less```\\n\\nDon\\'t edit files in ```scripts``` or ```css-dev``` as those are created during the build process.\\n\\nBuild CSS and JS (combines and minifies)\\n* Make sure you have lessc installed.\\n* ``` cd PROJECT_FOLDER/webroot/isle/includes``` (this step is important or the build files will not be saved to the correct location)\\n* ```./build.sh```\\n\\n## Keeping Database in Sync for Multiple Developer Teams\\n\\nChange Workflow:  \\n* Make changes.\\n* ```./dbup.sh``` (option 1)\\n* git add, git commit, git push.\\n\\nUpdate Workflow:  \\n* git pull\\n* ```./dbup.sh``` (option 2)\\n\\n## Basic Vagrant Commands\\n\\n\\n### Start or resume your server\\n```bash\\nvagrant up\\n```\\n\\n### Pause your server\\n```bash\\nvagrant suspend\\n```\\n\\n### Delete your server\\n```bash\\nvagrant destroy\\n```\\n\\n### SSH into your server\\n```bash\\nvagrant ssh\\n```\\n\\n\\n\\n## Database Access\\n\\n### MySQL \\n\\n- Hostname: localhost or 127.0.0.1\\n- Username: root\\n- Password: root\\n- Database: isle_dev\\n\\n## Updating the Box\\n\\nAlthough not necessary, if you want to check for updates, just type:\\n\\n```bash\\nvagrant box outdated\\n```\\n\\nIt will tell you if you are running the latest version or not, of the box. If it says you aren\\'t, simply run:\\n\\n```bash\\nvagrant box update\\n```\\n\\n\\n## Setting a Hostname\\n\\nIf you\\'re like me, you prefer to develop at a domain name versus an IP address. If you want to get rid of the some-what ugly IP address, just add a record like the following example to your computer\\'s host file.\\n\\n```bash\\n192.168.33.10 isle.local\\n```\\n\\nOr if you want \"www\" to work as well, do:\\n\\n```bash\\n192.168.33.10 isle.local www.isle.local\\n```\\n\\nTechnically you could also use a Vagrant Plugin like [Vagrant Hostmanager][4] to automatically update your host file when you run Vagrant Up. However, the purpose of Scotch Box is to have as little dependencies as possible so that it\\'s always working when you run \"vagrant up\".\\n\\n## Special Thanks To\\n\\n* [Scotch Box][5]\\n* [Require.js][6]\\n* [Bootstrap][7]\\n* [jQuery][8]\\n* [Modernizr][9]\\n* [tag-it][10]\\n* [jquery-trap-input](https://github.com/julienw/jquery-trap-input)\\n* [jquery-dateFormat](https://github.com/phstc/jquery-dateFormat)\\n* [AppLayout](https://github.com/bruffridge/AppLayout)\\n\\n [1]: https://www.vagrantup.com/downloads.html\\n [2]: https://www.virtualbox.org/wiki/Downloads\\n [3]: http://192.168.33.10/myinstance\\n [4]: https://github.com/smdahlen/vagrant-hostmanager\\n [5]: https://box.scotch.io/\\n [6]: http://requirejs.org/\\n [7]: http://getbootstrap.com/\\n [8]: https://jquery.com/\\n [9]: https://modernizr.com/\\n [10]: https://github.com/aehlke/tag-it\\n [11]: http://192.168.33.10/myinstance2\\n'},\n",
       " {'repo': 'nasa/openmct-map',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Open MCT Map\\n\\n## v0.3.0 TODO:\\n1. Retrieve timestamp from point.\\n2. Implement time based baselayers.\\n\\n## Known Bugs\\n1. Changing time system will will probably break realtime data until view is reloaded.\\n\\nA plugin for [Open MCT](https://nasa.github.io/openmct)\\nadding map style visualizations.  This plugin is experimental and not intended\\nfor production usage.\\n\\n## Usage\\n\\n\\n1. `npm install nasa/openmct-map`\\n2. include `node_modules/openmct-map/dist/openmct-map.js` and `node_modules/openmct-map/dist/openmct-map.css` in your `index.html`, or load with your favorite module loader.\\n    ```html\\n    <script src=\"node_modules/openmct-map/dist/openmct-map.js\"></script>\\n    <link rel=\"stylesheet\" href=\"node_modules/openmct-map/dist/openmct-map.css\" type=\"text/css\" media=\"screen\">\\n    ```\\n3. install plugin in OpenMCT before starting:\\n    ```javascript\\n    openmct.install(new OpenMCTMapPlugin());\\n    ```\\n\\n## Build\\n\\n```bash\\n$ npm install\\n```\\n\\nA UMD module with associated source maps will be written to the\\n`dist` folder. When installed as a global, the plugin will be\\navailable as `MapPlugin`.\\n\\n## Configuration\\n\\nThe Map Plugin exposes three new types for OpenMCT:\\n* Location Combiner: Takes two telemetry points (one for x, one for y) and returns a location telemetry object.  For testing, use one of these with two sine wave generators to get a \"location\".\\n* Measurement Location Synthesizer: Takes two telemetry points (one for location, one for measurement), and returns a location measurement telemetry object.\\n* Traverse Map: The actual map for users.\\n\\nThe traverse map has a JSON field where you can specify layers to add.\\n\\n\\n## Usage\\n\\nSee [`index.html`](index.html) for an example of use.\\n\\n## Developer Environment\\n\\nYou\\'ll need to install nasa/openmct, and then run the simple dev server.\\n\\nRollup seems to fail to detect changes in files on some systems, so you might \\nspend a lot of time restarting the dev server.\\n\\n```bash\\nnpm install nasa/openmct\\nnpm run dev\\n```\\n'},\n",
       " {'repo': 'nasa/nasapress',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': '# NASAPress (a WordPress Theme)\\n\\n![screen shot 2018-06-15 at 12 31 39 pm](https://user-images.githubusercontent.com/1322063/41479194-64b3bab2-7098-11e8-81cf-ccf93f472f61.png)\\n\\n## Sites using NASAPress\\n\\nIf you\\'d like your site to be added to this list please [create an issue](https://github.com/nasa/nasapress/issues/new) with your website name and URL.\\n\\n* [NASA Glenn Research Center](https://www1.grc.nasa.gov)\\n\\n## Features\\n\\n* Built on [Sage 9.0.0-beta.3](https://github.com/roots/sage/releases/tag/9.0.0-beta.3)\\n* Sass for stylesheets\\n* ES6 for JavaScript\\n* [Webpack](https://webpack.github.io/) for compiling assets, optimizing images, and concatenating and minifying files\\n* [Browsersync](http://www.browsersync.io/) for synchronized browser testing\\n* [Laravel\\'s Blade](https://laravel.com/docs/5.3/blade) as a templating engine\\n* [NASA Glenn Web Design System](https://nasa.github.io/nasawds-site/) based on the [U.S. Web Design System](https://designsystem.digital.gov) for CSS framework\\n* [Font Awesome](http://fontawesome.io/)\\n\\n### Required plugins\\n\\n* [Add Categories to Pages](https://wordpress.org/plugins/add-category-to-pages/)\\n* [Advanced Custom Fields](https://wordpress.org/plugins/advanced-custom-fields/)\\n\\n### Recommended plugins\\n\\n* [NASAPress Companion](https://github.com/nasa/nasapress-companion)\\n  * adds shortcodes for displaying nasa.gov news articles, spinoffs, and lists of pages.\\n* [Advanced TinyMCE Configuration](https://wordpress.org/plugins/advanced-tinymce-configuration/)\\n    * allows editors to add NASA Web Design Standards styles to elements in the visual text editor.\\n* [TinyMCE Advanced](https://wordpress.org/plugins/tinymce-advanced/)\\n* [Better WordPress External Links](https://wordpress.org/plugins/bwp-external-links/)\\n    * Plugin Settings:\\n      * Set External links\\' CSS class to `usa-external_link`\\n      * Uncheck \\'Use CSS provided by this plugin?\\'\\n* [Disable Search](https://wordpress.org/plugins/disable-search/)\\n  * if using DigitalGov Search\\n* [Responsive Lightbox](https://wordpress.org/plugins/responsive-lightbox/)\\n    * for viewing images in a lightbox.\\n* [Yet Another Related Posts Plugin](https://wordpress.org/plugins/yet-another-related-posts-plugin/)\\n* [Yoast SEO](https://wordpress.org/plugins/wordpress-seo/)\\n  * for breadcrumbs\\n* [Gravity Forms](http://www.gravityforms.com/) and [Gravity Forms Survey Add-On](http://www.gravityforms.com/add-ons/survey/)\\n  * for site feedback form and other forms.\\n* [Popup Maker](https://wordpress.org/plugins/popup-maker/)\\n  * for displaying site feedback form in a popup window.\\n* [Hide YouTube Related Videos](https://wordpress.org/plugins/hide-youtube-related-videos/)\\n* [Broken Link Checker](https://wordpress.org/plugins/broken-link-checker/)\\n\\n### Designed for use with these services\\n\\n* [Digital Analytics Program](https://www.digitalgov.gov/services/dap/)\\n* [DigitalGov Search](https://search.digitalgov.gov/)\\n\\n## Requirements\\n\\nMake sure all dependencies have been installed before moving on:\\n\\n* [WordPress](https://wordpress.org/) >= 4.7\\n* [PHP](http://php.net/manual/en/install.php) >= 5.6.4\\n* MySQL >= 5.6 or MariaDB >= 10.0\\n  * Earlier versions don\\'t support FULLTEXT index for InnoDB engine required by YARPP plugin. See [this explanation of issue](https://easyengine.io/tutorials/mysql/yarpp-innodb/).\\n* [Composer](https://getcomposer.org/download/)\\n* [Node.js](http://nodejs.org/) >= 6.9.x\\n* [Yarn](https://yarnpkg.com/en/docs/install)\\n\\n## Theme installation\\n\\nClone this repo into your WordPress themes directory.\\n\\nInstall Composer dependencies:\\n\\n```shell\\n# @ app/themes/nasapress or wp-content/themes/nasapress\\n$ composer install\\n```\\n\\nRun `yarn` from the theme directory to install dependencies. If you won\\'t be making changes to the theme\\'s static assets (css, javascript, images) then run `yarn install --production`.\\n\\nUpdate `resources/assets/config.json` settings:\\n  * `devUrl` should reflect your local development hostname\\n  * `publicPath` should reflect your WordPress folder structure (`/wp-content/themes/nasapress` for non-[Bedrock](https://roots.io/bedrock/) installs)\\n\\n## Theme setup\\n\\nSearch the theme folder for `todo-config`. These comments mark the locations where you\\'ll likely need to make customizations for your site.\\n\\n### Add top navigation\\n\\nCreate a menu and assign it to the \\'Primary Navigation\\' location.\\n\\n### Enable breadcrumbs\\n\\nInstall and activate the Yoast SEO plugin. Follow steps 1-5 [in this guide](https://kb.yoast.com/kb/implement-wordpress-seo-breadcrumbs/) to enable yoast breadcrumbs.\\n\\n### Enable related pages\\n\\nIf you want to show related pages at the bottom of pages install and activate the YARPP plugin. On the plugin settings, you might see a message about \\'consider titles\\' and \\'consider bodies\\' being disabled due to InnoDB... If you are using MySQL 5.6 or greater, expand the message and click the \\'Create FULLTEXT indices\\' button to enable them.\\n\\nUnder display options, select \\'Pages\\', then click the Custom button and make sure \\'You Might Also Like\\' is selected as the template file.\\n\\n### Add NASA Web Design Standards styles to Visual Editor\\n\\ntodo\\n\\n### Add site feedback form\\n\\ntodo\\n\\n## Using the theme\\n\\n### Page templates\\n\\n#### Home page\\n\\nAlthough not technically a template the theme expects a static front page and styles it differently than the others. Use the following as a starting point for this page.\\n```html\\n<div class=\"usa-overlay\"></div>\\n\\n<section class=\"usa-hero\">\\n  <div class=\"usa-grid\">\\n    <div class=\"usa-width-one-half\">\\n      <h1>Shaping the world of tomorrow</h1>\\n    </div>\\n  </div>\\n  <div class=\"usa-grid\">\\n    <div class=\"usa-width-one-half\">\\n      <p class=\"usa-font-lead\">By developing technologies that will enable further exploration of the universe and revolutionize air travel</p>\\n    </div>\\n  </div>\\n  <div class=\"usa-grid\">\\n    <div class=\"usa-width-two-thirds\">\\n      <div class=\"video-container\">\\n        https://www.youtube.com/watch?v=5VHPanW6F4E\\n      </div>\\n    </div>\\n  </div>\\n</section>\\n```\\n\\n#### Landing Page\\n\\nThe landing page template features a large hero image with leading paragraph followed by text. Make sure your featured image is large enough to not pixellate too much at larger screen sizes.\\n\\n#### Default template\\n\\nThe default template has no top hero section.\\n\\n### On this page navigation\\n\\nThe default and landing page templates automatically convert h2, h3, and h4 tags into left \\'in page\\' navigation. For shorter pages, this may not be desired, and can be turned off in the \"On this page\" settings on the edit page screen. In this section, you can also change which heading tags to convert to navigation.\\n\\n### Setting NASA Official\\n\\nA NASA Official can be added or changed on the edit page category screen. You can select from any users of your WordPress site.\\n\\n## Theme structure\\n\\n```shell\\nthemes/your-theme-name/   # → Root of your Sage based theme\\n├── app/                  # → Theme PHP\\n│   ├── lib/App/          # → NASAPress functions\\n│   ├── lib/Sage/         # → Blade implementation, asset manifest\\n│   ├── admin.php         # → Theme customizer setup\\n│   ├── filters.php       # → Theme filters\\n│   ├── helpers.php       # → Helper functions\\n│   └── setup.php         # → Theme setup\\n├── composer.json         # → Autoloading for `app/` files\\n├── composer.lock         # → Composer lock file (never edit)\\n├── dist/                 # → Built theme assets (never edit)\\n├── node_modules/         # → Node.js packages (never edit)\\n├── package.json          # → Node.js dependencies and scripts\\n├── resources/            # → Theme assets and templates\\n│   ├── assets/           # → Front-end assets\\n│   │   ├── config.json   # → Settings for compiled assets\\n│   │   ├── build/        # → Webpack and ESLint config\\n│   │   ├── fonts/        # → Theme fonts\\n│   │   ├── images/       # → Theme images\\n│   │   ├── scripts/      # → Theme JS\\n│   │   └── styles/       # → Theme stylesheets\\n│   ├── functions.php     # → Composer autoloader, theme includes\\n│   ├── index.php         # → Never manually edit\\n│   ├── screenshot.png    # → Theme screenshot for WP admin\\n│   ├── style.css         # → Theme meta information\\n│   └── views/            # → Theme templates\\n│       ├── layouts/      # → Base templates\\n│       └── partials/     # → Partial templates\\n└── vendor/               # → Composer packages (never edit)\\n```\\n\\n## Theme development\\n\\n### Build commands\\n\\n* `yarn run start` — Compile assets when file changes are made, start Browsersync session\\n* `yarn run build` — Compile and optimize the files in your assets directory\\n* `yarn run build:production` — Compile assets for production\\n\\n## Todos\\n\\nMake site title customizable in wp-admin.\\nMake right side of footer customizable in wp-admin.\\n'},\n",
       " {'repo': 'nasa/cumulus-message-adapter-js',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# @cumulus/cumulus-message-adapter-js\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter-js.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-message-adapter-js)\\n[![npm version](https://badge.fury.io/js/%40cumulus%2Fcumulus-message-adapter-js.svg)](https://badge.fury.io/js/%40cumulus%2Fcumulus-message-adapter-js)\\n\\n## What is Cumulus\\n\\nCumulus is a cloud-based data ingest, archive, distribution and management\\nprototype for NASA's future Earth science data streams.\\n\\nRead the [Cumulus Documentation](https://nasa.github.io/cumulus)\\n\\n## What is the Cumulus Message Adapter?\\n\\nThe Cumulus Message Adapter is a library that adapts incoming messages in the\\nCumulus protocol to a format more easily consumable by Cumulus tasks, invokes\\nthe tasks, and then adapts their response back to the Cumulus message protocol\\nto be sent to the next task.\\n\\n## Installation\\n\\nThe cumulus-message-adapter-js can be installed via Node Package Manager (NPM) and the package is located [here](https://www.npmjs.com/package/@cumulus/cumulus-message-adapter-js).\\n\\nThe package can be added to your project by running `npm install @cumulus/cumulus-message-adapter-js --save`.\\n\\n## Task definition\\n\\nIn order to use the Cumulus Message Adapter, you will need to create two\\nmethods in your task module: a handler function and a business logic function.\\n\\nThe handler function is a standard Lambda handler function which takes three\\nparameters (as specified by AWS): `event`, `context`, and `callback`.\\n\\nThe business logic function is where the actual work of your task occurs. It\\nshould take two parameters: `nestedEvent` and `context`.\\n\\nThe `nestedEvent` object contains two keys:\\n\\n* `input` - the task's input, typically the `payload` of the message,\\n    produced at runtime\\n* `config` - the task's configuration, with any templated variables\\n    resolved\\n\\nThe `context` parameter is the standard Lambda context as passed by AWS.\\n\\nThe return value of the business logic function will be placed in the\\n`payload` of the resulting Cumulus message.\\n\\nExpectations for input, config, and return values are all defined by the task,\\nand should be well documented. Tasks should thoughtfully consider their inputs\\nand return values, as breaking changes may have cascading effects on tasks\\nthroughout a workflow. Configuration changes are slightly less impactful, but\\nmust be communicated to those using the task.\\n\\n## Cumulus Message Adapter interface\\n\\nThe Cumulus Message adapter for Javascript provides one method:\\n`runCumulusTask`. It takes five parameters:\\n\\n* `taskFunction` - the function containing your business logic (as described\\n    above)\\n* `cumulusMessage` - the event passed by Lambda, and should be a Cumulus\\n    Message\\n* `context` - the Lambda context\\n* `callback` - the callback passed by Lambda\\n* `schemas` - JSON object with the locations of the task schemas\\n\\nThe `schemas` JSON should contain `input:`, `output:`, and `config:` with strings for each location. If the schema locations are not specified, the message adapter will look for schemas in a schemas directory at the root level for the files: input.json, output.json, or config.json. If the schema is not specified or missing, schema validation will not be performed.\\n\\n## Example Cumulus task\\n\\n```javascript\\nconst cumulusMessageAdapter = require('@cumulus/cumulus-message-adapter-js');\\n\\nfunction myBusinessLogic(nestedEvent, context) {\\n  console.log('Hello, example!');\\n  return { answer: 42 };\\n}\\n\\n// The handler function should rarely, if ever, contain more than this line\\nfunction handler(event, context, callback) {\\n  cumulusMessageAdapter.runCumulusTask(myBusinessLogic, event, context, callback, schemas);\\n}\\nexports.handler = handler;\\n```\\n\\n## Creating a deployment package\\n\\nTasks that use this library are just standard AWS Lambda tasks. Information on\\ncreating release packages is available [here](https://docs.aws.amazon.com/lambda/latest/dg/deployment-package-v2.html).\\n\\n## Usage in Cumulus Deployments\\n\\nFor documentation on how to utilize this package in a Cumulus Deployment, view the [Cumulus Workflow Documenation](https://nasa.github.io/cumulus/docs/workflows/input_output).\\n\\n## Environment variables\\n\\nThere are two environment variables that can be used with this library:\\n\\n* `CUMULUS_MESSAGE_ADAPTER_DISABLED=true`\\n  * Defaults to false. This env var disables Cumulus Message Adapter. This can be used to turn off the message adapter for tasks that adapt the message on their own, or for testing.\\n* `CUMULUS_MESSAGE_ADAPTER_DIR`\\n  * The default directory for Cumulus Message Adapter is the root directory of the lambda function.\\n\\n## Development\\n\\n### Running Tests\\n\\nTo run the tests for this package, run `npm run lint && npm test`\\n\\n## Why use this approach\\n\\nThis approach has a few major advantages:\\n\\n1. It explicitly prevents tasks from making assumptions about data structures\\n   like `meta` and `cumulus_meta` that are owned internally and may therefore\\n   be broken in future updates. To gain access to fields in these structures,\\n   tasks must be passed the data explicitly in the workflow configuration.\\n1. It provides clearer ownership of the various data structures. Operators own\\n   `meta`. Cumulus owns `cumulus_meta`. Tasks define their own `config`,\\n   `input`, and `output` formats.\\n1. The Cumulus Message Adapter greatly simplifies running Lambda functions not\\n   explicitly created for Cumulus.\\n1. The approach greatly simplifies testing for tasks, as tasks don't need to\\n   set up cumbersome structures to emulate the message protocol and can just\\n   test their business function.\\n\"},\n",
       " {'repo': 'nasa/pvslib',\n",
       "  'language': 'Common Lisp',\n",
       "  'readme_contents': 'NASALib\\n=\\n\\nNASALib is a continuing collaborative effort that has spanned over 3 decades,\\nto aid in research related to theorem proving  sponsored by NASA\\n(https://shemesh.larc.nasa.gov/fm/pvs/).\\nIt consists of a collection of formal development (i.e.,\\n<i>libraries</i>) written in the Prototype Verification System\\n([PVS](http://pvs.csl.sri.com)), contributed by SRI, NASA,NIA, and the PVS community, and maintained by the\\n[NASA/NIA Formal Methods Team at LaRC](http://shemesh.larc.nasa.gov/fm).\\n\\n# Release\\nThe current version of NASALib is 7.1.0 (11/05/20) and requires [PVS 7.1](http://pvs.csl.sri.com/).\\n\\n# Libraries\\n\\nCurrently, NASALib consists of 53 libraries and includes almost 30K lemmas.\\n\\n| Library  | Description | \\n| --- | --- | \\n| [ACCoRD](./ACCoRD/README.md) | Framework for the analysis of air traffic conflict detection and resolution algorithms | \\n| [affine_arith](./affine_arith/README.md) | Formalization of affine arithmetic and strategy for evaluating polynomial functions with variables on interval domains. |\\n| [algebra](./algebra/README.md) | Groups, monoids, rings, etc. |\\n| [analysis](./analysis/README.md) | Real analysis, limits, continuity, derivatives, integrals. |\\n| [ASP](./ASP/README.md) | Denotational semantics of Answer Set Programming. |\\n| [aviation](./aviation/README.md) | Support definitions and properties for aviation-related formalizations. |\\n| [Bernstein](./Bernstein/README.md) | Formalization of multivariate Bernstein polynomials. |\\n| [CCG](./CCG/README.md) | Formalization of diverse termination criteria. |\\n| [complex](./complex/README.md) | Complex numbers. |\\n| [complex_alt](./complex_alt/README.md) | Alternative formalization of complex numbers. |\\n| [complex_integration](./complex_integration/README.md) | Complex integration. |\\n| [co_structures](./co_structures/README.md) | Sequences of countable length defined as co-algebraic datatypes. |\\n| [digraphs](./digraphs/README.md) | Directed graphs: circuits, maximal subtrees, paths, DAGs. |\\n| [exact_real_arith](./exact_real_arith/README.md) | Exact real arithmetic including trig functions. |\\n| [examples](./examples/README.md) | Examples of application of the functionality provided by NASALib. |\\n| [extended_nnreal](./extended_nnreal/README.md) | Extended non-negative reals. |\\n| [fast_approx](./fast_approx/README.md) | Approximations of standard numerical functions. |\\n| [fault_tolerance](./fault_tolerance/README.md) | Fault tolerance protocols. |\\n| [float](./float/README.md) | Floating point numbers and arithmetic. |\\n| [graphs](./graphs/README.md) | Graph theory. |\\n| [groups](./groups/README.md) | Group theory. |\\n| [interval_arith](./interval_arith/README.md) | Interval arithmetic and numerical approximations. Includes automated strategies numerical for computing numerical approximations and interval for checking satisfiability and validity of simply quantified real-valued formulas. This development includes a formalization of Allen interval temporal logic. |\\n| [ints](./ints/README.md) | Integer division, gcd, mod, prime factorization, min, max. |\\n| [lebesgue](./lebesgue/README.md) | Lebesgue integral with connection to Riemann Integral. |\\n| [linear_algebra](./linear_algebra/README.md) | Linear algebra. |\\n| [lnexp](./lnexp/README.md) |  Logarithm, exponential and hyperbolic functions. & Foundational definitions of logarithm, exponential and hyperbolic functions. |\\n| [matrices](./matrices/README.md) | Executable specification of MxN matrices. This library includes computation of inverse and basic matrix operations such as addition and multiplication. |\\n| [measure_integration](./measure_integration/README.md) | Sigma algebras, measures, Fubini-Tonelli Lemmas. |\\n| [MetiTarski](./MetiTarski/README.md) | Integration of MetiTarski, an automated theorem prover for real-valued functions. |\\n| [metric_space](./metric_space/README.md) | Domains with a distance metric, continuity and uniform continuity. |\\n| [numbers](./numbers/README.md) | Elementary number theory. |\\n| [orders](./orders/README.md) | Abstract orders, lattices, fix points. |\\n| [power](./power/README.md) | Generalized Power function (without ln/exp). |\\n| [probability](./probability/README.md) | Probability theory. |\\n| [PVS0](./PVS0/README.md) | Formalization of fundamental computability concepts. |\\n| [PVSioChecker](./PVSioChecker/README.md) | Animation of PVS specifications. |\\n| [reals](./reals/README.md) | Summations, sup, inf, sqrt over the reals, absolute value, etc. |\\n| [Riemann](./Riemann/README.md) |  Riemann integral. |\\n| [scott](./scott/README.md) | Scott topology. |\\n| [series](./series/README.md) | Power series, comparison test, ratio test, Taylor\\'s theorem. |\\n| [sets_aux](./sets_aux/README.md) | Power sets, orders, cardinality over infinite sets. Includes functional and relational facts based on Axiom of Choice and refinement relations based on equivalence relations. |\\n| [shapes](./shapes/README.md) | 2D-Shapes: triangle, parallelogram, rectangle, circular segment |\\n| [sigma_set](./sigma_set/README.md) | Summations over countably infinite sets. |\\n| [sorting](./sorting/README.md) | Sorting algorithms. |\\n| [structures](./structures/README.md) | Bounded arrays, finite sequences, bags, and several other structures. |\\n| [Sturm](./Sturm/README.md) |  Formalization of Sturm\\'s theorem for univariate polynomials. Includes strategies `sturm` and `mono-poly` for automatically proving univariate polynomial relations over a real interval. |\\n| [Tarski](./Tarski/README.md) | Formalization of Tarski\\'s theorem for univariate polynomials. Includes strategy tarski for automatically proving systems of univariate polynomial relations on the real line. |\\n| [topology](./topology/README.md) | Continuity, homeomorphisms, connected and compact spaces, Borel sets/functions. |\\n| [trig](./trig/README.md) | Trigonometry: definitions, identities, approximations. |\\n| [TRS](./TRS/README.md) | Term rewrite systems and Robinson unification algorithm. |\\n| [TU_games](./TU_games/README.md) | Cooperative TU-games. |\\n| [vect_analysis](./vect_analysis/README.md) | Limits, continuity, and derivatives of vector functions. |\\n| [vectors](./vectors/README.md) | 2-D, 3-D, 4-D, and n-dimensional vectors. |\\n| [while](./while/README.md) | Semantics for the programming language While. |\\n\\n## Dependencies\\n\\nCheck the [NASALib dependency graph](docs/all-theories.svg \"Dependency Graph\").\\n\\n# Scripts\\n\\nNASALib also provides a collection of scripts that automates several tasks.\\n\\n* [`proveit`](./Scripts.md#proveit) (*) - Runs PVS in batch mode \\n* [`provethem`](./Scripts.md#provethem) (*) - Runs `proveit` on several libraries \\n* [`pvsio`](./Scripts.md#pvsio) (*) - Command-line utility to run the PVSio ground evaluator.\\n* [`prove-all`](./Scripts.md#prove-all) - Runs `proveit` on each library\\n  in NASALib by wrapping `provethem` in order to provide a specific kind of run. \\n* [`cleanbin-all`](./Scripts.md#cleanbin-all) - Clean `.pvscontext` and binary files from PVS libraries.\\n* [`find-all`](./Scripts.md#find-all) - Searches strings matching a given regular expressions in PVS libraries.\\n* [`dependencygraph`](./Scripts.md#dependencygraph) - Generates a library dependency graph for libraries in the current directory.\\n* [`dependency-all`](./Scripts.md#d#dependency-all) - Generates the dependency graphs for the PVS libraries in the current folder.\\n\\nClick [here](docs/Scripts.md) for more details on these scripts.\\n\\n(*) Already included in the PVS 7.1 distribution.\\n\\n# Getting NASALib\\n\\n## Via VSCode-PVS (recommended for new PVS users)\\n\\nNASALib (v7.0.1) is fully compatible with\\n[VSCode-PVS](http://github.com/nasa/vscode-pvs), a modern graphical\\ninterface to PVS based on\\n[Visual Studio Code](https://code.visualstudio.com). The latest\\nversion of NASALib can be installed from VSCode-PVS.\\n\\n## Development Version\\n\\nFor PVS advanced users, the development version is available from [GitHub](https://github.com/nasa/pvslib). \\nTo clone the development version, type the following command inside directory where PVS 7.0 is installed. Henceforth, that directory will be referred to as `<pvsdir>`. In the following commands, the dollar sign \\nrepresents the prompt of the operating system.\\n\\n```shell\\n$ git clone http://github.com/nasa/pvslib nasalib \\n```\\n\\nThe command above will put a copy of the library in the directory `<pvsdir>/nasalib`.\\n\\n### Major Recent Changes\\n\\n* **The library `trig_fnd` is now deprecated**. It\\'s still provided for backward compatibility, but it should be replaced by `trig`.  The new library `trig`, which used to be axiomatic, is now foundational. However, in contrast to `trig_fnd`, trigonometric definitions are based on infinite series, rather than integrals. This change considerably reduces the type-checking of theories involving trigonometric functions. The change from `trig_fnd` to `trig` should not have a major impact in your formal developments since names of definitions and lemmas are the same. However, theory importing may be slightly different.\\n\\n* The PVS developments `TCASII`, `WellClear`,  and `DAIDALUS` are now [available](https://github.com/nasa/WellClear/tree/master/PVS) as part of the [GitHub WellClear distribution](https://github.com/nasa/WellClear). The PVS development `PRECiSA`  is now [available](https://github.com/nasa/PRECiSA/tree/master/PVS) as part of the [GitHub PRECiSA distribution](https://github.com/nasa/PRECiSA). The PVS development `PolyCARP`  is now [available](https://github.com/nasa/PolyCARP/tree/master/PVS) as part of the [GitHub PolyCARP distribution](https://github.com/nasa/PolyCARP).\\n\\n\\n# Manual Installation\\n\\nThe following instructions assume that NASALib is located in the directory `<pvsdir>/nasalib`.\\n\\n## 1) Add this directory to the environment variable `PVS_LIBRARY_PATH`\\n\\nIf it does not exists, creates such variable and with the path of this directory as only content. It is usually very useful to have your shell systems creating this variable at startup. To this end, and depending upon your shell, you may want to add one of the following lines in your startup script.  For C shell (csh or tcsh), you may add this line in `~/.cshrc`:\\n```shell\\nsetenv PVS_LIBRARY_PATH \"<pvsdir>/nasalib\"\\n```\\nFor Borne shell (bash or sh), add this line in either `~/.bashrc` or `~/.profile`:\\n```shell\\nexport PVS_LIBRARY_PATH=\"<pvsdir>/nasalib\"\\n```\\n\\n## 2) Additional steps to protect previous NASALib configurations (optional)\\n\\nIf you had a previous installation of NASALib, either remove the file `~/.pvs.lisp` or, if you have a special configuration in that file, remove the following line  \\n```lisp\\n(load \"<pvsdir>/nasalib/pvs-patches.lisp\") \\n```\\n## 3) Install Scripts\\n\\nFinally, go to the directory `<pvsdir>/nasalib` and run the following shell scripts (the dollar sign represents the prompt of the operating system).\\n\\nThe `install-scripts` command will update and install NASALib scripts as needed.\\n~~~shell\\n$ ./install-scripts\\n~~~\\n\\n## Older Versions \\nOlder versions of NASALib are available from \\n[http://shemesh.larc.nasa.gov/fm/ftp/larc/PVS-library](http://shemesh.larc.nasa.gov/fm/ftp/larc/PVS-library).\\n\\n# Contributors\\n\\nNASALib has grown over the years thanks to the contribution of several people, among them:\\n\\n* [Aaron Dutle](http://shemesh.larc.nasa.gov/people/amd), NASA, USA\\n* Alfons Geser, HTWK Leipzig, Germany\\n* Amer Tahat, Michigan Technological University, USA\\n* Amy Isvik, Wartburg College, USA\\n* Ana Cristina Rocha Oliveira, University of Brasilia, Brazil\\n* André Galdino, Federal University of Goiás, Brazil\\n* Andreia Avelar Borges, University of Brasilia, Brazil\\n* Anthony Narkawicz, formerly at NASA, USA\\n* Ariane Alves Almeida, University of Brasilia, Brazil\\n* [Bruno Dutertre](http://www.csl.sri.com/users/bruno), SRI, USA\\n* Ben Di Vito, NASA (retired), USA\\n* [César Muñoz](http://shemesh.larc.nasa.gov/people/cam), NASA, USA\\n* Clément Blaudeau, EPFL, Switzerland and Ecole Polytechnique, France \\n* Concepción Vidal, University of La Coruña, Spain\\n* David Griffioen,CWI, The Netherlands\\n* [David Lester](http://apt.cs.man.ac.uk/people/dlester), Manchester University, UK\\n* Dragan Stosic, Ireland\\n* [Érik Martin-Dorel](http://erik.martin-dorel.org/), U. Montpellier 2 & U. of Perpignan (formerly), France\\n* Felicidad Aguado, University of La Coruña, Spain\\n* Flavio L.C. de Moura, University of Brasilia, Brazil\\n* [Gilles Dowek](https://who.rocq.inria.fr/Gilles.Dowek/index-en.html), INRIA, France\\n* [George Hagen](http://shemesh.larc.nasa.gov/people/geh), NASA, USA\\n* Gilberto Perez, University of La Coruña, Spain\\n* Gregory Anderson, University of Texas at Austin, USA\\n* Hanne Gottliebsen, formerly at NIA, USA\\n* Heber Herencia-Zapana, formerly at  NIA, USA\\n* J. Tanner Slagel, NASA, USA\\n* Jerry James, Utah State University, USA\\n* [Jeff Maddalon](http://shemesh.larc.nasa.gov/people/jmm), NASA, USA\\n* Jon Sjogren, Department of Defense, USA\\n* John Siratt, formerly at University of Arkansas at Little Rock, USA\\n* Katherine Cordwell, CMU, USA\\n* Kristin Rozier, formerly at NASA, USA\\n* [Lee Pike](http://corp.galois.com/lee-pike), formerly at Galois, USA\\n* [Marco A. Feliú](https://www.nianet.org/directory/research-staff/marco-feliu/), NIA, USA\\n* [Mariano Moscato](https://www.nianet.org/directory/research-staff/mariano-moscato/), NIA, USA\\n* [Mauricio Ayala-Rincón](http://www.mat.unb.br/~ayala), University of Brasilia, Brazil\\n* [Natarajan Shankar](http://www.csl.sri.com/users/shankar), SRI, USA\\n* Pablo Ascariz, formerly at University of La Coruña, Spain\\n* [Paul Miner](http://shemesh.larc.nasa.gov/people/psm), NASA, USA\\n* Pedro Cabalar, University of La Coruña, Spain\\n* Radu Siminiceanu, formerly at NIA, USA\\n* Ricky Butler, NASA (retired), USA\\n* [Silvie Boldo](https://www.lri.fr/~sboldo), INRIA, France\\n* [Sam Owre](http://www.csl.sri.com/users/owre), SRI, USA\\n* Thaynara de Lima, Federal University of Goiás, Brazil\\n* Thiago Mendonça Ferreira Ramos, University of Brasilia, Brazil\\n* Thomas Norris\\n* Víctor Carreño, NASA (retired), USA\\n\\nIf we have incorrectly attributed a PVS development or you have\\ncontributed to NASALib and your name is not included here, please let\\nus know.\\n\\nIf you want to contribute please read this [guide](docs/DEVEL-GUIDE.md).\\n\\nDISCLAIMER\\n--\\nNASALib is a collection of formal specifications most of\\nwhich have been in the public domain for several years. The Formal\\nMethods Team at NASA LaRC still\\nmaintains these developments. For the developments originally made by\\nthe Formal Methods Team, these\\ndevelopments are considered fundamental research that do not\\nconstitute software. Contributions made by others may have particular\\nlicenses, which are listed in the file `top.pvs` in each\\nrespective directory.  In case of doubt, please contact the developers\\nof each contribution, which are also listed in that file.\\n\\nPVS patches, which are included in the directory `pvs-patches`, are part of the\\nPVS source code and they are covered by the PVS open source license.\\n\\nSome proof strategies require third party research tools, e.g.,\\nMetiTarski and Z3. For convenience, they are included in this\\nrepository with permission from their authors. Licenses for these\\ntools are also included as appropriate.\\n\\nEnjoy it.\\n\\n[The NASA/NIA Formal Methods Team at LaRC](http://shemesh.larc.nasa.gov/fm)\\n\\nContact: [César A. Muñoz (NASA)](http://shemesh.larc.nasa.gov/people/cam)\\n'},\n",
       " {'repo': 'nasa/skeleton_app',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Core Flight System : Framework : App : Sample\\n\\nThis repository contains a sample application (sample_app), which is a framework component of the Core Flight System.\\n\\nThis sample application is a non-flight example application implementation for the cFS Bundle. It is intended to be located in the `apps/sample_app` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes sample_app as a submodule), which includes build and execution instructions.\\n\\nsample_app is an example for how to build and link an application in cFS.\\n\\n## Version Notes\\n- 1.1.5  \\n  - Fix to build on RASPBIAN OS\\n  - Minor updates (see https://github.com/nasa/sample_app/pull/47)\\n- 1.1.4  \\n  - Fix for a clean build with OMIT_DEPRECATED\\n  - Minor updates (see https://github.com/nasa/sample_app/pull/44)\\n- 1.1.3\\n  - Minor updates (see https://github.com/nasa/sample_app/pull/34)\\n- 1.1.2\\n  - Minor updates (see https://github.com/nasa/sample_app/pull/20)\\n- 1.1.1\\n  - Minor updates (see https://github.com/nasa/sample_app/pull/15)\\n- **1.1.0 OFFICIAL RELEASE**:\\n  - Minor updates (see https://github.com/nasa/sample_app/pull/11)\\n  - Not backwards compatible with OSAL 4.2.1\\n  - Released as part of cFE 6.7.0, Apache 2.0\\n- **1.0.0a OFFICIAL RELEASE**:\\n  - Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nAs a sample application, extensive testing is not performed prior to release and only minimal functionality is included.  Note discrepancies likely exist between this application and the example detailed in the application developer guide.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n'},\n",
       " {'repo': 'nasa/ow_autonomy',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': 'Notices:\\n--------\\nCopyright © 2020 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nDisclaimers\\n-----------\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n\\now_autonomy\\n===========\\n\\nThis package contains a candidate onboard autonomy component for an Ocean\\nWorlds lander, namely a ROS node (named `autonomy node`) embedding a PLEXIL plan\\nexecutive.\\n\\n\\nContents\\n--------\\n\\nsrc/plans directory contains the PLEXIL plans.\\n\\nsrc/plexil-adapter contains the supporting code needed to run the PLEXIL plans,\\nand also the ROS node implementation.\\n\\nSee the README.md files in each subdirectory for more information.\\n\\n\\nBuild\\n-----\\n\\nSee the \"Getting Started\" section in the parent repository\\'s README.md file,\\nviewable at https://github.com/nasa/ow_simulator, for complete installation and\\nbuild instructions as well as hardware/software requirements.  This file recaps\\na few key points and has some supplementary information.\\n\\nA prerequisite for building and running this package is a working PLEXIL\\ninstallation, which has its own prerequisites. The environment variable\\nPLEXIL_HOME must be set to PLEXIL\\'s installation pathname.\\n\\nYour ROS environment should also first be set up:\\n\\n```bash\\n source <catkin-workspace>/devel/setup.sh\\n```\\n\\nAssumed is this directory filed properly within an OceanWATERS ROS workspace\\n(see Conftluence for instructions).  Build the entire workspace with:\\n\\n```bash\\n catkin build\\n```\\n\\nBuild just the ow_autonomy package with:\\n\\n```bash\\n catkin build ow_autonomy\\n```\\n\\nNOTE: If any new PLEXIL plans (.plp or .ple files) have been added since your\\nlast build, a clean rebuild of ow_autonomy is needed.  See bottom of this file\\nfor instructions.\\n\\n\\nStart the autonomy node\\n-----------------------\\n\\n1. First you must start the simulator, e.g.\\n\\n```bash\\n  roslaunch ow europa_terminator.launch\\n```\\n   NOTES:\\n    - to omit the Gazebo GUI for faster performance, add `gzclient:=false`\\n    - for alternate terrains, other launch files are available:\\n      atacama_y1a.launch, europa_terminator_workspace.launch,\\n      europa_test_dem.launch.\\n\\n2. Next start the autonomy node.  Starting the autonomy node always runs a\\n   PLEXIL plan.  The simplest version is:\\n\\n   `roslaunch ow_autonomy autonomy_node.launch`\\n\\n   This invocation loads the default PLEXIL plan, Demo.plx.  A specific plan may\\n   be run by adding it to the command line, e.g.\\n\\n   `roslaunch ow_autonomy autonomy_node.launch plan:=ReferenceMission1.plx`\\n\\n   The argument given to the `plan` parameter must be a file found in :\\n\\n   `<ow_workspace>/devel/etc/plexil`\\n   \\n   See `plans/README.md` for a description of the PLEXIL plans.\\n\\n\\nFault Detection\\n---------------\\n\\nA rudimentary version does nothing more than check relevant fault parameters for\\neach lander operation.  Run any plan you like, while setting fault parameters as\\ndesired (see `ow_simulatyor/ow_faults/README.md` for instructions).  You can\\nalso use `scripts/set-faults.py` which by default sets every fault; edit it as\\nneeded.\\n\\nFaults are simply reported, as ROS warnings.\\n\\n\\nClean\\n-----\\n\\nTo clean (remove all build products from) just the ow_autonomy package:\\n\\n `cd <ow_workspace>/build`\\n `rm -rf ow_autonomy`\\n\\nTo clean the entire ROS workspace (not needed if you only want to rebuild\\now_autonomy):\\n\\n  `catkin clean`\\n'},\n",
       " {'repo': 'nasa/nos3',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# NASA Operational Simulator for Small Satellites\\nThe NASA Operational Simulator for Small Satellites (NOS3) is a suite of tools developed by NASA\\'s Katherine Johnson Independent Verification and Validation (IV&V) Facility to aid in areas such as software development, integration & test (I&T), mission operations/training, verification and validation (V&V), and software systems check-out. \\nNOS3 provides a software development environment, a multi-target build system, an operator interface/ground station, dynamics and environment simulations, and software-based models of spacecraft hardware.\\n\\n### Known Issues\\n1. Not all cFS delivered apps are included and supported at this time. Currently supported are: CI, TO, SCH, SC, HK, CFS_LIB\\n2. CentOS support not included in this release\\nThese issues will be addressed in future updates\\n\\n### Documentation\\nThe best source of documentation can be found at [NOS3](www.nos3.org), as well as a PDF Users Guide attached to this Release on Github\\n\\n### Prerequisites\\nEach of the applications listed below are required prior to performing the installation procedure:\\n* [Git 1.8+](https://git-scm.com/)\\n* [Vagrant 2.2.3+](https://www.vagrantup.com/)\\n* [VirtualBox 6.1+](https://www.virtualbox.org/)\\n\\n### Installing\\n1. Open a terminal\\n2. Navigate to the desired location for the repository\\n3. Clone the repository `git clone https://github.com/nasa/nos3.git`\\n4. Clone the submodules `git submodule init` and `git submodule update` \\n5. Navigate to `/nos3/deployment`\\n6. Run `vagrant up` and wait to return to a prompt\\n\\t- This can take anywhere from 20 minutes to hours depending on internet speeds and host PC specs\\n  \\t- The VM will reboot multiple times in order to finish install packages for you automatically so wait for that prompt!\\n\\t- **Sometimes ansible does not seem to install and there is an error like \"Could not get lock /var/lib/apt/lists/lock\".  If this happens run `vagrant provision` to install ansible and provision.**\\n7. Login to the nos3 user using the password `nos3123!` and get to work!\\n7. Try building and running following the instructions below\\n\\n### Getting started\\nIt is recommended to share the nos3 repository into the virtual machine\\n1. Open a terminal\\n2. To build use the `make` command from the nos3 repo\\n3. To run nos3 use the `make launch` command from the nos3 repo\\n4. To halt nos3 use the `make stop` command from the nos3 repo\\n\\n### Directory Layout\\n* `/nos3/deployment` contains the repository for generating the nos3 virtual environment\\n* `/nos3/fsw` contains the repositories needed to build cFS FSW\\n\\t- /apps - the open source cFS apps\\n\\t- /cfe - the core flight system (cFS) source files\\n\\t- /components - the hardware component apps\\n\\t- /osal - operating system abstraction layer (OSAL), enables building for linux and flight OS\\n\\t- /psp - platform support package (PSP), enables use on multiple types of boards\\n\\t- /tools - standard cFS provided tools\\n* `/nos3/gsw` contains the nos3 ground station files, and other ground based tools\\n\\t- /ait - Ammos Instrument Toolkit (Untested for 1.05.0)\\n\\t- /cosmos - COSMOS files\\n\\t- /OrbitInviewPowerPrediction - OIPP tool for operators\\n\\t- /scripts - convience scripts\\n* `/nos3/sims` contains the nos3 simulators and configuration files\\n\\t- /cfg - 42 files and NOS3 top level config file\\n\\t- /_sim - a component simulator\\n\\t- /nos_time_driver - time syncronization for all components\\n\\t- /sim_common - common files used by component simulators\\n\\t- /sim_server - NOS Engine Server config and build files\\n\\t- /sim_terminal - terminal for testing on NOS Engine busses\\n\\n## Support\\nIf this project interests you or if you have any questions, please feel free to contact any developer directly or email `support@nos3.org`.\\n\\n## Reporting Issues\\nPlease report issues to the tracking system on Github [NOS3 Issues](www.github.com/nasa/nos3/issues)\\n\\n### Frequently Asked Questions\\n* A GUI environment hasn\\'t shown up after an extended period (> 1.5 hours), what should I do?\\n  - Stop the provision, delete the existing VM (if it was created), and try again\\n  - `CTRL + C`, `vagrant destroy`, `y`, `vagrant up`, wait, `vagrant reload`\\n* What is the root username and password?\\n  - `vagrant` with password `vagrant`\\n* Why doesn\\'t the shared clipboard work?\\n  - You will most likely need to re-install / update the guest additions and reboot for this to function properly\\n  - In the VirtualBox menu select: Devices -> Insert Guest Additions CD Image...\\n  - Follow the instructions provided\\n* How can I mount a shared folder so that I edit on my host and compile / run in the VM?\\n  - In the VirtualBox menu select: Devices -> Shared Folders -> Shared Folders Settings...\\n  - Select the folder with a plus sign to add your folder\\n\\t  * Provide the path, name, mount point inside the VM\\n\\t\\t* Select `Auto-mount`, `Make Permanent`, and `OK`\\n* How can I run 42 without the GUI?\\n  - Edit the `/nos3/sims/cfg/InOut/Inp_Sim.txt` and set Graphics Front End to `FALSE` \\n* NOS Engine Standalone server reports `NOSEngine.Uart - close uart port failed` error?\\n\\t- This isn\\'t actually an error and is scheduled to be removed, proceed as usual.\\n\\n### Versioning\\nWe use [SemVer](http://semver.org/) for versioning. For the versions available, see the tags on this repository.\\n\\n### License\\nThis project is licensed under the NOSA (NASA Open Source Agreement) License. \\n\\n### Acknowledgments\\n* Special thanks to all the developers involved!\\n\\n'},\n",
       " {'repo': 'nasa/harmony-py',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# harmony-py\\n\\nHarmony-Py is a Python library for integrating with NASA\\'s [Harmony](https://harmony.earthdata.nasa.gov/) Services.\\n\\nHarmony-Py provides a Python alternative to directly using [Harmony\\'s RESTful API](https://harmony.earthdata.nasa.gov/docs/api/). It handles NASA [Earthdata Login (EDL)](https://urs.earthdata.nasa.gov/home) authentication and optionally integrates with the [CMR Python Wrapper](https://github.com/nasa/eo-metadata-tools) by accepting collection results as a request parameter. It\\'s convenient for scientists who wish to use Harmony from Jupyter notebooks as well as machine-to-machine communication with larger Python applications.\\n\\nHarmony-Py is a work-in-progress, is not feature complete, and should only be used if you would like to test its functionality. We welcome feedback on Harmony-Py via [GitHub Issues](https://github.com/nasa/harmony-py/issues)\\n\\n![Python package](https://github.com/nasa/harmony-py/workflows/Python%20package/badge.svg)\\n\\n[![Documentation Status](https://readthedocs.org/projects/harmony-py/badge/?version=latest)](https://harmony-py.readthedocs.io/en/latest/?badge=latest)\\n\\n# Using Harmony Py\\n\\n## Prerequisites\\n\\n* Python 3.7+\\n\\n\\n## Installing\\n\\nThe library is available from [PyPI](https://pypi.org/project/harmony-py/) and can be installed with pip:\\n\\n        $ pip install -U harmony-py\\n\\nThis will install harmony-py and its dependencies into your current Python environment. It\\'s recommended that you install harmony-py into a virtualenv along with any other dependencies you may have.\\n\\n\\n# Running Examples & Developing on Harmony Py\\n\\n## Prerequisites\\n\\n* Python 3.7+\\n* (optional,recommended) [pyenv](https://github.com/pyenv/pyenv) and [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv)\\n\\n\\n## Installing Development & Example Dependencies\\n\\nFirst, it\\'s recommended that you create a Python virtualenv so that Harmony Py and its dependencies are isolated in their own environment. To do so, you can either [create and activate a Python virtual environment with venv](https://docs.python.org/3/tutorial/venv.html), or--if you have pyenv and pyenv-virtualenv installed--use pyenv to create and activate one for you (`harmony-py`). There are `make` targets for both of these options--choose one.\\n\\n1a. Create a virtualenv with venv:\\n\\n        $ make venv-setup\\n        $ source .venv/bin/activate\\n\\nTo deactivate it:\\n\\n        $ source deactivate\\n\\n1b. Use pyenv & pyenv-virtualenv. This will install Python 3.9 & create a virtualenv using that version of Python. Important: if this is your first time using pyenv to install Python, be sure that you have the [Python build requirements installed](https://github.com/pyenv/pyenv/wiki#suggested-build-environment) first.\\n\\n        $ make pyenv-setup\\n\\nIf you\\'ve setup pyenv with your shell properly, it should automatically activate the environment. You can check if it\\'s activated by:\\n\\n        $ pyenv version\\n\\nIt should show `harmony-py`. Pyenv does auto-activation by creating a `.python-version` file in the project directory. Most shells can be setup to automatically activate & deactivate virtual environments when cd\\'ing into & out of directories by using the value found in `.python-version`. This is convenient since it ensures that the correct virtualenv has been activated (and deactivated) when starting work on a project. See the pyenv docs for more details. If you need to manually activate & deactivate:\\n\\n        $ pyenv activate harmony-py\\n        $ pyenv deactivate\\n\\n2. Install dependencies:\\n\\n        $ make install\\n\\n3. Optionally register your local copy with pip:\\n\\n        $ pip install -e ./path/to/harmony_py\\n\\n\\n## Running the Example Jupyter Notebooks\\n\\nJupyter notebooks in the `examples` subdirectory show how to use the Harmony Py library. Start up the Jupyter Lab notebook server and run these examples:\\n\\nThe Jupyter Lab server will start and [open in your browser](http://localhost:8888/lab). Double-click on a notebook in the file-browser sidebar and run the notebook. Note that some notebooks may have cells which prompt for your EDL username and password. Be sure to use your UAT credentials since most of the example notebooks use the Harmony UAT environment.\\n\\n        $ make examples\\n\\n\\n## Developing\\n\\n### Generating Documentation\\n\\nDocumentation on the Read The Docs site is generated automatically. It is generated by using `sphinx` with reStructuredText (.rst) and other files in the `docs` directory. To generate the docs locally and see what they look like:\\n\\n        $ make docs\\n\\nYou can then view the documentation in a web browser under `./docs/_build/html/index.html`.\\n\\nIMPORTANT: The documentation uses a notebook from the `examples` directory rendered as HTML. If you\\'ve modified that notebook (see `Makefile` for notebook that is currently rendered), you will need to run `make docs` locally. You will see a change to the `docs/user/notebook.html` file after doing so. This file should be committed to the git repo since it is used when the latest docs are pushed to the Read The Docs site (it can\\'t currently be generated as part of the build).\\n\\n### Running the Linter & Unit Tests\\n\\nRun the linter on the project source:\\n\\n        $ make lint\\n\\nRun unit tests and test coverage. This will display terminal output and generate an HTML coverage report in the `htmlcov` directory.\\n\\n        $ make test\\n\\nFor development, you may want to run the unit tests continuously as you update tests and the code-under-test:\\n\\n        $ make test-watch\\n\\n\\n### Generating Request Parameters\\n\\nThe `harmony.Request` constructor can accept parameters that are defined in the [Harmony OGC API schema](). If this schema has been changed and the `Request` constructor needs to be updated, you may run the generator utility. This tool reads the Harmony schema and generates a partial constructor signature with docstrings:\\n\\n        $ python internal/genparams.py ${HARMONY_DIR}/app/schemas/ogc-api-coverages/1.0.0/ogc-api-coverages-v1.0.0.yml\\n\\nEither set `HARMONY_DIR` or replace it with your Harmony project directory path. You may then write standard output to a file and then use it to update the `harmony.Request` constructor and code.\\n\\n## CI\\n\\nHarmony-py uses [GitHub\\nActions](https://github.com/nasa/harmony-py/actions) to run the Linter\\n& Unit Tests. The test coverage output is saved as a build artifact.\\n\\n## Building and Releasing\\n\\nIf a new version of Harmony-Py will be released then the `master` branch should be tagged with an updated version:\\n\\n        $ git checkout master\\n        $ git tag -a v1.2.3    # where v1.2.3 is the next version number\\n\\nIn order to generate new package and wheel files, do the following:\\n\\n        $ make build\\n\\n`make` reads the current version number based on git tag, populates the version in `harmony/__init__.py`, and `setup.py` reads the version number from `harmony/__init__.py` for packaging purposes.\\n\\nThis leaves us with a modifed __init\\\\__.py which must be committed and pushed to `master`.\\n\\n        $ git add harmony/__init__.py\\n        $ git commit -m \"Version bump to v1.2.3\"\\n        $ git tag -f\\n        $ git push\\n\\nThen, provided API tokens are in order, the following runs the build target and publishes to PyPI:\\n\\n        $ make publish\\n'},\n",
       " {'repo': 'nasa/zarr-eosdis-store',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"zarr-eosdis-store\\n=================\\n\\nThe zarr-eosdis-store library allows NASA EOSDIS Collections to be accessed efficiently\\nby the `Zarr Python library <https://zarr.readthedocs.io/en/stable/index.html>`_, provided they\\nhave a sidecar DMR++ metadata file generated.\\n\\nInstallation\\n============\\n\\nThis module requires Python 3.8 or greater::\\n\\n    $ python --version\\n    Python 3.8.2\\n\\nInstall from PyPI::\\n\\n    $ pip install zarr-eosdis-store\\n\\nTo install the latest development version::\\n\\n    $ pip install pip install git+https://github.com/nasa/zarr-eosdis-store.git@main#egg=zarr-eosdis-store\\n\\nEarthdata Login\\n===============\\n\\nTo access EOSDIS data, you need to sign in with a free NASA Earthdata Login account, which you can obtain at\\n`<https://urs.earthdata.nasa.gov/>`_.\\n\\nOnce you have an account, you will need to add your credentials to your ``~/.netrc`` file::\\n\\n    machine urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD\\n\\nIf you are accessing test data, you will need to use an account from the Earthdata Login test system at\\n`<https://uat.urs.earthdata.nasa.gov/>`_ instead, adding a corresponding line to your ``~/.netrc`` file::\\n\\n    machine uat.urs.earthdata.nasa.gov login YOUR_USERNAME password YOUR_PASSWORD\\n\\n\\nUsage\\n=====\\n\\nTo use the library, simply instantiate ``eosdis_store.EosdisStore`` with the URL to the data file you would\\nlike to access, pass it to the Zarr library as you would with any other store, and use the Zarr API as with any\\nother read-only Zarr file.  Note: the URL to the data file will typically end with an HDF5 or NetCDF4 extension,\\nnot .zarr.\\n\\n.. code-block:: python\\n\\n   from eosdis_store import EosdisStore\\n   import zarr\\n\\n   # Assumes you have set up .netrc with your Earthdata Login information\\n   f = zarr.open(EosdisStore('https://example.com/your/data/file.nc4'))\\n\\n   # Read metadata and data from f using the Zarr API\\n   print(f['parameter_name'][0:0:0])\\n\\nIf the data has _FillValue (to flag nodata), scale_factor, or add_offset set (defined in metadata using CF-conventions)\\nthey can be retrieved from the parameter attributes.\\n\\n.. code-block:: python\\n\\n  import numpy as np\\n\\n  scale_factor = f['parameter_name].scale_factor\\n  add_offset = f['parameter_name].add_offset\\n  nodata = f['parameter_name]._FillValue\\n\\n  arr = f['parameter_name'][] * scale_factor + add_offset\\n\\n  nodata_locs = np.where(arr == nodata)\\n\\n\\nA better way to handle these is to use XArray. Rather than reading the data immediately when a slice is requested, XArray\\ndefers the read until the data is actually accessed. With the Zarr backend to XArray, the scale and offset can be set so that\\nwhen the data is accessed it will apply those values. This is more efficient if the data is going to be used in other operations.\\n\\nThe scale_factor and get_offset will be used if specified in the NetCDF/HDF5 file.\\n\\n.. code-block:: python\\n\\n  import xarray\\n\\n  store = EosdisStore('https://example.com/your/data/file.nc4')\\n\\n  f = xarray.open_zarr(store)\\n\\n  # the data is not read yet\\n  xa = f['parameter_name'][<slice>]\\n\\n  # convert to numpy array, data is read\\n  arr = xa.values\\n\\nThe resulting array will have had scale and offset applied, and any element that is equal to the _FillValue attribute will be\\nset to numpy `nan`. To use XArray without apply the scale and offset or setting the nodata to `nan`, supply the `mask_and_scale`\\nkeyword to xarray.open_zarr to False:\\n\\n.. code-block:: python\\n\\n  store = EosdisStore('https://example.com/your/data/file.nc4')\\n\\n  f = xarray.open_zarr(store, mask_and_scale=False)\\n\\n\\nTechnical Summary\\n=================\\n\\nWe make use of a technique to read NetCDF4 and some HDF5 files that was prototyped by The HDF Group and USGS, described\\n`here <https://medium.com/pangeo/cloud-performant-reading-of-netcdf4-hdf5-data-using-the-zarr-library-1a95c5c92314)>`_.\\n\\nTo allow the technique to work with EOSDIS data, we have extended it and optimized access in the following key ways:\\n\\n* The ``EosdisStore`` reads a DMR++ file generated by OPeNDAP to present its metadata and determine byte offsets to the\\n  Zarr library. By reusing these, we avoid needing to generate new metadata sidecar files to support new data.\\n\\n* The store uses HTTPS and authenticates with a ``.netrc`` entry, rather than the S3 API, making it compatible with\\n  EOSDIS access patterns and requirements\\n\\n* The store caches redirect URLs for a period of time set by the Cache-Control header.  Doing this avoids the overhead\\n  of repeated redirects when accessing parts of files.\\n\\n* The store uses a parallel API that allows it to make more efficient access optimizations:\\n*\\n  * When the Zarr library accesses data that requires reading multiple near-sequential bytes in the file, the store combines\\n    these smaller requests into a single larger request.\\n\\n  * After an initial request to cache any authentication and redirect information, the store runs subsequent requests in\\n    parallel.\\n\\nDevelopment\\n===========\\n\\nClone the repository, then ``pip install`` its dependencies::\\n\\n    pip install -r requirements.txt\\n    pip install -r requirements-dev.txt\\n\\nTo check code coverage and run tests::\\n\\n    coverage run -m pytest\\n\\nTo check coding style::\\n\\n    flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\\n\\nTo build documentation, generated at ``docs/_build/html/index.html``::\\n\\n    cd docs && make html\\n\"},\n",
       " {'repo': 'nasa/Common-Metadata-Repository',\n",
       "  'language': 'Clojure',\n",
       "  'readme_contents': '# Common Metadata Repository\\n\\nVisit the CMR at [https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository](https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository)\\n\\n## About\\n\\nThe Common Metadata Repository (CMR) is an earth science metadata repository\\nfor [NASA](https://www.nasa.gov/) [EOSDIS](https://earthdata.nasa.gov) data. The CMR\\nSearch API provides access to this metadata.\\n\\n## Client-facing Components\\n\\n- Search\\n  - Allows the user to search by collections, granules, and concepts with a\\n    myriad of different query types\\n  - API Docs: https://cmr.earthdata.nasa.gov/search/site/search_api_docs.html\\n\\n- Ingest\\n  - Ingest refers to the process of validating, inserting, updating, or\\n    deleting metadata in the CMR system. It affects only the metadata for the\\n    specific Data Partner. The CMR allows Data Partners to ingest metadata.\\n    records through a RESTful API\\n  - API Docs: https://cmr.earthdata.nasa.gov/ingest/site/ingest_api_docs.html\\n\\n- Access Control\\n  - Access Control Lists (ACLs) are the mechanism which grants users\\n    access to perform different operations in the CMR. CMR ACLs follow the same\\n    design as ECHO ACLs, which are a superset of the generic ACL\\n    design pattern used in many other systems. An ACL is a\\n    mapping of actors (subjects) to resources (object) to operations\\n    (predicate).\\n  - Two quick examples of a CMR ACL could be:\\n    - All registered users have READ access to ASTER data\\n    - A provider\\'s operations team may ingest data for that provider\\n  - API Docs: https://cmr.earthdata.nasa.gov/access-control/site/access_control_api_docs.html\\n\\n## Our Development Environment\\n\\n- Mac OSX\\n- Atom: https://atom.io/\\n- Proto-Repl: https://atom.io/packages/proto-repl\\n  - Installed and configured according to this guide: https://git.io/atom_clojure_setup\\n\\n## Prerequisites\\n\\n- Java 1.8.0 (a.k.a. JAVA8) only; higher versions are not currently supported.\\n- Leiningen (https://leiningen.org) 2.5.1 or above.\\n  - We\\'ve had success with Homebrew and with the install script on the\\n    Leiningen website.\\n- Ruby (used to support two legacy apps)\\n- Maven (https://maven.apache.org/install.html)\\n    - Mac OS X devs can use `brew install maven`\\n    - Linux devs can use `sudo apt-get install maven`\\n- GCC and libc\\n- Docker\\n\\n## Obtaining the Code\\n\\nYou can get the CMR source code by cloning the repository from Github:\\n\\n```\\n$ git clone git@github.com:nasa/Common-Metadata-Repository.git cmr\\n```\\n\\n## Building and Running the CMR\\n\\nThe CMR is a system consisting of many services. The services can run\\nindividually or in a single process. Running in a single process makes\\nlocal development easier because it avoids having to start many different\\nprocesses. The dev-system project allows the CMR to run from a single REPL\\nor Jar file. If you\\'re developing a client against the CMR you can build and\\nrun the entire CMR with no external dependencies from this Jar file and use\\nthat instance for local testing. The sections below contain instructions for\\nrunning the CMR as a single process or as many processes.\\n\\n#### Using the `cmr` CLI Tool\\n\\nThis project has its own tool that is able to do everything from initial setup to\\nrunning builds and tests on the CI/CD infrastructure. To use the tool\\nas we do below, be sure to run the following from the top-level CMR directory:\\n\\n```\\nexport PATH=$PATH:`pwd`/bin\\nsource resources/shell/cmr-bash-autocomplete\\n```\\n\\n(If you use a system shell not compatible with Bash, we\\'ll accept a PR with\\nauto-complete for it.)\\n\\nTo make this change permanent:\\n\\n```\\necho \"export PATH=\\\\$PATH:`pwd`/bin\" >> ~/.profile\\necho \"source `pwd`/resources/shell/cmr-bash-autocomplete\" >> ~/.profile\\n```\\n\\n#### Oracle Dependencies\\n\\nEven if you\\'re not going to develop against a local Oracle database,\\nyou still need to have the Oracle libraries locally installed to use the\\nCMR.\\n\\nHere are the steps to do so:\\n\\n1. Ensure you have installed on your system the items listed above in the\\n   \"Prerequisites\" section.\\n1. Download the Oracle JDBC JAR files into `./oracle-lib/support` by\\n   following instructions in `./oracle-lib/README.md`. (The CMR must have these\\n   libraries to build but it does not depend on Oracle DB when running\\n   locally. It uses a local in-memory database by default.) If you\\'re reading this\\n   guide on the web, [here is a handy link to the instructions.](https://github.com/nasa/Common-Metadata-Repository/tree/master/oracle-lib)\\n1. With the JAR files downloaded to the proper location, you\\'re now ready\\n   to install them for use by the CMR:\" `cmr install oracle-libs`\\n\\n#### Building and Running CMR Dev System in a REPL with CMR CLI tool\\n\\n1. `cmr setup profile` and then update the new `./dev-system/profiles.clj` file.\\n   it will look something like this:\\n   ``` clojure\\n   {:dev-config {:env {:cmr-metadata-db-password \"<YOUR PASSWORD HERE>\"\\n                       :cmr-sys-dba-password \"<YOUR PASSWORD HERE>\"\\n                       :cmr-bootstrap-password \"<YOUR PASSWORD HERE>\"\\n                       :cmr-ingest-password \"<YOUR PASSWORD HERE>\"\\n                       :cmr-urs-password \"<YOUR PASSWORD HERE>\"}}}\\n   ```\\n\\n2. `cmr setup dev`\\n3. `cmr start repl`\\n4. Once given a Clojure prompt, run `(reset)`\\n\\nNote that the `reset` action could potentially take a while, not only due to\\nthe code reloading for a large number of namespaces, but for bootstrapping\\nservices as well as starting up worker threads.\\n\\n#### Building and Running CMR Dev System from a Jar\\n\\nAssuming you have already run the above steps (namely `cmr setup dev`), to\\nbuild and run the default CMR development system (`dev-system`) from a\\n`.jar` file:\\n\\n1. `cmr build uberjars`\\n2. `cmr build all`\\n3. `cmr start uberjar dev-system` will run the dev-system as a background task\\n\\nSee CMR Development Guide to read about specifying options and setting\\nenvironment variables\\n\\n#### Building and Running separate CMR Applications\\n\\nThe following will build every application but will put each jar into the\\nappropriate `target` directory for each application. The command shown in step\\n3 is an example. For the proper command to start up each application, see the\\n`Applications` section below. Note: You only need to complete steps 1 and 2 once.\\n\\n1. `cmr build uberjar APP`\\n2. `cmr run uberjar APP`\\n\\nWhere `APP` is any supported CMR app. You can touble-tap the `TAB` key on\\nyour keyboard to get the `cmr` tool to show you the list of available apps\\nafter entering `uberjar` in each step above.\\n\\nNote: building uberjars will interfere with your repl. If you want to use your repl post-build you will need to,\\n`rm -f ./dev-system/target/`\\n\\n## Checking Dependencies, Static Analysis, and Tests\\n\\nThere are several `lein` plugins within the CMR for performing\\nvarious tasks either at individual subproject levels or at the top-level for\\nall subprojects.\\n\\n#### Dependency Versions\\n\\nTo check for up-to-date versions of all project dependencies, you can use\\n`cmr test versions PROJ`, where `PROJ` is any CMR sub-project under the\\ntop-level directory.\\n\\nYou may run the same command without a project to check for all projects:\\n`cmr test versions`.\\n\\nNote that this command fails with the first project that fails. If many\\nsubprojects are failing their dependency version checks and you wish to see\\nthem all, you may use your system shell:\\n\\n```sh\\nfor PROJ in `ls -1d */project.clj|xargs dirname`\\ndo\\n  \"Checking $PROJ ...\"\\n  cmr test versions $PROJ\\n  cd - &> /dev/null\\ndone\\n```\\n\\n#### Dependency Ambiguities and `.jar` File Conflicts\\n\\nTo see if the JVM is having problems resolving which version of a\\ndependency to use, you can run `cmr test dep-tree PROJ`. To perform this\\nagainst all projects: `cmr test dep-trees`.\\n\\n#### Static Analysis and Linting\\n\\nTo perform static analysis and linting for a project, you can run\\n`cmr test lint PROJ`. As above with dependency version checking, by\\nnot passing a project, you can run for all projects: `cmr test lint`.\\n\\n#### Dependency Vulnerability Scanning\\n\\nYou can see if your currently installed version of CMR has any reported Common Vulnerabilities and Exploits (CVEs) by running the helpful alias `lein check-sec` that you can use in each application, or at the root folder to scan all CMR apps together.\\n\\nYou will find the vulnerability summary in `./target/dependency-check-report.html` in each application.\\n\\n#### Testing CMR\\n\\nTest files in CMR should follow the naming convention of ending in `-test`.\\n\\nThere are two modes of testing the CMR:\\n\\n* From the REPL\\n* Utilizing the CI/CD script to run against an Oracle database\\n\\nFor the first, the steps are as follows:\\n\\n1. Ensure you have set up your development environment in `dev-system`\\n2. If you have built any `.jar` files, run `cmr clean PROJ` (for a given\\n   project) or `cmr clean` to clean all projects.\\n3. Start the REPL: `cmr start repl`\\n4. Once in the REPL, start the in-memory services: `(reset)`\\n5. Run the tests: `(run-all-tests)` or `(run-all-tests-future)`\\n\\nYou have the option of substituting the last step with `(run-suites)`. This\\nuses a third-party tool to display clear test results which are\\neasier copy/paste should you want to run them on an individual basis.\\nThese results also contain easier to read\\nexception messages/stacktraces. Here\\'s an excerpt:\\n\\n```\\n   cmr.system-int-test.ingest.provider-ingest-test\\n     update-provider-test\\n       assertion 1 ........................................................ [OK]\\n       assertion 2 ........................................................ [OK]\\n       assertion 3 ........................................................ [OK]\\n       assertion 4 ........................................................ [OK]\\n     delete-provider-test\\n       assertion 1 ........................................................ [OK]\\n       assertion 2 ........................................................ [OK]\\n       assertion 3 ........................................................ [OK]\\n       assertion 4 ........................................................ [OK]\\n       assertion 5 ........................................................ [OK]\\n       assertion 6 ........................................................ [OK]\\n       assertion 7 ........................................................ [OK]\\n       assertion 8 ........................................................ [OK]\\n       assertion 9 ........................................................ [OK]\\n```\\n\\nFor non-terminal based dev, depending upon your IDE/editor, you may have\\nshortcuts available to you for starting/restarting the services and/or running the\\ntests. To find out what these are you can contact a CMR core dev.\\n\\nTo run the tests against an Oracle database, we recommend that\\nyou use an Oracle VM built for this purpose. You will also need\\nconfiguration and authentication information that will be set as environment\\nvariables. Be sure to contact a CMR core dev for this information.\\n\\nTo run only certain types of tests, you may run the following:\\n\\n##### Unit Tests\\n\\n``` sh\\nlein modules utest\\n```\\n\\n##### Integration Tests\\n\\nIf running CMR with the in-memory database (default)\\n``` sh\\nlein modules itest --skip-meta :oracle\\n\\n```\\n\\nIf running CMR with an external database\\n``` sh\\nlein modules itest --skip-meta :in-memory-db\\n```\\n\\nIf you want to run tests against Oracle, bring up the Oracle VM and execute\\nthe following to create the users and run the migrations:\\n\\n``` sh\\ncmr setup db\\n```\\n\\nThen, in the CMR REPL:\\n\\n```clj\\nuser=> (reset :db :external)\\n...\\nuser=> (run-all-tests)\\n...\\n```\\n\\nThose tests will take much longer to run than when done with the in-memory\\ndatabase (~25m vs. ~6m). To switch back to using the in-memory database,\\ncall `(reset :db :in-memory)`.\\n\\nThere is also a different, optional test runner you can use. For more details\\nsee the docstring for `run-suites` in `dev-system/dev/user.clj`.\\nIt will contain usage instructions\\n#### Testing in the CI Environment\\n\\nThroughout the modules, in the `project.clj` files there are additional `lein` aliseses for \\nexecuting the tests in the CI/CD environment. They are \\n* ci-itest\\n* ci-utest\\n\\nThese run the integration and unit tests, respectively, in the CI environment. The difference\\nbetween `itest` and `ci-itest` or `utest` and `ci-utest` are the settings passed to the\\nkaocha test runner.\\n\\nIn the CI environment, color is omitted, and certain tests that require an internal memory\\ndatabase are excluded. The aliases may be used locally as well. \\n\\nTo see the differnce in detail, inspect the `tests.edn` files for each module to see the\\nprofile in use in the CI environment. Kaocha supports the use of profiles so more may \\nbe added as necessary.\\n\\n### Test Development\\n\\nCMR uses the [Kaocha](https://github.com/lambdaisland/kaocha) test library.\\nIt provides plugins and grouping capabilities. Tests are organized in each module\\nwith the standard being `:unit` and `:integration`.\\n\\nNot all modules will contain `:integration` tests.\\n\\n## Code structure\\n\\nThe CMR comprises several small services called microservices. These are\\nsmall purposed-based services that do a small set of things well.\\n\\n- For more reading on microservices: https://martinfowler.com/articles/microservices.html\\n\\n### The Microservices\\n\\nEach microservice has a `README` file in its root directory, which provides a\\nshort overview of the service\\'s functionality. There are many main\\napplications, as well as several libraries and support applications.\\n\\n#### Applications:\\n\\n- access-control-app\\n  - The mechanism which grants users access to perform different\\n    operations in the CMR. It also maintains groups and access control rules.\\n    Note that ECHO and URS provide user access as an external dependency.\\n    The mock-echo application implements both of the necessary interfaces\\n    for local testing.\\n  - Main method: cmr.access_control.runner\\n\\n- bootstrap-app\\n  - Contains APIs for performing various bulk actions in the CMR\\n  - Main method: cmr.bootstrap.runner\\n  - See `/bootstrap-app/README.md` for a list of lein and uberjar commands\\n\\n- dev-system\\n  - An app that combines the separate microservices of the CMR into a single\\n  application. We use this to simplify development\\n  - Main method: cmr.dev_system.runner\\n\\n- indexer-app\\n  - This handles indexing collections, granules, and tags in Elasticsearch\\n  - Maintains the set of indexes in elasticsearch for each concept\\n  - Main method: cmr.indexer.runner\\n\\n- ingest-app\\n  - The Ingest app handles collaborating with metadata db and indexer systems.\\n  This maintains the lifecycle of concepts coming into the CMR\\n  - Main method: cmr.ingest.runner\\n\\n- search-app\\n  - Provides a public search API for concepts in the CMR\\n  - Main method: cmr.search.runner\\n\\n- search-relevancy-test\\n  - Tests to measure and report the effectiveness of CMR\\'s search relevancy algorithm\\n\\n- system-int-test\\n  - Black-box, system-level tests to ensure functionality of the CMR\\n\\n- virtual-product-app\\n  - Adds the concept of Virtual Products to the CMR. Virtual Products represent\\n  products that a provider generates on demand from users. This takes place when\\n   a user places an order or downloads a product through a URL\\n  - Main method: cmr.virtual_product.runner\\n\\n- metadata-db-app\\n  - A database that maintains revisioned copies of metadata for the CMR\\n  - Main method: cmr.metadata_db.runner\\n\\n- mock-echo-app\\n  - This mocks out the ECHO REST API and the URS API as well. Its purpose is to\\n  make it easier to integration test the CMR system without having to run a full\\n   instance of ECHO. It will only provide the parts necessary to enable\\n   integration testing. You should not expect a perfect or complete\\n   implementation of ECHO.\\n  - Main method: cmr.mock_echo.runner\\n\\n#### Libraries:\\n\\n- acl-lib\\n  - Contains utilities for retrieving and working with ACLs\\n\\n- common-app-lib\\n  - Contains utilities used within many CMR applications\\n\\n- common-lib\\n  - Provides common utility code for CMR projects\\n\\n- elastic-utils-lib\\n  - A library that handles most of the interfacing with Elasticsearch\\n\\n- es-spatial-plugin\\n  - An Elasticsearch plugin that enables spatial search within elastic\\n\\n- oracle-lib\\n  - Contains utilities for connecting to and manipulating data in Oracle\\n\\n- orbits-lib\\n  - Clojure wrapper of a Ruby implementation of the Backtrack Orbit Search\\n    Algorithm (BOSA)\\n\\n- message-queue-lib\\n  - A library for interfacing with RabbitMQ, AWS SQS, and an in-memory message queue\\n\\n- spatial-lib\\n  - The spatial libraries provide utilities for working with spatial areas in the CMR\\n\\n- transmit-lib\\n  - The Transmit Library defines functions for invoking CMR services\\n\\n- umm-lib\\n  - This is the old source of UMM schemas and translation code. Since the\\n    advent of umm-spec-lib we are planning to remove it\\n\\n- umm-spec-lib\\n  - The UMM Spec lib contains JSON schemas that define the Unified Metadata\\n    Model, as well as mappings to other supported formats, and code to migrate\\n    collections between any supported formats.\\n\\n## Further Reading\\n\\n- CMR Client Partner User Guide: https://wiki.earthdata.nasa.gov/display/ED/CMR+Client+Partner+User+Guide\\n- CMR Data Partner User Guide: https://wiki.earthdata.nasa.gov/display/CMR/CMR+Data+Partner+User+Guide\\n- CMR Client Developer Forum: https://wiki.earthdata.nasa.gov/display/CMR/CMR+Client+Developer+Forum\\n\\n## License\\n\\nCopyright © 2014-2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n'},\n",
       " {'repo': 'nasa/cumulus-api',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '## Cumulus API Docs\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-api.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-api)\\n\\nCumulus API documentaion: https://nasa.github.io/cumulus-api\\n\\n### Installation\\n\\n     $ npm install \\n\\n### Build\\n\\n     $ npm run build\\n\\n### Serve\\n\\n     $ npm run serve\\n\\n### Deploy\\n\\n     $ npm run deploy'},\n",
       " {'repo': 'nasa/cFE',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/cfe/workflows/Static%20Analysis/badge.svg)\\n\\n# Core Flight System : Framework : Core Flight Executive\\n\\nThis repository contains NASA\\'s Core Flight Executive (cFE), which is a framework component of the Core Flight System.\\n\\nThis is a collection of services and associated framework to be located in the `cfe` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes build and execution instructions.\\n\\nThe detailed cFE user\\'s guide can be viewed at <https://github.com/nasa/cFS/blob/gh-pages/cFE_Users_Guide.pdf>.\\n\\n## Version History\\n\\n### Development Build: v6.8.0-rc1+dev559\\n\\n- Adds tests for nominal use cases of the ES Critical Data Store API.\\n- Adds functional tests for nominal uses of FS Header API.\\n- Adds functional tests for Time Current API.\\n- [docs] Makes comment blocks in source and header files more consistent for all internal, CFE core APIs. Moves information about the function behavior to its prototype in the header in doxygen format. Comment blocks on the function implementation refer back to the prototype, it does not duplicate the info. Local helper functions that are not separately prototyped, are exceptions to this pattern. Adds intended scope to all functions: global, application-internal, or file/local.\\n- See <https://github.com/nasa/cFE/pull/1481> and <https://github.com/nasa/cFS/pull/252>\\n\\n### Development Build: v6.8.0-rc1+dev540\\n\\n- Changes the type of pointer for `MemPtr` in `CFE_ES_PoolCreateNoSem` API from uint8* to void* to be more consistent and easier to use. Should be backward compatible.\\nUpdates the doxygen documentation for this parameter, as it was incorrectly specifying a 32-bit alignment requirement.\\n- Adds new functional tests for ES Child Task API. Does not check edge cases. Fixed spelling mistake in `UtAssert_ResourceID_Undifeined` name\\n- Removes BUILDDIR reference and an old comment. No behavior changes\\n- Moves and renames `cfe_resourceid_basevalue.h` to `cfe_resourceid_basevalue.h`. Since it is is assumed/required that resource IDs follow the \"osal compatible\" pattern. Perhaps in a future version this could change, but\\n- Ensures that the `CFE_SUCCESS` constant is of the `CFE_Status_t` type. Since an explicit cast is required on all error codes that are expected to be negative values.\\n- Removes unused error codes: `CFE_ES_ERR_SHELL_CMD` and `CFE_SB_NO_MSG_RECV`. No impact to behavior.\\n- When a startup file has a line with too many tokens the build script will generate a concise warning including an indicator of which line is causing the problem.\\n- Confirm that the call to `CFE_ES_CDS_CachePreload` returns `CFE_SUCCESS` before continuing. No behavior changes. Now shows up as untested lines in the coverage report since error condition cannot be exercised through coverage.\\n- [docs] Clarify that `CFE_ES_DeleteCDS` does not wipe or erase the block, it only returns resources to the pool for re-use.\\n- [docs] Adds comments in `CFE_ES_RunExceptionScan` describing the logic when an exception cannot be traced back to a specific app, in that it should fall back to the PSP reset.\\n- `CFE_ES_GenPoolInitialize` now returns ` CFE_ES_BAD_ARGUMENT` error if the `AlignSize` passed-in value is not actually a power of two instead of \"fixing\" the alignment mask,\\n- Replace internal `CFE_ES_SYSLOG_APPEND` macro with the `CFE_ES_WriteToSysLog()` API since coding standards discourage use of multi-line macros.\\n- [docs] Improve Resource IDs documentation. Specifically on use of the various helper functions and common patterns Documents that the \"IsMatch()\" functions accept NULL pointers so they can be used with initial validation (gatekeeper). All other helper functions assume a non-NULL pointer.\\n- Compiler will catch if the `CFE_RESOURCEID_MAX` value changes in such a way that makes it not usable as a bit mask as intended. Add a compile time assert to ensure that `CFE_RESOURCEID_MAX` value is one less than a power of two  (i.e. an LSB-justified bit mask). Notes in the comments that it serves as both a numeric limit and a mask.\\n- See <https://github.com/nasa/cFE/pull/1431> and <https://github.com/nasa/cFS/pull/250>\\n\\n\\n### Development Build: v6.8.0-rc1+dev509\\n\\n- Separates the list of CFE core interface modules (e.g. core_api) from the list of CFE core implementation modules (e.g. msg). This allows the content of core_api to be expanded to locally include any additional modules the user has added to cFE core via the `MISSION_CORE_MODULES` list.\\n- Adds documentation for `CFE_ES_RegisterCDS()` regarding clearing.\\n- Removes the separate CFE \"testrunner\" module and moves the logic associated with running a test into cfe_assert library. Converts the \"testcase\" module from a library into an app, by calling into the runner logic that is now inside cfe_assert. Each functional test is a separate app, not a library, so it can be started and stopped via ES command like any other app.\\n- Removes check on `ENABLE_UNIT_TESTS=true` when building \"cfe_assert\", it should be built all the time.\\n- See <https://github.com/nasa/cfe/pull/1406> and <https://github.com/nasa/cfs/pull/248>\\n\\n### Development Build: v6.8.0-rc1+dev498\\n\\n- Reports test failures as CFE events. Test status messages are now sent as Events rather than Syslog. This allows for more processing capability, and allows failures to be received externally (e.g. ground system).\\n- See <https://github.com/nasa/cFE/pull/1295> and <https://github.com/nasa/cFS/pull/242>\\n\\n### Development Build: v6.8.0-rc1+dev494\\n\\n- Adds new tests for the ES Info APIs\\n- See <https://github.com/nasa/cFE/pull/1284> and <https://github.com/nasa/cFS/pull/238>\\n\\n### Development Build: v6.8.0-rc1+dev490\\n\\n- Removes `PspConfig` member from the `GLOBAL_CONFIGDATA` object. Updates the only remaining reference to this object inside the CFE_PSP_VERSION and uses the new Version API instead. Updates the OSAL version print to use the version API call and CFE uses the macro directly.\\n- Replaces duplicate mention of the removed `CFE_SB_ZeroCopyGetPtr` with the correct removal target of `CFE_SB_ZeroCopyReleasePtr`\\n- See <https://github.com/nasa/cFE/pull/1279> and <https://github.com/nasa/cFS/pull/233>\\n\\n### Development Build: v6.8.0-rc1+dev484\\n\\n- Removes cases in `cfe_es_apps.c` and `cfe_tbl_internal.c` that could never hit the alternate condition since the condition was already checked\\n- Removes all APIs deprecated in #777 and #998\\n- Resolves CodeQL warnings on uninitialized variables.\\n- Refactors a small portion of `CFE_TIME_UnregisterSynchCallback` and initializes variables to resolve \"uninitialized variable\" false alarms.\\n- Fixes a typo in initialization in `CFE_TBL_Validate( CFE_TBL_Handle_t TblHandle )`\\n- Initializes `TotalMsgSize` as 0 to avoid static analysis warning of \"use before initialized\"\\n- Increments the `CreatePipeErrorCounter` for all create pipe errors to eliminate a trivial static analysis warning\\n- Removes redundant or unreachable assignments and checks\\n- Updates header guards to standard format. Converts some file-scope block comments to a doxygen format to include a summary of the file.\\n- Enables the internal helper functions that determine table slot availability to handle NULL pointers.\\n- Resolves static analysis warnings by removing redundant check for `CFE_SUCCESS` in `CFE_EVS_EarlyInit`\\n- Moves the invocation of `CFE_PSP_AttachExceptions()` from the registration function to the pre-entry function and removes all references to task registration in code, docs, and tests. **This API change affects cFS apps.**\\n- Renames `CFE_TestRunner_AppMain` as `CFE_TR_AppMain` so it is less than 20 characters long. Updates App file names in documentation for `cfe_es_startup.scr`.\\n- Replace the call to `CFE_SB_MessageStringGet()` with the new filename-aware function `CFE_FS_ParseInputFileName()` for commands that contain file names like `CFE_ES_StopPerfDataCmd`. The default pathname/extension logic is now applied here too and only a \"basename\" is strictly necessary, although if a full/absolute path is given, it will be used as is.\\n- Removes the now-unnecessary `CFE_SB_ZeroCopyHandle_t` type and all APIs that refer or require it .Replaces `CFE_SB_ZeroCopyGetPtr()` and `CFE_SB_ZeroCopyReleasePtr()` with two new simplified functions `CFE_SB_AllocateMessageBuffer()` and `CFE_SB_ReleaseMessageBuffer()` , respectively.  These new functions do not use a separate handle. Updates the `CFE_SB_TransmitBuffer()` API to also remove the handle. **This breaks public APIs**.\\n- Internal cleanup localized to ES implementation. Consolidate all ES global variables under the `CFE_ES_Global` struct. Removes the separate `CFE_ES_TaskData` and some random pointers that were stored at global scope. Adjusts all references to the deprecated items accordingly (search and replace).\\n- Adds PSP version info to ES Housekeeping TLM messages. Changes both PSP and OSAL version info assignments on HK TLM to use the new version info API.\\n- Fixes check for \"NumBuckets\" member to use `<=` instead of `<`. `CFE_ES_GenPoolValidateState()` now returns `true` if using the max number of buckets (17 by default) and the pool structure using max value will correctly validate\\n- Replaces remaining `CFE_ES_ERR_BUFFER` with `CFE_ES_BAD_ARGUMENT` for when functions receive an invalid null-pointer argument. Adds null pointer checks in `cfe_es_api.c`.\\n- Adds branch coverage to html report when running `make lcov`\\n- See <https://github.com/nasa/cFE/pull/1258>\\n\\n### Development Build: v6.8.0-rc1+dev436\\n\\n- Adds a local definition of SOFTWARE_BIG/LITTLE_BIT_ORDER directly inside cfe_endian.h to provide a compatible symbol for apps that still require this. This allows CFE to build and run successfully when OSAL stops providing this in `common_types.h`.\\n- Removes incorrect statements from Application Developers Guide\\n- Fixes truncation handling on vsnprintf error by adding a cast to avoid implicit conversion\\n- Clarify the documentation on SB MsgId regarding requirements for command and telemetry messages\\n- Avoids undefined behavior and resolves static analysis warnings by casting isspace input to unsigned char.\\n- Updates message module and msgid v1, CFE_MSG_SetMsgId, to use mask instead of cast to alter value. Resolves static analysis warning.\\n- Updates CFE_ES_FileWriteByteCntErr to report status, not a size_t actual since OS_write returns int32. Use int16 for local type from CFE_TBL_FindTableInRegistry since it\\'s an index, not a status.\\n- Replaces <> with \" in local #includes\\n- Adds CONTRIBUING.md that links to the main cFS contributing guide.\\n- See <https://github.com/nasa/cFE/pull/1243>\\n\\n### Development Build: v6.8.0-rc1+dev412\\n\\n- Apply standard code style and format\\n- Add new continuous integration workflow to enforce this format\\n- See <https://github.com/nasa/cFE/pull/1219>\\n\\n### Development Build: v6.8.0-rc1+dev402\\n\\n- HOTFIX 20210312, updates to work with older CMake\\n- Fix #972, reorganize directory structure\\n- HOTFIX IC 2021-03-05: Correct static app build issue\\n- See <https://github.com/nasa/cFE/pull/1222>\\n\\n### Development Build: v6.8.0-rc1+dev392\\n\\n- Fix #665, update pipe name documentation.\\n- Fix #1165, remove configs about shells\\n- Fix #1094, Update CRC documentation\\n- Fix #979, add stack size and priority to task info …\\n- Fix #1170, refactor target config objects …\\n- Fix #1207, Add wrapper targets to simplify app builds …\\n- Fix #1211, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/cFE/pull/1213>\\n\\n### Development Build: v6.8.0-rc1+dev382\\n\\n- Refactors the SB buffer descriptor object `CFE_SB_BufferD_t` and simplify the zero-copy buffer paradigm. Combines the zero-copy and the normal CFE buffer descriptor into a single unified `CFE_SB_BufferD_t` object. Results in a simpler zero-copy design that is similarto the the standard, non-zero-copy message path. All message descriptor objects are now tracked in a list by SB. All changes are internal to SB. This does not affect API or behavior of any existing APIs (but see note). Corrects a minor issue where the `MsgSendErrorCounter` would get incremented if there were no subscribers, but only in the zero copy API.  \\n- Replaces `int_32` with `CFE_Status_t` for all error message codes  \\n- Removes references to `cfeesugshellsrv` from user guide\\n- Adds null pointer checks and size checks to validate method parameters. Returning the input instead of an error code\\n- Removes use of `LogEnabled` element in HK telemetry for EVS logic since log is always enabled now. On failures, reset area or semaphore will panic.\\n-  Fixes various build warnings when `BUILDTYPE=release`.\\n- See <https://github.com/nasa/cFE/pull/1196>\\n\\n### Development Build: v6.8.0-rc1+dev365\\n\\n- Implements a generic FS facility to perform file writes as a background job. Applications wanting to use this need to instantiate a state object (metadata) in global memory and two callback APIs, one to get a data record, another to send events. The following file requests are changed to use this facility:\\n  - ES ER Log dump\\n  - SB Pipe Info\\n  - SB Message Map\\n  - SB Route Info\\n  - TBL Registry Dump\\n- Changes the internal SB member names for consistency and thus fixes propagation of `Depth` and `CurrentDepth` into files:\\n  - `MaxQueueDepth` for maximum depth at queue creation time (previously was QueueDepth or Depth depending on context)\\n  - `CurrentQueueDepth` for the running count (previously was InUse or CurrentDepth depending on context)\\n  - `PeakQueueDepth` for the highest \"watermark\" (previously was PeakInUse or PeakDepth depending on context)\\n- Encapsulates all parameters for apps and tasks into a structure object. Cleans up internal APIs to pass this new object rather than individual parameters. Adds details to the relevant record (i.e. a task record has all relevant task details) which eliminates the need to traverse the app record to find some data.\\n- Enables items in `FILELIST` to be in a target name directory as well as symlinks. `arch_build.cmake` now checks a name-based subdirectory under `${MISSION_DEFS}` for files listed in the `FILELIST` for that target. If file is a symlink, the link should be followed so the correct content is installed, not a symlink.\\n- Adds documentation on  inclusion presence of null terminators for length parameters.\\n- Shortened `CFE_PLATFORM_ES_DEFAULT_TASK_LOG_FILE` name so it is within the `OSAL_MAX_FILE_NAME` size limit. Will now output task info to default filename if no filename is provided in command.\\n- Replaces `UT_Stub_SetForceFail` with `UT_Stub_SetDefaultReturnValue`. No behavior change.\\n- See <https://github.com/nasa/cFE/pull/1171>\\n\\n### Development Build: v6.8.0-rc1+dev348\\n\\n- Corrects reference to PSP header file location. Build now succesfully completes the build succeeds again when using `add_psp_module()` in custom CMakeLists file.\\n- Replace \"send\" with \"write\" in names for commands that write files. For example, `CFE_SB_**SEND**_ROUTING_INFO_CC` is now `CFE_SB_**WRITE**_ROUTING_INFO_CC`. Updates function names, command code names and comments.\\n- Removes incorrectly implemented deferred return code of `-1` for `CFE_SB_ReceiveBuffer` from software bus setup in `UT_InitData`.\\n- Implements more informative **assert messages** by making `SETUP, TEARDOWN, ASSERT` print `0x%lx` while `ASSERT_EQ` now prints both `%lf` and `0x%lx` format for the inputs\\n- Updates continuous-integration badges in `ReadMe.md`. The badges now reflect the success status of different runs.\\n- Remove `Test_SB_Cmds_SubRptUnexpCmdCode` which was a duplicate of `Test_SB_Cmds_CmdUnexpCmdCode` and did not implement any new tests.\\n- Initializes status in `CFE_ES_WaitForSystemState` and adds missing success test case so the function doesn\\'t return an uninitialized `Status`.\\n- Removes the `HkPacket` and `TblRegPacket` message initializations from `CFE_TBL_EarlyInit` since they are initialized in `CFE_TBL_InitData`. Moves the `NotifyMsg` message initialization to `CFE_TBL_InitData` and sets the message ID each time it\\'s sent from `CFE_TBL_SendNotificationMsg`. Possibly results in small performance improvement since the message isn\\'t initialized every call.\\n- Removes unimplemented `CFE_ES_AppGetList` and `CFE_ES_AppDumpAllInfo` prototypes.\\n- Adds a 15-minute timeout to continuous integration workflows to prevent excess resource utilization.\\n- Makes debug subscription events only print the Pipe ID, not a name, in the debug events.\\n- Updates the documentation and verification for `CFE_PLATFORM_SB_HIGHEST_VALID_MSGID` to allows the full range of values.\\n- Clarifies the difference between \"restart\" and \"reload\" in API/cmd and user\\'s guide documentation for `CFE_ES_RESTART_APP_CC`.\\n- Switches throttle indexes to use `CFE_SB_RouteId_Atom_t` and combines helper function given that msgid was removed due to being a resource hog. Resolves static analysis warning.\\n- `CFE_ES_RestartApp` now checks for file existence as part of command processing and does not remove the app if the file doesn\\'t exist (just avoids one error case). it also rejects the command and increments command error counter if file is missing.\\n- Removes `CFE_PLATFORM_SB_MAX_PIPE_DEPTH` in favor of `OS_QUEUE_MAX_DEPTH`. This depth parameter in command is now checked prior to attempting OSAL call.\\n- Filters pointer now `const` in API and reports truncation when registering filters with `CFE_EVS_Register`.\\n- Removes the ability to disable the log by not defining `CFE_PLATFORM_EVS_LOG_ON` so users are no longer able to disable log completely. For minimum memory use define `CFE_PLATFORM_EVS_LOG_MAX = 1`. Note: This could remove control based on LogEnabled, panic on reset area fail and limp along if \"sem create\" fails.\\n- Removes the remnants of the table service exclusion logic and documentation: `EXCLUDE_CFE_TBL` no longer available, even if defined, table services will still start.\\n- Set ES and EVS pipe message limit to defaults as opposed to the custom, unjustified, `CFE_SB_SubscribeEx`. This change might queue additional HK messages, but SCH loads after ES anyways.\\n- Replaces `CFE_SB_Default_Qos` with `CFE_SB_DEFAULT_QOS` macro that avoids global variable exposure. Removes SB-internal defines that are not implemented nor used.\\n- Explicity `memset` the task data to zero at the start of EarlyInit. Standardize the global typdef/variable names.\\n- Moves all functions, macros, types, and other definitions related to resource IDs and generic resource management into a separate module, like `CFE MSG`, `SBR`, etc. This allows a mission to elect \"strict\" implementations of these objects, where every ID type is unique, and assigning between them or `uint32` results in a compiler error. **API now has separate types for each resource type (Apps, Tasks, Libs, Counters, etc).** The user can elect at the mission level whether this is a simple typedef (all uint32, all interchangeable) or a wrapper type (separate/strict type, cannot be interchanged). The former is backward compatible but the latter is not - must use proper types.\\n- Adds Code QL analysis to continuous integration workflow.\\n- See <https://github.com/nasa/cFE/pull/1150>\\n\\n### Development Build: v6.8.0-rc1+dev290\\n\\n- Documentation: Add Security.md with instructions to report vulnerability\\n- Documentation: Update cpuname/MISSION_CPUNAMES documentation\\n- Fixes `UT_CheckEventHistoryFromFunc()` helper routine to read the correct number of IDs so it checks the correct number of events. Also correct bad event checks in TBL UT.\\n- Adds `OS_printf` to `CFE_ES_SYSLOG_APPEND` so it matches `CFE_ES_WriteToSysLog`\\n- Removes unused `SenderReporting` and `CFE_PLATFORM_SB_DEFAULT_REPORT_SENDER`\\n- Tests pass when debug events are enabled via `CFE_PLATFORM_EVS_DEFAULT_TYPE_FLAG` in platform config.\\n- Removes references to `UT_CheckForOpenSockets` which is no longer applicable since the UT framework resets the state for each unit test.\\n- Rename `UT_ClearForceFail` as `UT_ClearDefaultValue` given change in https://github.com/nasa/osal/issues/724\\n- Adds checks that ensure `CFE_SB_GetUserData` works with all payload data types.\\n- Adds header padding to 64-bit so that `CFE_SB_GetUserData` will work for message structures with elements up to 64 bit\\n- For primary-only header config: telemetry header required to 64 bit boundary (affects all receivers)\\n- For primary and extended header config: command header required padding to 64 bit boundary (affects all senders)\\n- Refactor `CFE_TIME_RegisterSynchCallback` to only have one return point and eliminates \"possible uninitialized variable\" static analysis warning\\n- None of these changes are expected to cause problematic.\\n- Addresses message delivery issues due to inconsistent locking by reworking cFE-SB API implementation. Ensures all events are generated and counters are incremented consistently by avoiding early returns in functions and using the `PendingEventID` register to record what event ID should be sent per the current operation.\\n- Employs the `CFE_ES_ResourceID_t` type and related patterns for managing the SB Pipe IDs.\\n- Will break code which directly accessed these items without going through the lookup function.\\n- **`CFE_SB_PipeId_t` type is no longer usable as a direct array index**, increased in size from 8 to 32 bits, and is now consistent with all other ID types in both behavior and size.\\n- **The \"pipe stats\" structure in the Pipe TLM is also changed**. This structure contained a `CFE_SB_PipeId_t` value, hence why it had to be updated because the type is now bigger. The spare bytes are also moved to the end of the struct.\\n- Removes `OS_printf` checks of stub calls in unit tests and checks for specific format string in history instead to confirm the right path was taken.\\n- Removes `CFE_MISSION_REV` from platform config.\\n- Removes the rest of the references and uses of `CFE_PLATFORM_ES_PERF_MAX_IDS` in favor of `CFE_MISSION_ES_PERF_MAX_IDS`\\n- Remove uses of strncpy and other minor hardcoded references\\n- Cleanup unit tests to reflect size changes in `CFE_MISSION_MAX_API_LEN` and `CFE_MISSION_MAX_PATH_LEN`.\\n- Moved ES pipe name and lengths to defines\\n- Removed PipeName and PipeDepth variables from app global\\n- Removed unnecessary (char *) casts\\n- Simplified `&stingname[0]` to `stringname` where observed\\n- Enables projects that have OSs with different limits to maintain a standard cmd/tlm and have unit tests pass.\\n- Make `CFE_ES_WriteToSysLog` stub unit test more informative by adding `UtDebug` output\\n- See <https://github.com/nasa/cFE/pull/1109>\\n\\n### Development Build: v6.8.0-rc1+dev248\\n\\n- Replace `OS_FileSysStatVolume()` with`OS_fsBlocksFree()` which will be deprecated. This call reports the number of total blocks, not just the free blocks, making the check more accurate and removing the need for a workaround for desktop machines.\\n- Instead of accessing `OS_time_t` values directly, use the OSAL-provided conversion and access methods. This provides independence and abstraction from the specific `OS_time_t` definition and allows OSAL to transition to a 64 bit value.\\n- Removes the spurious `CFE_SB_TimeOut_t` typedef from `cfe_sb.h`. May affect any apps that inappropriately rely on the private typedef.\\n- Removes unused `network_includes.h`. Not used by the framework anywhere,  apps should use OSAL Socket APIs instead.   \\n- Fixes deprecation directive typos\\n- See <https://github.com/nasa/cFE/pull/1088>\\n\\n### Development Build: v6.8.0-rc1+dev236\\n\\n- Resolved doxygen warnings for osalguide and updated header file references\\n- Corrects the documentation for the CFE_SB_GetPipeName() unit test stub function.\\n- Adds a new github actions workflow file to run cppcheck\\n- See <https://github.com/nasa/cFE/pull/1066>\\n\\n### Development Build: v6.8.0-rc1+dev228\\n\\n- Remove use of `osapi-os-loader.h` from ES UT.\\n- Use volatile `sig_atomic_t` for system state to avoid race issue if uint32 isn\\'t atomic on a system\\n- Set the flags parameter on the OS_ModuleLoad() properly to allow an app to be properly unloaded, which in turn allows the reload command to work as expected. Fixes problem where unload comand resulted in continuous restarting of the same app code.\\n- Replaced `Test_MSG_PrintMsg` with `UT_DisplayPkt`. Also removed unused `Test_MSG_Sum`.\\n- See <https://github.com/nasa/cFE/pull/1047>\\n\\n### Development Build: v6.8.0-rc1+dev218\\n\\n- Adds `CFE_SB_TransmitMsg`, `CFE_SB_TransmitBuffer`, `CFE_SB_ReceiveBuffer`\\n  - Main change is to utilize `CFE_SB_Buffer_t` and `CFE_MSG_Message_t` in a consistent manner to facilitate alignment\\n  - Deprecates multiple `CFE_SB_*` items\\n  - Redefines `CFE_MSG_Size_t` as `size_t` to minimize duplicated work and facilitate transition to `size_t`\\n- Use a generic void* as the interface type for the pool buffer pointers. This reduces the need for local type casting in the apps and makes it generally easier to use.\\n- Remove  reference to CEXP in RTEMS 4.11 i686 toolchain. Add an RTEMS 5.1 i686 toolchain file.\\n- See <https://github.com/nasa/cFE/pull/1045>\\n\\n\\n### Development Build: v6.8.0-rc1+dev204\\n\\n- Backward compatible API change. Replace many uses of generic uint16 and uint32 with a more purpose-specific type. Replace all sizes with size_t across the API.\\n- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing\\n- Deprecates many SB Elements and replaces them with the new MSG module. See https://github.com/nasa/cFE/issues/777 for list.\\n-  App and Lib info telemetry structures no longer contain the `ModuleId` value from OSAL.\\n- Add an extra write of a null char after `strncpy` which squelches a warning and appease compiler warning logic.\\n- Uses `CFE_PLATFORM_ES_DEFAULT_STACK_SIZE` as a default instead of a minimum. Affects the Start App command; if stack size is specified as zero, then the default stack size value from platform config is used. Otherwise the value in the command will be passed through and used as-is.\\n- Changes the type of the AppID parameter on \"Cleanup\" routines from `uint32` to `CFE_ES_ResourceID_t`.\\n- See <https://github.com/nasa/cFE/pull/1027>\\n\\n### Development Build: v6.8.0-rc1+dev179\\n\\n- Adds macros for more compact calls to `CFE_EVS_SendEvent`, making the type be part of the function name.\\n- The sample configs leap seconds default value is now up to date. (As of Oct 2020)\\n- Removed the clear=false logic (and clear parameter) `CFE_MSG_Init()` now always zeroes entire message and sets defaults.\\n- Adds flags parameter to calls to `OS_ModuleLoad()`. Initially just pass 0 (GLOBAL) to maintain old behavior.\\n- Updates `CFE_ES_RegisterCDSEx` stub to match current signature\\n- Includes `cfe_private.h` for stubs that implement related elements.\\n- See <https://github.com/nasa/cFE/pull/1008>\\n\\n### Development Build: v6.8.0-rc1+dev164\\n\\n- Keeps task names under 16 chars to make more debugger friendly, regardless\\nof the OSAL limit. Task name shows up as `ES_BG_TASK`\\n- Move ES typedefs shared across API and telemetry messages into the `cfe_es_extern_typedefs.h`.\\n- Move all ES typedefs that define the telemetry interface and structures that define the output of commands that write data files into this group (query all apps, query all tasks, query all CDS).\\n- Remove some localized definitions and replace with MISSION scope definitions where appropriate/necessary.\\n- Adjust `strncpy()` call to avoid compiler warning\\n- Cast fixed width types to the type used in the `printf` call. Removes `printf` type warnings on the 32-bit RTEMS build.\\n- See <https://github.com/nasa/cFE/pull/991>\\n\\n### Development Build: v6.8.0-rc1+dev150\\n\\n- Provide new Library API similar to App API\\n- Allows the existing CFE_ES_AppInfo_t structure to be extended to libraries as well as applications by introducing a new value (3) for the Type field.\\n- Allows Libraries to be queried via API calls similar to App API.\\n- Extends the Query All/Query One commands to operate on Libraries or Applications.\\n- Breaks up the monolithic AppCreate and LoadLibrary functions and have these call subroutines that operate on the common components.\\n- Fix race conditions in app request processing state machine.\\n- Adds SBR module which includes message map and routing table. The access APIs are on the SB side which still owns the destination logic\\n- Removes passing of route index or pointers being. Everything is now based on route and message id\\n- Oversized the hash message map (4x) to minimize collisions\\n- Hash designed for 32 bit, a change in CFE_SB_MsgId_Atom_t size may require implementation updates\\n- Adds debug event for collisions during add\\n- Dropped routing push/pop, dropped \"key\" in direct implementation\\n- Deletes unused code CFE_SB_FindGlobalMsgIdCnt\\n- Fixes variable declaration violations of coding standard\\n- Individual events for deleting destinations when deleting a pipe removed to avoid a race condition around a 10-20% performance hit to hash via rough comparison on a linux box, no memory impact\\n- See <https://github.com/nasa/cFE/pull/975>\\n\\n### Development Build: v6.8.0-rc1+dev139\\n\\n- For all resource types which have names, IDs are not re-issued after deletion, helping ensure safety as previously deleted IDs will not validate. Provides a consistent Name-ID translation API for all resource types. Enforces consistent argument/name validation on all resource types, and also enforces name uniqueness where relevant.\\n- Enhancement to use the full 16 bits of resource ID space, which avoids repeating ID values that have already been used. This significantly decreases the likelihood that a previously deleted ID will alias a newly allocated/valid ID.\\n- See <https://github.com/nasa/cFE/pull/959>\\n\\n### Development Build: v6.8.0-rc1+dev129\\n\\n- Rather than having a second pool implementation only for CDS, use the generic pool implementation. This also uses the abstract resource identifiers to identify CDS blocks, rather than a direct reference.\\n- Add the system-specific module suffix (.o, .so, .obj, etc) and the default CFE core executable name to the configdata structure.\\n- See <https://github.com/nasa/cFE/pull/944>\\n\\n### Development Build: v6.8.0-rc1+dev122\\n\\n- Adds the field `UnregAppID` to track whether an \"unregistered\" event was generated, un-overloading the EventCount field to serve its primary purpose of counting actual events generated from a valid/registered AppID.\\n- Move the AppID lookup execution to be early in the `CFE_SB_SendMsgFull` implementation. This avoids double locking between SB+ES and avoids a block-scope local variable.\\n- Instead of identifying a memory pool by its memory address, use a resource ID. IDs are a constant size, regardless of whether the host machine is 32 or 64 bits.\\n  - IDs can be put into commands/telemetry and maintain a more consistent format with consistent alignment requirements.\\n  - IDs can be independently verified without dereferencing memory. Previously the only way to validate a memory pool was to read the address pointed to, which results in a segfault if the address was bad.\\n- Change from `OS_MAX*` defines to appropriately-scoped CFE defines for array sizing\\n- This creates the new `CFE_Status_t` typedef for function\\'s return status codes. Also adds a note to `CFE_TBL_GetStatus` since its behavior will likely change in the future in the hopes of not having a non-zero \"info\" status.\\n- See <https://github.com/nasa/cFE/pull/936>\\n\\n### Development Build: v6.8.0-rc1+dev109\\n\\n- Add a new typedef `CFE_ES_ResourceID_t` that can replace `uint32` for all ID storage and manipulation. Initially this is just an alias to `uint32` for backward compatibility.\\n- See <https://github.com/nasa/cFE/pull/916>\\n\\n### Development Build: v6.8.0-rc1+dev105\\n\\n- Removes dependency on CCSDS version define.\\n- Removes old name and id defines.\\n- CFE_ES_CalculateCRC default stub behavior.\\n- Replaces calls to OS_open and OS_creat\\n- Replaces UT_Text with UtPrintf\\n- Updates variable checks in read_targetconfig\\n\\n- See <https://github.com/nasa/cFE/pull/912>\\n\\n### Development Build: v6.8.0-rc1+dev91\\n\\n- Sets Revision to 99 for development build.\\n- Installs unit test to target directory.\\n- Returns processor ID to default to unbreak toolchain\\n- Applies the appid/taskid/counterid pattern to Library resources.\\n- See <https://github.com/nasa/cFE/pull/891>\\n\\n### Development Build: v6.8.0-rc1+dev81\\n\\n- Deconflict CFE_ES_LIB_ALREADY_LOADED and CFE_ES_ERR_SYS_LOG_TRUNCATED EIDs\\n- Scrub all CFE references/uses of OSAL IDs to use the proper osal_id_t type. Any place that an OSAL ID is stored in memory or passed in an API call are changed to the osal_id_t type, rather than uint32. Conversions between this and other types (e.g. bare integer) is done using the OSAL-supplied conversion helpers.\\n- After the changes implemented in #101, there may be routing table entries with no subscribers (RoutePtr->ListHeadPtr would be NULL.) This could cause a seg-fault. Also, even if there are entries in the routing table, there will be no event generated if the unsubscribe does not find a matching route entry.\\n- Adds debug message.\\n- Applies the appid/taskid pattern to Generic Counter resources.\\n- Adds test for SB subscribe/unusubscribe/unsubscribe.\\n- See <https://github.com/nasa/cFE/pull/876>\\n\\n\\n### Development Build: v6.8.0-rc1+dev65\\n\\n- In the next major CFE release, this code will be no longer supported at all. It should be removed early in the cycle to avoid needing to maintain this compatibility code.\\n- The CFE_ES_FindCDSInRegistry function had an unusual loop control structure with mixed types of signed and unsigned. This has the possibility of being infinite if the MaxNumRegEntries is zero due to the way the end condition is structured. Simplify to be like other loops and use unsigned int control variable.\\n- Fixes the cast-align error (use the aligned Msg since it\\'s available already).\\n- HOTFIX-20200902 - Fix sb unit test setup issue.\\n- HOTFIX 20200902 - Update documentation links for deprecated symbols.\\n- HOTFIX 20200902 - Fix SB Test_CleanupApp_API AppID.\\n- See <https://github.com/nasa/cFE/pull/861>\\n\\n### Development Build: v6.8.0-rc1+dev42\\n\\n- Removes reference from documentation.\\n- CFE_SB_SendMsg stub now behaves the same as CFE_SB_TimeStampMsg (copies message pointer from local). No longer need to emulate CFE_SB_InitMsg from test code, set the API/stub data buffers directly.\\n- Removed iterator modification from within the loop... replaced with break.\\n- Resolves loop iterator size too small for comparison.\\n- Replaced CFE_MISSION_SPACECRAFT_ID use with CFE_PSP_GetSpacecraftId() and updated unit test\\n- See <https://github.com/nasa/cFE/pull/849>\\n\\n### Development Build: v6.8.0-rc1+dev28\\n\\n- Add msg stubs, update SB_UT to use them, and remove msg module include from unit tests\\n- Collapses time options down to just 32 bit second, 16 bit subsecond, always big endian. Removes old defines, and triggers an error if the configuration is set to a format that was removed.\\n- Enables source selection and out-of-tree mission-defined overrides in the msg directory\\n- Unit tests added from within unit tests will not execute, replaced this pattern with direct calls to the main subtest setup routine.\\n- See <https://github.com/nasa/cFE/pull/833>\\n\\n### Development Build: v6.8.0-rc1+dev13\\n\\n- Deprecates `CFE_SB_GetLastSenderId()` API by introducing new `CFE_OMIT_DEPRECATED_6_8` tag\\n- Documentation update remove deleted requiremements\\n- Add a new *cfe_assert* module for functional testing by making it possible to load the UT assert object code as a CFE library. These are compiled as separate, independent modules and only loaded on demand. Also includes a draft example for cFE testing, which calls some basic ES AppId functions.\\n- See <https://github.com/nasa/cFE/pull/816>\\n\\n### Development Build: v6.7.0+dev292\\n\\n- Add missing include path to the target/h and wrn/coreip directory.\\nSet and clarify difference between WIND_HOME and WIND_BASE variables.\\nRemove unrelated comment about CEXP (remnant from RTEMS). No more errors about missing headers.\\n- Version reporting is does not span multiple lines.\\n- See <https://github.com/nasa/cFE/pull/792>\\n\\n### Development Build: v6.7.0+dev289\\n\\n- Update `CFE_SB_TimeStampMsg` to save the message pointer argument `UT_Stub_CopyFromLocal` so that unit tests can check it\\n- Only affects build system. Fully backward compatible. The defaults are applied if a user has nothing specifically configured in their `targets.cmake`. The defaults will select osal, cfe-core, and psp as before. The user now has the option to explicitly configure and control the inclusion of these modules and also provide mission-specific search paths to override them as desired.\\n- Note this only affects UT stubs. Change the internal names of some stub arguments to match prototype. Ensure that:\\n  - All functions in the CFE public API have a stub function implemented\\n  - All parameters to the stub function are registered in the context object, so the values will be available to hook functions.\\n  - The names of all parameters match the prototype/documentation, so hook functions that use name-based argument value retrieval will work.\\n-  Adds to table search path in `arch_build.cmake`\\n- Calls to OS_open() now use the OSAL-defined symbol, not the POSIX symbol.\\n-  Defines new macros to report the build number and build baseline and new strings containing the version number of cFE and a combined string with the version number for OSAL, PSP, and CFE.\\n- Allow explicitly setting of the processor ID in `targets.cmake`. The `TGTx_PROCESSOR_ID` setting will be passed to the final build/link of CFE core as the CPU ID. If unspecified, then the CMake index value is used instead (backward compatible).\\n- `cmake` now detects conditions where no files were present to fulfill an config include file requirement and reports an error during `make prep` lists the files it checked for rather than generating an empty file.\\n- See <https://github.com/nasa/cFE/pull/765>\\n\\n### Development Build: 6.7.21\\n\\n- If a string is exactly the size of the field when using the `CFE_TBL_FILEDEF()` macro it will produce a compiler error\\n- Added cFE User\\'s Guide Reference to README.md\\n- Removes old license\\n- See <https://github.com/nasa/cFE/pull/743>\\n\\n### Development Build: 6.7.20\\n\\n- SB Unit use of the UT assert framework is closer to original design intent\\n- See <https://github.com/nasa/cFE/pull/743>\\n\\n### Development Build: 6.7.19\\n\\n- API Change: cFE ut_sb_stubs now has CFE_SB_DeletePipe available.\\nBehavior Change: App unit tests requiring this will not fail to build due to undefined reference to CFE_SB_DeletePipe\\n- Hook functions may now use the va_list form and obtain the full set of variable arguments passed to CFE_EVS_SendEvent and variants.\\n- Replace all direct references to data types defined in ccsds.h with the abstract type defined in cfe_sb.h.\\n- See <https://github.com/nasa/cFE/pull/729> for details.\\n\\n### Development Build: 6.7.18\\n\\n- Using ut stubs CFE_EVS_SendEvent and CFE_ES_WriteToSysLog, the register buffer will have the correct size. access to register buffer element will exist\\n- Both the main task and the child task(s) are successfully deleted and restarted after the exception occurs.\\n- Fixes doxygen warnings for the tbl subsystem.\\n- No compiler warnings or errors on cross build.\\n- Changes Message Key from uint16 to uint32 to avoid rollover and system hang\\n- See <https://github.com/nasa/cFE/pull/712> for more details\\n\\n### Development Build: 6.7.17\\n\\n- No longer automatically decompresses apps/libraries as part of load\\n- Deletes now unused CFE_ES_CountObjectCallback and CFE_ES_ListResourcesDebug. Flags were unused\\n- Removes all conditional preprocessing blocks related to CFE_ARINC653.\\n- Ensure clean build, no warnings on string operations using GCC 9.3.0.\\n- When OMIT_DEPRECATED = true attempt to send output to shell command will result in command error counter increment (unrecognized function code)\\n- SBN will need to init command with new MID\\n- Documentation links and references will now work properly\\n- API CFE_ES_ProcessCoreException is removed, replaced with async event.\\n- Removed duplicate prototype in cfe_time_utils.h\\n- Removes unused defines and adds documentation to TBL event defines.\\n- Deprecates CFE_TIME_CFE2FSSeconds and CFE_TIME_FS2CFESeconds.\\n- Unit tests now build and run when MESSAGE_FORMAT_IS_CCSDS_VER_2 is configured.\\n- Build now works with both extended headers and OMIT_DEPRECATED options set.\\n- No more alignment warnings\\n- Adds new unit test macros\\n- See <https://github.com/nasa/cFE/pull/692> for more details\\n\\n### Development Build: 6.7.16\\n\\n- Users must now select OSAL options via the CMake file in their defs directory, rather than the osconfig.h file.\\n- See <https://github.com/nasa/cFE/pull/672> for more details\\n\\n### Development Build: 6.7.15\\n\\n- Upon power on reset, default system log mode set to overwrite. Upon processor reset, default system log mode set to discard.\\n- No longer locks while locked (no issue observed on linux/posix, but user reported issue on FreeRTOS 10)\\n- Internal `CFE_TBL_LoadFromFile()` API changed slightly to add AppName as a parameter. Return value from `LoadFromFile()` no longer relevant for event generation.\\n- Updates `CFE_TBL_CleanUpApp` such that it now checks the \\'used flag\\' prior to calling `CFE_TBL_RemoveAccessLink` for a given TblHandle. Also sets the AppId to `CFE_TBL_NOT_OWNED` after removing the access descriptor link from linked list.\\n- Removed `OS_FS_SUCCESS, OS_FS_ERROR , OS_FS_ERR_INVALID_POINTER, OS_FS_ERR_NO_FREE_FDS , OS_FS_ERR_INVALID_FD, and OS_FS_UNIMPLEMENTED` from `osapi-os-filesys.h`\\n- See <https://github.com/nasa/cFE/pull/649> for more details\\n\\n### Development Build: 6.7.14\\n\\n- Exposes the `CFE_SB_IsValidMsgId()` for application usage.\\n- `CFE_SB_GetLastSenderID` will now detect if it is being called prior to a message being sent on a given pipe.\\n- Mismatches between PSP/BSP/OS are now detected and warned about during make prep. Only the `CFE_SYSTEM_PSPNAME` is actually required to be specified for a CFE build now. Others can be omitted.\\n- See <https://github.com/nasa/cFE/pull/635> for more details\\n\\n### Development Build: 6.7.13\\n\\n- RTEMS builds without error.\\n- Use the INTERFACE_COMPILE_DEFINITIONS and INTERFACE_INCLUDE_DIRECTORIES properties from the osal target and apply them to the entire CFE build as a directory-scope property. No impact until these are set in OSAL.\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/615>)\\n\\n### Development Build: 6.7.12\\n\\n- Cmd code (and checksum) are always in the same place (matches GSFC spec for command secondary header)\\n- No impact to behavior. Previously the perf log dump file frequently contained errors due to out of order or otherwise corrupted entries, which is now fixed.\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/586>)\\n\\n### Development Build: 6.7.11\\n\\n- Improve documentation\\n- Update makefile to report branch coverage\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/566>)\\n\\n### Development Build: 6.7.10\\n\\n- Fix potential unit test problems with name collisions\\n- Improve documentation\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/563>)\\n\\n### Development Build: 6.7.9\\n\\n- No longer requires sed \"hack\" to change the setting in default_config.h\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/544>)\\n\\n### Development Build: 6.7.8\\n\\n- Updates and cleaned up documentation and requirements\\n- Fixes incorrect debug messages\\n- Decrease child task count when one is deleted\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/530>)\\n\\n### Development Build: 6.7.7\\n\\n- Adds a new function, CFE_SB_GetPipeIdByName, which retrieves the pipe ID given a name of a pipe.\\n- Improvement in error reporting when using a pipe name that is already in use, or when the queue limit has been reached.\\n- Added userguide and osalguide to the local target list to avoid makefile warning\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/511>)\\n\\n### Development Build: 6.7.6\\n\\n- Adds OMIT_DEPRECATED prep flag\\n- Adds and enforces strict warnings\\n- Software Bus now increments sequence counter even if there are no subscribers\\n- Warning, comment, and code coverage cleanup (see <https://github.com/nasa/cFE/pull/490>)\\n\\n### Development Build: 6.7.5\\n\\n- Added CI static analysis check\\n- Resolved static analysis warnings\\n- Minor other updates (see <https://github.com/nasa/cFE/pull/479>)\\n\\n### Development Build: 6.7.4\\n\\n- Minor updates (see <https://github.com/nasa/cFE/pull/448>)\\n\\n### Development Build: 6.7.3\\n\\n- Minor updates (see <https://github.com/nasa/cFE/pull/413>)\\n\\n### Development Build: 6.7.2\\n\\n- Minor bugs and enhancements (see <https://github.com/nasa/cFE/pull/388>)\\n\\n### Development Build: 6.7.1\\n\\n- Fix strlen in CFE_ES_TaskInit <https://github.com/nasa/cFE/pull/23>\\n- Minor bug fixes (see <https://github.com/nasa/cFE/pull/378>)\\n\\n### **_OFFICIAL RELEASE: v6.7.0 - Aquila_**\\n\\n- This is a point release from an internal repository\\n- Changes are detailed in [cFS repo](https://github.com/nasa/cFS) release documentation\\n- Apache 2.0\\n\\n### **_OFFICIAL RELEASE: v6.6.0a_**\\n\\n- This is a point release from an internal repository\\n- Apache 2.0\\n- Additional release notes are found in [release notes](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_release_notes.md)\\n- See the [version description document](https://github.com/nasa/cFE/blob/v6.6.0a/docs/cFE_6_6_0_version_description.pdf) for the full document\\n- Test results can be found in [test results](https://github.com/nasa/cFE/tree/v6.6.0a/test-and-ground/test-review-packages/Results)\\n\\n## Known issues\\n\\nSee all open issues and closed to milestones later than this version.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/harmony',\n",
       "  'language': 'TypeScript',\n",
       "  'readme_contents': '# Harmony\\n\\nServices.  Together.\\n\\nHarmony has two fundamental goals in life:\\n1. **Services** - Increase usage and ease of use of EOSDIS\\' data, especially focusing on opportunities made possible now that data from multiple DAACs reside in AWS.  Users should be able to work seamlessly across data from different DAACs in ways previously unachievable.\\n2. **Together** - Transform how we, as a development community, work together to accomplish goal number 1.  Let\\'s reuse the simple, but necessary components (e.g. EDL, UMM, CMR and Metrics integration) and let\\'s work together on the stuff that\\'s hard (and fun) like chaining, scaling and cloud optimizations.\\n\\nFor general project information, visit the [Harmony wiki](https://wiki.earthdata.nasa.gov/display/Harmony). Harmony discussion and collaboration occurs in the EOSDIS #harmony Slack channel.\\n\\n\\n## Table of Contents\\n\\n1. [Development Prerequisites](#Development-Prerequisites)\\n    1. [Earthdata Login Application Requirement](#Earthdata-Login-Application-Requirement)\\n    2. [Software Requirements](#Software-Requirements)\\n2. [Running Harmony](#Running-Harmony)\\n    1. [Set Up Environment Variables](#Set-Up-Environment-Variables)\\n    2. [Run Tests](#Run-Tests)\\n    3. [Set Up A Database](#Set-Up-A-Database)\\n    4. [Set Up and Run Argo, Localstack](#Set-Up-and-Run-Argo,-Localstack)\\n    5. [Add A Service Backend](#Add-A-Service-Backend)\\n    6. [Run Harmony](#Run-Harmony)\\n    7. [Connect A Client](#Connect-A-Client)\\n3. [Local Development Of Workflows Using Visual Studio Code](#Local-Development-Of-Workflows-Using-Visual-Studio-Code)\\n4. [Running in AWS](#Running-in-AWS)\\n5. [Contributing to Harmony](#Contributing-to-Harmony)\\n6. [Additional Resources](#Additional-Resources)\\n\\n## Development Prerequisites\\n\\nFor developing Harmony on Windows follow this document as well as the information in [docs/dev_container/README.md](docs/dev_container/README.md).\\n\\n### Earthdata Login Application Requirement\\n\\nTo use Earthdata Login with a locally running Harmomy, you must first set up a new application in the Earthdata Login UAT environment using the Earthdata Login UI.  https://wiki.earthdata.nasa.gov/display/EL/How+To+Register+An+Application.  This is a four step process:\\n\\n1. Request and receive permission to be an Application Creator\\n2. Create a local/dev Harmony Application in the EDL web interface\\n3. Add the necessary Required Application Group\\n4. Update .env with credentials\\n\\nYou must select \"401\" as the application type for Harmony to work correctly with command line clients and clients like QGIS.  You will also need to add the \"eosdis_enterprise\" group to the list of required application groups in order for CMR searches issued by Harmony to be able to use your Earthdata Login tokens.  Update `OAUTH_CLIENT_ID` and `OAUTH_PASSWORD` in .env with the information from your Earthdata Login application. Additional information including other OAUTH values to use when creating the application can be found in the example/dotenv file in this repository.\\n\\n\\n### Software Requirements\\n\\nRequired:\\n* A local copy of this repository.  Using `git clone` is strongly recommended\\n* Node.js version 12.  We strongly recommend installing [NVM](https://github.com/nvm-sh/nvm) to add and manage node versions.\\n* Mac OSX, Linux, or similar command line tooling.  Harmony is tested to run on OSX >= 10.14 and Amazon Linux 2.  Command-line instructions and bash helper files under [bin/](bin/) are tested on OSX >= 10.14.\\n* [git](https://git-scm.com) - Used to clone this repository\\n* A running [Docker Desktop](https://www.docker.com/products/developer-tools) or daemon instance - Used to invoke docker-based services\\n* [Docker compose](https://docs.docker.com/compose/) version 1.20.0 or greater; preferably the latest version, which is v1.26 or greater.\\n* The [AWS CLI](https://aws.amazon.com/cli/) - Used to interact with both localstack and real AWS accounts\\n* [SQLite3 commandline](https://sqlite.org/index.html) - Used to create the local development and test databases. Install using your OS package manager, or [download precompiled binaries from SQLite](https://www.sqlite.org/download.html)\\n* PostgreSQL (required by the pg-native library) - `brew install postgresql` on OSX\\n* Earthdata Login application in UAT (Details below in the \\'Set up Earthdata Login application for your local Harmony instance\\' section)\\n* [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) - A command-line application for interfacing with a Kubenetes API.\\n\\n\\nHighly Recommended:\\n* An Amazon Web Services account - Used for testing Harmony against object stores and running Harmony in AWS\\n* An editor with syntax awareness of TypeScript.  If you do not have this or any preference, consider [Visual Studio Code](https://code.visualstudio.com)\\n\\n\\nOptional:\\n* [awscli-local](https://github.com/localstack/awscli-local) - CLI helpers for interacting with localstack\\n* [Python](https://www.python.org) version 3.7 - Useful for locally running and testing harmony-docker and other backend services\\n\\n## Running Harmony\\n\\n### Set up Environment\\nIf you have not yet cloned the Harmony repository, run\\n```\\n$ git clone https://github.com/nasa/harmony.git\\n```\\n\\nEnsure node is available and is the correct version, 12.x.y, where \"x\" >= 14.\\n\\n```\\n$ node --version\\nv12.22.1\\n```\\n\\nEnsure npm is available and is version 7 or later.\\n```\\n$ npm --version\\n7.11.2\\n```\\n\\nIf either are not the correct versions and you are using NVM, install them and ensure your `PATH` is up-to-date by running:\\n\\n```\\n$ nvm install && nvm use && npm install -g npm@7\\n```\\n\\nThe output should include node 12 and npm 7.\\n```\\nNow using node v12.22.1 (npm v7.11.2)\\n```\\n\\nBe sure to **verify the version on the final line** to make sure the NVM binary appears first in your `PATH`.\\n\\nFrom the harmony project root, install library dependencies:\\n```\\n$ npm install\\n```\\n\\nRecommended: Add `./node_modules/.bin` to your `PATH`.  This will allow you to run binaries from installed node modules.  If you choose not to do this, you will need to prefix node module calls with `npx`, e.g. `npx mocha` instead of just `mocha`\\n\\n### Set Up Environment Variables\\n\\nHarmony uses environment variables for managing much of its configuration. Most of the variables can be defaulted, and harmony provides those defaults suitable for local development in the `env-defaults` file. In order to set up the remaining variables, run the following from the harmony project root:\\n\\n```\\n$ bin/create-dotenv\\n```\\n\\nThe script will create a file named `.env` in the root project directory containing only those parameters that cannot be defaulted. Open the file and update the values for any of the variables that are currently blank. Detailed information for the environment variables can be found in the `env-defaults` file.\\n\\nHarmony reads both the `env-defaults` and `.env` files at startup to determine the configuration. To override any default values, set the desired value in the `.env` file. There is no need to duplicate parameters in the `.env` file if using the default value.\\n\\nNote: The defaults are suitable for running locally with Mac OS X. If running on Linux there are a couple of parameters that will also need to be overridden and are documented as such in the `env-defaults` file.\\n\\n### Run Tests\\n\\nTo run the linter, tests, and coverage checks as the CI environment will, run\\n\\n```\\n$ npm test\\n```\\n\\nHarmony uses [eslint](https://eslint.org) as a linter, which can be invoked as `$ npx eslint` (or `$ eslint` if you have set up your `PATH`).  It uses [mocha](https://mochajs.org) for tests, `$ npx mocha`, and [nyc](https://istanbul.js.org) for code coverage, `$ npx nyc mocha`.\\n\\n#### Test Fixtures\\nRather than repeatedly perform the same queries against the CMR, our test suite\\nuses [node-replay](https://github.com/assaf/node-replay) to record and play back\\nHTTP interactions.  All non-localhost interactions are recorded and placed in files\\nin the [fixtures directory](fixtures/).\\n\\nBy default, the test suite will playback interactions it has already seen and\\nrecord any new interactions to new files.  This behavior can be changed by setting\\nthe `REPLAY` environment variable, as described in the\\n[node-replay README](https://github.com/assaf/node-replay).\\n\\nTo re-record everything, remove the fixtures directory and run the test suite. This should be done to cull the recordings when a code change makes many of them obsolete, when CMR adds response fields that Harmony needs to make use of, and periodically to ensure no impactful CMR changes or regressions.\\n\\n### Set Up A Database\\n\\nTo setup a sqlite3 database with the correct schema for local execution, run\\n\\n```\\n$ bin/create-database development\\n```\\n\\nThis should be run any time the versioned contents of the `db/db.sql` file change.\\n\\nThis will create a file, `db/development.sqlite3`, which will contain your local data.  You can delete the above file to remove\\nall existing development data.\\n\\nIn production environments, we use PostgreSQL and use database migrations to modify the schema.  If you have a PostgreSQL\\ndatabase, you can create and/or migrate your database by setting `NODE_ENV=production` and\\n`DATABASE_URL=postgresql://your-postgres-connection-url` and running:\\n```\\n$ npx knex --cwd db migrate:latest\\n```\\n\\n### Set Up and Run Argo, Localstack\\n\\nHarmony uses [Argo Workflows](https://github.com/argoproj/argo) to manage job executions.  In development, we use [Localstack](https://github.com/localstack/localstack) to avoid allocating AWS resources.\\n\\n#### Prerequisites\\n\\n* Mac:\\n  * Install [Docker Desktop] https://www.docker.com/products/docker-desktop. Docker Desktop comes bundled with Kubernetes and `kubectl`.\\n    If you encounter issues running `kubectl` commands, first make sure you are running the version bunedled with Docker Desktop.\\n  * Run Kubernetes in Docker Desktop by selecting Preferences -> Kubernetes -> Enable Kubernetes\\n  * Install the [Argo CLI](https://github.com/argoproj/argo/releases/tag/v2.9.5), the command line interface to Argo\\n* Linux / Generic:\\n  * Install [minikube](https://kubernetes.io/docs/tasks/tools/install-kubectl/), a single-node kubernetes cluster useful for local development\\n  * Install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/), a command line interface to kubernetes.\\n  * Install the [Argo CLI](https://github.com/argoproj/argo/releases/tag/v2.9.5), the command line interface to Argo\\n\\n#### Installing and running Argo and Localstack on Kubernetes\\n\\n```\\n$ ./bin/start-argo\\n```\\n\\nThis will install Argo and forward port 2746 to localhost. It will take a few minutes the first time you run it. You will know when it has completed when it prints\\n\\n```\\nHandling connection for 2746\\n```\\n\\nYou can then connect to the Argo Server UI at `http://localhost:2746\\'.\\n\\nYou can change the startup port by adding the `-p` option like so for port 8080:\\n\\n```\\n$ ./bin/start-argo -p 8080\\n```\\n\\n`minikube` will default to using the `docker` driver. You can change the driver used by minikube by using the `-d` option with `start-argo` like so\\n\\n```\\n$ ./bin/start-argo -d DRIVER\\n```\\n\\nwhere `DRIVER` is one of the supported VM drivers found [here](https://kubernetes.io/docs/setup/learning-environment/minikube/#specifying-the-vm-driver).\\n\\n#### Deleting applications and stopping Kubernetes\\n\\nTo delete the argo and localstack deployment, run:\\n\\n```\\n$ kubectl delete namespaces argo\\n```\\n\\n`minikube` users can stop Kubernetes by pressing `ctrl-C` on the `bin/start-argo` process or run `minikube stop`.  Docker Desktop users will\\nneed to close Docker or disable Kubernetes support in the UI.  Note that the latter uninstalls `kubectl`.\\n\\n#### (minikube only) Configuring the callback URL for backend services\\n\\nYou can skip this step if you are using the default docker driver for minikube and set CALLBACK_URL_ROOT as described in the example dotenv file. If you are using a different driver such as virtualbox you may need to execute the following command to get the IP address minikube has bridged to localhost:\\n\\n```bash\\nminikube ssh grep host.minikube.internal /etc/hosts | cut -f1\\n```\\n\\nThis should print out an IP address. Use this in your .env file to specify the `CALLBACK_URL_ROOT` value, e.g., `CALLBACK_URL_ROOT=http://192.168.65.2:4001`.\\n\\n### Add A Service Backend\\n\\nClone the Harmony service example repository into a peer directory of the main Harmony repo\\n```\\n$ cd ..\\n$ git clone https://github.com/nasa/harmony-service-example.git\\n```\\n\\n(minikube only) From the harmony-service-example project root, run\\n```bash\\neval $(minikube docker-env)\\n```\\n\\nThis will set up the proper environment for building the image so that it may be used in minikube and Argo. Next run the following command to build and locally install the image:\\n\\n```bash\\n./bin/build-image\\n```\\n\\nThis may take some time, but ultimately it will produce a local docker image tagged `harmony/gdal:latest`.  You may choose to use another service appropriate to your collection if you have [adapted it to run in Harmony](docs/adapting-new-services.md).\\n\\n### Run Harmony\\n\\nTo run Harmony locally such that it reloads when files change (recommended during development), run\\n\\n```\\n$ npm run start-dev\\n```\\n\\nIn production, we use `$ npm run start` which does the same but does not add the file watching and reloading behavior.\\n\\nYou should see messages about the two applications listening on two ports, \"frontend\" and \"backend.\"  The frontend application receives requests from users, while the backend application receives callbacks from services.\\n\\n### Connect A Client\\n\\nYou should now be able to view the outputs of performing a simple transformation request.  Harmony has its own test collection\\nset up for sanity checking harmony with the harmony-gdal backend.  This will fetch a granule from that collection converted to GeoTIFF:\\n[http://localhost:3000/C1233800302-EEDTEST/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?granuleId=G1233800343-EEDTEST](http://localhost:3000/C1233800302-EEDTEST/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?granuleId=G1233800343-EEDTEST)\\n\\nYou can also set up a WMS connection in [QGIS](https://qgis.org/en/site/about/index.html), for example, by placing the\\n`http://localhost:3000/C1233800302-EEDTEST/wms` as the \"URL\" field input in the \"Connection Details\"\\ndialog when adding a new WMS connection.  Thereafter, expanding the connection should provide a list of layers obtained through a\\nGetCapabilities call to the test server, and double-clicking a layer should add it to a map, making a WMS call to retrieve an appropriate\\nPNG from the test server.\\n\\nYou can also use the Argo dashboard at http://localhost:2746 to visualize the workflows that were kicked off from your Harmony transformation requests.\\n\\n\\n## Local Development Of Workflows Using Visual Studio Code\\n\\nThis section describes a VS Code based approach to local development. The general ideas are, however, applicable to other editors.\\n\\nThere are two components to local development. The first is mounting your local project directory to a pod in a workflow so that changes to your code are automatically picked up whenever you run the workflow. The second is attaching a debugger to code running in a pod container (unless you prefer the print-debug method, in which case you can use the logs).\\n\\n#### Prerequisites\\n* [Visual Studio Code](https://code.visualstudio.com/) and the [Kubernetes plugin](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-kubernetes-tools)\\n\\n### Mounting a local directory to a pod running in a workflow\\n\\nThis is accomplished in two steps. The first step is to mount a local directory to a node in your `kubernetes/minikube` cluster. On a mac using the `virtualbox` driver the `/Users` directory is automatically mounted as `/Uses` on the single node in `minikube`. On Linux using the `virtualbox`driver the `/home` directory is automatically mounted at `/hosthome`. Other options for mounting a local directory can be found [here](https://minikube.sigs.k8s.io/docs/handbook/mount/).\\n\\nThe second step is to mount the directory on the node to a directory on the pod in your workflow. This can be done using a [hostPath](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) volume defined in your workflow template. The following snippet creates a volume using the `/Users/username/project_folder` directory from the `node` on which the pod runs, _not directory from the local filesystem_. Again, on a mac using `virtualbox` the local `/Users` folder is conveniently mounted to the `/Users` folder on the node.\\n\\n```yaml\\napiVersion: argoproj.io/v1alpha1\\nkind: Workflow\\nmetadata:\\n  generateName: hello-world-\\nspec:\\n  volumes:\\n  - name: test-volume\\n    hostPath:\\n      path: /Users/username/project_folder\\n  ...\\n```\\n\\nYou can then mount the volume in your pod using a `volumeMounts` entry in you container configuration:\\n\\n```yaml\\napiVersion: argoproj.io/v1alpha1\\nkind: Workflow\\nmetadata:\\n  generateName: hello-world-\\nspec:\\n  volumes:\\n  - name: test-volume\\n    hostPath:\\n      path: /Users/username/project_folder\\n  entrypoint: hello\\n  arguments:\\n    parameters:\\n    - name: message\\n      value: James\\n  templates:\\n  - name: hello\\n    inputs:\\n      parameters:\\n      - name: message\\n    container:\\n      image: node\\n      volumeMounts:\\n      - mountPath: /test-mount\\n        name: test-volume\\n```\\n\\nNow the pod will be able to access local code directly in the `/test-mount` directory. Updates to code in the developers local project will immediately show up in workflows.\\n\\n### Attaching a debugger to a running workflow\\n\\nArgo Workflow steps run as kubernetes [jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/), which means that the containers that run them are short-lived. This complicates the process of attaching a debugger to them somewhat. In order to attach the debugger to code running in a container in a workflow you have to start the code in a manner that will pause the code on the first line when it runs and wait for a debugger to attach.\\n\\nFor NodeJS code this is easily done by passing the `--inspect-brk` option to the `node` command. workflow template building on our previous example is given here\\n\\n```yaml\\napiVersion: argoproj.io/v1alpha1\\nkind: Workflow\\nmetadata:\\n  generateName: hello-world-\\nspec:\\n  volumes:\\n  - name: test-volume\\n    hostPath:\\n      path: /Users/username/project_folder\\n  entrypoint: hello\\n  arguments:\\n    parameters:\\n    - name: message\\n      value: James\\n  templates:\\n  - name: hello\\n    inputs:\\n      parameters:\\n      - name: message\\n    container:\\n      image: node\\n      volumeMounts:\\n      - mountPath: /test-mount\\n        name: test-volume\\n      command: [node]\\n      args: [\"--inspect-brk\", \"/test-mount/index.js\", \"{{inputs.parameters.message}}\"]\\n```\\n\\nIn this example the starting point for the step is in the `index.js` file.\\n\\nSimilar approaches are available for Python and Java, although they might require changes to the code.\\n\\nOnce you launch your workflow it will pause at the step (wait for the icon in the UI to change from yellow to blue and spinning), and you can attach the debugger. For VS Code this is easily done using the `kubernetes` plugin.\\n\\nOpen the plugin by clicking on the `kubernetes` icon in the left sidebar. Expend the `CLUSTERS` tree to show the pods in `CLUSTERS>minikube>Nodes>minikube` then ctrl+click on the pod with the same name as the step in your workflow, e.g., `hello-world-9th8k` (you may need to refresh the view). Select `Debug (Attach)` from the menu, then selecting the `wait` container (not `main`), and select the runtime environment (java, nodejs, or python).\\n\\nAt this point the editor should open the file that is the starting point for your applications and it should be stopped on the first line of code to be run. You can then perform all the usual debugging operations such as stepping trough code and examining variables.\\n\\n## Running in AWS\\n\\nNote: It is currently easiest to allow the CI/CD service to deploy the service remotely; it is deployed to the sandbox after each merge to `master`.\\nAs the deployment simply uploads the code, sets environment variables, kills the old server and runs `$ npm run start`, at present, there is not\\ntypically much to be gained by running remotely during development.\\n\\nWhen setting up a new environment, the first two steps need to be performed, but the CI environment should be set up to run the deployment rather than having\\nit done manually.\\n\\n#### Prerequisites\\n* Once per account, run `$ bin/account-setup` to create a service linked role for ECS.\\n* Upload the harmony/gdal Docker image somewhere accessible to an EC2 deployment.  This should be done any time the image changes.  The easiest way is to create an ECR in your account and push the image there.  Running `$ bin/build-image && bin/push-image` from the harmony-gdal repository will perform this step..\\n\\n### Stop here and set up CI/CD\\n\\nDeploying the code should be done using the harmony-ci-cd project from Bamboo rather than manually.  Apart from that project and CI/CD setup,\\nwe do not yet have automation scripts for (re)deploying to AWS manually, as it is typically not needed during development.\\n\\n### Deploy the code to AWS\\n\\nNote: The harmony-ci-cd repository contains automation code to do the following, usable from Bamboo.  You may use it locally by setting all\\nrelevant environment variables in a `.env` file, running `$ bin/build-image` in the root directory of the harmony-ci-cd project, and then\\nrunning the **harmony-ci-cd** `bin/deploy` script from your **harmony** codebase\\'s root directory.\\n\\n1. `scp` the Harmony codebase to the remote instance\\n2. `ssh` into the remote instance\\n3. Run `$ $(aws ecr get-login --region=$AWS_DEFAULT_REGION --no-include-email)` where `AWS_DEFAULT_REGION` is the region containing your harmony-gdal ECR instance.\\nSkip this step if harmony-gdal is not in an ECR.\\n4. Run `$ if pgrep node; then pkill node; fi` to stop any existing server that may be running\\n5. Run `$ nohup npm start >> ../server.log 2>&1 &` to start harmony\\n6. Run `$ docker pull $GDAL_IMAGE` to fetch harmony-gdal changes, where `GDAL_IMAGE` is the EC2-accessible location of your harmony-gdal Docker image. Repeat for any other docker images you want to use.\\n\\n### Connecting a client to an AWS instance\\n\\nThis process is identical to \"Connect a client\" above, except instead of `http://localhost:3000`, the protocol and host should be that of your\\nload balancer, e.g. `https://your-load-balancer-name.us-west-2.elb.amazonaws.com`.  Retrieve the precise load balancer details from the\\nAWS console.\\n\\n### Updating development resources after pulling new code\\n\\nOnce up and running, if you update code, you can ensure dependencies are correct, Argo is deployed, and necessary Docker images are built\\nby running\\n\\n```\\n$ npm run update-dev\\n```\\n\\n## Contributing to Harmony\\n\\nWe welcome Pull Requests from developers not on the Harmony\\nteam. Please follow the standard \"Fork and Pull Request\" workflow\\nshown below.\\n\\n### Submitting a Pull Request\\n\\nIf you are a developer on another team and would like to submit a Pull\\nRequest to this repo:\\n\\n1. Create a fork of the harmony repository.\\n2. In the fork repo\\'s permissions, add the `edc_snyk_user`\\n   with `Read` access\\n3. In the `#harmony-service-providers` Slack channel, ask a Harmony\\n   team member to import your fork repo into Snyk (see below).\\n4. When ready, submit a PR from the fork\\'s branch back to the harmony\\n   master branch. Ideally name the PR with a Jira ticket name (e.g.,\\n   HARMONY-314)\\n5. The PR\\'s \\'build\\' tab should not show errors\\n\\n### Importing a Fork Repo Into Snyk\\n\\nTo run Snyk on a fork of the repo (see above), the developer\\'s\\nfork needs to be imported into Snyk:\\n\\n1. Open [Snyk](https://app.snyk.io/org/esdis-cumulus-core-gibs-cmr-etc./reports/)\\n2. Click [Integrations](https://app.snyk.io/org/esdis-cumulus-core-gibs-cmr-etc./integrations)\\n   on the navbar at the top of the page\\n3. Click the integration type based on where the repo is hosted. E.g.:\\n   Bitbucket Server, GitHub, etc.\\n4. Search for \\'harmony\\' using the search box\\n5. Click the checkbox on the developer\\'s newly-created fork repo\\n6. Click the \\'Import selected repositories\\' button\\n\\nThis import should be done before the developer submits a PR. If it\\nhasn\\'t, the PR \\'build\\' will fail and the PR will be blocked. In this\\nsituation, the project can still be imported into Snyk, but then the\\nPR will need to be declined and resubmitted.\\n\\n## Additional Resources\\n\\n* [Adapting new services to Harmony](docs/adapting-new-services.md)\\n* [Harmony message schemas](app/schemas/data-operation)\\n* [EOSS protocol OpenAPI Specification](app/schemas/eoss)\\n* [Harmony NetCDF to Zarr service repository](https://github.com/nasa/harmony-netcdf-to-zarr)\\n* [Harmony GDAL example service repository](https://github.com/nasa/harmony-service-example)\\n* [Linux Container-based development with Harmony](docs/dev_container/README.md) (n.b. Windows users)\\n'},\n",
       " {'repo': 'nasa/CARA_Analysis_Tools',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# CARA_Analysis_Tools\\nThe tools provided here are free and open-source.\\n\\n**Conjunction Consequence Assessment:**\\n> Risk is properly considered as the combination of likelihood and consequence; but conjunction assessment has usually limited itself to the consideration of only collision likelihood. When considered from an orbital regime protection perspective, the focus shifts to the question of the amount of debris that a collision might produce (the “consequence”). An operational algorithm for determining the expected amount of debris production should a conjunction result in a collision, has been proposed and previously validated.\\n\\n**Monte Carlo from TCA:**\\n> A method that may be employed as a method of determining the probability of collision is to perform a Monte Carlo simulation of both the primary and secondary object states at the time of close approach and statistically determine the probability of collision based on the number of trials which violate a predetermined proximity threshold.\\nA Monte Carlo simulation is a computational technique that allows a probabilistic process to be modelled using random sampling from a known multivariate distribution. As the process of orbit determination yields both a best estimate of a satellite state and its associated uncertainty, the problem of collision probability lends itself well to using Monte Carlo sampling methodology.\\n\\n**Single Covariance Max Pc:**\\n> Frisbee (2015) proposed a method by which the maximum possible probability of collision could be determined for a close approach event for which only one object has position uncertainty information. This is of particular use in determining whether an encounter may be of risk to an asset, as the maximum probability of collision may be below an actionable threshold. To determine the maximum probability of collision, the covariance ellipsoid of the object possessing a covariance matrix is mapped to the conjunction plane and distended so that the Mahalanobis distance between the two objects is equal to one. To do this, the covariance of the secondary object is oriented along a one dimensional position uncertainty along the miss vector between the two objects.\\n\\n**Two-Dimension Pc:**\\n> One method that may be employed as a method of determining the probability of collision is by transforming a close approach event from a three dimensional problem to a two dimensional problem which greatly simplifies the calculation of the probability of collision. This calculation is widely used to characterize and analyze close approach events and determine resultant probabilities of collision as a result of mitigation actions.\\n'},\n",
       " {'repo': 'nasa/NPSS-Power-System-Library',\n",
       "  'language': 'PLSQL',\n",
       "  'readme_contents': '# NPSS - Power System Library\\n\\n## NASA - Glenn Research Center\\n\\n## Introduction\\n\\nThis repository contains a power system library (PSL) for the Numerical\\nPropulsion System Simulation (NPSS) software framework.\\nIt contains circuit components such as resistors and capacitors, electric\\nmachines, and ports that are used to link them together.\\nThe library also includes bus components that allow more than two elements to be\\nconnected to a single node.\\n\\nSeveral examples are included in the library to demonstrate the use of the\\nvarious components, and how they may be connected to an NPSS gas-turbine engine\\nmodel.\\n\\n## Usage\\n\\nIf you have your NPSS environment set up, you can run a model like so:\\n\\n```bat\\nrunnpss-psl run\\\\[file_name].run\\n```\\n\\n> NOTE: You must run this command from the project root\\ndirectory (containing \"src\", \"model\", etc).\\n\\n## Example\\n\\nIf you run the baseline example, \"baseline.run,\" you should get an output that\\nlooks like this in the file \"engine.viewOut\":\\n\\n```txt\\n*******************************************************************************                                                                 \\nNCP                   NPSS_2.7.1    model:         Baseline   run by:     glthoma1   solutionMode= STEADY_STATE     converge=    1    CASE:    0\\ntime:  0.0000   timeStep:0.05000    therm_package:   GasTbl   Mode:         DESIGN   itr/pas/Jac/Bry=  15/  20/  1/ 13    run: 12/01/20 12:33:00\\n\\n                                        FLOW STATION DATA                                                                               \\n                                W        Pt        Tt       ht     FAR       Wc        Ps        Ts      rhos     Aphy      MN      gamt\\nSt0-St1   Atm.Fl_O         219.74    14.696    518.67   123.95  0.0000   219.74     0.000      0.00  0.000000      0.0  0.0000   1.40052\\nSt1-St2   Prop.Fl_O        219.74    20.574    574.97   137.46  0.0000   165.25     0.000      0.00  0.000000      0.0  0.0000   1.39944\\nSt00-St10 TurbineAtm.Fl>   500.00   500.000   2200.00   574.10  0.0200    30.27     0.000      0.00  0.000000      0.0  0.0000   1.30774\\nSt10-End0 Turb.Fl_O        500.00   476.711   2177.85   567.66  0.0200    31.58     0.000      0.00  0.000000      0.0  0.0000   1.30837\\nSt2-End   Noz.Fl_O         219.74    20.574    574.97   137.46  0.0000   165.25    14.000    515.05  0.073366    508.3  0.7625   1.39944\\n\\n                                        ELECTRICAL PORT DATA                                          \\n                                   Complex Power Data           |               Misc Data             \\n             |S|, kVA  /_S, deg    P, kW  Q, kVAR     Power Type  Power Factor  frequency, Hz         \\nGen1.EP_O     3332.51    11.567  3264.83   668.19            AC3        0.9797        400.000         \\nCable1.EP_I   3332.51    11.567  3264.83   668.19            AC3        0.9797        400.000         \\nCable1.EP_O   3328.84    11.478  3262.27   662.43            AC3        0.9800        400.000         \\nEM1.EP_I      3328.84    11.478  3262.27   662.43            AC3        0.9800        400.000         \\n                                                                                                      \\n                                   Complex Voltage Data  (V_LL)   |     Complex Current Data  (I_Line)\\n                 |V|, V  /_V, deg      V.r      V.j      |I|, A  /_I, deg      I.r      I.j           \\nGen1.EP_O       1200.00     0.000  1200.00     0.00     1603.35   -41.567  1199.61  ----.--           \\nCable1.EP_I     1200.00     0.000  1200.00     0.00     1603.35   -41.567  1199.61  ----.--           \\nCable1.EP_O     1198.68    -0.088  1198.68    -1.85     1603.35   -41.567  1199.61  ----.--           \\nEM1.EP_I        1198.68    -0.088  1198.68    -1.85     1603.35   -41.567  1199.61  ----.--           \\n\\n                                        ELECTRICAL POWER SYSTEM COMPONENT DATA  \\n            eff  Mass, kg  Loss_r, kW  Loss_j, kVAR  Q_heat, BTU/s              \\nGen1    1.00000    251.14      136.03          0.00         128.94              \\nCable1  0.99921    452.55        2.57          5.76           5.98              \\nEM1     1.00000    240.91      130.49        662.43         639.93              \\n                                                                                \\n                            ELECTRICAL POWER SYSTEM -- COMPONENT SPECIFIC DATA  \\n                                                                                \\nAeroCable Data                                                                  \\n           R, Ohms  L, Henries  X, Reactance  cable_size   #parallel    ampacity\\nCable1   3.33E-004   2.97E-007     7.47E-004         2/0       6.000    1608.000\\n```\\nFor more information, see the NPSS Power System Library [wiki page](../../wiki/Home/).\\n'},\n",
       " {'repo': 'nasa/prog_models',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Prognostics Model Python Package\\n\\nThe NASA Prognostic Model Package is a python modeling framework focused on defining and building models for prognostics (computation of remaining useful life) of engineering systems, and provides a set of prognostics models for select components developed within this framework, suitable for use in prognostics applications for these components.\\n\\nThis is designed to be used with the [Prognostics Algorithms Package](https://github.com/nasa/prog_algs).\\n\\n## Installation \\n`pip3 install prog_models`\\n\\n## Documentation\\nSee documentation [here](https://nasa.github.io/prog_models/)\\n \\n## Repository Directory Structure \\nHere is the directory structure for the github repository \\n \\n`prog_models/` - The prognostics model python package<br />\\n&nbsp;&nbsp; |-`models/` - Example models<br /> \\n&nbsp;&nbsp; |-`prognostics_model.py` - Physics-based model superclass of degraded system behavior<br />\\n&nbsp;&nbsp; |-`visualize.py` - Visualization tools<br />\\n`docs/` - Project documentation<br />\\n`sphinx_config/` - Configuration for automatic documentation generation<br />\\n`examples/` - Example python scripts using prog_models<br />\\n`tests/` - Tests for prog_models<br />\\n`README.md` - The readme (this file)<br />\\n`requirements.txt` - python library dependiencies required to be met to use this package. Install using `pip install -r requirements.txt`<br />\\n`prog_model_template.py` - Template for Prognsotics Model<br />\\n`tutorial.ipynb` - Tutorial (Juypter Notebook)\\n\\n## Citing this repository\\nUse the following to cite this repository:\\n\\n```\\n@misc{2020_nasa_prog_models,\\n    author    = {Christopher Teubert and Matteo Corbetta and Chetan Kulkarni and Matthew Daigle},\\n    title     = {Prognostics Models Python Package},\\n    month     = Apr,\\n    year      = 2021,\\n    version   = {1.0},\\n    url       = {https://github.com/nasa/prog_models}\\n    }\\n```\\n\\nThe corresponding reference should look like this:\\n\\nC. Teubert, M. Corbetta, C. Kulkarni, and M. Daigle, Prognostics Models Python Package, v1.0, Apr. 2021. URL https://github.com/nasa/prog_models.\\n\\n## Acknowledgements\\nThe structure and algorithms of this package are strongly inspired by the [MATLAB Prognostics Model Library](https://github.com/nasa/PrognosticsModelLibrary). We would like to recognize Matthew Daigle and the rest of the team that contributed to the Prognostics Model Library for the contributions their work on the MATLAB library made to the design of prog_models\\n\\n## Notices\\n\\nCopyright © 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\n## Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/harmony-netcdf-to-zarr',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# harmony/netcdf-to-zarr\\n\\nA Harmony service to convert NetCDF4 files to Zarr files.  Takes conventional Harmony messages and translates\\ntheir input granules to Zarr using xarray.\\n\\nThis library intentionally does very little checking of the input files and file extensions.  It is designed\\nto work on NetCDF granules.  It ought to work with any other file type that can be opened with\\n[xarray.open_mfdataset](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html) using the\\n`h5netcdf` driver.  This includes some HDF5 EOSDIS datasets.  Individual collections must be tested to ensure\\ncompatibility.\\n\\n\\n## Development\\n\\n### Setup\\n\\n#### Docker\\n\\nIt is possible to develop and run this service locally using only Docker.  This is the recommended option\\nfor validation and small changes. Install [Docker](https://www.docker.com/get-started) on your development\\nmachine.\\n\\n#### Environment file\\n\\nThis service uses the\\n[harmony-service-lib-py](https://github.com/nasa/harmony-service-lib-py),\\nand requires that certain environment variables be set, as shown in the Harmony Service Lib README. For example,\\n`STAGING_BUCKET` and `STAGING_PATH` are required, and `EDL_USERNAME` and `EDL_PASSWORD` are required for any\\ndata behind Earthdata Login. For local testing (not integrated into Harmony in a dev environment or AWS\\ndeployment), use the example `.env` file in this repo:\\n\\n    $ cp example/dotenv .env\\n\\nand update the `.env` with the correct values.\\n\\n#### Python & Project Dependencies (Optional)\\n\\nIf you would like to do local development outside of Docker, install Python, create a Python virtualenv,\\nand install the project dependencies.\\n\\nIf you have [pyenv](https://github.com/pyenv/pyenv) and\\n[pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv) installed (recommended), install Python and\\ncreate a virtualenv:\\n\\n    $ pyenv install 3.7.4\\n    $ pyenv virtualenv 3.7.4 harmony-ntz\\n    $ pyenv activate harmony-ntz\\n    $ pyenv version > .python-version\\n\\nThe last step above creates a local .python-version file which will be automatically activated when cd\\'ing into the\\ndirectory if pyenv-virtualenv has been initialized in your shell (See the pyenv-virtualenv docs linked above).\\n\\nInstall project dependencies:\\n\\n    $ pip install -r requirements/core.txt -r requirements/dev.txt\\n\\nNOTE: All steps in this README which install dependencies need to be performed while on the NASA VPN\\nin order to download and install the Harmony Service Lib, which is published on the\\n[Nexus artifact repository](https://maven.earthdata.nasa.gov/).\\n\\n### Development with Docker\\n\\n#### Testing & Running the Service Independently\\n\\nTo run unit tests, coverage reports, or run the service on a sample message _outside_ of the\\nentire Harmony stack, start by building new runtime and test images:\\n\\n*IMPORTANT*: Be sure to do these steps in a shell in which has *not* been updated to point to\\nthe Minikube Docker daemon. This is usually done via a shell `eval` command. Doing so will\\ncause tests and the service to fail due to limitations in Minikube.\\n\\n    $ bin/build-image\\n    $ bin/build-test-image\\n\\nRun unit tests and generate overage reports. This will mount the local directory into the\\ncontainer and run the unit tests. So all tests will reflect local changes to the service.\\n\\n    $ bin/test-in-docker\\n\\nYou may be concurrently making changes to the Harmony Service Lib. To run the unit tests using\\nthe local clone of that Harmony Service Lib and any changes made to it:\\n\\n    $ LOCAL_SVCLIB_DIR=../harmony-service-lib-py bin/test-in-docker\\n\\nFinally, run the service using an example Harmony operation request\\n([example/harmony-operation.json](example/harmony-operation.json) as input.  This will reflect\\nlocal changes to this repo, but will not include local changes to the Harmony Service Lib.\\n\\n    $ bin/run-in-docker example/harmony-operation.json\\n\\nTo run the example and also include local Harmony Service Lib changes:\\n\\n    $ LOCAL_SVCLIB_DIR=../harmony-service-lib-py bin/run-in-docker example/harmony-operation.json\\n\\n#### Testing & Running the Service in Harmony\\n\\n*Without local Harmony Service Lib changes*:\\n\\nBe sure your environment is pointed to the Minikube Docker daemon:\\n\\n    $ eval $(minikube docker-env)\\n\\nBuild the image:\\n\\n    $ bin/build-image\\n\\nYou can now run a workflow in your local Harmony stack and it will execute using this image.\\n\\n*With local Harmony Service Lib changes*:\\n\\nTo run this service in Harmony *with* a local copy of the Service Lib, build\\nthe image, but specify the location of the local Harmony Service Lib clone:\\n\\n    $ LOCAL_SVCLIB_DIR=../harmony-service-lib-py bin/build-image\\n\\nYou can now run a workflow in your local Harmony stack and it will execute using this image.\\nNote that this also means that the image needs to be rebuilt (using the same command) to\\ninclude any further changes to the Harmony Service Lib or this service.\\n\\n### Development without Docker\\n\\n#### Testing & running the Service Independently\\n\\nRun tests with coverage reports:\\n\\n    $ bin/test\\n\\nRun an example:\\n\\n    $ dotenv run python3 -m harmony_netcdf_to_zarr --harmony-action invoke --harmony-input \"$(bin/replace.sh example/harmony-operation.json)\"\\n\\n#### Installing `harmony-service-lib-py` in Development Mode\\n\\nYou may be concurrently developing on this service as well as the `harmony-service-lib-py`. If so, and you\\nwant to test changes to it along with this service, install the `harmony-service-lib-py` in \\'development mode\\'.\\nInstall it using pip and the path to the local clone of the service library:\\n\\n```\\npip install -e ../harmony-service-lib-py\\n```\\n\\nNow any changes made to that local repo will be visible in this project when you run tests, etc.\\n\\nFinally, you can test & run the service in Harmony just as shown in the `Development with Docker` section above.\\n'},\n",
       " {'repo': 'nasa/cmr-csw',\n",
       "  'language': 'Ruby',\n",
       "  'readme_contents': '# [CMR CATALOGUE SERVICE FOR THE WEB (CSW)](https://cmr.earthdata.nasa.gov/csw)\\n\\nVisit NASA\\'s CSW based on the EOSDIS COMMON METADATA REPOSITORY (CMR) at\\n[https://cmr.earthdata.nasa.gov/csw](https://cmr.earthdata.nasa.gov/csw)\\n\\n[![Build Status](https://travis-ci.org/nasa/cmr-csw.svg?branch=master)](https://travis-ci.org/nasa/cmr-csw)\\n\\n## About\\nThe CMR CSW is a web application developed by [NASA](http://nasa.gov) [EOSDIS](https://earthdata.nasa.gov)\\nto enable data discovery, search, and access across EOSDIS\\' Earth Science data holdings.\\nIt provides an interface compliant with the [OpenGIS Catalogue Service Implementation Specification v 2.0.2](http://portal.opengeospatial.org/files/?artifact_id=20555)\\nby taking advantage of NASA\\'s [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/) APIs for data discovery and access.\\n\\n## License\\n\\n> Copyright © 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n>\\n> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\n> You may obtain a copy of the License at\\n>\\n>    http://www.apache.org/licenses/LICENSE-2.0\\n>\\n> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\n> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n\\n## Third-Party Licenses\\n\\nSee public/licenses.txt\\n\\n## Installation\\n\\n* Ruby 2.5.3\\n* A Ruby version manager such as [RVM](http://rvm.io/) or [rbenv](https://github.com/rbenv/rbenv) is strongly recommended.\\n\\n### Initial setup\\nOnce the repository is cloned locally and Ruby 2.5.3 is installed, you must install the dependencies.\\nIf you don\\'t have the [bundler](http://bundler.io/) gem already installed, execute the command below in the project root directory:\\n   \\n    gem install bundler   \\n\\nor if you wish to install the bundler without documentation:\\n\\n    gem install bundler --no-rdoc --no-ri\\n\\nInstall all the gem dependencies:\\n\\n    bundle install    \\n\\n### Set up the required environment\\nThe application requires the environment variables below to be set.\\n\\nString that uniquely identifies your specific CMR CSW installation:\\n    \\n    client_id = <your client identifier>\\n    \\n### Run the automated [Rspec](http://rspec.info/) tests\\nExecute the command below in the project root directory:\\n\\n    bin/rspec\\n\\nAll tests should pass.\\n\\n### Run the application\\nExecute the command below in the project root directory:\\n\\n    rails server\\n\\nOpen `http://localhost:3000/csw` in a local browser.\\n'},\n",
       " {'repo': 'nasa/harmony-service-lib-py',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# harmony-service-lib\\n\\nA library for Python-based Harmony services to parse incoming messages, fetch data, stage data, and call back to Harmony\\n\\n## Installing\\n\\n### Using pip\\n\\nInstall the latest version of the package from PyPI using pip:\\n\\n    $ pip install harmony-service-lib\\n\\n### Other methods:\\n\\nThe package is installable from source via\\n\\n    $ pip install git+https://github.com/harmony/harmony-service-lib-py.git#egg=harmony-service-lib\\n\\nIf using a local source tree, run the following in the source root directory instead:\\n\\n    $ pip install -e .\\n\\n## Usage\\n\\nServices that want to work with Harmony can make use of this library to ease\\ninterop and upgrades.  To work with Harmony, services must:\\n\\n1. Receive incoming messages from Harmony.  Currently the CLI is the only\\nsupported way to receive messages, though HTTP is likely to follow.  `harmony.cli`\\nprovides helpers for setting up CLI parsing while being unobtrusive to non-Harmony\\nCLIs that may also need to exist.\\n2. Extend `harmony.BaseHarmonyAdapter` and implement the `#invoke` to\\nadapt the incoming Harmony message to a service call and adapt the service\\nresult to call to one of the adapter\\'s `#completed_with_*` methods. The adapter\\nclass provides helper methods for retrieving data, staging results, and cleaning\\nup temporary files, though these can be overridden or ignored if a service\\nneeds different behavior, e.g. if it operates on data in situ and does not\\nwant to download the remote file.\\n\\nA full example of these two requirements with use of helpers can be found in\\n[example/example_service.py](example/example_service.py)\\n\\n## Environment\\n\\nThe following environment variables can be used to control the behavior of the\\nlibrary and allow easier testing:\\n\\nREQUIRED:\\n\\n* `STAGING_BUCKET`: When using helpers to stage service output and pre-sign URLs, this\\n       indicates the S3 bucket where data will be staged\\n* `STAGING_PATH`: When using helpers to stage output, this indicates the path within\\n       `STAGING_BUCKET` under which data will be staged\\n* `ENV`: The name of the environment.  If \\'dev\\' or \\'test\\', callbacks to Harmony are\\n       not made and data is not staged unless also using localstack\\n* `OAUTH_UID`, `OAUTH_PASSWORD`: Used to acquire a shared EDL token\\n       needed for downloading granules from EDL token-aware data\\n       sources. Services using data in S3 do not need to set this.\\n\\n       NOTE: If `FALLBACK_AUTHN_ENABLED` is set to True (CAUTION!)\\n       these credentials will be used to download data *as* the EDL\\n       application user. This may cause problems with metrics and can\\n       result in users getting data for which they\\'ve not approved a\\n       EULA.\\n* `OAUTH_CLIENT_ID`: The Earthdata application client ID.\\n* `OAUTH_HOST`: Set to the correct Earthdata Login URL, depending on\\n       where the service is being deployed. This should be the same\\n       environment where the `OAUTH_*` credentials are valid. Defaults\\n       to UAT.\\n* `OAUTH_REDIRECT_URI`: A valid redirect URI for the EDL application.\\n* `SHARED_SECRET_KEY`: The 32-byte encryption key shared between Harmony and backend services.\\n       This is used to encrypt & decrypt the `accessToken` in the Harmony operation message.\\n       In a production environment, this should be injected into the container running the service\\n       Docker image. When running the service within Harmony, the Harmony infrastructure will\\n       ensure that this environment variable is set with the shared secret key, and the Harmony\\n       service library will read and use this key. Therefore, the service developer need not\\n       be aware of this variable or its value.\\n\\nOPTIONAL:\\n\\n* `APP_NAME`: Defaults to first argument on commandline. Appears in log records.\\n* `AWS_DEFAULT_REGION`: (Default: `\"us-west-2\"`) The region in which S3 calls will be made\\n* `USE_LOCALSTACK`: (Development) If \\'true\\' will perform S3 calls against localstack rather\\n       than AWS\\n* `LOCALSTACK_HOST`: (Development) If `USE_LOCALSTACK` `true` and this is set, will\\n       establish `boto` client connections for S3 & SQS operations using this hostname.\\n* `TEXT_LOGGER`: (Default: True) Setting this to true will cause all\\n       log messages to use a text string format. By default log\\n       messages will be formatted as JSON.\\n* `HEALTH_CHECK_PATH`: Set this to the path where the health check file should be stored. This\\n       file\\'s mtime is set to the current time whenever a successful attempt is made to to read the\\n       message queue (whether or not a message is retrieved). This file can be used by a container\\'s\\n       health check command. The container is considered unhealthy if the mtime of the file is old -\\n       where \\'old\\' is configurable in the service container. If this variable is not set the path\\n       defaults to \\'/tmp/health.txt\\'.\\n\\nOPTIONAL -- Use with CAUTION:\\n\\n* `FALLBACK_AUTHN_ENABLED`: Default: False. Enable the fallback authentication that\\n  uses the EDL application credentials. See CAUTION note above.\\n* `EDL_USERNAME`: The Earthdata Login username used for fallback authn.\\n* `EDL_PASSWORD`: The Earthdata Login password used for fallback authn.\\n\\n## Development Setup\\n\\nPrerequisites:\\n  - Python 3.7+, ideally installed via a virtual environment such as `pyenv`\\n  - A local copy of the code\\n\\nInstall dependencies:\\n\\n    $ make develop\\n\\nRun linter against production code:\\n\\n    $ make lint\\n\\nRun tests:\\n\\n    $ make test\\n\\nBuild & publish the package:\\n\\n    $ make publish\\n\\n## Releasing\\n\\nGitHub release notes will automatically be generated based on pull request subjects.\\nPull request subject lines should therefore concisely emphasize library\\nuser-facing behavior and updates they should appear in the changelog.  If more\\ninformation is needed for release notes, note that in the PR content.\\n'},\n",
       " {'repo': 'nasa/koviz',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Get the Code\\n```sh\\ngit clone https://github.com/nasa/koviz.git\\n```\\n# Qt Prerequisites\\n\\n`koviz` is built upon the Qt framework.\\n\\n## Redhat 7+\\n\\n```sh\\nsudo yum install qt5-qtbase-devel gcc gcc-c++ make flex bison\\n```\\n\\n## Redhat 8+\\n\\n```sh\\nsudo dnf install -y qt5-qtbase-devel bison flex make gcc gcc-c++\\n```\\n\\n## Ubuntu latest\\n```sh\\nsudo apt-get install qtbase5-dev qt5-default\\n```\\n\\n# Build\\n\\n## Redhat 7 & 8\\n\\n```sh\\nqmake-qt5\\nmake\\n```\\n## Ubuntu\\n\\n```sh\\nqmake\\nmake\\n```\\n\\n# Run\\n\\n```sh\\nbin/koviz -h                  # for usage\\nbin/koviz /path/to/RUN_dir    # View trick run\\nbin/koviz /path/to/MONTE_dir  # View trick MONTE dir (set of runs)\\n```\\n'},\n",
       " {'repo': 'nasa/icarous',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '![](ICAROUS-logo.jpeg \"\")\\n\\nIndependent Configurable Architecture for Reliable Operations of\\nUnmanned Systems (ICAROUS)\\n========\\n\\nICAROUS (Independent Configurable Architecture for Reliable Operations of\\nUnmanned Systems) is a software architecture that enables the robust integration\\nof mission specific software modules and highly assured core software\\nmodules for building safety-centric autonomous unmanned aircraft\\napplications. The set of core software modules includes formally\\nverified algorithms that detect, monitor, and control conformance\\nto safety criteria; avoid stationary obstacles and maintain a safe\\ndistance from other users of the airspace; and compute resolution\\nand recovery maneuvers, autonomously executed by the autopilot, when\\nsafety criteria are violated or about to be violated. ICAROUS is implemented using the\\nNASA\\'s core Flight Systems (cFS) middleware. The aforementioned functionalities are implemented as\\ncFS applications which interact via a publish/subscribe messaging\\nservice provided by the cFS Software Bus.\\n\\n### User Guide\\n\\nhttps://nasa.github.io/icarous/\\n\\n### Current Releases\\n\\n- ICAROUS  V-2.2.5 - March 8, 2021\\n\\n### Pycarous\\n\\nRefer to [Python/pycarous/README.md](Python/pycarous/README.md) for more information about the Icarous python framework.\\n\\n\\n### License\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. \\n\\n### Contact\\n\\n[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.\\n\\n### Detect and Avoid (DAA) and Geofencing Capabilities\\n\\nICAROUS integrates NASA\\'s open source software packages [DAIDALUS](http://shemesh.larc.nasa.gov/fm/DAIDALUS)\\n(Detect and Avoid Alerting Logic for Unmanned Systems) and\\n[PolyCARP](http://shemesh.larc.nasa.gov/fm/PolyCARP) (Algorithms and Software\\nfor Computations with Polygons). DAIDALUS provides detect and avoid\\ncapabilities, while PolyCARP provides geofencing capabilities.\\n\\n## Logo\\n\\nThe ICAROUS logo was designed by \\n[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC), NASA Langley Research Center.\\n\\n### Copyright Notice\\n\\nCopyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/eo-metadata-tools',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# EO Metadata Tools #\\n\\nThis repository is for an upcoming project to provide tools for interfacing with\\nNASA metadata systems. Each directory possibly written in different programing\\nlanguages.\\n\\n## Projects ##\\n\\n* [CMR Python Wrapper](CMR/python)\\n\\n## Community\\n\\nPlease review the following files:\\n\\n* [Code of Conduct](CODE_OF_CONDUCT.md)\\n* [Contributing](CONTRIBUTING.md)\\n* [License](LICENSE)\\n* [Security](SECURITY.md)\\n'},\n",
       " {'repo': 'nasa/nasa.github.io',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': \"# NASA's Public GitHub Organization\\n\\n## Purpose\\nThe github.com/nasa organizational account is intended to publicly host NASA code that has been SRA-approved for open source. \\n\\n# IF YOU ARE LOOKING FOR INFORMATION, PLEASE CHECK OUT THE GITHUB PAGES PAGE CREATED BY THIS REPO https://nasa.github.io/\\n\\n<!-- <b><i>If you are a NASA Staff member, please check out these instructions for adding a code repository or getting made a collaborator: https://nasa.github.io/</i></b> -->\\n\\n## Visualizations of NASA Code\\nAn exploration of user engagement with NASA code repositories. Use this to find out which code has active development and might be good place to submit a pull request! https://observablehq.com/@justingosses/public-engagement-with-nasas-open-source-code-projects-on-g?collection=@justingosses/nasa-metadata\\n\\nA collection of data explorations on observable using NASA open-source code metadata: https://observablehq.com/collection/@justingosses/nasa-metadata\\n\\n## Owners\\nWe currently have two active owners in the org:\\n+ Taylor Yates (evan.t.yates@nasa.gov)\\n+ Justin Gosses (justin.c.gosses@nasa.gov)\\n\\nPlease reach out to us if you have any questions not covered in https://nasa.github.io/ or https://code.nasa.gov/#/guide\\n\\n<!-- ## Related Information & Sites\\n\\n#### Please make sure any repos added here are also tracked in code.nasa.gov! \\nIn addition to being a congressional mandate, these will then get harvested into [code.gov](https://code.gov/) enabling tracking of government written code provided to the public. \\n\\n#### Other Related Sites\\n- [code.nasa.gov](https://code.nasa.gov)\\n- [data.nasa.gov](https://data.nasa.gov)\\n- [api.nasa.gov](https://api.nasa.gov)\\n- [open.nasa.gov](https://open.nasa.gov)\\n- [nasa.gov/data](https://nasa.gov/data) -->\\n\"},\n",
       " {'repo': 'nasa/trick',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '<p align=center>\\n<a href=\"https://nasa.github.io/trick\">\\n<img src=\"https://raw.github.com/nasa/Trick/master/TrickLogo.png\" alt=\"Trick Logo\" height=150px>\\n</a>\\n</p>\\n\\n<p align=left>\\n<a href=\"https://github.com/nasa/trick/actions?query=workflow%3ALinux\">\\n<img src=\"https://github.com/nasa/trick/workflows/Linux/badge.svg?branch=master\" alt=\"Linux\" height=30px>\\n</a>\\n</p>\\n\\n<p align=left>\\n<a href=\"https://github.com/nasa/trick/actions?query=workflow%3AmacOS\">\\n<img src=\"https://github.com/nasa/trick/workflows/macOS/badge.svg?branch=master\" alt=\"macOS\" height=30px>\\n</a>\\n</p>\\n\\n<p align=left>\\n<a href=\"https://github.com/nasa/trick/actions?query=workflow%3A32-bit\">\\n<img src=\"https://github.com/nasa/trick/workflows/32-bit/badge.svg?branch=master\" alt=\"macOS\" height=30px>\\n</a>\\n</p>\\n\\n<p align=justify>\\nThe Trick Simulation Environment, developed at the NASA Johnson Space Center, is a powerful simulation development framework that enables users to build applications for all phases of space vehicle development. Trick expedites the creation of simulations for early vehicle design, performance evaluation, flight software development, flight vehicle dynamic load analysis, and virtual/hardware in the loop training. Trick\\'s purpose is to provide a common set of simulation capabilities that allow users to concentrate on their domain specific models, rather than simulation-specific functions like job ordering, input file processing, or data recording.\\n</p>\\n\\n<table>\\n    <col width=\"33%\">\\n    <col width=\"33%\">\\n    <col width=\"33%\">\\n    <thead>\\n        <tr>\\n            <th><a href=\"https://nasa.github.io/trick/documentation/install_guide/Install-Guide\">Install Guide</a></th>\\n            <th><a href=\"https://nasa.github.io/trick/tutorial/Tutorial\">Tutorial</a></th>\\n            <th><a href=\"https://nasa.github.io/trick/documentation/Documentation-Home\">Documentation</a></th>\\n        </tr>\\n    </thead>\\n    <tbody>\\n        <tr align=\"center\">\\n            <td>Follow the installation guide to properly configure Trick on your operating system.</td>\\n            <td>Complete the tutorial to become familiar with the basics.</td>\\n            <td>Visit the documentation for a more complete understanding of Trick.</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\n<table>\\n    <col width=\"33%\">\\n    <col width=\"33%\">\\n    <col width=\"33%\">\\n    <thead>\\n        <tr>\\n            <th><a href=\"https://nasa.github.io/trick/related_projects/Related-Projects\">Related Projects</a></th>\\n            <th><a href=\"https://nasa.github.io/trick/faq/FAQ\">Frequently Asked Questions</a></th>\\n            <th><a href=\"https://nasa.github.io/trick/howto_guides/How-To-Guides\">How-To Guides</a></th>\\n        </tr>\\n    </thead>\\n    <tbody>\\n        <tr align=\"center\">\\n            <td>View some of the many projects that use Trick.</td>\\n            <td>Read some of the most frequently asked questions pertaining to Trick.</td>\\n            <td>See a collection of how-to guides detailing common Trick processes.</td>\\n        </tr>\\n    </tbody>\\n</table>\\n\\n---\\n\\nTrick is released under the NASA Open Source Agreement Version 1.3 [license](https://github.com/nasa/trick/blob/master/LICENSE).\\n'},\n",
       " {'repo': 'nasa/ow_europa',\n",
       "  'language': 'GLSL',\n",
       "  'readme_contents': 'Notices:\\n--------\\nCopyright © 2020 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nDisclaimers\\n-----------\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n\\now_europa\\n============\\nThis repository contains models of mission sites on Europa. It is a separate\\nrepository because it will potentially contain very large amounts of data that\\nwe do not want to mix with smaller code repositories.\\n\\n'},\n",
       " {'repo': 'nasa/cmr-opensearch',\n",
       "  'language': 'Ruby',\n",
       "  'readme_contents': '# [Common Metadata Repository (CMR) OpenSearch](https://cmr.earthdata.nasa.gov/openseaarch)\\n\\nVisit NASA\\'s EOSDIS CMR OpenSearch at:\\n[https://cmr.earthdata.nasa.gov/opensearch](https://cmr.earthdata.nasa.gov/opensearch)\\n\\nThe CMR OpenSearch documentation page is at:\\n[https://cmr.earthdata.nasa.gov/opensearch/home/docs](https://cmr.earthdata.nasa.gov/opensearch/home/docs)\\n\\n[![Build Status](https://travis-ci.org/nasa/cmr-opensearch.svg?branch=master)](https://travis-ci.org/nasa/cmr-opensearch)\\n\\n## About\\nCMR OpenSearch is a web application developed by [NASA](http://nasa.gov) [EOSDIS](https://earthdata.nasa.gov)\\nto enable data discovery, search, and access across Earth Science data holdings by using an open standard.\\nIt provides an interface compliant with the [OpenSearch 1.1 (Draft 5) specification](http://www.opensearch.org/Home)\\nby taking advantage of NASA\\'s [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/) APIs for data discovery and access.\\n\\n## License\\n\\n> Copyright © 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n>\\n> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\n> You may obtain a copy of the License at\\n>\\n>    http://www.apache.org/licenses/LICENSE-2.0\\n>\\n> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\n> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n\\n## Third-Party Licenses\\n\\nSee public/licenses.txt\\n\\n## Installation\\n\\n* Ruby 2.5.3\\n* A Ruby version manager such as [RVM](http://rvm.io/) or [rbenv](https://github.com/rbenv/rbenv) is strongly recommended.\\n\\n### Initial setup\\nOnce the repository is cloned locally and Ruby 2.5.3 is installed, you must install the dependencies.\\nIf you don\\'t have the [bundler](http://bundler.io/) gem already installed, execute the command below in the project root directory:\\n\\n    gem install bundler   \\n\\nor if you wish to install the bundler without documentation:\\n\\n    gem install bundler --no-rdoc --no-ri\\n\\nInstall all the gem dependencies:\\n\\n    bundle install    \\n\\nIn some cases, depending on your operating system type and/or version, the above command will fail while trying to install\\nthe libv8 and therubyracer gems.  While there might be lots of causes for the errors and lots of\\nsolutions to fix the errors, we found that on some versions of OS X, you can overcome the problem by trying to use the existing\\noperating system version of the libv8 library, rather than trying to build a new one during the normal gem install.\\nWe found the following workarounds to the _**bundle install**_ failures due to libv8:\\n\\n    $ brew install v8@3.15\\n    $ bundle config build.libv8 --with-system-v8\\n    $ bundle config build.therubyracer --with-v8-dir=$(brew --prefix v8@3.15)\\n    $ bundle install\\n\\nLocal problems with mimemagic on MACOSX?\\n    brew install shared-mime-info\\n\\n### Set up the required environment\\nThe application requires the environment variables below to be set in order to run the web application:  \\n\\nURL of the internal / back-end CMR API instance endpoint.  In a hosted environment, the application\\ntakes advantage of the direct access back-end interal URLs for increased performance in comparison to\\nthe public CMR API instance endpoint. For local installs or installs in non-CMR hosting environments,\\nthe _catalog_rest_endpoint_ and the _public_catalog_rest_endpoint_ should both point to the public\\nCMR search API endpoint.\\n\\n    catalog_rest_endpoint = <internal endpoint for the CMR Search API instance used by the application>\\n\\nURL of your specific CMR OpenSearch install:\\n\\n    opensearch_url = <CMR OpenSearch application URL>\\n\\nURL of the public CMR search API instance used in OpenSearch results links in the response ATOM feed:\\n\\n    public_catalog_rest_endpoint = <public endpoint for the CMR Search API instance>\\n\\nURL for the release page of the CMR OpenSearch application.\\nThe release page references appear on the user interface as well as in the search results:\\n\\n    release_page = <CMR OpenSearch EarthData release page>\\n\\nThe ATOM feed author email to be used for CMR entries in the matching ATOM results feed:\\n\\n    contact = <atom feed author email for each feed entry>\\n\\nThe environment specific postfix (such as dev,PT, TB etc.) for the internally generated client_id sent to echo in\\norder to obtain an echo token:\\n\\n    mode = <postfix for client_id string used in obtaining echo tokens>\\n\\nThe ECHO REST endpoint used for obtaining user access tokens:\\n\\n    echo_rest_endpoint = <echo REST endpoint>\\n\\nA CMR token with CMR collection tagging permissions in the respective CMR environment that the\\n_public_catalog_rest_endpoint_ points to:\\n\\n    CMR_ECHO_SYSTEM_TOKEN = <value>\\n\\nURL for the CMR documentation page which appears in the OpenSearch web application footer:\\n\\n    documentation_page = <CMR documentation page URL>\\n\\nEmail for the NASA official responsible for the CMR OpenSearch application:\\n\\n    organization_contact_email = <email>\\n\\nFull name for the NASA official responsible for CMR OpenSearch application:\\n\\n    organization_contact_name = <full name>\\n\\nWe provide default values for the above environment variables to enable running of the automated tests during\\nCI (continuous integrations) builds.\\nThe application first looks for the configuration file:\\n\\n    config/application.yml\\n\\nIf the file exists, the application loads the values of variables in the file in the Rails environment.  Having\\na local _config/application.yml_ file is an effective way to populate the environment variables\\nthat the application needs in order to run.  A sample _config/application.yml_ file is below:\\n\\n    current: &current\\n        opensearch_url: http://localhost:3000/opensearch\\n        catalog_rest_endpoint: https://cmr.earthdata.nasa.gov/search/\\n        echo_rest_endpoint: https://api.echo.nasa.gov/echo-rest/\\n        contact: echodev@echo.nasa.gov\\n        mode: dev\\n        public_catalog_rest_endpoint: https://cmr.earthdata.nasa.gov/search/\\n        release_page: https://wiki.earthdata.nasa.gov/display/echo/Open+Search+API+release+information\\n        documentation_page: https://wiki.earthdata.nasa.gov/display/CMR/Common+Metadata+Repository+Home\\n        organization: Sample Organization Name\\n        organization_contact_email: contact@example.com\\n        organization_contact_name: ContactFirstName ContactLastName\\n\\n    development:\\n        <<: *current\\n        CMR_ECHO_SYSTEM_TOKEN: \"CMR system token with tagging permissions in the CMR environment that development uses\"\\n\\n    production:\\n        <<: *current\\n        CMR_ECHO_SYSTEM_TOKEN: \"CMR system token with tagging permissions in the CMR PROD environment\"\\n\\n    test: &test\\n        # test values are already defaulted to enable CI automated Rspec and cucumber tests\\n\\n### Run the automated [Rspec](http://rspec.info/) and [cucumber](https://github.com/cucumber/cucumber-rails) tests\\nExecute the commands below in the project root directory:\\n\\n    bundle exec rspec\\n    bundle exec cucumber\\n\\nAll tests should pass in less than 2 minutes.\\n\\n### Run the application\\nExecute the command below in the project root directory:\\n\\n    rails server\\n\\nOpen `http://localhost:3000/opensearch` in a local browser.\\n'},\n",
       " {'repo': 'nasa/harmony-service-example',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# harmony-gdal\\n\\nA demonstration of a subsetter capability to be used with Harmomy.\\n\\n## Prerequisites\\n\\nFor building & pushing the image locally:\\n\\n1. [Docker](https://www.docker.com/get-started)\\n\\nFor local development:\\n\\n1. [pyenv](https://github.com/pyenv/pyenv)\\n2. [pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv)\\n\\n## Development\\n\\n### Install dependencies\\n\\n1. Install & use miniconda:\\n\\n        $ pyenv install miniconda3-4.7.12\\n        $ pyenv local miniconda3-4.7.12\\n\\n2. Create and activate a conda environment containing the development dependencies:\\n\\n        $ conda env create -n hgdal -f environment.yml\\n        $ pyenv activate miniconda3-4.7.12/envs/hgdal\\n\\n3. (Optional) To use an unreleased version of the `harmony-service-lib-py`, e.g., when testing changes to it, install it as a dependency from the filesystem:\\n\\n        $ git clone https://git.earthdata.nasa.gov/projects/HARMONY/repos/harmony-service-lib-py/browse ../harmony-service-lib-py\\n        $ pip3 install ../harmony-service-lib-py/ --target deps/harmony-service-lib-py\\n\\n### Run unit tests:\\n\\n        # Run the tests once:\\n        $ pytest --ignore deps\\n\\n        # Run the tests continuously in watch mode:\\n        $ ptw -c --ignore deps\\n\\n### Manually building & deploying\\n\\n1. Build the Docker image:\\n\\n        $ bin/build-image\\n\\n2. Deploy (publish) the Docker image to Amazon ECR:\\n\\n        $ bin/push-image\\n\\n### Building from Dev Container\\n\\nIf you plan to build the Docker image from a container, in addition to the above instructions, you'll want to create a .env file and populate it with the following:\\n\\n```\\n# Harmony-GDAL Environment Variables\\n\\n# Set to 'true' if running Docker in Docker and the docker daemon is somewhere other than the current context\\nDIND=true\\n\\n# Indicates where docker commands should find the docker daemon\\nDOCKER_DAEMON_ADDR=host.docker.internal:2375\\n```\\n\\n## CI\\n\\nThe project has a [Bamboo CI job](https://ci.earthdata.nasa.gov/browse/HARMONY-HG) running\\nin the Earthdata Bamboo environment.\\n\"},\n",
       " {'repo': 'nasa/apod-api',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Astronomy Picture of the Day (APOD) microservice\\n\\nA microservice written in Python with the [Flask micro framework](http://flask.pocoo.org).\\n\\n## NOTES: \\n### Code re-organization has occurred [2020-05-04]!\\nCode was reorganized to make it work more easily on AWS\\'s Elastic Beanstalk service.\\n\\nThe changes over previous version were :\\n1. Moved main code out of the APOD folder and into the top level directory as Elastic Beanstalk had a hard time finding the initial python file unless it was in the top-level folder. \\n2. Changed service.py to application.py\\n3. Changed references to app in application.py to application\\n\\nYou can find a frozen version of the previous code in the branch called <a href=\"https://github.com/nasa/apod-api/tree/prevCodeOrganization\">\"prevCodeOrganization\"</a>\\n\\n#### API Reliability\\nA very large number of people use the instance of this API that NASA has set up. If you need a extremely reliable version of this API, you likely want to stand up your own version of the API. You can do that with this code! All information that this API returns is actually just grabbed from the <a href=\\'https://apod.nasa.gov/apod/astropix.html\\'>Astronomy Photo of the Day Website</a> (APOD).\\n\\n#### Content Related Issues\\nNo one watching this repository has anything to do with Astronomy Photo of the Day website, so we\\'re unable to deal with issues directly related to their content. Please contact them directly.\\n\\n\\n# Table of contents\\n1. [Getting Started](#getting_started)\\n    1. [Standard environment](#standard_env)\\n    2. [`virtualenv` environment](#virtualenv)\\n    3. [`conda` environment](#conda)\\n2. [Docs](#docs)\\n3. [APOD parser](#TheAPODParser)\\n4. [Deployed](#Deployed)\\n5. [Feedback](#feedback)\\n6. [Author](#author)\\n\\n&nbsp;\\n## Getting started <a name=\"getting_started\"></a>\\n\\n### Standard environment <a name=\"standard_env\"></a>\\n\\n1. Clone the repo\\n```bash\\ngit clone https://github.com/nasa/apod-api\\n```\\n2. `cd` into the new directory\\n```bash\\ncd apod-api\\n```\\n3. Install dependencies into the project\\'s `lib`\\n```bash\\npip install -r requirements.txt -t lib\\n```\\n4. Add `lib` to your PYTHONPATH and run the server\\n```bash\\nPYTHONPATH=./lib python application.py\\n```\\n&nbsp;\\n### `virtualenv` environment <a name=\"virtualenv\"></a>\\n\\n1. Clone the repo\\n```bash\\ngit clone https://github.com/nasa/apod-api\\n```\\n2. `cd` into the new directory\\n```bash\\ncd apod-api\\n```\\n3. Create a new virtual environment `env` in the directory\\n```bash\\npython -m virtualenv env\\n```\\n4. Activate the new environment\\n```bash\\nsource env/bin/activate\\n```\\n5. Install dependencies in new environment\\n```bash\\npip install -r requirements.txt\\n```\\n6. Run the server locally\\n```bash\\npython application.py\\n```\\n&nbsp;\\n### `conda` environment <a name=\"conda\"></a>\\n\\n1. Clone the repo\\n```bash\\ngit clone https://github.com/nasa/apod-api\\n```\\n2. `cd` into the new directory\\n```bash\\ncd apod-api\\n```\\n3. Create a new virtual environment `env` in the directory\\n```bash\\nconda create --prefix ./env\\n```\\n4. Activate the new environment\\n```bash\\nconda activate ./env\\n```\\n5. Install dependencies in new environment\\n```bash\\npip install -r requirements.txt\\n```\\n6. Run the server locally\\n```bash\\npython application.py\\n```\\n\\n### Run it in Docker\\n\\n1. Clone the repo\\n```bash\\ngit clone https://github.com/nasa/apod-api.git\\n```\\n2. `cd` into the new directory\\n```bash\\ncd apod-api\\n```\\n3. Build the image\\n```bash\\ndocker build . -t apod-api\\n```\\n4. Run the image\\n```bash\\ndocker run -p 5000:5000 apod-api\\n```\\n\\n&nbsp;\\n## Docs <a name=\"docs\"></a>\\n\\n### Endpoint: `/<version>/apod`\\n\\nThere is only one endpoint in this service which takes 2 optional fields\\nas parameters to a http GET request. A JSON dictionary is returned nominally.\\n\\n**Fields**\\n\\n- `date` A string in YYYY-MM-DD format indicating the date of the APOD image (example: 2014-11-03).  Defaults to today\\'s date.  Must be after 1995-06-16, the first day an APOD picture was posted.  There are no images for tomorrow available through this API.\\n- `concept_tags` A boolean indicating whether concept tags should be returned with the rest of the response.  The concept tags are not necessarily included in the explanation, but rather derived from common search tags that are associated with the description text.  (Better than just pure text search.)  Defaults to False.\\n- `hd` A boolean parameter indicating whether or not high-resolution images should be returned. This is present for legacy purposes, it is always ignored by the service and high-resolution urls are returned regardless.\\n- `count` A positive integer, no greater than 100. If this is specified then `count` randomly chosen images will be returned in a JSON array. Cannot be used in conjunction with `date` or `start_date` and `end_date`.\\n- `start_date` A string in YYYY-MM-DD format indicating the start of a date range. All images in the range from `start_date` to `end_date` will be returned in a JSON array. Cannot be used with `date`.\\n- `end_date` A string in YYYY-MM-DD format indicating that end of a date range. If `start_date` is specified without an `end_date` then `end_date` defaults to the current date.\\n- `thumbs` If set to `true`, the API returns URL of video thumbnail. If an APOD is not a video, this parameter is ignored.\\n\\n**Returned fields**\\n\\n- `resource` A dictionary describing the `image_set` or `planet` that the response illustrates, completely determined by the structured endpoint.\\n- `concept_tags` A boolean reflection of the supplied option.  Included in response because of default values.\\n- `title` The title of the image.\\n- `date` Date of image. Included in response because of default values.\\n- `url` The URL of the APOD image or video of the day.\\n- `hdurl` The URL for any high-resolution image for that day. Returned regardless of \\'hd\\' param setting but will be omitted in the response IF it does not exist originally at APOD.\\n- `media_type` The type of media (data) returned. May either be \\'image\\' or \\'video\\' depending on content.\\n- `explanation` The supplied text explanation of the image.\\n- `concepts` The most relevant concepts within the text explanation.  Only supplied if `concept_tags` is set to True.\\n- `thumbnail_url` The URL of thumbnail of the video. \\n- `copyright` The name of the copyright holder.\\n- `service_version` The service version used.\\n\\n**Example**\\n\\n```bash\\nlocalhost:5000/v1/apod?date=2014-10-01&concept_tags=True\\n```\\n<details><summary>See Return Object</summary>\\n<p>\\n\\n```jsoniq\\n{\\n    resource: {\\n        image_set: \"apod\"\\n    },\\n    concept_tags: \"True\",\\n    date: \"2013-10-01\",\\n    title: \"Filaments of the Vela Supernova Remnant\",\\n    url: \"http://apod.nasa.gov/apod/image/1310/velafilaments_jadescope_960.jpg\",\\n    explanation: \"The explosion is over but the consequences continue. About eleven\\n    thousand years ago a star in the constellation of Vela could be seen to explode,\\n    creating a strange point of light briefly visible to humans living near the\\n    beginning of recorded history. The outer layers of the star crashed into the\\n    interstellar medium, driving a shock wave that is still visible today. A roughly\\n    spherical, expanding shock wave is visible in X-rays. The above image captures some\\n    of that filamentary and gigantic shock in visible light. As gas flies away from the\\n    detonated star, it decays and reacts with the interstellar medium, producing light\\n    in many different colors and energy bands. Remaining at the center of the Vela\\n    Supernova Remnant is a pulsar, a star as dense as nuclear matter that rotates\\n    completely around more than ten times in a single second.\",\\n    concepts: {\\n        0: \"Astronomy\",\\n        1: \"Star\",\\n        2: \"Sun\",\\n        3: \"Milky Way\",\\n        4: \"Hubble Space Telescope\",\\n        5: \"Earth\",\\n        6: \"Nebula\",\\n        7: \"Interstellar medium\"\\n    }\\n}\\n```\\n\\n</p>\\n</details>\\n\\n\\n```bash\\nhttps://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&count=5\\n```\\n\\n<details><summary>See Return Object</summary>\\n<p>\\n\\n\\n```jsoniq\\n[\\n  {\\n    \"copyright\": \"Panther Observatory\",\\n    \"date\": \"2006-04-15\",\\n    \"explanation\": \"In this stunning cosmic vista, galaxy M81 is on the left surrounded by blue spiral arms.  On the right marked by massive gas and dust clouds, is M82.  These two mammoth galaxies have been locked in gravitational combat for the past billion years.   The gravity from each galaxy dramatically affects the other during each hundred million-year pass.  Last go-round, M82\\'s gravity likely raised density waves rippling around M81, resulting in the richness of M81\\'s spiral arms.  But M81 left M82 with violent star forming regions and colliding gas clouds so energetic the galaxy glows in X-rays.  In a few billion years only one galaxy will remain.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/0604/M81_M82_schedler_c80.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Galaxy Wars: M81 versus M82\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/0604/M81_M82_schedler_c25.jpg\"\\n  },\\n  {\\n    \"date\": \"2013-07-22\",\\n    \"explanation\": \"You are here.  Everyone you\\'ve ever known is here. Every human who has ever lived -- is here. Pictured above is the Earth-Moon system as captured by the Cassini mission orbiting Saturn in the outer Solar System. Earth is the brighter and bluer of the two spots near the center, while the Moon is visible to its lower right. Images of Earth from Saturn were taken on Friday. Quickly released unprocessed images were released Saturday showing several streaks that are not stars but rather cosmic rays that struck the digital camera while it was taking the image.  The above processed image was released earlier today.  At nearly the same time, many humans on Earth were snapping their own pictures of Saturn.   Note: Today\\'s APOD has been updated.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/1307/earthmoon2_cassini_946.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Earth and Moon from Saturn\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/1307/earthmoon2_cassini_960.jpg\"\\n  },\\n  {\\n    \"copyright\": \"Joe Orman\",\\n    \"date\": \"2000-04-06\",\\n    \"explanation\": \"Rising before the Sun on February 2nd, astrophotographer Joe Orman anticipated this apparition of the bright morning star Venus near a lovely crescent Moon above a neighbor\\'s house in suburban Phoenix, Arizona, USA. Fortunately, the alignment of bright planets and the Moon is one of the most inspiring sights in the night sky and one that is often easy to enjoy and share without any special equipment. Take tonight, for example. Those blessed with clear skies can simply step outside near sunset and view a young crescent Moon very near three bright planets in the west Jupiter, Mars, and Saturn. Jupiter will be the unmistakable brightest star near the Moon with a reddish Mars just to Jupiter\\'s north and pale yellow Saturn directly above. Of course, these sky shows create an evocative picture but the planets and Moon just appear to be near each other -- they are actually only approximately lined up and lie in widely separated orbits. Unfortunately, next month\\'s highly publicized alignment of planets on May 5th will be lost from view in the Sun\\'s glare but such planetary alignments occur repeatedly and pose no danger to planet Earth.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/0004/vm_orman_big.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Venus, Moon, and Neighbors\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/0004/vm_orman.jpg\"\\n  },\\n  {\\n    \"date\": \"2014-07-12\",\\n    \"explanation\": \"A new star, likely the brightest supernova in recorded human history, lit up planet Earth\\'s sky in the year 1006 AD. The expanding debris cloud from the stellar explosion, found in the southerly constellation of Lupus, still puts on a cosmic light show across the electromagnetic spectrum. In fact, this composite view includes X-ray data in blue from the Chandra Observatory, optical data in yellowish hues, and radio image data in red. Now known as the SN 1006 supernova remnant, the debris cloud appears to be about 60 light-years across and is understood to represent the remains of a white dwarf star. Part of a binary star system, the compact white dwarf gradually captured material from its companion star. The buildup in mass finally triggered a thermonuclear explosion that destroyed the dwarf star. Because the distance to the supernova remnant is about 7,000 light-years, that explosion actually happened 7,000 years before the light reached Earth in 1006. Shockwaves in the remnant accelerate particles to extreme energies and are thought to be a source of the mysterious cosmic rays.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/1407/sn1006c.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"SN 1006 Supernova Remnant\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/1407/sn1006c_c800.jpg\"\\n  },\\n  {\\n    \"date\": \"1997-01-21\",\\n    \"explanation\": \"In Jules Verne\\'s science fiction classic A Journey to the Center of the Earth, Professor Hardwigg and his fellow explorers encounter many strange and exciting wonders. What wonders lie at the center of our Galaxy? Astronomers now know of some of the bizarre objects which exist there, like vast dust clouds,\\\\r bright young stars, swirling rings of gas, and possibly even a large black hole. Much of the Galactic center region is shielded from our view in visible light by the intervening dust and gas. But it can be explored using other forms of electromagnetic radiation, like radio, infrared, X-rays, and gamma rays. This beautiful high resolution image of the Galactic center region in infrared light was made by the SPIRIT III telescope onboard the Midcourse Space Experiment. The center itself appears as a bright spot near the middle of the roughly 1x3 degree field of view, the plane of the Galaxy is vertical, and the north galactic pole is towards the right. The picture is in false color - starlight appears blue while dust is greenish grey, tending to red in the cooler areas.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/9701/galcen_msx_big.gif\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Journey to the Center of the Galaxy \\\\r\\\\nCredit:\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/9701/galcen_msx.jpg\"\\n  }\\n]\\n```\\n\\n</p>\\n</details>\\n\\n\\n\\n```bash\\nhttps://api.nasa.gov/planetary/apod?api_key=DEMO_KEY&start_date=2017-07-08&end_date=2017-07-10\\n```\\n\\n<details><summary>See Return Object</summary>\\n<p>\\n\\n```jsoniq\\n[\\n  {\\n    \"copyright\": \"T. Rector\",\\n    \"date\": \"2017-07-08\",\\n    \"explanation\": \"Similar in size to large, bright spiral galaxies in our neighborhood, IC 342 is a mere 10 million light-years distant in the long-necked, northern constellation Camelopardalis. A sprawling island universe, IC 342 would otherwise be a prominent galaxy in our night sky, but it is hidden from clear view and only glimpsed through the veil of stars, gas and dust clouds along the plane of our own Milky Way galaxy. Even though IC 342\\'s light is dimmed by intervening cosmic clouds, this sharp telescopic image traces the galaxy\\'s own obscuring dust, blue star clusters, and glowing pink star forming regions along spiral arms that wind far from the galaxy\\'s core. IC 342 may have undergone a recent burst of star formation activity and is close enough to have gravitationally influenced the evolution of the local group of galaxies and the Milky Way.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/1707/ic342_rector2048.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Hidden Galaxy IC 342\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/1707/ic342_rector1024s.jpg\"\\n  },\\n  {\\n    \"date\": \"2017-07-09\",\\n    \"explanation\": \"Can you find your favorite country or city?  Surprisingly, on this world-wide nightscape, city lights make this task quite possible.  Human-made lights highlight particularly developed or populated areas of the Earth\\'s surface, including the seaboards of Europe, the eastern United States, and Japan.  Many large cities are located near rivers or oceans so that they can exchange goods cheaply by boat.  Particularly dark areas include the central parts of South America, Africa, Asia, and Australia.  The featured composite was created from images that were collected during cloud-free periods in April and October 2012 by the Suomi-NPP satellite, from a polar orbit about 824 kilometers above the surface, using its Visible Infrared Imaging Radiometer Suite (VIIRS).\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/1707/EarthAtNight_SuomiNPP_3600.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Earth at Night\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/1707/EarthAtNight_SuomiNPP_1080.jpg\"\\n  },\\n  {\\n    \"date\": \"2017-07-10\",\\n    \"explanation\": \"What\\'s happening around the center of this spiral galaxy? Seen in total, NGC 1512 appears to be a barred spiral galaxy -- a type of spiral that has a straight bar of stars across its center.  This bar crosses an outer ring, though, a ring not seen as it surrounds the pictured region. Featured in this Hubble Space Telescope image is an inner ring -- one that itself surrounds the nucleus of the spiral.  The two rings are connected not only by a bar of bright stars but by dark lanes of dust. Inside of this inner ring, dust continues to spiral right into the very center -- possibly the location of a large black hole. The rings are bright with newly formed stars which may have been triggered by the collision of NGC 1512 with its galactic neighbor, NGC 1510.\",\\n    \"hdurl\": \"https://apod.nasa.gov/apod/image/1707/NGC1512_Schmidt_1342.jpg\",\\n    \"media_type\": \"image\",\\n    \"service_version\": \"v1\",\\n    \"title\": \"Spiral Galaxy NGC 1512: The Nuclear Ring\",\\n    \"url\": \"https://apod.nasa.gov/apod/image/1707/NGC1512_Schmidt_960.jpg\"\\n  }\\n]\\n```\\n\\n\\n</p>\\n</details>\\n\\n## The APOD Parser<a name=\"TheAPODParser\"></a>\\n\\n<i>The APOD Parser is not part of the API itself. </i> Rather is intended to be used for accessing the APOD API quickly with Python without writing much additional code yourself. It is found in the apod_parser folder.\\n\\n### Usage\\n\\n1. First import the `apod_object_parser.py` file.\\n\\n2. Now use the `get_data` function and pass your API key as the only argument. You can get the API key <a href=\"https://api.nasa.gov/#signUp\">here</a>\\n\\n```python\\nresponse = apod_object_parser.get_data(<your_api_key>)\\n```\\n\\n3. Now you can use the following functions:\\n\\n-> `apod_object_parser.get_date(response)`\\n\\n-> `apod_object_parser.get_explaination(response)`\\n\\n-> `apod_object_parser.get_hdurl(response)`\\n\\n-> `apod_object_parser.get_media_type(response)`\\n\\n-> `apod_object_parser.get_service_version(response)`\\n\\n-> `apod_object_parser.get_title(response)`\\n\\n-> `apod_object_parser.get_url(response)`\\n\\n**for full docs and more functions visit the readme of  the apod parser by clicking <a href=\"apod_parser/apod_parser_readme.md\">here</a>**\\n\\n## Deployed <a name=\"Deployed\"></a>\\nThe deployed version of this API is based on the `eb` branch. The version that was deployed before that is in the `eb_previous` branch. The `master` branch is used as development as that\\'s where most of the pull requests will come into anyways.\\n\\nThis API is deployed on AWS using elastic beanstalk due to large number of people who use the service. However, if you\\'re planning on using it just yourself, it is small enough to be stood up on a single micro EC2 or any other small size cloud compute machine.\\n\\n## Feedback <a name=\"feedback\"></a>\\n\\nStar this repo if you found it useful. Use the github issue tracker to give\\nfeedback on this repo.\\n\\n## Author <a name=\"author\"></a>\\n- Brian Thomas (based on code by Dan Hammer) \\n- Justin Gosses (made changes to allow this repository to run more easily on AWS Elastic Beanstalk after heroku instance was shut-down)\\n- Please checkout the <a href=\"https://github.com/nasa/apod-api/graphs/contributors\">contributers</a> to this repository on the righthand side of this page. \\n\\n## Contributing\\nWe do accept pull requests from the public. Please note that we can be slow to respond. Please be patient. \\n\\nAlso, **the people with rights on this repository are not people who can debug problems with the APOD website itself**. If you would like to contribute, right now we could use some attention to the tests. \\n\\n'},\n",
       " {'repo': 'nasa/GlennOPT',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': \"# GlennOPT\\n## Objective\\nThe objective of this project is to develop a standalone optimization tool that can be easily integrated into openmdao at a later stage. \\nThis tool will be entirely written in python making it compatible with windows and linux. \\n\\n## Why this tool?\\nThis tool overcomes some of the limitations of gradient based optimization where F(x) = y. This isn't always true for all simulations. If you need gradient optimization then go with OpenMDAO.\\n\\n## Summary\\nMany optimization packages seem like a compile of past tools written in other langauges, they lack unverisal features described above that can make big data really happen at Glenn\\n\\n## Project Goals and Tasks\\nKey Features\\n*  Keeps track of each evaluation\\n*  Restarts from an Population folder\\n*  Execution timeouts (If CFD is stuck, kill it, but keep eval folder)\\n*  Exports all results to tecplot, csv \\n*  Track performance parameters\\n*  Apply constraints to performance parameters \\nFuture Features\\n*  Addition of other optimization algorithms\\n*  Incorporate metamodels (machine learning, kriging) directly in the optimization\\n\\n\\n# Tutorials\\n[Multi-Objective Kursawe Function](https://colab.research.google.com/github/nasa/GlennOPT/blob/main/test/KUR/multi_objective_example.ipynb)\\n\\n[Single Objective Rosenbrock](https://colab.research.google.com/github/nasa/GlennOPT/blob/main/test/Rosenbrock/RosenbrockExample.ipynb)\\n\\n[Multi and Single Objective Probe Placement](https://colab.research.google.com/github/nasa/GlennOPT/blob/main/test/ProbePlacement_multi/ProbePlacementExample.ipynb)\\n\\n# Contributors \\n|                   | Position      | Dates            | Responsibility                      |   |\\n|-------------------|---------------|------------------|-------------------------------------|---|\\n| Justin Rush       | LERCIP Intern | Fall 2020        | CCD DOE                             |   |\\n| Nadeem Kever      | LERCIP Intern | Summer-Fall 2020 | Single Objective DE                 |   |\\n| Trey Harrison     | Researcher    | Fall 2020-       | Probe Optimization test case        |   |\\n| Paht Juangphanich | Researcher    | Spring 2020-     | Architect                           |   |\\n\\n\\n\"},\n",
       " {'repo': 'nasa/cmr-metadata-review',\n",
       "  'language': 'Ruby',\n",
       "  'readme_contents': '# CMR Metadata Review\\nThe CMR Metadata Review tool is used to curate NASA EOSDIS collection and granule level metadata in CMR for correctness, completeness and consistency.\\n\\n## Synopsis\\n\\nThe CMR Metadata Review tool is designed to aid in the process of performing quality checks on metadata records in the Common Metadata Repository (CMR). A series of initial automated metadata quality checks are performed when ingesting a selected record into the tool, after which the record is available for manual review. Metadata reviewers perform manual quality checks in the tool, with the option to flag identified issues as high (red), medium (yellow), or low (blue) priority. Additionally, the tool allows reviewers to leave specific recommendations on how to resolve identified issues. Metrics are tracked for all of the data entered into the tool (e.g. number of high, medium, and low priority issues flagged) allowing for the generation of reports on the fly. Detailed reports can be generated for individual record reviews, which summarize the number of red, yellow and blue flagged elements along with recommendations made on how to resolve identified issues. Higher level reports, summarizing metrics across all records in the tool or from a certain data provider, can also be generated. Reports can be shared directly in the tool with other users.\\n\\n## Motivation\\n\\nThis tool was specifically developed for the Analysis and Review of CMR (ARC) Team, which is responsible for conducting quality evaluations of NASA\\'s metadata records in the Common Metadata Repository (CMR). The overarching goal of this project is to ensure that NASA data in the CMR is discoverable and accessible, and that the user experience is consistent when exploring data holdings across NASA\\'s multiple data centers. Achieving this goal involves evaluating metadata for completeness, correctness, and consistency; a process largely supported by this tool.\\n\\n## Development Environment Set Up\\n\\nThis can be found (with pictures) [here](https://wiki.earthdata.nasa.gov/display/CMRARC/Dev+Environment+Set+Up).\\n\\n## Setting Up Local Development Project\\n\\nAll development work is done on local copies of the code base.  Follow the steps below to prepare a local copy on your machine.\\n\\n1. Install Ruby on your machine.  The production version of the project uses Ruby v 2.7.2. Other versions may work, but only 2.7.2 is currently fully supported.\\n\\n- You can check which version of ruby you are running with the command `ruby -v`, and the location with `which ruby`\\n\\n- If the version of ruby that comes up is not 2.7.2 and you are using rvm use the command `rvm list` to list which ruby versions you have. To set the ruby version for the current directory use the command `rvm use 2.7.2`, or to set your default use `rvm --default use 2.7.2`\\n\\n2. Download the source code from this github repo.\\n\\n3. The user will then need to setup a local Postgres DB to connect to the app. _\\\\*NOTE: make sure to install Postgres 9.6, because 10.\\\\* is known to cause issues with running tests._\\n\\n- Macs come pre-installed with PSQL (and a common way to manage it is with [PGAdmin](https://www.pgadmin.org/download/).\\n\\n- _From the command line/Terminal_ - enter the psql command line with the command `psql`\\n\\n- You will then be prompted for your password\\n\\n- If you run into the issue of `password authentication failed for user` – try the command `psql -U postgres -h localhost` and enter in the password for your computer.  If that command does not work, you may find it easier to alter the pg_hba.conf file.  The location of this file varies based on where the postgres is trying to run from.  It should be located in the data directory.  Using `ps aux | grep postgres` should list the active processes, one of which should contain something vaguely like: \\' /usr/local/opt/postgresql@9.6/bin/postgres -D /usr/local/var/postgresql@9.6\\'.  The location after the -D is the data directory for the instance that is currently running.  After you alter that file, you will need to restart postgres for the changes to take effect.  \\n\\n- If you run into the issue of `psql: FATAL: database \"<user>\" does not exist` - see [this link](https://stackoverflow.com/questions/17633422/psql-fatal-database-user-does-not-exist)\\n\\n- Some people may find they need to create an initial database to connect to with `/usr/local/opt/postgresql@9.6/bin/createdb -h localhost`\\n\\n4. From the PSQL console, you will need to add a `SUPERUSER` matching the username values shown in `config/database.yml` within the project repo. The development & test DB\\'s both are in local Postgres. There are many ways to create a postgres db and superuser.  These links can help –  [here](https://launchschool.com/blog/how-to-install-postgresql-on-a-mac) and [here](https://www.postgresql.org/docs/9.1/static/sql-createrole.html).\\n\\n- With user information from `config/database.yml`, the commands will look like: `CREATE ROLE <user> WITH SUPERUSER PASSWORD \\'<password>\\';`\\n\\n- Create both the development and test users\\n\\n- To see if you have a successfully created a user, use the command `\\\\du` in the psql console\\n\\n- If you have error connecting with the two  accounts, run the following command to grant login right: `ALTER ROLE \"<user>\" WITH LOGIN;`\\n\\n- After you have finished creating the users, exit out of the psql console with the command `\\\\q`\\n\\n5. Start the Postgres server on your machine. This command will depend on how you installed.\\n\\n6. The user should then navigate into the project directory and run the \\'bundle install\\' command to install all needed dependencies (you might need to do the command `gem install bundler` to get the bundler gem). Sometimes there are build conflicts on the local machine during this process.  Most solutions can be found on stack overflow. If you encounter any bundle install failures, please post the error notice and solution here so they can be updated in the source directory.\\n\\n\\t1. If Puma is problematic (as observed on the Mac OS 10.13) , try the following: `gem install puma -v \\'3.6.2\\' -- --with-opt-dir=/usr/local/opt/openssl`.\\n\\n7. Once installation of gems is complete, to create the Database the user should run the commands `rake db:create`, `rake db:migrate`, and rake `db:seed` in that order. These commands will create the db required to run the project.\\n\\n- The `seeds.rb` file currently will only seed some user data required for testing.\\n\\n8. To ensure that you\\'ve created the proper databases – you can go back into the psql console and use the command `\\\\l`\\n\\n9. An application.yml file will be needed before starting the server.  This file can only be obtained from a teammate on the project, it does not reside on the repo.  Once received, copy the file into your config folder. The application.yml is set to be ignored by git.  However, if somehow you accidentally commit the file or send the file to the cloud repo, let someone on the team know immediately.  All env variables will need to be reset across platforms to ensure safety of the system.\\n\\n10. The last piece of software that needs to be installed is python. The production version uses 2.7. Using pip you\\'ll need to install the \"requests\" package, e.g,:\\n/usr/bin/pip install requests\\n\\n11. The application uses react:rails which has a dependency on having [yarn](https://tecadmin.net/install-yarn-macos/) installed on your system.  Yarn is a package manager used to install Javascript dependencies needed by the frontend.   To install these dependencies, issue the command: `yarn install` from the root directory of the application.\\n    \\n11. Now the project should be ready to go. Navigate to the home directory and execute `rails s` to start the server.\\n\\n12. The homepage will be served at the address `localhost:3000` in your browser. To use the tool locally you will need to:\\n\\n- Register for an [Earthdata Login account](https://sit.urs.earthdata.nasa.gov/) for the SIT environment.\\n\\n- Request ACL permissions to access the tool in the SIT environment by emailing [Earthdata Support](mailto:support@earthdata.nasa.gov). In order to Ingest collections into the tool, you may need Admin or Arc Curator permissions as well..  Also see section below entitles \"Authentication\"\\n\\n## Quick Start Guide - Installation on Mac\\n\\n## Changing Python to default to 2.7\\n    echo ‘export PATH=“/System/Library/Frameworks/Python.framework/Versions/2.7/bin/:$PATH”’ >> ~/.zshrc\\n    source ~/.zshrc\\n    \\n## Installing pip (if necessary)\\n    curl https://bootstrap.pypa.io/pip/2.7/get-pip.py -o get-pip.py\\n    python get-pip.py --user\\n    pip install requests\\n\\n## Installing rvm\\n    \\\\curl -sSL https://get.rvm.io | bash\\n    rvm install 2.7.2\\n    rvm use 2.7.2 --default \\n\\n### Installing Postgresql 9.6\\n    brew install postgresql@9.6\\n    echo \\'export PATH=\"/usr/local/opt/postgresql@9.6/bin:$PATH\"\\' >> ~/.zshrc\\n    createuser -s postgres\\n    createuser -s cmrdash\\n    createuser -s cmrdash_test\\n\\n### Installing source code\\n    git clone https://[userid]@github.com/nasa/cmr-metadata-review.git\\n    cd cmr-metadata-review\\n    rake db:create:all\\n    rake db:migrate\\n    cp ~/application.yml config (yml is supplied by another developer)\\n\\n### Install Gems\\n    gem install bundle\\n    bundle install\\n\\n### Install reactjs deps\\n    brew install yarn\\n    yarn install\\n\\n### Startup the server\\n    rails s\\n\\n## Authentication\\nThe application uses [devise](https://github.com/heartcombo/devise) which is a flexible authentication solution for \\nRails.  The authorization mechanism uses [omniauth](https://github.com/omniauth/omniauth) which is a library/module that standardizes \\nmulti-provider authentication for web applications.  Any developer can create strategies for OmniAuth that can \\nauthenticate users via disparate systems.  \\n\\nOne strategy, called [Developer](https://github.com/omniauth/omniauth/blob/master/lib/omniauth/strategies/developer.rb), is included with OmniAuth and provides a completely \\ninsecure, non-production-usable strategy that directly prompts an user for authentication information and then \\npasses it straight through.    If the server is run in \"development\" mode, you will see a button called \"Login with Developer\",\\nyou can click that, provide your name and email, and it will log you in using the information you provide.  It will bypass all\\ntypical authorizations you would get with production and assign you the role of `admin`.\\n\\nThe second strategy, called [URS](https://git.earthdata.nasa.gov/projects/CMRARC/repos/omniauth-edl/browse), provides \\na production-usable strategy that directly authenticates with Earth Data Login.   You can register for a account for \\n[SIT](https://sit.urs.earthdata.nasa.gov/home), [UAT](https://uat.urs.earthdata.nasa.gov/home), and [PROD](https://urs.earthdata.nasa.gov/home).  Please note, SIT and UAT both require PIV access.   If you want to configure authentication with the production, you\\'ll need to modify these 2 lines \\nin `cmr-metadata-review/config/environments/development.rb`:\\n\\n    config.cmr_base_url = \\'https://cmr.earthdata.nasa.gov\\'\\n    config.kms_base_url = \\'https://gcmd.earthdata.nasa.gov\\'\\n\\nThis will tell the authorization mechanism to access production CMR for access control rather than SIT. \\n\\n## Importing Database \\n\\n    psql -U postgres -c \\'drop database cmrdash\\'\\n    psql -U postgres -c \\'create database cmrdash\\'\\n    psql --user cmrdash cmrdash < sit-db-dump-2018-09-27.sql\\n(Note you may see a bunch of ERROR like role \"arcdashadmin\" does not exist, this can be ignored)\\n\\nAfter the import is complete, you\\'ll need to run `db:migrate` again.\\n\\nAdditional note: If you login and view the dashboard home page (with all the sections) and the sections are empty, check the server logs, if you see lots of:\\n\\n    Record without DAAC: #<Record id: 4489, recordable_id: 1891, recordable_type: \"Collection\", revision_id: \"30\", closed_date: nil, format: \"echo10\", state: \"in_daac_review\", associated_granule_value: nil, copy_recommendations_note: nil, released_to_daac_date: \"2021-03-27 17:00:12\", daac: nil, campaign: [], native_format: nil>\\n\\nIt means one of the migration scripts did not run properly (not entirely sure why), but if you\\nredo the migration for that one script, it will work:\\n\\n    rake db:migrate:redo VERSION=20200227150658\\n\\n## Other Known Issues\\n\\nDuring the RAILS 5.2 upgrade, there was an issue with the CSRF authenticity tokens.   Namely, this specific\\nworkflow:  if a user clicks See Review Details, then clicks Curation Home, then clicks Revert Record,\\nthey will get a Invalid Authenticity Token.   Workaround is to tell form_with it should not auto include the\\ntoken, rather we should explicitly include it ourselves. i.e.,\\n`<%= hidden_field_tag :authenticity_token, form_authenticity_token %>`\\nNote: The above work-around is not necessary on GET requests, only POST, PUT, and DELETE.\\n\\nSee [https://bugs.earthdata.nasa.gov/browse/CMRARC-484] and [https://github.com/rails/rails/issues/24257]\\nfor more details.\\n'},\n",
       " {'repo': 'nasa/concept-tagging-training',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Concept Tagging Training\\n\\nThis software enables the creation of concept classifiers, to be utilized by an \\naccompanying [service](https://github.com/nasa/concept-tagging-api). If you don\\'t have your own data to train, you can use the pretrained models <a href=\"https://data.nasa.gov/Software/STI-Tagging-Models/jd6d-mr3p\">described here</a>. This project was written about [here](https://strategy.data.gov/proof-points/2019/05/28/improving-data-access-and-data-management-artificial-intelligence-generated-metadata-tags-at-nasa/) for the Federal Data Strategy Incubator Project.\\n\\n### What is Concept Tagging\\nBy concept tagging, we mean you can supply text, for example:` Volcanic activity, or volcanism, has played a significant role in the geologic evolution of Mars.[2] Scientists have known since the Mariner 9 mission in 1972 that volcanic features cover large portions of the Martian surface.` and get back predicted keywords, like `volcanology, mars surface, and structural properties`, as well as topics like `space sciences, geosciences`, from a standardized list of several thousand NASA concepts with a probability score for each prediction.\\n\\n## Requirements\\n\\nYou can see a list of options for this project by navigating to the root of the project and executing `make` or `make help`.\\n\\nThis project requires:\\n* [docker](https://docs.docker.com/install/) -- [tested with this version](docker-versions.txt)\\n* [GNU Make](https://www.gnu.org/software/make/) -- tested with 3.81 built for i386-apple-darwin11.3.0\\n\\n## Index:\\n1. [installation](#installation)\\n2. [how to run](#how-to-run)\\n3. [managing experiments](#managing-experiments)\\n4. [advanced usage](#advanced-usage)\\n\\n## installation\\nYou have several options for installing and using the pipeline. \\n1) [pull existing docker image](#pull-existing-docker-image)\\n2) [build docker image from source](#build-docker-image-from-source)\\n3) [install in python virtual environment](#install-in-python-virtual-environment)\\n \\n### pull existing docker image\\nYou can just pull a stable docker image which has already been made:\\n```bash\\ndocker pull storage.analytics.nasa.gov/abuonomo/concept_trainer:stable\\n```\\nIn order to do this, you must be on the NASA network and able to connect to the <https://storage.analytics.nasa.gov> docker registry.\\n\\\\* <sub> There are several versions of the images. You can see them [here](https://storage.analytics.nasa.gov/repository/abuonomo/rat_trainer). \\nIf you don\\'t use \"stable\", some or all of this guide may not work properly. </sub>\\n\\n\\n### build docker image from source\\nTo build from source, first clone this repository and go to its root.\\n\\nThen build the docker image using:\\n```bash\\ndocker build -t concept_trainer:example .\\n```\\nSubstitute `concept_trainer:example` for whatever name you would like. Keep this image name in mind. It will be used elsewhere. \\n\\n\\\\* If you are actively developing this project, you should look at the `make build` in [Makefile](Makefile). This command automatically tags the image with the current commit url and most recent git tag. The command requires that [setuptools-scm](https://pypi.org/project/setuptools-scm/) is installed.\\n\\n### install in python virtual environment\\n\\\\* tested with python3.7\\nFirst, clone this repository. \\nThen create and activate a virtual environment. For example, using [venv](https://docs.python.org/3/library/venv.html):\\n```bash\\npython -m venv my_env\\nsource my_env/bin/activate\\n```\\nNext, while in the root of this project, run `make requirements`.\\n\\n\\n## how to run\\nThe pipeline takes input document metadata structured like [this](data/raw/STI_public_metadata_records_sample100.jsonl) and a config file like [this](config/test_config.yml). The pipeline produces interim data, models, and reports.\\n\\n1. [using docker](#using-docker) -- if you pulled or built the image\\n2. [using python in virtual environment](#using-python-in-virtual-environment) -- if you are running in a local virtual environment\\n\\n### using docker\\nFirst, make sure `config`, `data`, `data/raw`, `data/interim`, `models`, and `reports` directories. If they do not exist, make them (`mkdir config data models reports data/raw`). These directories will be used as docker mounted volumes. If you don\\'t make these directories beforehand, they will be created by docker later on, but their permissions will be unnecessarily restrictive.  \\n\\nNext, make sure you have your input data in the `data/raw/` directory. [Here](data/raw/STI_public_metadata_records_sample100.jsonl) is an example file with the proper structure. You also need to make sure the `subj_mapping.json` file [here](data/interim/subj_mapping.json) is in `data/interim/` directory.\\n\\nNow, make sure you have a config file in the `config` directory. [Here](config/test_config.yml) is an example config which will work with the above example file.\\n\\nWith these files in place, you can run the full pipeline with this command:\\n```bash\\ndocker run -it \\\\\\n     -v $(pwd)/data:/home/data \\\\\\n     -v $(pwd)/models:/home/models \\\\\\n     -v $(pwd)/config:/home/config \\\\\\n     -v $(pwd)/reports:/home/reports \\\\\\n    concept_trainer:example pipeline \\\\\\n        EXPERIMENT_NAME=my_test_experiment \\\\\\n        IN_CORPUS=data/raw/STI_public_metadata_records_sample100.jsonl \\\\\\n        IN_CONFIG=config/test_config.yml\\n```\\nSubstitute `concept_trainer:example` with the name of your docker image.\\nYou can set the `EXPERIMENT_NAME` to whatever you prefer.\\n`IN_CORPUS` and `IN_CONFIG` should be set to the paths to the corpus and to the configuration file, respectively.\\n\\n\\\\* Developers can also use the `container` command in the [Makefile](Makefile). Note that this command requires [setuptools-scm](https://pypi.org/project/setuptools-scm/). Note that this command will use the image defined by the `IMAGE_NAME` variable and version number equivalent to the most recent git tag. \\n\\n\\n### using python in virtual environment\\n\\nAssuming you have cloned this repository, files for testing the pipeline should be in place. In particular, `data/raw/STI_public_metadata_records_sample100.jsonl` and `config/test_config.yml` should both exist. Additionally, you should add the `src` directory to your `PYTHONPATH`:\\n```\\nexport PYTHONPATH=$PYTHONPATH:$(pwd)/src/\\n``` \\n\\nThen, you can run a test of the pipeline with: \\n```\\nmake pipeline \\\\\\n    EXPERIMENT_NAME=test \\\\\\n    IN_CORPUS=data/raw/STI_public_metadata_records_sample100.jsonl \\\\\\n    IN_CONFIG=config/test_config.yml\\n```\\nIf you are not using the default values, simply substitute the proper paths for `IN_CORPUS` and `IN_CONFIG`. Choose whatever name you prefer for `EXPERIMENT_NAME`.\\n\\n## managing experiments\\n\\nIf you have access to the `hq-ocio-ci-bigdata` moderate s3 bucket, you can sync local experiments with those in the s3 bucket.\\n\\nFor example, if you created a local experiment with `EXPERIMENT_NAME=my_cool_experiment`, you can upload your local results to the appropriate place on the s3 bucket with:\\n```bash\\nmake sync_experiment_to_s3 EXPERIMENT_NAME=my_cool_experiment PROFILE=my_aws_profile\\n```\\nwhere `my_aws_profile` is the name of your awscli profile which has access to the given bucket.\\n\\nAfterwards, you can download the experiment interim files and results with:\\n```bash\\nmake sync_experiment_from_s3 EXPERIMENT_NAME=my_cool_experiment PROFILE=my_aws_profile\\n```\\n## use full sti metadata records\\nIf you have access to the moderate bucket and you want to work with the full STI metadata records, you can download them to the `data/raw` folder with:\\n```bash\\nmake sync_raw_data_from_s3 PROFILE=my_aws_profile\\n``` \\nWhen using these data, you will want to use a config file which is different from the test config file. You can browse previous experiments at `s3://hq-ocio-ci-bigdata/home/DataSquad/classifier_scripts/` to see example config files. You might try:\\n```yaml\\nweights:  # assign weights for term types specified in process section\\n  NOUN: 1\\n  PROPN: 1\\n  NOUN_CHUNK: 1\\n  ENT: 1\\n  ACRONYM: 1\\nmin_feature_occurrence: 100\\nmax_feature_occurrence: 0.6\\nmin_concept_occurrence: 500\\n```\\nSee [config/test_config.yml](config/test_config.yml) for details on these parameters.\\n\\n## advanced usage\\nFor more advanced usage of the project, look at the [Makefile](Makefile) commands and their associated scripts. You can learn more about these python scripts by them with help flags. For example, you can run `python src/make_cat_models.py -h`. \\n\\n'},\n",
       " {'repo': 'nasa/pigans-material-ID',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Inverse Estimation of Elastic Modulus Using Physics-Informed Generative Adversarial Networks\\n\\nThis repo contains the code and data used to generate the results in the report:\\n```\\nJ. Warner, J. Cuevas, G. Bomarito, P. Leser, and W. Leser. Inverse Estimation of Elastic Modulus \\nUsing Physics-Informed Generative Adversarial Networks. NASA/TM-2020-5001300. May 2020.\\n```\\n\\n## Dependencies\\n*  Python 3\\n*  Tensorflow 2\\n\\nAll additional modules required that can be found in requirements.txt\\n\\n```python\\npip install -r requirements.txt\\n```\\n\\n## Usage\\n\\n### 1) Dataset Generation\\nThe training/testing data used for the report can be found in the `data/` directory, \\nso this step is optional. If you would like to create your own data (change the\\nnumber of sensors, collocation points, etc. used), see the **generate_pigan.py** script.\\n\\n### 2) Training\\n\\nUse the **train_pigan.py** file to train the PIGAN using the generated training data.\\nTo visualize realtime metrics, use Tensorboard. Use the command:\\n```\\n tensorboard --logdir \"data/tensorboard/\"\\n```\\nand then follow the link provided in Chrome to see the graphs.\\n\\n### 3) Generating Predicitons and Analyzing Results\\n\\nTo generate samples with the trained PIGAN and assess accuracy versus the testing data, \\nuse the **evaluate_pigan.py** script. Be sure to specify the location of your trained models, \\nor use the models in the paper, which reside in the `data/paper_models/` folder.\\nThis python script will generate data using the model and recreate many of the \\nreport. After the script runs, the plots will be saved as pdfs in `data/plots/`.\\n\\n\\n## PI-GAN Architecture\\n\\n### The Generator\\n\\nThe Generator is composed of two feed forward deep neural networks (DNNs) that generate values for u and E, respectively. <br>\\nIt learns to estimate the probability distribution through 3 seperate loss terms: <br>\\n\\n**WGAN Loss** is the loss associated with the Discriminator:<br> <br>\\n<img src=\"images/wgan_gen_loss.png\" title=\"Equation 7\" alt=\"Generator Loss\" width=\"auto\" height=\"30\"/> <br>\\n\\n\\n**PDE Loss** is found by evaluating the generated samples at collocation points and computing the PDE\\'s residual: <br><br>\\n<img src=\"images/residual.png\" title=\"Equation 11\" alt=\"Residual\" width=\"auto\" height=\"30\"/> <br> <br>\\nThe residual is then averaged over all samples and collocation points: <br><br>\\n<img src=\"images/loss_pde.png\" title=\"Equation 13\" alt=\"PDE Loss\" width=\"auto\" height=\"70\"/> <br>\\n\\nThe implementation of PDE loss can be found in [pde.py](pigan/components/pde.py).<br>\\n\\n**Boundary Condition Loss** is calculated similarly. First generated samples are evaulated at collocation points on the boundary: <br><br>\\n<img src=\"images/boundary.png\" title=\"Equation 12\" alt=\"Boundary\" width=\"auto\" height=\"30\"/> <br><br>\\nThe residual is then averaged over all samples and boundary collocation points: <br><br>\\n<img src=\"images/loss_bc.png\" title=\"Equation 14\" alt=\"Boundary Loss\" width=\"auto\" height=\"70\"/> <br><br>\\n\\nThe implementation of Boundary Condition loss can be found [boundary_conditions.py](pigan/components/boundary_conditions.py).\\n\\nThese losses are combined to find the **Total Loss**: (Note: Our results equally weighted all three loss types)  <br><br>\\n<img src=\"images/gen_tot_loss.png\" title=\"Equation 15\" alt=\"Total Loss\" width=\"auto\" height=\"40\"/> <br>\\n\\n\\n### The Discriminator\\nThe Discriminator looks at the samples of u being generated and scores them on how close they are to the real expected values of u. <br>\\n\\nIt also enforces the gradient penalty, which encourages the discriminator gradients towards unity:\\n\\n<img src=\"images/wgan_disc_loss.png\" title=\"Equation 8\" alt=\"Discriminator Loss\" width=\"auto\" height=\"65\"/> \\n\\n<br>\\nSee the report for more details on the PIGAN formulation.\\n\\n## Acknowledgements\\n\\n* The authors thank Theodore Lewitt (USC) for his help in preparing the code in this repo.\\n\\n'},\n",
       " {'repo': 'nasa/openmct-hello',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Index\\n\\nBasic starter plugin for Open MCT using webpack.\\n\\nIncludes basic linting rules, jasmine + karma for testing.\\n\\n# Running\\n\\n```\\nnpm install\\n\\nnpm start\\n```\\n'},\n",
       " {'repo': 'nasa/cumulus-template-deploy',\n",
       "  'language': 'HCL',\n",
       "  'readme_contents': '# Cumulus Template Deployment Project\\n## How To Deploy\\nDocumentation for how to use the Cumulus template project can be read online:\\n\\n[https://nasa.github.io/cumulus/docs/deployment/deployment-readme](https://nasa.github.io/cumulus/docs/deployment/deployment-readme)'},\n",
       " {'repo': 'nasa/EMTAT',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# EMTAT\\nElectrical Modeling and Thermal Analysis Toolbox<br>\\n<meta name=\"keywords\" content=\"T-MATS, TMATS, EMTAT, Control System, Numerical Methods, Newton-Raphson, Jacobian Calculation, Propulsion, Aircraft Engine, Jet, Turbofan, Turbojet, Compressor, Turbine, Nozzle, Inlet, open source, simulation, modeling, NASA, electrical, EAP, power system, hybrid, thermodynamics, turbomachinery, MATLAB, Simulink, jet, engine,  etc.\">\\n<b> <a href= \"https://github.com/nasa/EMTAT/releases\" >Click Here</a> for stable release download</b> <br> <br>\\n<b>Introduction</b> <br>\\nThe Electrical Modeling and Thermal Analysis Toolbox \\nis a Simulink toolbox intended for use in the modeling and simulation of electrical \\nsystems and their controls. EMTAT contains generic electrical \\ncomponents that may be combined with a variable input iterative solver and optimization \\nalgorithm to create complex systems to meet the needs of a developer. Development of this tool\\nwas initiated on behalf of the NASA Transformative Aeronautics Concepts Program (TACP)/\\nTransformational Tools and Technologies (TTT) projects.\\n<br><br>\\n<b>Description</b> <br>\\nThis simulation toolbox has been developed to model the electrical portions of Electrified Aircraft  Propulsion   (EAP)   systems,   including   self-\\nheating   of   electrical   systems.   Dynamic models of EAP systems  often  include turbomachinery, an electrical  power  system, and a thermal \\nmanagement system. These portions of the propulsion system can operate on timescales   covering   six   orders   of   magnitude,   which   poses   a   \\nchallenging   modeling   task. Assumptions and approximations are made in an effort to capture the dynamics relevant to engine,   electrical   \\nsystem,   and   thermal   system   interactions,   while   still   allowing   for   fast simulations. These assumptions and approximations enable the electrical \\ncomponent models to run at the time scale of the turbomachinery, neglecting the very high frequency electrical dynamics, while demonstrating \\nrepresentative behavior of the end-to-end system. Thus the models execute in an acceptable time frame for what would otherwise be an \\nextremely stiff simulation.   The   resulting   simulations   of   EAP   systems   are   appropriate   for   system-level control  design and analysis. \\n<br><br>\\nEMTAT is written in MATLAB/Simulink (The Mathworks, Inc.), is open source, \\nand is intended for use by industry, government, and academia. All EMTAT equations \\nwere developed from public sources and all default maps and constants provided in the \\nEMTAT software package are nonproprietary and available to the public. The software \\nis released under the Apache V2.0 license agreement. \\n<br><br>\\n<b>Getting Started</b> <br>\\nStable releases of EMTAT are located under the <a href= \"https://github.com/nasa/EMTAT/releases\" >releases tab</a>. It is encouraged that a user\\ndownload the most up to date version using the appropriate software download button (green button). \\nInstallation instructions are detailed in the user\\'s guide which is included within the package (Documentation/TM-20205008125.pdf). \\n<br><br>\\nEMTAT encourages open collaboration and if a user wishes to become a developer the software \\nmay be forked at any time via the main page link.\\n'},\n",
       " {'repo': 'nasa/XPlaneConnect',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# X-Plane Connect\\nThe X-Plane Connect (XPC) Toolbox is an open source research tool used to\\ninteract with the commercial flight simulator software X-Plane. XPC allows users\\nto control aircraft and receive state information from aircraft simulated in\\nX-Plane using functions written in C, C++, Java, MATLAB, or Python in real time over the\\nnetwork. This research tool has been used to visualize flight paths, test control\\nalgorithms, simulate an active airspace, or generate out-the-window visuals for\\nin-house flight simulation software. Possible applications include active control\\nof an XPlane simulation, flight visualization, recording states during a flight,\\nor interacting with a mission over UDP.\\n\\n### Architecture\\nXPC includes an X-Plane plugin (xpcPlugin) and clients written in several\\nlanguages that interact with the plugin.\\n\\n#### Quick Start\\nTo get started using X-Plane Connect, do the following.\\n\\n1. Purchase and install X-Plane 9, 10 or 11.\\n2. Download the `XPlaneConnect.zip` file from the latest release on the [releases](https://github.com/nasa/XPlaneConnect/releases) page.\\n3. Copy the contents of the .zip archive to the plugin directory (`[X-Plane Directory]/Resources/plugins/`)\\n4. Write some code using one of the clients to manipulate X-Plane data.\\n\\nEach client is located in a top-level directory of the repository named for the\\nclient\\'s language. The client directories generally include a `src` folder\\ncontaining the client source code, and an `Examples` folder containing sample\\ncode demonstrating how to use the client.\\n\\n#### Additional Information\\nFor detailed information about XPC and how to use the XPC clients, refer to the\\n[XPC Wiki](https://github.com/nasa/XPlaneConnect/wiki).\\n\\n#### Capabilities\\nThe XPC Toolbox allows the user to manipulate the internal state of X-Plane by\\nreading and setting DataRefs, a complete list of which can be found on the\\n[X-Plane SDK wiki](http://www.xsquawkbox.net/xpsdk/docs/DataRefs.html).\\n\\nIn addition, several convenience functions are provided, which allow the user to\\nefficiently execute common commands. These functions include the ability to set\\nthe position and control surfaces of both player and multiplayer aircraft. In\\naddition, the pause function allows users to easily pause and un-pause X-Plane\\'s\\nphysics simulation engine.\\n\\n### Compatibility\\nXPC has been tested with the following software versions:\\n* Windows: Vista, 7, & 8\\n* Mac OSX: 10.8-10.14\\n* Linux: Tested on Red Hat Enterprise Linux Workstation release 6.6\\n* X-Plane: 9, 10 & 11\\n\\n### Contributing\\nAll contributions are welcome! If you are having problems with the plugin, please\\nopen an issue on GitHub or email [Chris Teubert](mailto:christopher.a.teubert@nasa.gov).\\nIf you would like to contribute directly, please feel free to open a pull request\\nagainst the \"develop\" branch. Pull requests will be evaluated and integrated into\\nthe next official release.\\n\\n\\n### Notices\\nCopyright ©2013-2018 United States Government as represented by the Administrator\\nof the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY\\nKIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY\\nWARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED\\nWARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY\\nWARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE.\\nTHIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT\\nAGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE,\\nSOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT\\nSOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\nREGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND\\nDISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n#### X-Plane API\\nCopyright (c) 2008, Sandy Barbour and Ben Supnik  All rights reserved.\\nPermission is hereby granted, free of charge, to any person obtaining a copy of\\nthis software and associated documentation files (the \"Software\"), to deal in the\\nSoftware without restriction, including without limitation the rights to use,\\ncopy, modify, merge, publish, distribute, sublicense, and/or sell copies of the\\nSoftware, and to permit persons to whom the Software is furnished to do so,\\nsubject to the following conditions:\\n\\n* Redistributions of source code must retain the above copyright notice, this\\n  list of conditions and the following disclaimer.\\n* Neither the names of the authors nor that of X-Plane or Laminar Research may\\n  be used to endorse or promote products derived from this software without\\n  specific prior written permission from the authors or Laminar Research,\\n  respectively.\\n\\nX-Plane API SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS\\nOF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\\nIF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'},\n",
       " {'repo': 'nasa/HyperInSPACE',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Hyperspectral In situ Support for PACE\\n<html lang=\"en\">\\n\\n<center><img src=\"Data/banner2.png\" alt=\"Banner\"></center>\\n\\nHyperInSPACE is designed to provide hyperspectral in situ support for the <a href=\\'https://pace.gsfc.nasa.gov/\\'>PACE mission</a> by processing automated, above-water, hyperspectral ocean color radiometry data using state-of-the-art methods and protocols for quality assurance, uncertainty estimation/propagation, sky/sunglint correction, convolution to satellite wavebands, and ocean color product retrieval. Data output are formatted to text files for submission to the SeaBASS database and saved as comprehensive HDF5 records with automated processing reports. The package is designed to facilitate rigorous, flexible, and transparent data processing for the ocean color remote sensing community, particularly PIs funded by NASA to submit such radiometric data to SeaBASS. Radiometry processed in HyperInSPACE are used for water optical characterization, ocean color product retrieval algorithm development, and orbital platform validation. \\n\\nCurrently, HyperInSPACE supports <a href=\\'https://www.seabird.com/\\'>Sea-Bird Scientific</a> HyperSAS packages with and without SolarTracker or pySAS platforms. If you are interested in integrating support for your platform, contact us at the email address below the copyright.\\n\\n## Version 1.0.7 (see Changelog.md)\\n\\n---\\n```\\n                 NASA Goddard Space Flight Center (GSFC) \\n         Software distribution policy for Public Domain Software\\n\\n The HyperInSPACE code is in the public domain, available without fee for \\n educational, research, non-commercial and commercial purposes. Users may \\n distribute this code to third parties provided that this statement appears\\n on all copies and that no charge is made for such copies.\\n\\n NASA GSFC MAKES NO REPRESENTATION ABOUT THE SUITABILITY OF THE SOFTWARE\\n FOR ANY PURPOSE. IT IS PROVIDED \"AS IS\" WITHOUT EXPRESS OR IMPLIED\\n WARRANTY. NEITHER NASA GSFC NOR THE U.S. GOVERNMENT SHALL BE LIABLE FOR\\n ANY DAMAGE SUFFERED BY THE USER OF THIS SOFTWARE.\\n\\n Copyright © 2020 United States Government as represented by the Administrator \\n of the National Aeronautics and Space Administration. All Rights Reserved.\\n```\\n---\\nPrimary Author: Dirk Aurin, USRA @ NASA Goddard Space Flight Center <dirk.a.aurin@nasa.gov>\\\\\\nAcknowledgements: N.Vandenberg (PySciDON; https://ieeexplore.ieee.org/abstract/document/8121926)\\n\\n## Requirements and Installation\\n\\nClone this repository (branch: \"master\") to a convenient directory on your computer. When HyperInSPACE/Main.py is launched for the first time, sub-directories will be created and databases downloaded and moved into them as described below. No system files will be changed.\\n\\nRequires Python 3.X installed on a Linux, MacOS, or Windows computer. The <a href=\\'https://www.anaconda.com/\\'>Anaconda</a> distribution is encouraged. (If you are unfamiliar with Anaconda, a nice walkthrough can be found here: https://youtu.be/YJC6ldI3hWk. If you are running a minimal, bootstrap distribution such as Miniconda, several additional packages may be needed including: scipy, matplotlib, pyqt5, h5py, fpdf, and requests.)\\n\\nHDF5 data files will be read and written using the h5py module (2.9.0 at the time of writing) included in a standard Anaconda Python package. The Zhang et al. (2017) sky/sunglint correction also requires Xarray, which requires installation (complete details here: http://xarray.pydata.org/en/stable/installing.html). To install Xarray with Anaconda:\\n```\\nprompt$ conda install xarray dask netCDF4 bottleneck\\n```\\n Utilization of <a href=\\'https://gmao.gsfc.nasa.gov/\\'>GMAO</a> atmospheric models for use in the Zhang et al. (2017) glint correction and filling in environmental conditions not otherwise provided in field logs will require a user account on the <a href=\\'https://earthdata.nasa.gov/\\'>NASA EARTHDATA</a> server. These are the same credentials used to download satellite imagery from oceancolor.gsfc.nasa.gov. New profiles can be created here: https://urs.earthdata.nasa.gov/users/new (but HyperInSPACE will prompt and link you when run). The Pysolar module (https://anaconda.org/conda-forge/pysolar) must be installed unless the raw data includes solar geometries (e.g. SolarTracker with SATNAV and associated device file as described below). To install Pysolar with Anaconda:\\n```\\nprompt$ conda install -c conda-forge pysolar\\n```\\nTo generate PDF reports for each file processed at Level-2, the package fpdf will need to be installed:\\n```\\nprompt$ pip install fpdf\\n```\\nThe new (v1.0.5) Anomaly Analysis tool requires pyqtgraph (http://www.pyqtgraph.org/) which can be installed with pip:\\n```\\nprompt$ pip install pyqtgraph\\n```\\nor conda\\n```\\nprompt$ conda install -c conda-forge pyqtgraph\\n```\\n---\\n## Launching\\n\\n<center><img src=\"Data/banner.jpg\" alt=\"banner\"></center>\\n<!-- ![test](Data/banner.jpg) -->\\n\\nHyperInSPACE is a Main-View-Controller Python package with a pyQt5 GUI that can be launched in several ways, such as by navigating to the program folder on the command line and typing the following command after the prompt:\\n```\\nprompt$ python Main.py\\n```\\nHowever you launch the GUI, *watch for important feedback at the command line terminal* in addition to informational GUI windows.\\n\\nThe following sub-directories will be created automatically (if not present) when you first run the program: \\n\\n- Config - Configuration and instrument files (by subdirectory - auto-created), SeaBASS header configuration files, Main view configuration file\\n- Logs - Most command line output messages generated during processing are captured for later reference in .log text files here\\n- Plots - A variety of optional plotting routines are included which create name-appropriate sub-directories (i.e. \\'L1C_Anoms\\', \\'L1D\\', \\'L1E\\', \\'L2\\', \\'L2_Spectral_Filter\\'). As with the Data, this path for outputting plots is optional and will be overwritten by choosing an alternate Data/Plots parent directory (see below).\\n- Data - This directory now comes unpacked in the distribution. By default, it contains only Pope & Fry/Smith & Baker water absorption properties, Thuillier and satellite spectral response functions, banner images for the GUI, and the Zhang glint correction database. This is also the optional fallback location for input and/or output radiometry data, though setting up separate locations for field data is highly recommended (see below).\\n- Source - This directory (which comes unpacked with the distribution) holds the majority of the Python source code.\\n\\n(Note: Data, Plots, and Logs directories are not tracked with git.)\\n\\n### Database download\\nWhen you first launch the software, it will need to download a large (2.3 GB) database (Zhang_rho_db.mat) for use in glint correction. If this database is not found in Data, a dialog window will appear before the Main.py GUI with guidance on how to proceed. If this download should fail for any reason, further instructions will be given at the command line terminal where Main.py was launched.\\n\\n---\\n## Guide\\n\\n### Quick Start Overview\\n1. Identify the research cruise, relevant calibration files, and ancillary data files to be used in processing\\n2. Launch HyperInSPACE and set up the Main window for data directories and the ancillary file\\n3. Create a new Configuration (or edit and existing Configuration)\\n4. Add and enable only *relevant* calibration and instrument files to the Configuration; there is no such thing as a standard instrument package\\n5. Choose appropriate processing parameters for L1A-L2 (do not depend on software defaults; there is no such thing as a standard data collection)\\n6. HDF files will be produced at each level of processing, plus optional SeaBASS text files for radiometry at L1E and L2. Plots can be produced at L1D, L1E, and L2. Processing logs and plots are aggregated into PDF Reports at L2 (covering all processing from RAW to L2) written to a dedicated Reports directory in the selected Output directory.\\n\\n\\n### Main Window\\n\\nThe Main window appears once Main.py is launched. It has options to specify a configuration file, input/output directories, ancillary input files (i.e. environmental conditions and relevant geometries in a SeaBASS file format), single-level processing, and multi-level processing. For batch processing, pop-up windows from \"failed\" (no output due to either corrupt raw binary data or stringent quality control filtering) files can be suppressed. Producing no output file at a given processing level is often a normal result of quality control filtering, so this option allows batches to continue uninterrupted.\\n\\nThe \\'New\\' button allows creation of a new configuration file. \\'Edit\\' allows editing the currently selected configuration file. \\'Delete\\' is used to delete the currently selected configuration file *and* corresponding auto-created calibration directories (see Configuration). After creating a new configuration file, select it from the drop-down menu, and select \\'Edit\\' to launch the Configuration module and GUI. \\n\\nThe \\'Input...\\' and \\'Output Data/Plots Parent Directory\\' buttons are self explanatory and allow optional selection of data and directories from any mounted/mapped drive. Note that output data and plot sub-directories (e.g. for processing levels) are also auto-created during processing as described below. The parent directory is the directory containing the sub-directories for processing levels (e.g. \"/L1A\", \"/L1B\", etc.) If no input or output data directories are selected, \\'/Data\\' and \\'/Plots\\' under the HyperInSPACE directory structure will be used by default as the parent directories.\\n\\nAncillary data files for environmental conditions and relevant geometries used in L2 processing must be text files in SeaBASS format with columns for date, time, lat, and lon. See https://seabass.gsfc.nasa.gov/ for a description of SeaBASS format. Optional data fields include station number, ship heading, relative sensor azimuth, aerosol optical depth, cloud cover, salinity, water temperature, and wind speed. An example ancillary file is included for use as a template. It is recommended that ancillary files are checked with the \\'FCHECK\\' utility as described on the SeaBASS website. They will be interpreted using the included SB_support.py module from NASA/OBPG. \\n\\nIn case environmental conditions were not logged in the field, or for filling in gaps in logged data, they will be retrieved from GMAO models as described below. The ancillary data file is optional (though strongly advised for adding wind speed at a minimum) provided the sensor suite is equipped with a SolarTracker or equivalent to supply the relevant sensor/solar geometries. If no SolarTracker-type instrument is present to report the relative sensor/solar geometries, the ancillary file must be provided with at least the ship heading and relative angle between the bow of the ship and the sensor azimuth as a function of time. \\n\\n### Configuration\\n\\nLaunch the configuration module and GUI (ConfigWindow.py) from the Main window by selecting/editing a configuration file or creating a new one. This file will be instrument-suite-specific, and is also deployment-specific according to which factory calibration files are needed, as well as how the instrument was configured on the platform or ship. Some cruises (e.g. moving between significantly different water types) may also require multiple configurations to obtain the highest quality ocean color products at Level 2. Sharp gradients in environmental conditions could also warrant multiple configurations for the same cruise (e.g. sharp changes in temperature may effect how data deglitching is parameterized, as described below).\\n\\n##### Calibration & Instrument Files:\\n***NOTE: IT IS IMPORTANT THAT THESE INSTRUCTIONS FOR SELECTING AND ACTIVATING CALIBRATION AND INSTRUMENT FILES ARE FOLLOWED CAREFULLY OR PROCESSING WILL FAIL***\\n\\n**Note: You do not need to move/copy/paste your calibration and instrument files; HyperInSPACE will take care of that for you.**\\n\\nIn the \\'Configuration\\' window, click \\'Add Calibration Files\\' to add the *relevant* calibration or instrument files (date-specific HyperOCR factory calibrations or ancillary instrument Telemetry Definition Files; i.e. the \\'.cal\\' and \\'.tdf\\' files). *Only add and enable those calibration and instrument files that are relevant to the cruise/package you wish to process (see below).* Each instrument you add here -- be it a radiometer or an external data instrument such as a tilt-heading sensor -- requires at least one .cal or .tdf file for raw binary data to be interpreted. Two .cal files are required in the case of radiometers calibrated seperately for shutter open (light) and shutter closed (dark) calibrations, as is typical with Satlantic/Seabird HyperOCRs. Instruments with no calibrations (e.g. GPS, SolarTracker, etc.) still require a Telemetry Definition File (.tdf) to be properly interpreted. \\n\\nAdding new .cal and .tdf files will automatically copy these files from the directory you identify on your machine into the HyperInSPACE directory structure once the Configuration is saved. \\n\\nThe calibration or instrument file can now be selected using the drop-down menu. Enable (in the neighboring checkbox) only the files that correspond to the data you want to process with this configuration. You will need to know which .cal/.tdf files correspond to each sensor/instrument, and which represent light and dark shutter measurements. For example:\\n\\n- SATMSG.tdf > SAS Solar Tracker status message string (Frame Type: Not Required)\\n\\n- SATTHSUUUUA.tdf > Tilt-heading sensor (Frame Type: Not Required) (Note: Use of built-in flux-gate compass is extremely inadviseable on a steel ship or platform. Best practice is to use externally supplied heading data from the ship\\'s NMEA datastream or from a seperate, external dual antenna GPS incorporated into the SolarTracker. DO NOT USE COURSE DATA FROM SINGLE GPS SYSTEMS.)\\n\\n- SATNAVVVVA.tdf > Solar Tracker (Frame Type: Not Required)\\n\\n- GPRMC_NMEAWWW.tdf > GPS (Frame Type: Not Required)\\n\\n- SATPYR.tdf > Pyrometer (Frame Type: Not Required)\\n\\n- HEDXXXAcal > Es (Frame Type: Dark)\\n\\n- HSEXXXA.cal > Es (Frame Type: Light)\\n\\n- HLDYYYA.cal > Li (Frame Type: Dark)\\n\\n- HSLYYYA.cal > Li (Frame Type: Light)\\n\\n- HLDZZZA.cal > Lt (Frame Type: Dark)\\n\\n- HSLZZZA.cal > Lt (Frame Type: Light)\\n\\nwhere UUUU, VVV, WWW, XXX, YYY, and ZZZ are the serial numbers of the individual instruments, which are followed where appropriate by factory calibration codes (usually A, B, C, etc. associated with the date of calibration) ***Be sure to choose the factory calibration files appropriate to the date of data collection.***\\n\\nSelections:  \\n-Add Calibration Files - Allows loading calibration/instrument files (.cal/.tdf) into HyperInSPACE. Once loaded the drop-down box can be used to select the file to enable the instrument and set the frame type.\\n-Enabled checkbox - Used to enable/disable loading the file in HyperInSPACE.\\n-Frame Type - ShutterLight/ShutterDark/Not Required can be selected. This is used to specify shutter frame type (ShutterLight/ShutterDark) for dark correction.\\n\\nFor each calibration file:  \\nClick \\'Enable\\' to enable the calibration file. Select the frame type used for dark data correction, light data, or \\'Not Required\\' for navigational and ancillary data. in version 1.0.4 (which only fully supports Satlantic/Seabird HyperSAS) each radiometer will require two calibration files (light and dark). Data from the GPS and SATNAV instruments, etc. are interpreted using the corresponding Telemetry Definition Files (\\'.tdf\\').\\n\\nOnce you have created your new Configuration, CAL/TDF files are copied from their chosen locations into the /Config directory HyperInSPACE directory structure within an automatically created sub-directory named for the Configuration (i.e. a configuration named \"KORUS\" creates a KORUS.cfg configuration file in /Config and creates the /Config/KORUS_Calibration/directory with the chosen calibration & TDF files.\\n\\nLevel 1A through Level 2 processing configurations are adjusted in the Configuration window. If you are reading this for the first time, the Configuration Window is a good reference to accompany the discussion below regarding processing. *The values set in the configuration file should be considered carefully. They will depend on your viewing geometry and desired quality control thresholds. Do not use default values without consideration.* Level 1d includes a module that can be launched from the Configuration window to assist with data deglitching parameter selection (\\'Anomaly Analysis\\'). Spectral filters are also plotted in L2 to help with filter parameterization factors. More details with citations and default setting descriptions are given below. A separate module to assist in the creation of SeaBASS output files is launched in Level 1E processing, and applied to L1E and L2 SeaBASS output as described below.\\n\\nClick \\'Save/Close\\' or \\'Save As\\' to save the configuration file. As of version 1.0.4, SeaBASS headers will be updated automatically to reflect your selection in the Configuration window. The configuration file is saved to the /Config directory under the HyperInSPACE main directory with a .cfg extension.\\n\\n### Processing Overview\\n\\nIt will be helpful to set your \\'Input Data Parent\\' and \\'Output Data Parent\\' directories from the Main window. As an example, one could use a cruise directory containing RAW HyperSAS data as the Input Parent Directory, and then create another directory to use as the Output Parent Directory when processing from L0 (raw binary). Files will be automatically sorted by processing level in the automatically created sub-directories (i.e. the software automatically creates and looks for L1A, L1B, L1C, L1D, L1E, and L2 directories under the parent directory). If not selected, the Input/Output parent directories will default to the /Data directory within HyperInSPACE. Your Main window set-up (including configuration file, Input/Output directories, and Ancillary File) will be saved in Config/main.config using the Save button or upon closing the Main window, and reopened the next time you launch Main.py.\\n\\nProcess the data by clicking on one of the buttons for single-level or multi-level processing. A file selection dialogue will appear. Multiple data files can be processed together (successively) by selecting them together in the GUI (e.g. Shift- or Ctrl- click, or Ctrl-A for all, depending on your platform). Input files will be checked for match to expected input level (e.g. L1A file input for for L1B processing). Multi-level processing works the same as single-level by processing each input raw file through all levels before moving on to the next raw file. However, it will only continue with a given file if the preceding level was created immediately (within 1 minute) prior. In other words, if -- due to changes in QA/QC parameterization -- a file is entirely discarded at a given level, but an old file of the same name still exists in that directory, it will be ignored, and processing for that file will be terminated for higher levels. \\n\\n\\n*Bug: Very rarely, when running the program for the first time, the first RAW binary data file opened for processing is not read in properly. Processing will fail with the error message: [filename] does not match expected input level for outputing L2. The file will process properly if run a second time (assuming it is a healthy file). Cause unknown.*\\n\\n#### Level 1A - Preprocessing\\n\\nProcess data from raw binary (Satlantic HyperSAS \\'.RAW\\' collections) to L1A (Hierarchical Data Format 5 \\'.hdf\\'). Calibration files and the RawFileReader.py script allow for interpretation of raw data fields, which are read into HDF objects.\\n\\n**Solar Zenith Angle Filter**: prescreens data for high SZA (low solar elevation) to exclude files which may have been collected post-dusk or pre-dawn from further processing. *Triggering the SZA threshold will skip the entire file, not just samples within the file, so do not be overly conservative with this selection, particularly for files collected over a long period.* Further screening for SZA min/max at a sample level is available in L2 processing. This option is currently only applied when using Satlantic SolarTracker (SATNAV) raw data; it is available again for all platforms at L2.\\n**Default: 60 degrees (e.g. Brewin et al., 2016)**\\n\\n#### Level 1B\\n\\nProcess data from L1A to L1B. Factory calibrations are applied and data arranged in a standard HDF5 format.\\n\\nL1B Format:\\n*Data being processed from non-HyperSAS SOLARTRACKER systems should be formatted in HDF5 to match L1B in order to be successfully processed to L2 in HyperInSPACE. HyperSAS data with no SolarTracker can be processed from raw data, provided the relative geometries are included in the ancillary data file as described above and below.* An example of an L1B HDF file is provided in /Data for reference. Datasets are grouped by instrument and contain their data in an array referenced \\'.data\\'. For example, latitude data is stored in the group \\'GPS\\' under \\'LATPOS.data\\'. Data should also contain \\'dtype\\', a list of column headers strings (e.g. the wavelength of sample \\'304.37\\', or for many instruments simply \\'NONE\\' if the data is already well described in the group name; see example file). The following datasets and attributes and groups are generally required:\\n\\nRoot level attributes:\\n\\n- \\'CAL_FILE_NAMES\\' - list of TDF and CAL file names\\n- \\'ES\\'/\\'LI\\'/\\'LT_UNITS\\' - calibrated data units for Es, Li, and Lt\\n- \\'FILE_CREATION_TIME\\' - DD-MMM-YYYY HH:MM:SS in UTC\\n- \\'PROCESSING_LEVEL\\' - \\'1b\\'\\n- \\'WAVELENGTH UNITS\\' - \\'nm\\'\\n\\nGroup id: \\'GPS\\', attributes: \\'CalFileName\\' (TDF file), \\'Frametag\\' (\\'$GPRMC\\' or \\'$GPGGA\\'), \\'InstrumentType\\' (\\'GPS\\')\\n\\ndatasets: (where y is the number of samples in the file)\\n\\n- \\'LATPOS\\' - y-length vector of latitude, DDMM.MM (D-Degree(positive), M-Decimal minute)\\n- \\'LATHEMI\\' - y-length vector of latitude hemisphere \\'N\\' or \\'S\\'\\n- \\'LONPOS\\' - y-length vector of longitude, DDDMM.MM (D-Degree(positive) , M-Decimal minute)\\n- \\'LONHEMI\\' - y-length vector of longitude hemisphere \\'W\\' or \\'E\\'\\n- \\'UTCPOS\\' - y-length vector of UTC, HHMMSS.SS (no leading zeroes, e.g. 12:01:00AM is 100.0)\\n- Several optional datasets including, \\'COURSE\\', \\'SPEED\\', etc.\\n\\nGroup id: \\'ES_DARK\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SAT\\'+instrument code+serial number), \\'FrameType\\' (\\'ShutterDark\\'), \\'InstrumentType\\' (\\'Reference\\')\\n        \\ndatasets: \\n\\n- \\'DATETAG\\'* - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\'* - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'ES\\' - Array x by y, where x is waveband, y is series. Calibrated Es dark shutter data\\n\\n*Note that HDF5 does not support Python datetime, so datetime is calculated for use at each level using Datetag and Timetag2.\\n\\nGroup id: \\'ES_LIGHT\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SAT\\'+instrument code+serial number), \\'FrameType\\' (\\'ShutterLight\\'), \\'InstrumentType\\' (\\'Reference\\')\\n        \\ndatasets: \\n\\n- \\'DATETAG\\' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'ES\\' - Array x by y, where x is waveband, y is series. Calibrated Es light shutter data\\n\\nGroup id: \\'LI_DARK\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SAT\\'+instrument code+serial number), \\'FrameType\\' (\\'ShutterDark\\'), \\'InstrumentType\\' (\\'SAS\\')\\n        \\ndatasets: \\n\\n- \\'DATETAG\\' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'LI\\' - Array x by y, where x is waveband, y is series. Calibrated Li dark shutter data\\n\\nGroup id: \\'LI_LIGHT\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SAT\\'+instrument code+serial number), \\'FrameType\\' (\\'ShutterLight\\'), \\'InstrumentType\\' (\\'SAS\\')\\n        \\ndatasets: \\n\\n- \\'DATETAG\\' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'LI\\' - Array x by y, where x is waveband, y is series. Calibrated Li light shutter data\\n\\nGroup id: \\'LT_DARK\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SAT\\'+instrument code+serial number), \\'FrameType\\' (\\'ShutterDark\\'), \\'InstrumentType\\' (\\'SAS\\')\\n\\ndatasets: \\n\\n- \\'DATETAG\\' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'LT\\' - Array x by y, where x is waveband, y is series. Calibrated Lt dark shutter data\\n\\nGroup id: \\'LT_LIGHT\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SAT\\'+instrument code+serial number), \\'FrameType\\' (\\'ShutterLight\\'), \\'InstrumentType\\' (\\'SAS\\')\\n\\ndatasets: \\n\\n- \\'DATETAG\\' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'LT\\' - Array x by y, where x is waveband, y is series. Calibrated Lt light shutter data\\n\\nGroup id: \\'SOLARTRACKER\\', attributes: \\'CalFileName\\' (CAL file), \\'Frametag\\' (\\'SATNAV\\'+instrument code+serial number), \\'FrameType\\' (\\'SATNAVNNNN\\'), \\'InstrumentType\\' (\\'SAS\\')\\n\\ndatasets:\\n\\n- \\'DATETAG\\' - Vector of length y, YYYYDOY.0, where DOY is the UTC sequential day of the year\\n- \\'TIMETAG2\\' - Vector of length y, HHMMSSUUU UTC (hour, minute, second, millisecond)\\n- \\'AZIMUTH\\' - Vector of length y, Solar Azimuth Angle (dtype \\'SUN\\')\\n- \\'ELEVATION\\' - Vector of length y, Solar Elevation (dtype \\'SUN\\')\\n- \\'HEADING\\' - Array 2 by y, True Sensor Azimuth (dtype \\'SAS_TRUE\\'), Ship Heading (dtype \\'SHIP_TRUE\\', optional)\\n- \\'PITCH\\' - Vector of lenght y, Ship pitch\\n- \\'ROLL\\' - Vector of lenght y, Ship roll\\n\\n\\n#### Level 1C\\n\\nProcess data from L1B to L1C. Data are filtered for vessel attitude (pitch, roll, and yaw when available), viewing and solar geometry. *It should be noted that viewing geometry should conform to total radiance (Lt) measured at about 40 degrees from nadir, and sky radiance (Li) at about 40 degrees from zenith* **(Mobley 1999, Mueller et al. 2003 (NASA Protocols))**. Unlike other approaches, HyperInSPACE eliminates data flagged for problematic pitch/roll, yaw, and solar/sensor geometries *prior* to deglitching the time series (L1D), thus increasing the relative sensitivity of deglitching for the removal of non-environmental anomalies.\\n\\n**SolarTracker**: Select when using the Satlantic SolarTracker package. In this case sensor and solor geometry data will come from the SolarTracker (i.e. SATNAV**.tdf). If deselected, solar geometries will be calculated from GPS time and position with Pysolar, while sensor azimuth (i.e. ship heading and sensor offset) must either be provided in the ancillary data or (eventually) from other data inputs. Currently, if SolarTracker is unchecked, the Ancillary file chosen in the Main Window will be read in, subset for the relevant dates/times, held in the ANCILLARY_NOTRACKER group object, and carried forward to subsequent levels (i.e. the file will not need to be read in again at L2). If the ancillary data file is very large (e.g. for a whole cruise at high temporal resolution), this process of reading in the text file and subsetting it to the radiometry file can be slow.\\n\\n**Rotator Home Angle Offset**: Generally 0. This is the offset between the neutral position of the radiometer suite and the bow of the ship. This *should* be zero if the SAS Home Direction was set at the time of data collection in the SolarTracker as per Satlantic SAT-DN-635. If no SolarTracker was used, the offset can be set here if stable (e.g. pointing angle on a fixed tower), or in the ancillary data file if changeable in time. Without SolarTracker, L1C processing will require at a minimum ship heading data in the ancillary file. Then the offset can be given in the ancillary file (dynamic) or set here in the GUI (static). *Note: as SeaBASS does not have a field for this angle between the instrument and the bow of the ship, the field \"relaz\" (normally reserved for the relative azimuth between the instrument and the sun) is utilized for the angle between the ship heading (NOT COG) and the sensor.*\\n\\n**Rotator Delay**: Seconds of data discarded after a SolarTracker rotation is detected. Set to 0 to ignore. Not an option without SolarTracker.\\n**Default: 60 seconds (Vandenberg 2017)**\\n\\n**Pitch & Roll Filter** (optional): Data outside these thresholds are discarded if this is enabled in the checkbox. Not currently an option without SolarTracker. \\n*{To Do: see what other accelerometer data are being collected and accommodate.}*\\n**Default: 5 degrees (IOCCG Draft Protocols; Zibordi et al. 2019; 2 deg \"ideal\" to 5 deg \"upper limit\")**\\n\\n**Absolute Rotator Angle Filter** (optional): Angles relative to the SolarTracker neutral angle beyond which data will be excluded due to obstructions blocking the field of view. These are generally set in the SolarTracker software when initialized for a given platform. Not an option without SolarTracker.\\n**Default: -40 to +40 (arbitrary)**\\n\\n**Relative Solar Azimuth Filter** (optional): Relative azimuth angle in degrees between the viewing Li/Lt and the sun.  \\n**Default: 90-135 deg (Mobley 1999, Zhang et al. 2017); 135 deg (Mueller 2003 (NASA Protocols)); 90 deg unless certain of platform shadow (Zibordi et al. 2009, IOCCG Draft Protocols)**\\n\\n\\n#### Level 1D\\n\\nProcess data from L1C to L1D. Light and dark data are screened for electronic noise (\"deglitched\" - see Anomaly Analysis), which is then removed from the data (optional, but strongly advised). Shutter dark samples are then subtracted from shutter light frames after dark data have been interpolated in time to match light data. \\n**(e.g. Brewin et al. 2016, Sea-Bird/Satlantic 2017)**\\n\\nResulting dark-corrected spectra for Es, Li, and Lt can optionally be plotted, and will be carried forward into the PDF processing report produced at L2.\\n\\n*{To Do: Allow provisions for above water radiometer set-ups that do not have a dark shutter correction.}*\\n*{To Do: discard dark data outside thresholds as hinted at in Zibordi 2009 (no actual threshold or methods given)}*\\n*Currently, spectra with anomalies in any band are deleted in their entirety, but this might be overkill -- certainly it is very conservative. It may be sufficient to set the anomalous values to NaNs, and only delete the entire spectrum if more than, say, 25% of wavebands are anomalous. TBD.*\\n\\n##### Anomaly Analysis (optional)\\n\\nDeglitching the data (which must follow after L1C processing, as it is evaluated on L1C files) is highly sensitive to the deglitching parameters described below, as well as some environmental conditions not controlled for in L1C and the variability of the radiometric data itself. Therefore, a separate module was developed to tune these parameters for individual files, instruments, and/or field campaigns and conditions. A sharp temperature change, shutter malfunction, or heavy vibration, for example, could impact \"glitchy-ness\" and change the optimal deglitching parameterization. \\n\\nDue to high HyperOCR noise in the NIR, deglitching is currently hard-coded to only perform deglichting between 350 - 850 nm. Deglitching is conservative: i.e. if a value in any waveband within a timeseries is flagged, all data for that timestamp are removed.\\n\\nThe tool is launched pressing the Anomaly Analysis button in the Configuration Window. A dialog will appear to select an L1C file for deglitching, after which a GUI will display timeseries plots of the light (shutter open) and dark (shutter closed) data for a given waveband. Metadata including date, time, wind, cloud, waves, solar and sensor geometry are shown in the top of the window. In addition, the software allows the user to define the file naming scheme of photographs collected in the field, presuming they are named with date and time. The software will look in a directory called /Photos in the designated input directory structure and match all photos within 90 minutes of the mean collection time for the file. Matched photos can by scanned using the button on the right to launch the viewer. The slider below the metadata allows for adjustment of the wavelength to be screened (the Update button will update the figures for any changes in sensor or parameterization), and radio buttons allow selection between Es, Li, or Lt sensors. Sensors can be parameterized independently of each other, and seperately for the light and dark signals. Plots are interactive and can be explored in higher detail by panning with the left mouse button or zooming with the right mouse button (a small \"A\" box in the bottom left of the plot restores it to all data, or right-click for more options).\\n\\nFor each waveband of each sensor, and for both light and dark shutter measurements, the time series of radiometric data are low-pass filtered with a moving average over time using discrete linear convolution of two one dimensional sequences with adjustable window sizes (number of samples in the window). For darks, a *STATIONARY** standard deviation anomaly (from the moving average in time) is used to assess whether data are within an adjustable \"sigma factor\" multiplier within the window. For lights, a *MOVING* standard deviation anomaly (from the moving average of separately adjustable window size) is used to assess whether data are within a separately adjustable sigma. The low-band filter is passed over the data twice. First and last data points for light and dark data cannot be accurately filtered with this method, and are discarded.  \\n\\nAdjust the window size and sigma parameters for each instrument and hit Update (or keyboard Enter) to see which data (black circles) are retained or discarded (red \\'x\\' or \\'+\\' for first and second pass, respectively). Move the slider and hit update to see how these factors impact data in various portions of the spectrum. The field \\'% Loss (all bands)\\' shows how application of the current parameterization decimates the entire spectral/temporal dataset for the given sensor.\\n\\nIn addition to the low-pass filters, light and dark data from each sensor can be filtered with a high and low value threshold. These are chosen by selecting the desired band (and hit Set Band) independently for light and dark data, and choosing a minimum and/or maximum threshold value in the appropriate boxes. Leave value as \"None\" if a particularly threshold should be ignored. For example, to filter only Li data on thresholds only for a high threshold for dark data based on 555 nm, select the Threshold checkbox, select the Li Radio button, move the slider to 555 nm, and hit Update. Now, you can enter a value (e.g. 1.0) into the lefthand \"Max\" textbox and hit \"Update\" (or keyboard Enter). The filtered data should show in blue. Keep in mind, they will only show in the waveband for which they were set, but like the low-pass filter, if they fall outside the thresholds in that band, that timestamp will be deleted for all bands.\\n\\nCurrently, to threshold data from any of the three instruments, Threshold must be left checked, but leaving the min/max values as None in the other sensors will still work to ignore thresholding those sensors.\\n\\nTo see the results plotting when reviewing the threshold parameters on a file, make sure the waveband slider is on the appropriate waveband (and hit Update).\\n\\nOnce the parameters have been adjusted for each sensor, they can be saved (Save Sensor Params button) to the current software configuration and to a backup configuration file for later use. This means that once you have \\'tuned\\' these parameters for a given file, the software will be able to load the file (from the Config directory) to reference those parameters. This is useful for reprocessing; *you should only need to tune these once for each file.* If you find that a given set of deglitching parameterizations is working sufficiently well for all your L1C files for a given cruise, simply save them once, save the Configuration from the Configuration Window, and the software configuration will reuse them for all files (i.e. it only applies alternate values for files that were specifically saved). Saved file-specific parameterization can be viewed/editted in the CSV file named after the Configuration in the Config directory (e.g. \"KORUS_anoms.csv\" for the \"KORUS.cfg\").\\n\\nFor record keeping and the PDF processing report, plots of the delitching (similar to those shown in realtime) can be saved to disk. Select the waveband interval at which to save plots (e.g. at 3.3 nm resolution and 20 interval, plots are produced every 66 nm, or 48 PNG files for a typical HyperSAS system), and click Save Anomaly Plots. Results of the anomaly detection are saved to [output_directory]/Plots/L1C_Anoms. Data flagged for removal given the parameterizations chosen in the Configuration window are shown for the filter first pass (red box) and second pass (blue star) and thresholds (red circles only shown in the band for which they were chosen).\\n\\nFor convenience a shortcut to processing the currently active L1C file to L1D is provided (Process to L1D). \\n\\nTo save the current values from the Anomaly Analysis tool as the defaults for the given cruise, Save Sensor Params > Close > Save/Close the Configuration Window.\\n\\n*{KNOWN BUG: the pyqtgraph GUI interface does not always update as expected on macOS Catalina. Hitting Update again or switching the sensor radio button generally resolves the issue.}*\\n\\n**Defaults: TBD; experimental**\\n**(Abe et al. 2006, Chandola et al. 2009)**  \\n**(API Reference: https://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html)**\\n\\n*{A problem with instrument sensitivity to integration time was recently revealed, and a patch to adjust the response to the integration time is under development. This may be applied here, or alternatively in L1B.}*\\n\\n#### Level 1E\\n\\nProcess data from L1D to L1E. Interpolates radiometric data to common timestamps and wavebands, optionally generates temporal plots of Li, Lt, and Es, and ancillary data to show how data were interpolated. L1E also optionally outputs text files (see \\'SeaBASS File and Header\\' below) containing the data and metadata for submission to the SeaWiFS Bio-optical Archive and Storage System (SeaBASS; https://seabass.gsfc.nasa.gov/)\\n\\nEach HyperOCR collects data at unique time intervals and requires interpolation for inter-instrument comparison. Satlantic ProSoft 7.7 software interpolates radiometric data between radiometers using the OCR with the fastest sampling rate (Sea-Bird 2017), but here we use the timestamp of the slowest-sampling radiometer (typically Lt) to minimize perterbations in interpolated data (i.e. interpolated data in HyperInSPACE are always closer in time to actual sampled data) **(Brewin et al. 2016, Vandenberg 2017)**. \\n\\nEach HyperOCR radiometer collects data in a unique set of wavebands nominally at 3.3 nm resolution. For merging, they must be interpolated to common wavebands. Interpolating to a different (i.e. lower) spectral resolution is also an option. No extrapolation is calculated (i.e. interpolation is between the global minimum and maximum spectral range for all HyperOCRs). Spectral interpolation is linear by default, but has an option for univariate spline with a smoothing factor of 3 (see ProcessL1e.interpolateL1e in ProcessL1e.py).\\n**(API: https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html)**\\n\\n*Note: only the datasets specified in ProcessL1e.py in each group will be interpolated and carried forward. For radiometers, this means that ancillary instrument data such as SPEC_TEMP and THERMAL_RESP will be dropped at L1E and beyond. See ProcessL1e.py at Perform Time Intepolation comment.*\\n\\nOptional plots of Es, Li, and Lt of L1E data can be generated which show the temporal interpolation for each parameter and each waveband to the slowest sampling radiometer timestamp. They are saved in [output_directory]/Plots/L1E. Plotting is time and memory intensive, and so may not be adviseable for processing an entire cruise.\\n\\n*{To Do: Allow provision for above water radiometers that operate simultaneously and/or in the exact same wavebands.}*\\n\\n##### SeaBASS File and Header\\n\\nTo output SeaBASS formatted text files, check the box. A SeaBASS subfolder within the L1E directory will be created, and separate files generated for Li, Lt, and Es hyperspectral data.\\n\\nAn eponymous, linked module allows the user to collect information from the data and the processing configuration (as defined in the Configuration window) into the SeaBASS files and their headers. The module is launched by selecting the \\'Edit SeaBASS Header\\' button in the Configuration window. A SeaBASS header configuration file is automatically stored in the /Config directory with the name of the Configuration and a .hdr extension. Instructions are given at the top of the SeaBASS Header window. Within the SeaBASS Header window, the left column allows the user to input the fields required by SeaBASS. Calibration files (if they have been added at the time of creation) are auto-populated. In the right hand column, the HyperInSPACE parameterizations defined in the Configurations window is shown in the \\'Config Comments\\' box, and can be editted (though this should rarely ever be necessary). Additional comments can be added in the second comments field, and the lower fields are autopopulated from each data file as it is processed. To override auto-population of the lower fields in the right column, enter the desired value here in the SeaBASS Header window.\\n\\n*{To Do: Populate the left column using values in the Ancillary file, if present.}*\\n\\n#### Level 2\\n\\nProcess L1E to L2. Further quality control filters are applied to data, and data are then averaged within optional time interval ensembles prior to calculating the remote sensing reflectance within each ensemble. A typical field collection file for the HyperSAS SolarTracker is one hour, and the optimal ensemble periods within that hour will depend on how rapidly conditions and water-types are changing, as well as the instrument sampling rate. While the use of ensembles is optional (set to 0 to avoid averaging), it is highly recommended, as it allows for the statistical analysis required for Percent Lt calculation (radiance acceptance fraction; see below) within each ensemble, rather than %Lt across an entire (e.g. one hour) collection, and it also improves radiometric uncertainty estimation.\\n\\nPrior to ensemble binning, individual spectra may be filtered out for:\\n**Lt(NIR)>Lt(UV)**\\nSpectra with Lt higher in the UV (average from 780-850) than the UV (350-400) are eliminated.\\n*{Unable to find citiation for the Lt(NIR)> Lt(UV) filter...}*\\n\\n**Maximum Wind Speed**.  \\n**Default 7 m/s (IOCCG Draft Protocols 2019; D\\'Alimonte pers.comm 2019); 10 m/s Mueller et al. 2003 (NASA Protocols); 15 m/s (Zibordi et al. 2009);**\\n\\n**Solar Zenith Angle** may be filtered for minimum and maximum values.  \\n**Default Min: 20 deg (Zhang et al 2017); Default Max: 60 deg (Brewin et al. 2016)**\\n\\n**Spectral Outlier Filter** may be applied to remove noisy data prior to binning. This simple filter examines only the spectra of Es, Li, and Lt from 400 - 700 nm, above which the data are noisy in the HyperOCRs. Using the standard deviation of the normalized spectra for the entire sample ensemble, together with a multiplier to establish an \"envelope\" of acceptable values, spectra with data outside the envelop in any band are rejected. Currently, the arbitrary filter factors are 5.0 for Es, 8.0 for Li, and 3.0 for Lt. Results of spectral filtering are saved as spectral plots in [output_directory]/Plots/L2_Spectral_Filter. The filter can be optimized by studying these plots for various parameterizations of the filter.\\n\\n**Meteorological flags** based on **(Ruddick et al. 2006, Mobley, 1999, Wernand et al. 2002, Garaba et al. 2012, Vandenberg 2017)** can be optionally applied to screen for undesirable data. Specifically, data are filtered for cloud cover, unusually low downwelling irradiance at **480 nm < default 2.0 uW cm^-2 nm^-1** for data likely to have been collected near dawn or dusk, or **(Es(470)/Es(680) < 1.0**), and for data likely to have high relative humidity or rain (**Es(720)/Es(370) < 1.095**). Cloud screening (**Li(750)/Es(750) >= 0.05**) is optional and not well parameterized. Clear skies are approximately 0.02 (Mobley 1999) and fully overcast are of order 0.3 (Ruddick et al. 2006). However, the Ruddick skyglint correction (below) can partially compensate for clear versus cloudy skies, so to avoid eliminating non-clear skies prior to glint correction, set this high (e.g. 1.0). Further investigation with automated sky photography for cloud cover is warranted.\\n\\n**Ensembles**\\n**Extract Cruise Stations** can be selected if station information is provided in the ancillary data file identified in the Main window. If selected, only data collected on station will be processed, and the output data/plot files will have the station number appended to their names. At current writing, stations must be numeric, not string-type. If this option is deselected, all automated data (underway and on station) will be included in the ensemble processing.\\n\\n**Ensemble Interval** can be set to the user\\'s requirements depending on sampling conditions and instrument rate (**default 300 sec**). Setting this to zero avoids temporal bin-averaging, preserving the common timestamps established in L1E. Processing the data without ensenble averages can be very slow, as the reflectances are calculated for each spectrum collected (i.e. nominally every 3.3 seconds of data for HyperSAS). The ensemble period is used to process the spectra within the lowest percentile of Lt(780) as defined/set below. The ensemble average spectra for Es, Li, and Lt is calculated, as well as variability in spectra within the ensemble, which is used to help estimate sample uncertainty.\\n\\n**Percent Lt Calculation** Data are optionally limited to the darkest percentile of Lt data at 780 nm within the sampling interval (if binning is performed; otherwise across the entire file) to minimize the effects of surface glitter from capillary waves. The percentile chosen is sensitive to the sampling rate. The 5% default recommended in Hooker et al. 2002 was devised for a multispectral system with rapid sampling rate.\\n**Default: 5 % (Hooker et al. 2002, Zibordi et al. 2002, Hooker and Morel 2003); <10% (IOCCG Draft Protocols)**.\\n\\n**Skyglint/Sunglint Correction (rho)**\\nUse of the Ruddick et al. 2006 and the Zhang et al. 2017 glint corrections require wind data, and Zhang (2017) also requires aerosol optical depth, salinity, and sea surface temperature. Since most field collections of above water radiometry are missing some or all of these anchillary parameters, an embedded function allows the user to download model data from the NASA EARTHDATA server. These data are generated by the NASA Global Modeling and Assimilation Office (GMAO) as hourly, global \\'MERRA2\\' HDF files at 0.5 deg (latitude) by 0.625 deg (longitude) resolution (https://gmao.gsfc.nasa.gov/reanalysis/MERRA-2/). Two files will be downloaded for each hour of data processed (total ~8.3 MB for one hour of field data) and stored in /Data/Anc. Global ancillary data files from GMAO will be reused, so it is not recommended to clear this directory unless updated models are being released by GMAO. Details for how these data are applied to above water radiometry are given below.\\n\\nAs of January 2020, access to these data requires a user login and password, which can be obtained here (https://oceancolor.gsfc.nasa.gov/registration). A link to register is also provided in the Configuration window. When the user selects \\'Download Ancillary Models\\', pop-up windows will allow the user to enter a login and password. Once this has been done once, canceling the login pop-up dialog will force the program to use the current configuration (i.e. it is only necessary to re-enter the password if it has changed.)\\n\\nThe default value for sea-surface reflectance (**Rho_sky**, sometimes called the Fresnel factor) is set by default to 0.0256 based on **(Mobley 1999, Mueller et al. 2003 (NASA Protocols))**, which can be optionally adjusted for wind speed and cloud cover using the relationship found in **(Ruddick et al. 2006)** (i.e. Li(750)/Es(750)< 0.05 for clear skies, Rho_sky = 0.0256 + 0.00039* U + 0.000034* U^2, else Rho_sky = 0.0256). The default wind speed (U) should be set by the user depending on in situ conditions, for instances when the ancillary data and models are not available (more information is given below on how ancillary wind data are applied). This correction does not account for the spectral dependence **(Lee et al. 2010, Gilerson et al. 2018)** or polarization sensitivity **(Harmel et al. 2012, Mobley 2015, Hieronymi 2016, D\\'Alimonte and Kajiyama 2016, Foster and Gilerson 2016, Gilerson et al. 2018)** in Rho_sky. Uncertainty in rho is estimated from Ruddick et al. 2006 at +/- 0.003.\\n\\nThe third option provided for glint correction is based on **Zhang et al. 2017**. This model explicitly accounts for spectral dependence in rho, separates the glint contribution from the sky and the sun, and accounts for polarization in the skylight term. This approach requires knowledge of environmental conditions during sampling including: wind speed, aerosol optical depth, solar and sensor azimuth and zenith angles, water temperature and salinity. To accomodate these parameters, HyperInSPACE uses either the ancillary data file provided in the main window, GMAO models, or the default values set in the Configuration window as follows: field data ancillary files are screened for wind, water temperature, and salinity. These are each associated with the nearest timestamps of the radiometer suite to within one hour. Radiometer timestamps still lacking wind and aerosol data will extract it from the GMAO models, if available. Otherwise, the default values set in the Configuration window will be used as a last resort.\\n\\n*{To Do: Work to optimize the processing of Zhang rho. Currently, this proceeds far more slowly in Python than Matlab. See notes in ZhangRho.py for gen_vec_quad and prob_reflection near L165}*\\n*{To Do: Include additional skylight/skyglint corrections, such as Groetsch et al. 2017/2020}*\\n*{To Do: Include a bidirectional correction to Lw based on, e.g. Lee 2011, Zibordi 2009 (for starters; ultimately the BRDF will need to be better parameterized for all conditions and water types.)}*\\n*{To Do: Improve uncertainty estimates (e.g. Zibordi et al. 2009). The uncertainty in rho within the Zhang model is not well constrained.}*\\n\\nRemote sensing reflectance is calculated as Rrs = (Lt - rho_sky* Li) / Es (e.g. **(Mobley 1999, Mueller et al. 2003, Ruddick et al. 2006)**). Normalized water leaving radiance (nLw) is calculated as Rrs*F0, where F0 is the top of atmosphere incident radiation adjusted for the Earth-Sun distance on the day sampled.\\n\\nUncertainties in Li, Lt, and Es are estimated using the standard deviation of spectra in the ensemble or full-file average. Uncertainty in rho is estimated at +/- 0.003 from Ruddick et al. 2006. Uncertainty in Rrs and nLw are estimated using propagation of uncertainties from Li, Lt, Es, and rho assuming random, uncorrelated error.\\n\\nAdditional glint may be removed from the Rrs and nLw by subtracting the value in the NIR from the entire spectrum **(Mueller et al. 2003 (NASA Protocols))**. This approach, however, assumes neglible water-leaving radiance in the 750-800 nm range (not true of turbid waters), and ignores the spectral dependence in sky glint, and **should therefore only be used in the clearest waters and with caution**. Here, a minimum in Rrs(750-800) or nLw(750-800) is found and subtracted from the entire spectrum.\\n\\nAn alternate NIR residual correction can be applied based on **Ruddick et al. 2005, Ruddick et al. 2006**. This utilizes the spectral shape in water leaving reflectances in the NIR to estimate the residual glint correction for turbid waters with NIR reflectances from about 0.0001 to 0.03\\n\\nNegative reflectances can be removed as follows: any spectrum with any negative reflectances between 380 nm and 700 nm is removed from the record entirely. Negative reflectances outside of this range (e.g. noisy data deeper in the NIR) are set to 0.\\n\\nSpectral wavebands for a few satellite ocean color sensors can be optionally calculated using their spectral weighting functions. These will be included with the hyperspectral output in the L2 HDF files. Spectral response functions are applied to convolve the (ir)radiances prior to calculating reflectances. **(Burgghoff et al. 2020)**.\\n\\nPlots of processed L2 data from each radiometer and calculated reflectances can be created and stored in [output_directory]/Plots/L2. Uncertainties are shown for each spectrum as shaded regions, and satellite bands (if selected) are superimposed on the hyperspectral data.\\n\\nSelect the \"Derived L2 Ocean Color Products\" button to choose, calculate, and plot derived biochemical and inherent optical properties using a variety of ocean color algorithms. Algorithms largely mirror those available in SeaDAS with a few additions. They include OC3M, PIC, POC, Kd490, iPAR, GIOP, QAA, and the Average Visible Wavelength (Vandermuellen et al. 2020) and GOCAD-based CDOM/Sg/DOC algorithms (Aurin et al. 2018), as well as the Rrs spectral QA score (Wei et al 2016).\\n\\nTo output SeaBASS formatted text files, check the box. A subfolder within the L2 directory will be created, and separate text files will be made for Li, Lt, Es, and Rrs hyperspectral data and satellite bands, if selected. Set-up for the SeaBASS header is managed with the \\'Edit/Update SeaBASS Header\\' in the L1E configuration.\\n\\n\\n**PDF Reports**\\n\\nUpon completion of L2 processing for each file (or lower level if that is the terminal processing level), a PDF summary report will be produced and saved in [output_directory]/Reports. This contains metadata, processing parameters, processing logs, and plots of QA analysis, radiometry, and derived ocean color products. These reports should be used to evaluate the choices made in the configuration and adjust them if necessary.\\n\\n\\n## References\\n- Abe, N., B. Zadrozny and J. Langford (2006). Outlier detection by active learning. Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. Philadelphia, PA, USA, Association for Computing Machinery: 504–509.\\n- Brewin, R. J. W., G. Dall\\'Olmo, S. Pardo, V. van Dongen-Vogels and E. S. Boss (2016). \"Underway spectrophotometry along the Atlantic Meridional Transect reveals high performance in satellite chlorophyll retrievals.\" Remote Sensing of Environment 183: 82-97.\\n- Burggraaff, O. (2020). \"Biases from incorrect reflectance convolution.\" Optics Express 28(9): 13801-13816.\\n- Chandola, V., A. Banerjee and V. Kumar (2009). \"Anomaly detection: A survey.\" ACM Comput. Surv. 41(3): Article 15.\\n- D’Alimonte, D. and T. Kajiyama (2016). \"Effects of light polarization and waves slope statistics on the reflectance factor of the sea surface.\" Optics Express 24(8): 7922-7942.\\n- Foster, R. and A. Gilerson (2016). \"Polarized transfer functions of the ocean surface for above-surface determination of the vector submarine light field.\" Applied Optics 55(33): 9476-9494.\\n- Garaba, S. P., J. Schulz, M. R. Wernand and O. Zielinski (2012). \"Sunglint Detection for Unmanned and Automated Platforms.\" Sensors 12(9): 12545.\\n- Gilerson, A., C. Carrizo, R. Foster and T. Harmel (2018). \"Variability of the reflectance coefficient of skylight from the ocean surface and its implications to ocean color.\" Optics Express 26(8): 9615-9633.\\n- Harmel, T., A. Gilerson, A. Tonizzo, J. Chowdhary, A. Weidemann, R. Arnone and S. Ahmed (2012). \"Polarization impacts on the water-leaving radiance retrieval from above-water radiometric measurements.\" Applied Optics 51(35): 8324-8340.\\n- Hieronymi, M. (2016). \"Polarized reflectance and transmittance distribution functions of the ocean surface.\" Optics Express 24(14): A1045-A1068.\\n- Hooker, S. B., G. Lazin, G. Zibordi and S. McLean (2002). \"An Evaluation of Above- and In-Water Methods for Determining Water-Leaving Radiances.\" Journal of Atmospheric and Oceanic Technology 19(4): 486-515.\\n- Hooker, S. B. and A. Morel (2003). \"Platform and Environmental Effects on Above-Water Determinations of Water-Leaving Radiances.\" Journal of Atmospheric and Oceanic Technology 20(1): 187-205.\\n- Lee, Z., Y.-H. Ahn, C. Mobley and R. Arnone (2010). \"Removal of surface-reflected light for the measurement of remote-sensing reflectance from an above-surface platform.\" Optics Express 18(25): 26313-26324.\\n- Mobley, C. D. (1999). \"Estimation of the remote-sensing reflectance from above-surface measurements.\" Applied Optics 38(36): 7442-7455.\\n- Mobley, C. D. (2015). \"Polarized reflectance and transmittance properties of windblown sea surfaces.\" Applied Optics 54(15): 4828-4849.\\n- Mueller, J. L., A. Morel, R. Frouin, C. O. Davis, R. Arnone, K. L. Carder, Z. P. Lee, R. G. Steward, S. B. Hooker, C. D. Mobley, S. McLean, B. Holbert, M. Miller, C. Pietras, K. D. Knobelspiesse, G. S. Fargion, J. Porter and K. J. Voss (2003). Ocean Optics Protocols for Satellite Ocean Color Sensor Validation, Revision 4, Volume III. Ocean Optics Protocols for Satellite Ocean Color Sensor Validation. J. L. Mueller. Greenbelt, MD, NASA Goddard Space Flight Center.\\n- Ruddick, K., V. De Cauwer and B. Van Mol (2005). Use of the near infrared similarity reflectance spectrum for the quality control of remote sensing data, SPIE.\\n- Ruddick, K. G., V. De Cauwer, Y.-J. Park and G. Moore (2006). \"Seaborne measurements of near infrared water-leaving reflectance: The similarity spectrum for turbid waters.\" Limnology and Oceanography 51(2): 1167-1179.\\n- Vandenberg, N., M. Costa, Y. Coady and T. Agbaje (2017). PySciDON: A python scientific framework for development of ocean network applications. 2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM).\\n- Wernand, M. R. (2002). GUIDELINES FOR (SHIP BORNE) AUTO-MONITORING\\nOF COASTAL AND OCEAN COLOR. Ocean Optics XVI. S. Ackleson and C. Trees. Santa Fe, NM, USA.\\n- Zhang, X., S. He, A. Shabani, P.-W. Zhai and K. Du (2017). \"Spectral sea surface reflectance of skylight.\" Optics Express 25(4): A1-A13.\\n- Zibordi, G., S. B. Hooker, J. F. Berthon and D. D\\'Alimonte (2002). \"Autonomous Above-Water Radiance Measurements from an Offshore Platform: A Field Assessment Experiment.\" Journal of Atmospheric and Oceanic Technology 19(5): 808-819.\\n- Zibordi, G., F. Mélin, J.-F. Berthon, B. Holben, I. Slutsker, D. Giles, D. D’Alimonte, D. Vandemark, H. Feng, G. Schuster, B. E. Fabbri, S. Kaitala and J. Seppälä (2009). \"AERONET-OC: A Network for the Validation of Ocean Color Primary Products.\" Journal of Atmospheric and Oceanic Technology 26(8): 1634-1651.\\n- Zibordi, G., K. J. Voss, B. Johnson and J. L. Mueller (2019). Protocols for Satellite Ocean Colour Data Validation: In Situ Optical Radiometry. IOCCG Ocean Optics and Biogeochemistry Protocols for Satellite Ocean Colour Sensor Validation. IOCCG. Dartmouth, NS, Canada, IOCCG.\\n'},\n",
       " {'repo': 'nasa/ominas',\n",
       "  'language': 'IDL',\n",
       "  'readme_contents': '## OMINAS:  Open-source Multiple INstrument Analysis Software\\n\\nSee the full OMINAS documentation here:  https://nasa.github.io/ominas/guides/userguide.html\\n\\n\\n###Current release: v1.0.5\\n\\n\\nhttps://github.com/nasa/ominas/releases/latest\\n'},\n",
       " {'repo': 'nasa/JHU-PIV-data',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': '# JHU-PIV-data\\n\\n## REMEMBER: Pull Remote Repo changes frequently using this command:\\n```bash\\ngit pull\\n```\\n\\n## Upload changes to this repo\\n\\n1. Change directories in your Command Prompt or Terminal application\\n```bash\\ncd \\\\Users\\\\blucci\\\\Desktop\\\\piv\\n```\\n2. See changed files (they should show up as red)\\n```bash\\ngit status\\n```\\n3. Add all locally changed files to the staging area\\n```bash\\ngit add *\\n```\\n4. Commit this set of file changes with a message describing the changes made\\n```bash\\ngit commit -m \"<a description of the changes>\"\\n```\\n5. Push the new commits to your remote repository\\n```bash\\ngit push\\n```\\n'},\n",
       " {'repo': 'nasa/sample_lib',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/sample_lib/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/sample_lib/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : App : Sample Lib\\n\\nThis repository contains a sample library (sample_lib), which is a framework component of the Core Flight System.\\n\\nThis sample library is a non-flight example library implementation for the cFS Bundle. It is intended to be located in the `apps/sample_lib` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes sample_lib as a submodule), which includes build and execution instructions.\\n\\nsample_lib implements SAMPLE_Function, as an example for how to build and link a library in cFS.\\n\\n## Version History\\n\\n### Development Build: v1.2.0-rc1+dev34\\n\\n- Replace direct ref to ArgPtr with `UT_Hook_GetArgValueByName` macro. Reading the pointer directly is not advised.\\n- See <https://github.com/nasa/sample_lib/pull/61> and <https://github.com/nasa/cFS/pull/250>\\n\\n### Development Build: v1.2.0-rc1+dev30\\n\\n- Replace <> with \" for local includes\\n- Adds CONTRIBUTING.md that links to the main cFS contributing guide.\\n- See <https://github.com/nasa/sample_lib/pull/55>\\n\\n### Development Build: v1.2.0-rc1+dev24\\n\\n- Fix #46, simplify build to use wrappers and interface libs\\n- Fix #48, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/sample_lib/pull/50>\\n\\n### Development Build: v1.2.0-rc1+dev10\\n\\n- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing\\n- See <https://github.com/nasa/sample_lib/pull/38>\\n\\n### Development Build: v1.2.0-rc1+dev8\\n\\n- No behavior changes. All identifiers now use the prefix `SAMPLE_LIB_`. Changes the name of the init function from SAMPLE_LibInit to SAMPLE_LIB_Init which affects the CFE startup script.\\n- Set REVISION to \"99\" to indicate development version status\\n- See <https://github.com/nasa/sample_lib/pull/35>\\n\\n### Development Build: v1.2.0-rc1+dev3\\n\\n- Installs unit test to target directory.\\n- See <https://github.com/nasa/sample_lib/pull/32>\\n\\n### Development Build: 1.1.0+dev27\\n\\n- Install unit test as part of cmake recipe. Sample lib test runner now shows up in expected install directory\\n- Add build number and baseline to version reporting\\n- See <https://github.com/nasa/sample_lib/pull/28>\\n\\n### Development Build: 1.1.4\\n\\n- Apply code style\\n- See <https://github.com/nasa/sample_lib/pull/24>\\n\\n### Development Build: 1.1.3\\n\\n- Coverage data `make lcov` includes the sample_lib code\\n- See <https://github.com/nasa/sample_lib/pull/22>\\n\\n### Development Build: 1.1.2\\n\\n- Added coverage test and a stub library to facilitate unit test\\n- See <https://github.com/nasa/sample_lib/pull/16>\\n\\n### Development Build: 1.1.1\\n\\n- See <https://github.com/nasa/sample_lib/pull/14>\\n\\n### ***OFFICIAL RELEASE: 1.1.0 - Aquila***\\n\\n- Released as part of cFE 6.7.0, Apache 2.0\\n- See <https://github.com/nasa/sample_lib/pull/6>\\n\\n### ***OFFICIAL RELEASE: 1.0.0a***\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nAs a lab library, extensive testing is not performed prior to release and only minimal functionality is included.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n'},\n",
       " {'repo': 'nasa/CFL3D',\n",
       "  'language': 'Fortran',\n",
       "  'readme_contents': '## CFL3D\\n\\nCFL3D \\nis a structured-grid, cell-centered, upwind-biased, Reynolds-averaged Navier-Stokes (RANS) code. It can be run\\nin parallel on multiple grid zones with point-matched, patched, overset, or embedded connectivities. Both\\nmultigrid and mesh sequencing are available in time-accurate or steady-state modes.\\n\\nThe most up-to-date information can be found on the web at:\\n\\nhttps://cfl3d.larc.nasa.gov\\n\\n-------------\\n\\nCopyright 2001 United States Government as represented by the Administrator\\nof the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nThe CFL3D platform is licensed under the Apache License, Version 2.0 \\n(the \"License\"); you may not use this file except in compliance with the \\nLicense. You may obtain a copy of the License at \\nhttp://www.apache.org/licenses/LICENSE-2.0. \\n\\nUnless required by applicable law or agreed to in writing, software \\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT \\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \\nLicense for the specific language governing permissions and limitations \\nunder the License.\\n'},\n",
       " {'repo': 'nasa/edsc-timeline',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Earthdata Search Components: Timeline\\n\\n[![npm version](https://badge.fury.io/js/%40edsc%2Ftimeline.svg)](https://badge.fury.io/js/%40edsc%2Ftimeline)\\n![Build Status](https://github.com/nasa/edsc-timeline/workflows/CI/badge.svg?branch=master)\\n[![codecov](https://codecov.io/gh/nasa/edsc-timeline/branch/master/graph/badge.svg?token=KQNTU9DTFD)](https://codecov.io/gh/nasa/edsc-timeline)\\n\\nTry out the [online demo](http://nasa.github.io/edsc-timeline/)\\n\\nA React plugin implementing a timeline view of data, allowing\\ntime range selection as well as keyboard and touch interaction\\nFor a basic usage example and a testbed for changes,\\nsee `example/src`\\n\\nThe edsc-timeline plugin was developed as a component of\\n[Earthdata Search](https://github.com/nasa/earthdata-search).\\n\\n## Installation\\n\\n    npm install @edsc/timeline\\n\\n## Usage\\n\\nAfter installing you can use the component in your code.\\n\\n```javascript\\nimport EDSCTimeline from \\'@edsc/timeline\\'\\n\\nconst Component = () => {\\n  const data = [\\n    {\\n      id: \\'row1\\',\\n      title: \\'Example title\\',\\n      intervals: [\\n        [\\n          new Date(\\'2019-08-12\\').getTime(), // Start of the interval\\n          new Date(\\'2019-12-20\\').getTime() // End of the interval\\n        ],\\n        [\\n          new Date(\\'2020-01-04\\').getTime(),\\n          new Date(\\'2020-05-20\\').getTime()\\n        ]\\n      ]\\n    }\\n  ]\\n\\n  return (\\n    <EDSCTimeline\\n      data={data}\\n    />\\n  )\\n}\\n```\\n\\n### Props\\n\\n| Prop | Type | Required | Default Value | Description\\n| ---- |:----:|:--------:|:-------------:| -----------\\ndata | array | true | | Array of rows to be displayed on the timeline\\ncenter | number | | new Date().getTime() | Center timestamp of the timeline\\nminZoom | number | | 1 | Minimum zoom level\\nmaxZoom | number | | 5 | Maximum zoom level\\nzoom | number | | 3 | Active zoom level\\ntemporalRange | object | | {} | Temporal range ({ start, end }) that is displayed on the timeline\\nfocusedInterval | object | | {} | Focused interval ({ start, end }) that is displayed on the timeline\\nonFocusedSet | function | | | Callback function that returns the focused interval when it is set\\nonTemporalSet | function | | | Callback function that returns the temporal range when it is set\\nonTimelineMove | function | | | Callback function called when the timeline is moved\\nonTimelineMoveEnd | function | | | Callback function called when the timeline is finished moving\\nonArrowKeyPan | function | | | Callback function called when arrow keys are used to change the focused interval\\nonButtonPan | function | | | Callback function called when buttons are used to change the focused interval\\nonButtonZoom | function | | | Callback function called when buttons are used to change the zoom level\\nonDragPan | function | | | Callback function called when the timeline is panned using dragging\\nonFocusedIntervalClick | function | | | Callback function called when a focused interval is clicked\\nonScrollPan | function | | | Callback function called when the mouse wheel is used to pan the timeline\\nonScrollZoom | function | | | Callback function called when the mouse wheel is used to change the zoom level\\n\\n### Callback function return value\\n\\nEvery callback function returns this object\\n\\n```javascript\\n{\\n  center,\\n  focusedEnd,\\n  focusedStart,\\n  temporalEnd,\\n  temporalStart,\\n  timelineEnd,\\n  timelineStart,\\n  zoom\\n}\\n```\\n\\n## Development\\n\\nTo compile:\\n\\n    npm install\\n\\nTo start the example project for local testing:\\n\\n    npm start\\n\\nTo run the Jest tests:\\n\\n    npm test\\n\\nTo run the Cypress tests:\\n\\n    npm run cypress:run\\n\\n## Contributing\\n\\nSee CONTRIBUTING.md\\n\\n## License\\n\\n> Copyright © 2007-2014 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n>\\n> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\n> You may obtain a copy of the License at\\n>\\n>    http://www.apache.org/licenses/LICENSE-2.0\\n>\\n>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\n>WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n'},\n",
       " {'repo': 'nasa/ipv6_python',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Advanced IPv6 Socket Manipulation for Python\\n\\nDescription:\\n\\nThis rather simplistic extension module is intended to allow for more advanced\\nmanipulation of IPv6 sockets in Python.  In particular, Python did not have an\\neasy means to interact with and obtain the flow label of a particular IPv6\\nsocket.  This extension currently allows for flow labels to be enabled on a\\nsocket and a random flow label can be requested from the kernel.  In the future,\\nadditional options may be added to facilitate additional flow label actions.\\n\\nA collection of tools are also included which can be used to generate test\\ntraffic under strict patterns, similar to how many telemetry systems operate.\\nSee the documentation under tools for more details.\\n\\nInstallation:\\n\\nThis package is installed using distutils.  The most common approach is to run:\\n\\n  python setup.py install\\n    or\\n  python3 setup.py install\\n\\nUsage:\\n\\n  import ipv6\\n\\nYou can then pass an IPv6 socket to the function get_flow_label() to\\napply a random flow label assigned by the kernel.  The first parameter\\nis the socket object or the integer file descriptor.  The remaining\\nparameters are optional depending on the state of your socket.  Here is\\nan example:\\n\\n  sockaddr = ipv6.get_flow_label(sock,*sockaddr)\\n\\nSee https://docs.python.org/2/library/socket.html for info on sockaddr\\n\\nThe get_flow_label call returns a new sockaddr structure with the flowinfo\\nfield set to newly created flow label assigned by the kernel.  You can then\\nsend data as normal.\\n\\nLicense:\\n\\nThis code is released under the NASA Open Source Agreement version 1.3\\n\\nSimple Example:\\n\\n# Send a single UDP packet\\nimport socket\\nimport ipv6\\nhost = \"::1\"\\nport = 3300\\ndata = \"NASA\"\\nresolve = socket.getaddrinfo(host, port, socket.AF_INET6, socket.SOCK_DGRAM)\\n(family, socktype, proto, _, sockaddr) = resolve[0]\\nsock = socket.socket(family, socktype, proto)\\nsockaddr = ipv6.get_flow_label(sock,*sockaddr)\\nprint \"Flow Label:\",hex(sockaddr[2])\\nsock.sendto(data,sockaddr)\\n\\n'},\n",
       " {'repo': 'nasa/cFS-GroundSystem',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"![Static Analysis](https://github.com/nasa/cFS-GroundSystem/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/cFS-GroundSystem/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : Tool : Ground System\\n\\nThis repository contains NASA's Lab Ground System (cFS-GroundSystem), which is a framework component of the Core Flight System.\\n\\nThis lab application is a non-flight utility ground system to interact with the cFS. It is intended to be located in the `tools/cFS-GroundSystem` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes cFS-GroundSystem as a submodule), which includes build and execution instructions.\\n\\nSee [Guide-GroundSystem.md](https://github.com/nasa/cFS-GroundSystem/blob/master/Guide-GroundSystem.md) for more information.\\n\\n## Version History\\n\\n### Development Build: v2.2.0-rc1+dev46\\n\\n- Changes executable command from 'startg' to 'cFS-GroundSystem'\\n- Changes version to be the version stated in version.py\\n- Adds executable installation instructions to Guide-GroundSystem.md\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/178> and <https://github.com/nasa/cFS/pull/248>\\n\\n### Development Build: v2.2.0-rc1+dev41\\n\\n- Corrects values in sb and tbl hk-tlm.txt to allow the TBL and SB tlm pages to open.\\n- Adds a contributing guide that links to the main cFS contributing guide.\\n- See <https://github.com/nasa/cfs-groundsystem/pull/171>\\n\\n### Development Build: v2.2.0-rc1+dev33\\n\\n- Fix #163, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/cfs-groundsystem/pull/167>\\n\\n### Development Build: v2.2.0-rc1+dev18\\n\\n- Documentation: Add `Security.md` with instructions to report vulnerabilities\\n- **Breaking change**, CmdUtil, Rounds header up to match <https://github.com/nasa/cFE/pull/1077>\\n- **Breaking change**, GUI, Rounds header up to match <https://github.com/nasa/cFE/pull/1077>\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/150>\\n\\n### Development Build: v2.2.0-rc1+dev11\\n\\n- Updated CHeaderParser.py to address specific issues.\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/135>\\n\\n### Development Build: v2.2.0-rc1+dev8\\n\\n- Replaces old code that caused a cast-align warning when strict. Refactored and removed unnecessary code while also following recommended model for getaddrinfo. Removed old windows support/defines/etc (likely not tested for years, no longer supported).\\n- Reduce the size of the strncpy so that it ensures there's a null byte at the end of the string buffer.\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/133>\\n\\n### Development Build: v2.2.0+dev2\\n\\n - Fixes multiple typos\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/127>\\n\\n### Development Build: v2.1.0+dev85\\n\\n- Remove unused code/packages to fix LGTM warnings\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/120>\\n\\n### Development Build: v2.1.0+dev76\\n\\n- Fixes more lgtm warnings\\n- Allows users to change the byte offsets for sending commands and parsing telemetry, to support different header versions or other implementations of cFS\\n- Adds a file to store version information and reports version upon ground-system startup.\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/109>\\n\\n### Development Build: 2.1.12\\n\\n- Change all individual UI elements to table widgets. Update backend code accordingly\\n- Temporary fix for implicit declaration of endian functions on some systems (RH/CentOs). No build errors on CentOS\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/107>\\n\\n### Development Build: 2.1.11\\n\\n- Default behavior is the same except adds checksum and doesn't actually require fields. Adds all the packet fields, overrides, more supported data types, etc.\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/101>\\n\\n### Development Build: 2.1.10\\n\\n- Change documentation for table loading guide to markdown\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/94>\\n\\n### Development Build: 2.1.9\\n\\n- Upgrading PyQt4 to PyQt5 and includes a lot of cleanup/refactoring, and changes to the GUI itself\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/90>\\n\\n### Development Build: 2.1.8\\n\\n- No warnings when building with GCC9\\n- Event messages now display both Event type and ID.\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/86>\\n\\n### Development Build: 2.1.7\\n\\n- Commands and Telemetry definitions now match code\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/74>\\n\\n### Development Build: 2.1.6\\n\\n- Cmd code (and checksum) are always in the same place\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/69>\\n\\n### Development Build: 2.1.5\\n\\n- Updated build instructions for Python 3\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/64>\\n\\n### Development Build: 2.1.4\\n\\n- Finish conversion to python 3\\n- cmdutil now accepts --word as alias to --long\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/54>\\n\\n### Development Build: 2.1.3\\n\\n- Minor updates to work with python 3\\n- No longer compatible with python 2.7\\n- Note issue #50 is to update the related documentation\\n- See <https://github.com/nasa/cFS-GroundSystem/pull/47>\\n\\n### Development Build: 2.1.2\\n\\n- Minor updates (see <https://github.com/nasa/cFS-GroundSystem/pull/39>)\\n\\n### Development Build: 2.1.1\\n\\n- Minor updates (see <https://github.com/nasa/cFS-GroundSystem/pull/36>)\\n\\n### **_OFFICIAL RELEASE 2.1.0 - Aquila_**\\n\\n- Minor updates (see <https://github.com/nasa/cFS-GroundSystem/pull/26>)\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### **_OFFICIAL RELEASE 2.0.90a_**\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nAs a lab application, extensive testing is not performed prior to release and only minimal functionality is included.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n\"},\n",
       " {'repo': 'nasa/prog_algs',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Prognostics Algorithm Python Package\\n\\nThe Prognostic Algorithm Package is a python framework for model-based prognostics (computation of remaining useful life) of engineering systems, and provides a set of algorithms for state estimation and prediction, including uncertainty propagation. The algorithms take as inputs prognostic models (from NASA\\'s Prognostics Model Package), and perform estimation and prediction functions. The library allows the rapid development of prognostics solutions for given models of components and systems. Different algorithms can be easily swapped to do comparative studies and evaluations of different algorithms to select the best for the application at hand.\\n\\n## Installation\\n`pip3 install prog_algs`\\n\\n## Documentation\\nSee documentation [here](https://nasa.github.io/prog_algs/)\\n\\n## Repository Directory Structure \\n\\n`prog_algs/` - The prognostics algorithm python package<br />\\n&nbsp;&nbsp; |- `predictors/` - Algorithms for performing the prediction step of model-based prognostics<br />\\n&nbsp;&nbsp; |- `samplers/` - Standard tools for performing state sampling<br />\\n&nbsp;&nbsp; |- `state_estimators/` - Algorithms for performing the state estimation step of model-based prognostics<br />\\n`benchmarking_example` - An example using metrics for benchmarking<br />\\n`example.py` - An example python script using prog_algs<br />\\n`README.md` - The readme (this file)<br />\\n`requirements.txt` - python library dependiencies required to be met to use this package. Install using `pip install -r requirements.txt`<br />\\n`tutorial.ipynb` - Tutorial (Juypter Notebook)\\n\\n## Citing this repository\\nUse the following to cite this repository:\\n\\n```\\n@misc{2020_nasa_prog_algs,\\n    author    = {Christopher Teubert and Matteo Corbetta and Chetan Kulkarni},\\n    title     = {Prognostics Algorithm Python Package},\\n    month     = Apr,\\n    year      = 2021,\\n    version   = {0.3.0},\\n    url       = {https://github.com/nasa/prog_algs}\\n    }\\n```\\n\\nThe corresponding reference should look like this:\\n\\nC. Teubert, M. Corbetta, C. Kulkarni, Prognostics Algorithm Python Package, v0.3.0, Apr. 2021. URL https://github.com/nasa/prog_algs.\\n\\n## Acknowledgements\\nThe structure and algorithms of this package are strongly inspired by the [MATLAB Prognostics Algorithm Library](https://github.com/nasa/PrognosticsAlgorithmLibrary) and the [MATLAB Prognostics Metrics Library](https://github.com/nasa/PrognosticsMetricsLibrary). We would like to recognize Matthew Daigle, Shankar Sankararaman and the rest of the team that contributed to the Prognostics Model Library for the contributions their work on the MATLAB library made to the design of prog_algs\\n\\n## Notices\\n\\nCopyright © 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\n## Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/exoscene',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# exoscene\\n\\nInstallation: `pip install exoscene`\\n\\n**exoscene** is a library of classes and utility functions for simulating\\ndirect images of exoplanetary systems. The package was developed by Neil\\nZimmerman (NASA/GSFC), with source code contributions from Maxime Rizzo,\\nChristopher Stark, and Ell Bogat. This work was funded in part by a WFIRST/Roman Science\\nInvestigation Team contract (PI: Margaret Turnbull). \\n\\n**exoscene** makes significant use of the Astropy, NumPy, SciPy, and \\nScikit-image packages.\\n\\nA jupyter notebook providing usage examples for much of the functionality is included under the docs subdirectory:\\n[exoscene/docs/notebooks/Roman-CGI_scene_demo.ipynb](exoscene/docs/notebooks/Roman-CGI_scene_demo.ipynb)\\n\\nThe functions are organized in 3 modules: [exoscene/planet.py](exoscene/planet.py), \\n[exoscene/star.py](exoscene/star.py), and [exoscene/image.py](exoscene/image.py).\\n\\n## 1. [exoscene/planet.py](exoscene/planet.py)\\n\\n* a Planet() class with a data structure for containing the basic physical \\nparameters of a planet, its orbit, its host star, and associated methods for\\ncomputing its relative astrometry ephemeris, its phase function, and flux ratio.\\n\\n* A function for modeling the orbital position and the Lambert sphere phase function,\\nbased on the Keplerian orbital elements and date of observation.\\n\\n* A function for mapping the time-dependent sky-projected position and \\nLambert phase factor.\\n\\n## 2. [exoscene/star.py](exoscene/star.py)\\n\\n* Functions for computing the band-integrated irradiance of a star based on its \\napparent magnitude and spectral type, and instrument bandpass, using the built-in \\nBruzual-Persson-Gunn-Stryker (BPGS) Spectral Atlas (under \\n[exoscene/data/bpgs/](exoscene/data/bpgs/))\\n\\n* A function for computing the approximate parallax and proper motion offset for \\na star, based on the celestial coordinates and observing dates.\\n\\n## 3. [exoscene/image.py](exoscene/image.py)\\n\\n* A function for accurately resampling an image model array to a detector array.\\n\\n* Functions for translating a coronagraph PSF model to an arbitrary field point,\\ntaking into account position-dependent properties included in the model.\\n\\n* Functions for applying a noise model to a detector intensity map, to simulate\\nan image with photon counting noise, read noise, and dark current, for a given \\nintegration time.\\n\\nCopyright © 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Other Rights Reserved.\\n'},\n",
       " {'repo': 'nasa/MFISPy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# MFISPy\\nMultifidelity Importance Sampling with Python\\n\\n\\n\\n## License\\n\\nNotices:\\nCopyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/TrickHLA',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# TrickHLA is an IEEE 1516 High Level Architecture (HLA) Simulation Interoperability Standard Middleware for the Trick Simulation Environment\\n\\n## Brief Abstract:\\n\\nThe TrickHLA software supports the IEEE-1516 High Level Architecture (HLA) simulation interoperability standard for the [Trick Simulation Environment](https://github.com/nasa/trick/). The TrickHLA software abstracts away the details of using HLA, allowing the user to concentrate on the simulation and not worry about having to be an HLA distributed simulation expert. The TrickHLA software is data driven and provides a simple Application Programming Interface (API) making it relatively easy to take an existing Trick simulation and make it a HLA distributed simulation.\\n\\n## Copyright:\\nCopyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\n## Disclaimers:\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n## Responsible Organization:\\nSimulation and Graphics Branch, Mail Code ER7  \\nSoftware, Robotics & Simulation Division  \\nNASA, Johnson Space Center  \\n2101 NASA Parkway, Houston, TX  77058  \\n\\n---\\n\\nTrickHLA is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/TrickHLA/blob/master/LICENSE.txt).\\n\\n'},\n",
       " {'repo': 'nasa/dorado-sensitivity',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# Dorado sensitivity and exposure time calculator\\n\\nDorado is a proposed space mission for ultraviolet follow-up of gravitational\\nwave events. This repository contains a simple sensitivity and exposure time\\ncalculator for Dorado.\\n\\nThis package can estimate the signal to noise, exposure time, or limiting\\nmagnitude for an astronomical source with a given spectrum using the [CCD\\nsignal to noise equation]. It models the following noise contributions:\\n\\n*   Zodiacal light\\n*   Airglow (geocoronal emission)\\n*   Standard CCD noise (shot noise, read noise, dark current)\\n\\n## Installation\\n\\nTo install with [Pip]:\\n\\n    $ pip install dorado-sensitivity\\n\\n## Examples\\n\\nFor examples, see the [Jupyter notebook].\\n\\n## Dependencies\\n\\n*   [Astropy]\\n*   [Synphot] for combining bandpasses and source spectra\\n*   [PyYAML] for reading [ECSV] data files\\n\\n[CCD signal to noise equation]: https://hst-docs.stsci.edu/stisihb/chapter-6-exposure-time-calculations/6-4-computing-exposure-times\\n[Pip]: https://pip.pypa.io\\n[Astropy]: https://www.astropy.org\\n[Synphot]: https://synphot.readthedocs.io/\\n[PyYAML]: https://pyyaml.org/\\n[ECSV]: https://github.com/astropy/astropy-APEs/blob/master/APE6.rst\\n[Jupyter notebook]: https://github.com/nasa/dorado-sensitivity/blob/master/example.ipynb\\n'},\n",
       " {'repo': 'nasa/Mixed-Reality-Exploration-Toolkit',\n",
       "  'language': 'C#',\n",
       "  'readme_contents': '<p align=\"center\"><img src=https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/blob/master/Unity/Assets/Media/Images/MRET-Banner.png width=250></p>\\r\\n\\r\\n# Mixed Reality Exploration Toolkit (MRET)\\r\\n\\r\\nThe Mixed Reality Exploration Toolkit (MRET) is an XR toolkit designed for rapidly building environments for NASA domain problems in science, engineering and exploration. It works with CAD-derived models and assemblies, LiDAR data and scientific models.\\r\\n\\r\\n## Installation (Built Package)\\r\\n\\r\\nDownload the release package from the [release page](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/tag/v21.1.0). Simply extract the zip folder and run MRET.exe.\\r\\n\\r\\n### Non-HDRP\\r\\n* [VR](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1VR.zip)\\r\\n* [Desktop](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1Desktop.zip)\\r\\n\\r\\n### HDRP (experimental)\\r\\n* [VR](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1HDRPVR.zip)\\r\\n* [Desktop](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/releases/download/v21.1.0/MRET21.1HDRPDesktop.zip) (Raytracing)\\r\\n\\r\\n## Installation (Development Environment)\\r\\n\\r\\n`git clone https://github.com/nasa/Mixed-Reality-Exploration-Toolkit` into the folder you would like the MRET project to be for your Unity **2019.4.17f1**. Yes, that specific Unity version is important!\\r\\n\\r\\nIn Unity Hub “ADD” the MRET project from the folder where you cloned it to.\\r\\nOnce Unity opens, **DO NOT** change the scene to MRET scene, because you want the necessary assets to be included in the project before. \\r\\nHence import all these assets first into the project under [Unity/Assets/AssetsandLibraries/Non-Distributable](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/tree/master/Unity/Assets/AssetsandLibraries/Non-Distributable):\\r\\n\\r\\n### Free assets:\\r\\n\\r\\n* [SteamVR](https://github.com/ValveSoftware/steamvr_unity_plugin/releases/tag/2.6.1) Version 2.6.1\\r\\n* [VR Capture](https://assetstore.unity.com/packages/tools/video/vr-capture-75654) Version 11.6\\r\\n\\r\\n### Paid assets:\\r\\n\\r\\nWhile MRET is free and [open-source](https://opensource.gsfc.nasa.gov/documents/NASA_Open_Source_Agreement_1.3.pdf), it does rely on third party assets that aren\\'t.\\r\\n\\r\\nPrices give a ballpark estimate for building MRET as of 2021.03.30. The prices of these assets fluctuate over time and may differ from what is listed.\\r\\n\\r\\n* [Easy Build System](https://assetstore.unity.com/packages/templates/systems/easy-build-system-v5-2-5-45394) Version 5.2.5 - $20.\\r\\n* [Embedded Browser](https://assetstore.unity.com/packages/tools/gui/embedded-browser-55459) Version 3.1.0 - $75\\r\\n* [Final IK](https://assetstore.unity.com/packages/tools/animation/final-ik-14290) Version 2.0 - $90\\r\\n* [Graph & Chart](https://assetstore.unity.com/packages/tools/gui/graph-and-chart-78488) Version 1.95 - $35\\r\\n* [Non Convex Mesh Collider](https://assetstore.unity.com/packages/tools/physics/non-convex-mesh-collider-84867) Version 1.0 - $9\\r\\n* [Universal Media Player](https://assetstore.unity.com/packages/tools/video/ump-win-mac-linux-webgl-49625) Version 2.0.3 - $45\\r\\n* [Universal Sound FX](https://assetstore.unity.com/packages/audio/sound-fx/universal-sound-fx-17256) Version 1.4 - $40 \\r\\n\\r\\n### Tricky assets:\\r\\n\\r\\n* [Pointcloud viewer and tools](https://assetstore.unity.com/packages/tools/utilities/point-cloud-viewer-and-tools-16019) Version 2.40 - $100 When importing this one, uncheck the “Editor” folder (otherwise MRET will run in Unity but not compile).\\r\\n* [Runtime OBJ Importer](https://assetstore.unity.com/packages/tools/modeling/runtime-obj-importer-49547) Version 2.02 - Free\\r\\nAfter importing this asset, modify the string in \"Shader.Find\" on line 160 of MTLLoader.cs to \"Standard\" (i.e. Shader.Find(\"Standard\")).\\r\\n\\r\\n### Unity Packages\\r\\n* If the OpenVR Unity XR package doesn\\'t install correctly using the Package Manager, it can also be downloaded from [GitHub](https://github.com/ValveSoftware/unity-xr-plugin). Note, you will need to remove the reference to the OpenVR Unity XR package from the package.manifest if going this route.\\r\\n\\r\\nNow navigate in Unity to `Project > Assets > Framework > Scenes > MainScene` and open either VR or Desktop scene.\\r\\n\\r\\n(you will still need the MRET Components to be placed, where they need to be placed to..)\\r\\n\\r\\n### Additional Configuration\\r\\nEnsure that Project Settings->Player->Other Settings->Active Input Handling is set to \"Both\".\\r\\n\\r\\n## Contributing\\r\\n\\r\\nPull requests are welcome. For more information on contributing, please see [Contributing.md](https://github.com/nasa/Mixed-Reality-Exploration-Toolkit/blob/master/CONTRIBUTING.md).\\r\\n\\r\\n## License\\r\\n\\r\\n[NOSA](https://opensource.gsfc.nasa.gov/documents/NASA_Open_Source_Agreement_1.3.pdf)\\r\\n'},\n",
       " {'repo': 'nasa/refine',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Description\\n\\n`refine` is a 3D mesh adaptation tool implemented in the C language.\\n\\n# Quick Start Compile from Git Repo and Basic Usage\\n\\n`refine` can function without depencies, but the typical use cases of\\nparallel execution and geometry evaluation require an MPI implementation\\nand [Engineering Sketch Pad](https://acdl.mit.edu/ESP/ESPreadme.txt) (ESP).\\nA native implementaion of a recursive coordinate bisection partition\\nalgorithm is included, but better results are expected with\\n[ParMETIS](http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview).\\nInitial mesh generation assumes\\n[TetGen](http://wias-berlin.de/software/tetgen/) or\\n[AFLR](http://www.simcenter.msstate.edu/research/cavs_cfd/aflr.php) is in\\nthe shell path.\\nConfiguration and compliation is supported with Autoconf and CMake.\\n\\n## Automake 1.7 (or later) and Autoconf 2.53 (or later):\\n```\\n ./bootstrap\\n mkdir -p build\\n cd build\\n ../configure --prefix=`pwd` \\\\\\n   --with-mpi=/mpi/path \\\\\\n   --with-parmetis=/parmetis/path \\\\\\n   --with-EGADS=/egads/path \\\\\\n   --with-OpenCASCADE=/opencascade/path\\n make\\n make install\\n```\\nSee the INSTALL file in the top directory or `./configure --help`\\nfor additional build instructions.\\n\\n## CMake 3.0 (or later):\\n```\\n mkdir -p build\\n cd build\\n cmake .. -DCMAKE_INSTALL_PREFIX=`pwd` \\\\\\n   -DCMAKE_PREFIX_PATH=\"/mpi/path;/parmetis/path;/egads/path;/opencascade/path\"\\n make\\n make install\\n```\\n\\n## Usage\\n\\nThe installed `bin` directory will include the `ref` executable.\\nInvoking `ref` with no arguments will list available subcommands.\\nHelp on a particular subcommand is available via a `-h`, i.e.,\\n`ref adapt -h`. If MPI is provided, `refmpi` will allow for parallel\\nexecution. If ESP is provided, `ref` and `refmpifull` includes\\nEGADS built with OpenCASCADE and `refmpi` includes EGADSlite.\\n\\n# Examples\\n\\nThe following examples assume that `ref` is in your shell path.\\n`mpiexec ... refmpi` or `mpiexec ... rempifull` can be substituted for\\n`ref` in each of these examples if MPI and/or ESP is configured. The\\n[.meshb and .solb file extensions](https://github.com/LoicMarechal/libMeshb)\\nare used generically. Other formats are supported, e.g.,\\nAFLR `*.ugrid`.\\n\\n## Bootstrapping Mesh Adaptation on an EGADS Model\\n\\nAn `.egads` file can be dumped from OpenCSM in the ESP package.\\n```\\nref bootstrap project.egads\\n```\\nor\\n```\\nmpiexec ... refmpifull bootstrap project.egads\\n```\\nwhich assume that `tetgen` is in your shell path or\\n`aflr3` is in your shell path with `--mesher aflr` option.\\nA `project-vol.meshb` is output that includes the surface mesh,\\nvolume mesh, mesh-to-geometry associtivity, and EGADSlite data.\\n\\n## Mesh Adaptation\\n\\nThe mesh is adapted with\\n```\\nref adapt input.meshb -x output.meshb [-m metric.solb] [-g geometry.egads]\\n```\\nor\\n```\\nmpiexec ... refmpi adapt input.meshb -x output.meshb [-m metric.solb]\\n```\\nwhere a surface curvature metric is used if the `-m` argument is not present.\\n\\n## Multiscale Metric for Control of Interpolation Error in Lp-norm\\n\\nIn conjunction with the\\n[Unstructured Grid Adaptation Working Group](https://ugawg.github.io/),\\nan implementation of the multiscale metric is provided.\\n```\\nref multiscale input.meshb scalar.solb complexity output-metric.solb\\n```\\nor\\n```\\nmpiexec ... refmpi multiscale input.meshb scalar.solb complexity output-metric.solb\\n```\\n\\n## Field Interpolation\\nThe fields in a .solb file paired with a donor mesh can be interpolated to\\na receptor mesh. This utility can be executed in serial or parallel.\\n\\n```\\nref interp donor-mesh.ext donor-field.solb receptor-mesh.ext receptor-field.solb\\n```\\nor \\n```\\nmpiexec ... refmpi interp donor-mesh.ext donor-field.solb receptor-mesh.ext receptor-field.solb\\n```\\nwhere the output is `receptor-field.solb`.\\n\\n# Description\\n\\n`refine` is a 2D and 3D mesh adaptation tool implemented in the C\\nlanguage.  Mesh adaptation mechanics are provided where the primary\\ntarget is linear and curved simplex (triangle and tetrahedra)\\nmeshes. A limited capability to store, modify, and insert\\nmixed-element types is also provided. Typical use is via an executable\\nthat interacts with files, and linking to a library form is also\\navailable. Mesh adaptation metrics can be computed by reconstructing\\ngradients and Hessians from a field. Visualization files and multiple\\nmesh formats can be exported. Solutions can be interpolated between\\nmeshes. The distance to lower-dimensional elements can be computed.\\nInterfaces are available to multiple geometry sources and an internal\\nsurrogate geometry source. Parallel execution is supported with\\npartitioning and load balancing. Solution fields are provided to\\nverify the mesh adaptation process.\\n\\n'},\n",
       " {'repo': 'nasa/SMCPy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'SMCPy - **S**equential **M**onte **C**arlo with **Py**thon \\n==========================================================================\\n[![Build Status](https://travis-ci.com/nasa/SMCPy.svg?branch=master)](https://travis-ci.com/nasa/SMCPy) &nbsp;[![Coverage Status](https://coveralls.io/repos/github/nasa/SMCPy/badge.svg?branch=master)](https://coveralls.io/github/nasa/SMCPy?branch=master)\\n\\nPython module for uncertainty quantification using a parallel sequential Monte\\nCarlo sampler.\\n\\nTo operate the code, the user supplies a computational model built in Python\\n3.6+, defines prior distributions for each of the model parameters to be\\nestimated, and provides data to be used for calibration. SMC sampling can then\\nbe conducted with ease through instantiation of the SMCSampler class and a call\\nto the sample() method. The output of this process is an approximation of the\\nparameter posterior probability distribution conditional on the data provided.\\n\\nThe primary sampling algorithm implemented in this package is an MPI-enabled\\nversion of that presented in the following IEEE article by Nguyen et al.:\\n\\n> Nguyen, Thi Le Thu, et al. \"Efficient sequential Monte-Carlo samplers for Bayesian\\n> inference.\" IEEE Transactions on Signal Processing 64.5 (2015): 1305-1319.\\n\\n[Link to Article](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339702) | [BibTeX Reference](https://scholar.googleusercontent.com/scholar.bib?q=info:L7AZJvppx1MJ:scholar.google.com/&output=citation&scisdr=CgUT24-FENXorVVNYK0:AAGBfm0AAAAAXYJIeK1GJKW947imCXoXAkfc7yZjQ7Oo&scisig=AAGBfm0AAAAAXYJIeNYSGEVCrlauowP6jMwVMHB_blTp&scisf=4&ct=citation&cd=-1&hl=en)\\n\\nThis software was funded by and developed under the High Performance Computing\\nIncubator (HPCI) at NASA Langley Research Center.\\n\\n------------------------------------------------------------------------------\\n## Example Usage\\n\\n```python\\nimport numpy as np\\n\\nfrom scipy.stats import uniform\\n\\nfrom spring_mass_model import SpringMassModel\\nfrom smcpy.utils.plotter import plot_pairwise\\nfrom smcpy import SMCSampler, VectorMCMC, VectorMCMCKernel\\n\\n\\n# Load data\\nstd_dev = 0.5\\ndisplacement_data = np.genfromtxt(\\'noisy_data.txt\\')\\n\\n# Define prior distributions & MCMC kernel\\npriors = [uniform(0, 10), uniform(0, 10)]\\nvector_mcmc = VectorMCMC(model.evaluate, displacement_data, priors, std_dev)\\nmcmc_kernel = VectorMCMCKernel(vector_mcmc, param_order=(\\'K\\', \\'g\\'))\\n\\n# SMC sampling\\nsmc = SMCSampler(mcmc_kernel)\\nstep_list, mll_list = smc.sample(num_particles=500,\\n                                 num_mcmc_samples=5,\\n                                 phi_sequence=np.linspace(0, 1, 20),\\n                                 ess_threshold=0.8,\\n                                 progress_bar=True)\\n\\n# Display results\\nprint(f\\'parameter means = {step_list[-1].compute_mean()}\\')\\n\\nplot_pairwise(step_list[-1].params, step_list[-1].weights, save=True,\\n              param_labels=[\\'K\\', \\'g\\'])\\n```\\n\\nThe above code produces probabilistic estimates of K, the spring stiffness\\ndivided by mass, and g, the gravitational constant on an unknown planet. These\\nestimates are in the form of weighted particles and can be visualized by\\nplotting the pairwise weights as shown below. The mean of each parameter is\\nmarked by the dashed red line. The true values for this example were K = 1.67\\nand g = 4.62. More details can be found in the spring mass example\\n(smcpy/examples/spring_mass/).\\n\\nTo run this model in parallel using MPI, the MCMC kernel just needs to be built\\nwith the ParallelMCMC class in place of VectorMCMC. More details can be found\\nin the MPI example (smcpy/examples/mpi_example/).\\n\\n![Pairwise](https://github.com/nasa/SMCPy/blob/main/examples/spring_mass/pairwise.png)\\n\\nTests\\n-----\\n\\nThe tests can be performed by running \"pytest\" from the tests/unit directory to ensure a proper installation.\\n\\nDevelopers\\n-----------\\n\\nNASA Langley Research Center <br /> \\nHampton, Virginia <br /> \\n\\nThis software was funded by and developed under the High Performance Computing Incubator (HPCI) at NASA Langley Research Center. <br /> \\n\\nContributors: Patrick Leser (patrick.e.leser@nasa.gov) and Michael Wang\\n\\n------------------------------------------------------------------------------\\n\\nLicense\\n-----------\\nNotices:\\nCopyright 2018 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration. No copyright is claimed in\\nthe United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S\\nUSE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR\\nANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS\\nAGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/Kamodo',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Kamodo\\n\\n[![codecov](https://codecov.io/gh/asherp/Kamodo/branch/master/graph/badge.svg?token=W1B3L19REF)](https://codecov.io/gh/asherp/Kamodo)\\n\\nKamodo is a CCMC tool for access, interpolation, and visualization of space weather models and data in python. Kamodo allows model developers to represent simulation results as mathematical functions which may be manipulated directly by end users. Kamodo handles unit conversion transparently and supports interactive science discovery through jupyter notebooks with minimal coding and is accessible through python.\\n\\n\\nThe project page is located at the Community Coordinated Modeling Center, located at NASA Goddard Space Flight Center.\\n\\n* Official site page [https://ccmc.gsfc.nasa.gov/Kamodo/](https://ccmc.gsfc.nasa.gov/Kamodo/)\\n\\nKamodo's official source code is hosted on github under a permissive NASA open source license:\\n\\n* [https://github.com/nasa/Kamodo](https://github.com/nasa/Kamodo)\\n\\nPeriodic contributions to Kamodo are made from the unofficial repo located here\\n\\n* [https://github.com/asherp/Kamodo](https://github.com/asherp/Kamodo)\\n\\n\\n## Usage\\nSuppose we have a vector field defined by a function of positions in the x-y plane:\\n\\n```python\\nfrom kamodo import kamodofy\\nimport numpy as np\\n\\nx = np.linspace(-np.pi, np.pi, 25)\\ny = np.linspace(-np.pi, np.pi, 30)\\nxx, yy = np.meshgrid(x,y)\\npoints = np.array(zip(xx.ravel(), yy.ravel()))\\n\\n@kamodofy(units = 'km/s')\\ndef fvec(rvec = points):\\n    ux = np.sin(rvec[:,0])\\n    uy = np.cos(rvec[:,1])\\n    return np.vstack((ux,uy)).T\\n```\\n\\nThe @kamodofy decorator lets us register this field with units to enable unit-conversion downstream:\\n```python\\nfrom kamodo import Kamodo\\n\\nkamodo = Kamodo(fvec = fvec)\\nkamodo\\n```\\nWhen run in a jupyter notebook, the above kamodo object will render as a set of equations:\\n\\n$$\\\\vec{f}{\\\\left (\\\\vec{r} \\\\right )} [km/s] = \\\\lambda{\\\\left (\\\\vec{r} \\\\right )}$$\\n\\nWe can now evaluate our function using dot notation:\\n\\n```python\\nkamodo.fvec(np.array([[-1,1]]))\\n```\\n```console\\narray([[-0.84147098,  0.54030231]])\\n```\\nWe can perform unit conversion by function composition:\\n```python\\nkamodo['gvec[m/s]'] = 'fvec'\\n```\\nkamodo automatically generates the appropriate multiplicative factors:\\n$$\\\\vec{g}{\\\\left (\\\\vec{r} \\\\right )} [m/s] = 1000 \\\\vec{f}{\\\\left (\\\\vec{r} \\\\right )}$$\\nwe can verify these results through evaluation\\n\\n```python\\nkamodo.gvec(np.array([[-1,1]]))\\n```\\n```console\\narray([[-841.47098481,  540.30230587]])\\n```\\nKamodo also generates quick-look graphics via function inspection.\\n```python\\nimport plotly.io as pio\\n\\nfig = kamodo.plot('fvec')\\npio.write_image(fig, 'images/fig2d-usage.svg')\\n```\\n![usage](notebooks/images/fig2d-usage.svg)\\n\\nHead over to the [Introduction](notebooks/Kamodo.ipynb) page for more details.\\n\\n\\n## Getting started\\n\\nKamodo may be installed from pip\\n\\n```console\\npip install kamodo\\n```\\n\\nTo get the latest version, install from Asher's fork:\\n\\n```console\\npip install git+https://github.com/asherp/Kamodo.git\\n```\\n\\n!!! note\\n    Asher's fork is periodically merged into the CCMC's official NASA version.\\n\\n### Kamodo Environment \\n\\nWe strongly recommend using the conda environment system to avoid library conflicts with your host machine's python.\\n\\nDownload and install miniconda from [here](https://conda.io/miniconda.html). The advantage to using miniconda is that each new environment includes the bare-minimum for a project. This allows you to keep many different projects on a single work station.\\n\\n#### Create Kamodo environment\\n\\nCreate a new environment for kamodo\\n\\n```console\\nconda create -n kamodo python==3.7\\nconda activate kamodo\\n(kamodo) pip install kamodo\\n```\\n!!! note\\n    The leading (kamodo) in your prompt indicates that you have activated the `kamodo` environment.\\n    From here on, anything you install will be isolated to the `kamodo` environment.\\n\\n#### Loading example notebooks\\n\\nIf you want to run any of the notebooks in docs, you will need to install `jupyter`:\\n\\n```console\\n(kamodo) conda install jupyter\\n```\\n\\nNavigate to the top-level of the kamodo repo, then point jupyter to `docs/notebooks`:\\n\\n    (kamodo) jupyter notebook docs/notebooks\\n\\nThis should open a browser window that will allow you to load any of the example notebooks.\\n\\n#### Requirements\\n\\nThe following requirements are obtained by running `pip install kamodo`\\n\\n* numpy\\n* scipy\\n* sympy\\n* pandas\\n* plotly==3.3 \\n* pytest\\n* psutil\\n* conda install antlr-python-runtime (rendering latex)\\n* conda install -c plotly plotly-orca (for writing images)\\n\\n!!! note\\n    plotly version in flux\\n\\n\\n## Generating Docs\\n\\nKamodo's documentation site is a good example of how to embed your own plots in your own website.\\nThe documentation site is generated by the `mkdocs` package with some addons\\n\\n* mkdocs - handles site generation and deployment (configured by top-level `mkdocs.yaml`)\\n* markdown-include - allows for embedding of markdown files (and graph divs) outside the docs folder\\n* python-markdown-math - enables LaTeX rendering\\n* mknotebooks - allows for the embedding of jupyter notebooks\\n\\nAll of the above requirements can be installed with this line:\\n\\n```console\\npip install mkdocs python-markdown-math markdown-include mknotebooks\\n```\\n\\nYou can then generate the docs and serve locally with\\n\\n`mkdocs serve`\\n\\nTo deploy your own documentation on github-pages:\\n\\n`mkdocs gh-deploy`\\n\\nThis generates a gh-pages branch with the static site files and pushes it to github. Github automatically creates a website url based on that branch.\\n\\n\"},\n",
       " {'repo': 'nasa/CCDD',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': \"# CCDD\\nCore Flight System (CFS) Command and Data Dictionary (CCDD) utility\\n\\n*** CCDD Version 1.5.37 ***\\n\\n*** CCDD works with JAVA 7-13 ***\\n\\nCCDD is a software tool for managing the command and telemetry data for CFS and CFS applications.  CCDD is written in Java™ and interacts with a PostgreSQL database, so it can be used on any operating system that supports the Java Runtime Environment (JRE) and PostgreSQL.  CCDD is released as open source software under the NASA Open Source Software Agreement, version 1.3, and is hosted on GitHub.\\n\\nThe CCDD application uses tables, similar to a spreadsheet, to display and allow manipulation of telemetry data structures, command information, and other data pertinent to a CFS project.  The data is stored in a PostgreSQL database for manipulation and data security.  The PostgreSQL database server can be run locally or centralized on a remote host for easier access by multiple users.  Data can be imported into the application from files in comma-separated values (CSV), JavaScript Object Notation (JSON), electronic data sheet (EDS), and extensible markup language (XML) telemetric and command exchange (XTCE) formats.  Data can be exported from the application to files in CSV, JSON, EDS, and XTCE formats.  The CCDD tables also allow simple cut and paste operations from the host operating system’s clipboard.  To make use of the project’s data, CCDD can interact with Java Virtual Machine (JVM)-based scripting languages via a set of supplied data access methods.  Using scripts, the user can translate the data stored in the CCDD’s database into output files.  Example scripts for creating common CFS related output files are provided in four of these scripting languages.  An embedded web server can be activated, allowing web-based application access to the data.\\n\\nSee the CCDD user's guide for details on set up and use.\\n\\n## CCDD version 2 \\n\\n*** Version 2.0.16 is now released (see below for details) ***\\n\\n*** CCDD version 2 works with JAVA 7-13 ***\\n\\n*** CCDD version 2 has changed the way that the json import/export works. You can now import and export entire databases. Check CCDDv2 users guide for more details ***\\n\\nVersion 2 redefines the behavior of command tables.  Command arguments are no longer defined as columns within a command table.  Instead, the command table has a column that is a reference to a structure table; this structure defines the command argument(s).  The version 2 user's guide is updated to provide further details.\\n\\nWhen version 2 attempts to open a version 1.x.x version project database then a dialog appears asking to convert the project.  Unlike previous patches, this patch alters user-defined tables and table definitions, and creates new ones.  The argument columns in any command tables are replaced with the argument structure reference column, and the argument structure is created and populated using the original argument information.  Many of the command table script data access methods no longer exist, so existing scripts may need to be updated. Before this patch is applied to the version 1.x.x database a backup will be performed to ensure no data loss on the chance that something does not work as anticipated. \\n\"},\n",
       " {'repo': 'nasa/SROMPy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'SROMPy - **S**tochastic **R**educed **O**rder **M**odels with **Py**thon \\n==========================================================================\\n\\n<a href=\\'https://travis-ci.com/nasa/SROMPy\\'><img src=\\'https://travis-ci.com/nasa/SROMPy.svg?branch=master\\' alt=\\'Coverage Status\\' /></a> <a href=\\'https://coveralls.io/github/lukemorrill/SROMPy?branch=master\\'><img src=\\'https://coveralls.io/repos/github/lukemorrill/SROMPy/badge.svg?branch=master\\' alt=\\'Coverage Status\\' /></a>\\n\\nGeneral\\n--------\\n\\nPython module for generating Stochastic Reduced Order Models (SROMs) and applying them for uncertainty quantification problems. See documentation in `docs/` directory for details. \\n\\nDependencies\\n-------------\\nSROMPy is intended for use with Python 2.7 and relies on the following packages:\\n* numpy\\n* scipy\\n* matplotlib\\n* mpi4py (optional for running in parallel)\\n* pytest (optional if the testing suite is to be run)\\n\\nA requirements.txt file is included for easy installation of dependecies with pip:\\n\\n```\\npip install -r requirements.txt\\n```\\n\\nExample Usage\\n--------------\\n\\n```python\\nfrom SROMPy.postprocess import Postprocessor\\nfrom SROMPy.srom import SROM\\nfrom SROMPy.target import NormalRandomVariable\\n\\n#Initialize Normal random variable object to be modeled by SROM:\\nnormal = NormalRandomVariable(mean=3., std_dev=1.5)\\n\\n#Initialize SROM & optimize to model the normal random variable:\\nsrom = SROM(size=10, dim=1)\\nsrom.optimize(normal)\\n\\n#Compare the CDF of the SROM & target normal variable:\\npost_processor = Postprocessor(srom, normal)\\npost_processor.compare_CDFs()\\n```\\n  \\nThe above code snippet produces the following CDF comparison plot: \\n  \\n![CDF comparison](https://github.com/nasa/SROMPy/blob/master/examples/basic_tests/normal_rv_srom.png)\\n\\nGetting Started\\n----------------\\nSROMPy can be installed via pip from [PyPI](https://pypi.org/project/SROMPy/):\\n\\n```\\npip install srompy\\n```\\n\\nSROMPy can also be installed using the `git clone` command:\\n\\n```\\ngit clone https://github.com/nasa/SROMPy.git\\n```\\n\\nThe best way to get started with SROMPy is to take a look at the scripts in the examples/ directory. A simple example of propagating uncertainty through a spring mass system can be found in the examples/spring_mass/, while the examples/phm18/ directory contains scripts necessary to reproduce the results in the following conference paper on probabilistic prognostics: https://www.phmpapers.org/index.php/phmconf/article/view/551. For more information, see the source code documentation in docs/SROMPy_doc.pdf (a work in progress) or the technical report below that accompanied the release of SROMPy.\\n\\nTests\\n------\\nThe tests can be performed by running \"py.test\" from the tests/ directory to ensure a proper installation.\\n\\nReference\\n-------------\\nIf you use SROMPy for your research, please cite the technical report:\\n\\nWarner, J. E. (2018). Stochastic reduced order models with Python (SROMPy). NASA/TM-2018-219824. \\n\\nThe report can be found in the `docs/references` directory. Thanks!\\n\\nDevelopers\\n-----------\\n\\nUQ Center of Excellence <br />\\nNASA Langley Research Center <br /> \\nHampton, Virginia <br /> \\n\\nThis software was funded by and developed under the High Performance Computing Incubator (HPCI) at NASA Langley Research Center. <br /> \\n\\nContributors: James Warner (james.e.warner@nasa.gov), Luke Morrill, Juan Barrientos\\n\\nLicense\\n---------\\n\\nCopyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nThe Stochastic Reduced Order Models with Python (SROMPy) platform is licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. \\n \\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n\\n\\n\\n'},\n",
       " {'repo': 'nasa/IDF',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Input Device Framework\\n\\nIDF is a software library that provides an infrastructure for interfacing\\nsoftware with physical input devices. Examples of common devices include hand\\ncontrollers, joysticks, foot pedals, computer mice, game controllers, etc.\\nConceptually, the framework can be extended to support any device that produces\\ndigital output. IDF additionally presents, and is itself an implementation of, a\\ndesign methodology that encourages application developers to program against\\ndomain-specific interfaces rather than particular hardware devices. This\\nabstraction frees the application from the details of communicating with the\\nunderlying devices, resulting in robust and flexible code that is\\ndevice-agnostic. IDF ensures that devices meet application interface\\nrequirements, supports many-to-many relationships between application interfaces\\nand devices, allows for flexible and dynamic interpretation of device inputs,\\nand provides methods for transforming and combining inputs.\\n\\n# The Problem\\n\\nSoftware often requires input from external physical devices. The range of\\ndevices, data formats, and mediums of communication are virtually limitless.\\nThere thus exists a need for a method by which to deliver data to applications\\nin a consistent, well-defined manner, and in an application-defined format. The\\nvast array of ways that devices output data necessitates a separation between\\nthe application, which is concerned only with what the data represents, and the\\ndevices themselves, which have no notion of how their data is to be used. There\\nmust then be a way to bridge this disconnect without burdening either side with\\ndetails of the other. Barring such a tool, applications may be written against\\nspecific devices, but this pollutes the program\\'s primary functionality with a\\nsecondary task and forces the application interface to conform to the device\\'s\\noutput format. As the consumer of the data, it is preferable for the\\napplication to define its own context-specific interface through which the data\\nis received. Changes in the device, data format, or communication medium should\\nnot alter the application interface.\\n\\n# A Solution\\n\\nIDF serves to completely separate the \"business\" logic of a program from the\\nlogic responsible for reading raw input from physical devices. Beyond simply\\nisolating the code, it further aims to abstract each side from the other, so\\nthat both pieces can be developed fully independently. Its goal is to enable\\napplication developers to write robust, flexible code with clearly delineated\\ninterface points which are defined in a domain and context most meaningful to\\nthe application.\\n\\n# Architecture\\n\\n### Simulation Interface Layer\\n\\nThis layer defines the point at which external inputs enter the application\\'s\\nprimary functional logic. It is here that a developer decides what kind of\\ninputs his system will accept, and in what format it will accept them. Once this\\ninterface is established, the developer encodes it as a \"controller\" by\\nextending IDF and specializing it for this particular application. Devices that\\nwish to service this interface must meet this controller\\'s contract. This layer\\nis the only one with which the primary functional logic should interact.\\n\\n### Hardware Interface Layer\\n\\nThis layer represents the physical input devices and is responsible for all\\naspects of communication with the hardware, primarily reading and decoding raw\\ndata. Devices are classified by their communication medium, and a representation\\nexists for each specific device supported.\\n\\n### Input Abstraction Layer\\n\\nThis layer provides a means by which to tie the above two layers together. It\\nrepresents device input layouts without regard to their particular medium of\\ncommunication. A device\\'s available data, represented in this layer, is\\npopulated by the Hardware Interface Layer as it is received from the physical\\ndevice. This data can then be presented to a controller from the Simulation\\nInterface Layer, provided that it can be made to meet that controller\\'s\\ncontract.\\n\\n# Wiki\\n\\nSee the [wiki](https://github.com/nasa/IDF/wiki) for installation instructions\\nand lots of additional information.\\n\\n# License\\nIDF is released under the [NASA Open Source Agreement, Version 1.3](LICENSE).\\n'},\n",
       " {'repo': 'nasa/PSP',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/psp/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/psp/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : Platform Support Package\\n\\nThis repository contains NASA\\'s Platform Support Package (PSP), which is a framework component of the Core Flight System.\\n\\nThis is a collection of APIs abstracting platform specific functionality to be located in the `psp` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes build and execution instructions.\\n\\n## Version History\\n\\n\\n### Development Build: v1.5.0-rc1+dev112\\n\\n- Cleans up stale code from the previous methods of generating 1Hz. Adds a new PSP module that instantiates an OSAL abstract timebase for use with cFE services. This single module is used across all psp implementations (mcp750, pc-linux, pc-rtems). Results in 1Hz timing tick on MCP750 will be more accurate. No changes to Linux or RTEMS\\n- Fixes segfaults when `CFE_PSP_Port` routines are invoked on Linux.\\n- Converts `cfe_psp_ram.c` and `cfe_psp_port.c` into modular components and removes from the \"shared\" directory. The existing implementations become the corresponding \"direct\" module, and are enabled based on the psp module selection. Adds a \"notimpl\" variant where all the functions return `CFE_PSP_ERR_NOT_IMPLEMENTED`. This is used on Linux\\nor any other system where direct access is not possible.  Renames the existing `eeprom_stub` module to be `eeprom_notimpl` for consistency and to avoid confusion with the unit test stubs.\\n- Implements two PSP modules to provide `CFE_PSP_GetTime` and `CFE_PSP_GetTimeBase`, one for POSIX-compliant RTOS using `clock_gettime()` and the other specifically for PowerPC processors on VxWorks that have the `vxTimeBaseGet()` routine. Clarifies and documents the difference and use cases for `CFE_PSP_GetTime` and `CFE_PSP_GetTimeBase`. No impact to behavior.\\n- Adds a coverage test for the VxWorks PSP timebase module and provides an example of how this can be implemented for other modules.\\n- See <https://github.com/nasa/PSP/pull/289> and <https://github.com/nasa/cFS/pull/238>\\n\\n### Development Build: v1.5.0-rc1+dev101\\n\\n- Removes unnecessary global config structure `Target_PspConfigData` and associated elements infavor of the new version API.\\n- The mem pool stats TLM command now works on 64-bit Linux and sends out the expected telemetry packet.\\nConverts `CFE_PSP_MemoryTable` to internal object (instead of external) that should only be accessed via the PSP API. Replace `uint32`s with `size_t`. Use full range (SIZE_MAX) in the Linux/RTEMS implementation.\\n- See <https://github.com/nasa/PSP/pull/288> and <https://github.com/nasa/cFS/pull/233>\\n\\n### Development Build: v1.5.0-rc1+dev95\\n\\n- Includes `cfe_psp_version.c` in the cmake source list, which was mistakenly omitted previously.\\n- Applied the patch and confirmed that CS Cmake unit tests build. Unit tests relying on `CFE_PSP_GetKernelTextSegmentInfo` will build.\\n- See <https://github.com/nasa/PSP/pull/279>\\n\\n### Development Build: v1.5.0-rc1+dev90\\n\\n- Addresses the issue of incompatible/non-portable code blobs in the \"shared\" directory. It uses the same modular init pattern as is used elsewhere in cFE: CMake generates a list of \"base\" modules correlating with the selected PSP (i.e. pc-linux, mcp750-vxworks, etc) and these modules are then initialized (in order) before the rest of PSP runs. The \"direct write\" EEPROM is not used unconditionally. Instead the proper eeprom implementation module is selected based on which PSP is selected. MCP750 uses direct write, pc-linux uses an mmap file, and pc-rtems uses a stub (not implemented).\\n- Replaces \" used on non-system header #includes with <>\\n- Adds a contributing guide that links to the main cFS contributing guide.\\n- See <https://github.com/nasa/PSP/pull/273>\\n\\n\\n### Development Build: v1.5.0-rc1+dev82\\n\\n- HOTFIX 20210312, updates to work with older CMake\\n- See <https://github.com/nasa/PSP/pull/268>\\n\\n### Development Build: v1.5.0-rc1+dev76\\n\\n- Fix #246, remove unused code.\\n- Fix #254, use CMake to publish interface details\\n- Fix #256, add PSP version API\\n- Fix #258, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/PSP/pull/260>\\n\\n### Development Build: 1.5.0-rc1+dev68\\n\\n- Updates continuous integration workfow by adding static analysis with timeout and code format check. Adds status badges to ReadMe and removes old TravisCI artifacts.\\n- Adds CodeQL analysis to continuous integration\\n- Apply standard formatting to psp codebase. Only changes whitespace.\\n- Adds missing \"+dev\" to development version output\\n- See <https://github.com/nasa/PSP/pull/250>\\n\\n### Development Build: 1.5.0-rc1+dev58\\n\\n- Add `Security.md` with instructions on reporting vulnerabilities.\\n- Rename `UT_ClearForceFail` as `UT_ClearDefaultValue` to reflect <https://github.com/nasa/osal/issues/724>\\n- Remove unused elements in `CFE_PSP_ModuleType_t` enum to avoids irregular enum warning\\n- See <https://github.com/nasa/PSP/pull/243>\\n\\n### Development Build: 1.5.0-rc1+dev50\\n\\n-  Instead of accessing `OS_time_t` member values directly, use the OSAL-provided conversion and access methods. This provides independence and abstraction from the specific `OS_time_t` definition and allows OSAL to transition to a 64 bit value.\\n- See <https://github.com/nasa/psp/pull/240>\\n\\n### Development Build: 1.5.0-rc1+dev46\\n\\n- Add cppcheck GitHub Actions workflow file\\n- See <https://github.com/nasa/PSP/pull/234>\\n\\n\\n### Development Build: 1.5.0-rc1+dev42\\n\\n- Updates the Readme for RTEMS and adds `README_RTEMS_5.txt`. The changes include removing references to the CEXP module loader, and describing the development environment setup for RTEMS 5.  \\n- Remove obsolete OS_TaskRegister comment.  \\n- See <https://github.com/nasa/PSP/pull/226>\\n\\n\\n### Development Build: 1.5.0-rc1+dev36\\n\\n- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing.\\n- Use of the size_t type instead of uint32 in unit-tests to avoid a compiler type mismatch error on some platforms.\\n- See <https://github.com/nasa/PSP/pull/221>\\n\\n### Development Build: 1.5.0-rc1+dev30\\n\\n- - Use event callback mechanism to invoke pthread_setname_np() such that the OS kernel is informed of the OSAL task name. `/proc` filesystem on Linux now has actual task name, instead of all being core-cpu1. The `pthread_setname_np` API requires `_GNU_SOURCE` to be defined when compiling - this can be local to PSP.\\n- Set REVISION to \"99\" to indicate development version\\n- See <https://github.com/nasa/PSP/pull/213>\\n\\n### Development Build: 1.5.0-rc1+dev24\\n\\n- Improves the module ID lookup when getting the CFE core text segment info. VxWorks PSP should use the real module name, not assume cfe-core.o when getting text segment info\\n- See <https://github.com/nasa/PSP/pull/209>\\n\\n### Development Build: 1.5.0-rc1+dev19\\n\\n- Use the osal_id_t typedef whenever dealing with an OSAL ID value.\\n- Resolves build error regarding redefinition of PPC macros in the coverage test, when building on the actual ppc/vxworks target.\\n- See <https://github.com/nasa/PSP/pull/206>\\n\\n### Development Build: 1.5.0-rc1+dev14\\n\\n- Sets the stub config data spacecraft id to historical value 0x42, was 42.\\n- Installs unit test to target directories.\\n- See <https://github.com/nasa/PSP/pull/196>\\n\\n### Development Build: 1.5.0-rc1+dev6\\n\\n- Adds CFE_PSP_GetProcessorName\\n- Removes classic make artifacts\\n- See <https://github.com/nasa/PSP/pull/190>\\n\\n### Development Build: 1.4.0+dev76\\n\\n- Provide a new framework and perform PSP coverage tests. New coverage test executable is built and several files within PSP are targeted.\\n- See <https://github.com/nasa/PSP/pull/184>\\n\\n### Development Build: 1.4.0+dev71\\n\\n- Restructure code to make more amicable for rebuilding in a unit test environment. No major changes, primarily just shifting code between locations/headers to support unit testing.\\n-  Adds a char element `Version` to `CFE_PSP_VersionInfo_t` containing the version number expressed as a string. Defines new macros for the Build Number and the Build Baseline.\\n- See <https://github.com/nasa/PSP/pull/176>\\n\\n### Development Build: 1.4.14\\n\\n- Changes the PSP reference to be compatible with the change in nasa/osal#449 making the BSP modules more generic and changes the name.\\n- See <https://github.com/nasa/PSP/pull/175>\\n\\n### Development Build: 1.4.13\\n\\n- Changes the PSP reference to be compatible with the change in nasa/osal#449 making the BSP modules more generic and changes the name.\\n- See <https://github.com/nasa/PSP/pull/167>\\n\\n### Development Build: 1.4.12\\n\\n- Replace \\'OS_VolumeTable\\' with OS_FileSysAddFixedMap() in all PSPs.\\n- See <https://github.com/nasa/PSP/pull/166>\\n\\n### Development Build: 1.4.11\\n\\n- Removes non-termination string warnings when building with GCC9.\\n- Exception handling is now implemented on POSIX. There is no longer a separate handler for SIGINT - it is now treated as an exception and goes through the normal process which ends up \"restarting\" CFE. On pc-linux causes the process to exit normally. There is now a mechanism to capture the CTRL+C exception code and use it during normal test cycles.\\n- See <https://github.com/nasa/PSP/pull/160>\\n\\n### Development Build: 1.4.10\\n\\n- Implements full-precision microsecond conversion\\n- See <https://github.com/nasa/PSP/pull/155>\\n\\n### Development Build: 1.4.9\\n\\n- RTEMS builds successfully without errors\\n- Build script uses a proper CMakeLists.txt instead of the aux_source directory\\n- Minor updates (see <https://github.com/nasa/PSP/pull/153>)\\n\\n### Development Build: 1.4.8\\n\\n- Minor updates (see <https://github.com/nasa/PSP/pull/151>)\\n\\n### Development Build: 1.4.7\\n\\n- Fixed some build warnings for MCP750\\n- Minor updates (see <https://github.com/nasa/PSP/pull/142>)\\n\\n### Development Build: 1.4.6\\n\\n- Minor updates (see <https://github.com/nasa/PSP/pull/141>)\\n\\n### Development Build: 1.4.5\\n\\n- Simplifies array handling in VxWorks\\n- Minor updates (see <https://github.com/nasa/PSP/pull/138>)\\n\\n### Development Build: 1.4.4\\n\\n- Minor updates (see <https://github.com/nasa/PSP/pull/132>)\\n\\n### Development Build: 1.4.3\\n\\n- Minor updates (see <https://github.com/nasa/PSP/pull/130>)\\n\\n### Development Build: 1.4.2\\n\\n- Minor updates (see <https://github.com/nasa/PSP/pull/127>)\\n\\n### Development Build: 1.4.1\\n\\n- Minor updates (see <https://github.com/nasa/PSP/pull/115>)\\n\\n### **_1.4.0 OFFICIAL RELEASE - Aquila_**\\n\\n- This is a point release from an internal repository\\n- Changes are detailed in [cFS repo](https://github.com/nasa/cFS) release documentation\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### **_1.3.0a OFFICIAL RELEASE_**\\n\\n- This is a point release from an internal repository\\n- See [version description document](https://github.com/nasa/PSP/blob/v1.3.0a/doc/PSP%201.3.0.0%20Version%20Description%20Document.pdf)\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\nThe open source release does not include all PSPs that have been developed. Only the three PSPs included are managed by the community CCB. PSPs developed by other organizations can be requested through the mechanisms listed below. Note the framework PSPs delivered may change in the future as platforms become obsolete.\\n\\n## Known issues\\n\\nSee all open issues and closed to milestones later than this version.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/radbelt',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# radbelt: An Astropy-friendly wrapper for the AE-8/AP-8 Van Allen belt model\\n\\nThis is small Python library to model the fluxes of charged particles trapped\\nin the Van Allen belt. It provides a fast, simple, and convenient Python\\ninterface to the [International Geomagnetic Reference Field (IGRF)] model and\\nNASA\\'s AE-8/AP-8 models of electron and proton fluxes, which are both\\nimplemented in Fortran. The package is integrated with the [Astropy] ecosystem\\nfor easy conversion of coordinate systems, time scales, and units. With this\\npackage, it is easy and fast to determine the flux of particles above any given\\nenergy, at any position, at any time.\\n\\n## Acknowledging radbelt\\n\\nThis package is wraps the following Fortran codes, which have been retrieved\\nfrom NASA Goddard Space Flight Center\\'s (GSFC) [Community Coordinated Modeling\\nCenter (CCMC)]:\\n\\n- https://ccmc.gsfc.nasa.gov/models/modelinfo.php?model=IGRF\\n- https://ccmc.gsfc.nasa.gov/models/modelinfo.php?model=AE-8/AP-8%20RADBELT\\n\\nWhen publishing results derived from this Python package, please cite the\\nfollowing articles:\\n\\n- [Vette, J.I., Lucero, A.B., Wright, J.A., et al. 1966, \"Models of the Trapped Radiation Environment.\" NASA SP-3024.](https://ui.adsabs.harvard.edu/abs/1966NASSP3024.....V)\\n- [Sawyer, D.M. & Vette, J.I. 1976, \"AP-8 trapped proton environment for solar maximum and solar minimum.\" NASA WDC-A-R&S 76-06, NASA-TM-X-72605.](https://ui.adsabs.harvard.edu/abs/1976STIN...7718983S)\\n- [Vette, J.I. 1991, \"The AE-8 trapped electron model environment.\" NSSDC/WDC-A-R&S 91-24.](https://ui.adsabs.harvard.edu/abs/1991STIN...9224228V)\\n- [Thébault, E., Finlay, C.C., Beggan, C.D., et al. 2015, \"International Geomagnetic Reference Field: the 12th generation.\" Earth, Planets, and Space, 67, 79.](https://ui.adsabs.harvard.edu/abs/2015EP&S...67...79T)\\n\\n## To install\\n\\n    $ pip install .\\n\\n## Example\\n\\n```pycon\\n>>> from radbelt import get_flux\\n>>> from astropy import units as u\\n>>> from astropy.coordinates import EarthLocation\\n>>> from astropy.time import Time\\n>>> coords = EarthLocation(-45 * u.deg, -30 * u.deg, 500 * u.km)\\n>>> time = Time(\\'2021-03-01\\')\\n>>> energy = 20 * u.MeV\\n>>> get_flux(coords, time, energy, \\'p\\', \\'max\\')\\n<Quantity 2642.50268555 1 / (cm2 s)>\\n```\\n\\n## Known issues\\n\\nThe CCMC IGRF code has spatially varying errors of a few percent, which will\\nresult in a striped pattern in the resulting particle flux.\\n\\n[International Geomagnetic Reference Field (IGRF)]: https://www.ngdc.noaa.gov/IAGA/vmod/igrf.html\\n[Astropy]: https://www.astropy.org\\n[Community Coordinated Modeling Center (CCMC)]: https://ccmc.gsfc.nasa.gov/\\n'},\n",
       " {'repo': 'nasa/GMSEC_API',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '\\ufeffThis GMSEC API README file contains technical notes and the change summary\\nfor the current release and historical releases.\\n\\nSee the associated Version Description Document (VDD) for further details\\nof each release.\\n\\nFor technical support regarding this release, or any previous version, please\\ncontact the API Team at gmsec-support@lists.nasa.gov.\\n\\n\\n==============================================================================\\n= GMSEC API 4.8 release notes (October 2020)\\n==============================================================================\\no A Ruby binding for the GMSEC API is now offered\\n\\no Support for Apache Artemis is now offered\\n\\no Python3 binding is built using Python 3.8.5\\n\\n\\n--- Change summary -----------------------------------------------------------\\n* API-5275 : C# binding GmsecException missing constructor that accepts Status object\\n* API-5276 : C# binding Status missing constructor that accepts GmsecException object\\n* API-5277 : Java binding GMSEC_Exception missing constructor that accepts Status object\\n* API-5278 : Java binding Status missing constructor that accepts a GMSEC_Exception object\\n* API-5289 : Provide support for Apache Artemis\\n* API-5304 : Resource Info Generator is not adding certain fields related to network interfaces\\n* API-5321 : Add getConnectionEndpoint() method to Connection and ConnectionManager\\n* API-5323 : ConfigFile does not reset doc-loaded flag on error\\n* API-5324 : InternalMistMessage should not add fields while iterating over existing fields\\n* API-5325 : InternalMessage should avoid unnecessary copying of fields\\n* API-5326 : InternalMistMessage setValue() cannot convert value to BinaryField\\n* API-5327 : gmsub subscribing to C2MS.TERMINATE when given a custom subject\\n* API-5329 : Java binding\\'s JNIFieldConverter.cloneField() does not copy header attribute\\n* API-5330 : Improve efficiency by capturing host name, user name, and process ID only once\\n* API-5331 : Need to check values/parameters in U16 and U32 classes of Java binding\\n* API-5334 : Improve efficiency by not forcing Field names to uppercase\\n* API-5344 : Memory leak in Java binding setStandardFields()\\n* API-5346 : API should be able to parse XML configuration file that contains lowercase Subcription, Config, and Message definitions\\n* API-5347 : Augment documentation for shutdownAllMiddlewares() and shutdownMiddleware()\\n* API-5351 : Create a Ruby binding for the GMSEC API\\n* API-5355 : StringUtil getValue() can return NaN on macOS when failing to parse a floating point number\\n* API-5356 : Config getValue() methods that accept default value should not generate error when name is NULL or empty string\\n* API-5358 : Provide ability for API utility apps to accept configuration file and connection config name\\n* API-5360 : Flatten NDM message templates\\n* API-5362 : Message Bus client that has lost its connection is not provided error when attempting to publish\\n* API-5363 : Improve efficiency of the Message addField() methods in the Java binding\\n* API-5364 : OpenDDS m/w wrapper is returning incorrect library root name\\n* API-5365 : ProductFile can throw bad_alloc when it is unable to reserve space for binary file contents\\n* API-5366 : Internal Heartbeat Generator should use thread-safe objects\\n* API-5368 : Java JNIHeartbeatGenerator constructor should not be setting MistMessage standard fields\\n* API-5370 : Config getDoubleValue() in Java binding is calling wrong method to acquire the value\\n* API-5371 : Config getNext() seg-faults when called before getFirst() when there are no entries to fetch\\n* API-5372 : Build the Python3 binding of the GMSEC API using Python 3.8.5 or later\\n* API-5373 : Fix documentation for ConnectionManager\\'s createHeartbeatMessage and createLogMessage\\n* API-5374 : ConnectionManager setStandardFields() should not throw exception if list of fields is empty\\n* API-5396 : ZeroMQ m/w wrapper does not remove ZMQ-REPLY-ADDRESS field from request message\\n* API-5397 : Resource Info Generator should handle improper division when generating metrics\\n* API-5404 : Update Environment Validator script for Apache Artemis\\n* API-5406 : MistMessage setValue() should not throw exception when converting GMSEC_I64 to GMSEC_U64\\n* API-5409 : Scan GMSEC API 4.8 using Fortify and address any issues\\n* API-5411 : Create middleware wrapper for Apache Artemis\\n* API-5412 : Allow for authentication credentials to be used with Generic JMS\\n* API-5416 : API needs to handle exception that may be thrown by message decoder\\n* API-5417 : IBM MQ m/w wrapper should not attempt to reconnect if user indicates no retries\\n* API-5418 : The GMSEC API maximum encoded message size limit cannot be represented using native int type\\n* API-5422 : Incorrect function being referenced to get NSS error string\\n* API-5425 : Floating point dependency check is calling incorrect Message method to acquire value\\n* API-5428 : Javadoc generated for Java binding fails on class search\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.7 release notes (May 2020)\\n==============================================================================\\no Support for 32-bit distributions of the GMSEC API has been dropped.\\n\\no Java-related facets of the GMSEC API are built using OpenJDK 11. Users can continue to use Java 8 through 11 to run Java applications.\\n\\no Support for Python (2.7) has been dropped; users should use the Python3 binding instead.\\n\\no Perl binding under Windows is no longer built with Active State Perl; Strawberry Perl is being used instead.\\n\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4766 : The API should be able to validate dependent fields as defined by C2MS and GMSEC Addendum\\n* API-5242 : API should follow soft-link when attempting to determine install directory\\n* API-5245 : Under Windows, build the Perl binding of the GMSEC API using Strawberry Perl\\n* API-5248 : OpenDDS m/w occassionally crashes on shutdown\\n* API-5249 : Retrieval of binary data is not thread-safe in C# binding\\n* API-5250 : Add example programs that demonstrate basic usage of C2/C2-NSS capabilities of the GMSEC API\\n* API-5251 : Message validation can fail when comparing supposedly equal F32 values\\n* API-5252 : Tracking field insertion for C2MS (2019) Heartbeat message\\n* API-5253 : API should use the 2019 C2MS as the default for XSD files\\n* API-5255 : API is not referencing C++ header files on MacOSX\\n* API-5256 : Optimize InternalSpecification to skip loading unused message template files\\n* API-5258 : Avoid the unnecessary throwing of exception within the GMSEC API\\n* API-5259 : Condition class reports conflicting status\\n* API-5260 : By default, gmreq should not be republishing request messages\\n* API-5261 : Typo in error message when attempting to publish message with a non-PUBLISH message kind\\n* API-5266 : AMQP m/w wrapper provides vague error message when it fails to initialize\\n* API-5268 : Validation produces malformed error message when array fields are involved\\n* API-5269 : Add support for GMSEC-style wildcard characters in OpenDDS m/w wrapper\\n* API-5270 : Allow users to register custom message validation function\\n* API-5271 : Functions to create Connection Manager in C binding are not handling exceptions\\n* API-5272 : Perl and Python3 bindings are not properly handling GMSEC API exceptions\\n* API-5274 : Add a hasField() method to Message class\\n* API-5284 : Develop string utils that can be used to validate header string, timestamp, and IP address values that go into message fields\\n* API-5285 : Algorithm to determine leap-year reports incorrect results\\n* API-5288 : The gmreq and gmrpl utilities do not create valid C2MS messages\\n* API-5290 : ResourceInfoGenerator should not give up if it cannot query stats on a disk\\n* API-5291 : Tracking fields are not cleared from message upon receiving error from m/w wrapper\\n* API-5292 : ConnectionManager requestDirective() methods do not add REQUEST-ID or MSG-ID\\n* API-5295 : Provide the means for users to use an alias in lieu of a schema ID when building a MistMessage\\n* API-5296 : Provide utility app to display all available message schema IDs and associated alias (if any)\\n* API-5297 : Fix documentation for publish() method that accepts a Config object\\n* API-5299 : ConnectionManager publishLog() does not include miscellaneous elements in message subject\\n* API-5301 : Validate Header String, IP Address, and Timestamp contents in Fields\\n* API-5302 : Add getSchemaLevel() method to Specification\\n* API-5306 : NODE and USER-NAME fields are not C2MS compliant\\n* API-5307 : Add C2MS-Pipeline message templates\\n* API-5308 : Add middleware wrapper for ActiveMQ CMS 3.9.5\\n* API-5309 : API does not create message templates for all schema levels\\n* API-5312 : Field getIntegerValue() and getUnsignedIntegerValue() report incorrect error when a value cannot be produced\\n* API-5313 : MistMessage does not properly convert legacy message into newer format\\n* API-5315 : Duplicate message in error string when validating fields in an array\\n* API-5316 : Validation incorrectly identifies name of field when comparing to field template in array\\n* API-5317 : API\\'s JNI MistMessage should not set standard fields when field count is zero\\n* API-5318 : Bolt wrapper is not reporting connection endpoint when multiple connections are used\\n* API-5319 : Fix Python3 and C# documentation regarding ConnectionManager\\'s setStandardFields()\\n* API-5332 : Message Bus includes homemade field in request message\\n* API-5333 : IBM MQ m/w wrapper includes homemade field in request messages\\n* API-5342 : API fails to handle subscription with callback with certain topics ending with \\'+\\'\\n* API-5345 : Do not associate a Specification object with an InternalMistMessage\\n\\n\\n==============================================================================\\n= GMSEC API 4.6 release notes (June 2019)\\n==============================================================================\\n\\n--- Change summary -----------------------------------------------------------\\n* API-5067 : Allow for user to set custom value for COUNTER field that is used by Heartbeat Service\\n* API-5102 : Concatenate Specification\\'s parse errors into string summary to be thrown\\n* API-5107 : Update InternalDeviceMessage::init to leverage message templates\\n* API-5109 : Update InternalMnemonicMessage::init to leverage message templates\\n* API-5129 : Simplify Message Validation\\'s exception string\\n* API-5130 : MistMessage needs data constructor that takes in data + specification\\n* API-5139 : Define a public-facing HeartbeatGenerator class\\n* API-5178 : API should check tracking fields when performing message validation\\n* API-5181 : Replace usages of std::auto_ptr with StdUniquePtr\\n* API-5183 : Update API example Heartbeat Service and Resource Service programs\\n* API-5185 : Add logging arguments to Bolt\\n* API-5186 : Fix Perl 4.x documentation\\n* API-5187 : Add Config constructor to the C# binding that accepts a string array\\n* API-5189 : MistMessage setValue() methods do not indicate if Field is a header field\\n* API-5190 : Internal Product File Message consructors are not initializing class member data\\n* API-5193 : Fix documentation issues within the SWIG-generated C# binding.\\n* API-5196 : INSTALL.txt does not include instructions for installing libxml2 and libxslt under Ubuntu and macOS\\n* API-5197 : Include JavaDoc for Java binding within gmsecapi.jar file\\n* API-5198 : The JAVA_LIBS definition in GMSEC API Platform configuration files is incorrect for OpenJDK use\\n* API-5199 : Message Bus client closes stdin file descriptor when connection to server fails\\n* API-5201 : Add setName() and clone() methods to Field class\\n* API-5203 : Log messages are always displayed when using SWIG-generated C# binding, regardless of the logging level\\n* API-5204 : Fix documentation issues within the SWIG-generated Python3 binding\\n* API-5205 : Add Config() constructor to Python3 binding that accepts a string list\\n* API-5206 : TimeUtil.format_time() returns SwigPyObject instead of string in Python3 binding\\n* API-5208 : Update GMSEC API utility apps to use ConnectionManager\\n* API-5209 : Trim the directory path from filename used in log messages\\n* API-5210 : Display file name and line number within Python3 log messages\\n* API-5211 : Remove AutoMutex from Perl and Python bindings\\n* API-5212 : Use of the Python3 binding shutdown_all_middlewares() results in dead-lock\\n* API-5213 : Boolean Field not created correctly when XML content uses \"1\" for field value\\n* API-5214 : Python3 binding does not register custom log handler with Core API\\n* API-5215 : Message addField(String, String) in Java binding does not allow for empty string value\\n* API-5216 : C# binding does not register custom log handler with Core API\\n* API-5217 : Identify Config.Initialize() in C# binding as being obsolete\\n* API-5218 : ProductFile GetContents() can throw unmanaged exception if object has no data contents\\n* API-5220 : Resource Service should not increment COUNTER field with each sample taken\\n* API-5221 : Resource Message published by Resource Service should include PUB-RATE field\\n* API-5223 : Scan GMSEC API 4.6 using Fortify and correct/disposition any deficiencies found\\n* API-5224 : Message Bus Reader Thread does not properly destroy data buffer\\n* API-5225 : MSG-ID field not included within C2CX HB messages generated by the API when operating with the 2014 GMSEC ISD\\n* API-5226 : C++ log macros cannot be compiled in extended namespace\\n* API-5227 : Internal ValueMap does not handle GMSEC_U32 value type\\n* API-5228 : Java applications cannot run on macOS when SIP is enabled\\n* API-5230 : Use python-config and python3-config to get compiler and linker flags\\n* API-5232 : GMSEC API Project File improperly references certain source modules\\n* API-5233 : AMQP m/w wrapper fails to free resource\\n* API-5234 : Generic JMS m/w wrapper failes to free resources\\n* API-5235 : IBM MQ (formerly WebSphere) m/w wrapper does not always retry connecting to broker\\n* API-5236 : Certain example programs for the SWIG-generated language bindings do not demonstrate usage of release() for disposing of received messages\\n* API-5237 : MistMessage needs to identify header fields when applying standard fields to message\\n* API-5238 : Environment Validator does not fully check if Java is installed on macOS\\n\\n\\n==============================================================================\\n= GMSEC API 4.5.1 release notes (February 26 2019)\\n==============================================================================\\n--- Change summary -----------------------------------------------------------\\n* API-5192 : API 3.x Request Message Kind should not be altered when message is published\\n* API-5202 : TYPE attribute should be ignored if KIND attribute is provided with Message definition\\n\\n\\n==============================================================================\\n= GMSEC API 4.5 release notes (February 12 2019)\\n==============================================================================\\no Support for Microsoft Visual Studio 2008, 2010, 2012, and 2013 has been dropped. Currently the API is built only with Visual Studio 2017.\\n\\no Support for Solaris 10 SPARC has been dropped.\\n\\no Support for Ubuntu 18.04 LTS is being provided.\\n\\no Java 8 (or later version) must be used to run Bolt and/or to use the Java binding of the GMSEC API.\\n\\no This release contains an additional interface for the C# language that can be\\n    used under Linux (using Mono) or under Windows.\\n\\n    Please refer to the example programs in GMSEC_API/examples/csharp and the\\n    code documentation in GMSEC_API_DOCS/manual/csharp_4x to learn more about\\n    the use of the new binding.\\n\\n    Note: In future releases, the separate and distinct C# .NET/CLI language\\n    binding will no longer be provided with the GMSEC API.\\n\\no This release contains an additional interface for the Python 3 language.\\n    This interface is based on the Python 3.6.5 interface.\\n\\n    Please refer to the example programs in GMSEC_API/examples/python3 and the\\n    code documentation in GMSEC_API_DOCS/manual/python3_4x to learn more about\\n    the use of the new binding.\\n\\no An OpenDDS middleware wrapper is now available for Windows and RHEL7.\\n\\no An IBM MQ 9.0 middleware wrapper is now available.\\n\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4730 : The API should be able to validate the values of fields as defined by the GMSEC ISD\\n* API-4977 : The C# LogEntry does not contain file name or line number information for the C# source file\\n* API-5036 : Provide support for IBM MQ (formerly WebSphere) 9.0\\n* API-5043 : Build and test the GMSEC API under Ubuntu 18.04 LTS\\n* API-5052 : Support message validation logic in the GMSEC API using XSD files\\n* API-5075 : Add SWIG-generated C# binding for Linux-based Mono projects\\n* API-5076 : Generate Javadoc JAR file for the Java binding of the GMSEC API\\n* API-5080 : Update API User\\'s Guide to remove certain unsupported features\\n* API-5085 : Update API User\\'s Guide to include all tracking field enable/disable options\\n* API-5088 : Tracking fields are not removed from C2CX Heartbeat messages\\n* API-5089 : The PROCESS-ID tracking field should not always be set to a U32 field type\\n* API-5091 : Upgrade Visual Studio 2008 Project/Solution files to the Visual Studio 2017 format\\n* API-5092 : Messages with excluded subject patterns should not be aggregated when Message Binning is enabled\\n* API-5094 : The API does not support multi-layered schema addendums\\n* API-5095 : Newly defined C2CX HB message tracking fields should not be added if 2014 or 2016 ISD is specified\\n* API-5096 : Allow for customizable time out period for receiving WebSphere messages\\n* API-5097 : API should not remove user-supplied tracking field(s) from messages\\n* API-5098 : StringUtil\\'s getValue<T>() method does not always report errors when given bad input\\n* API-5099 : InternalMistMessage not converting data values correctly with setValue()\\n* API-5100 : Field GetType() in C#/CLI (dotnet) binding should be renamed to GetFieldType()\\n* API-5101 : MistMessage needs a constructor that can convert a Message object into a MistMessage object\\n* API-5103 : Specification (class) is doing unnecessary work when validate() is called with NO_ENFORCEMENT configured\\n* API-5105 : Removed ConnectionManager\\'s setStandardFields() exception specification\\n* API-5110 : Clarify API documentation to mention user needs to first add COUNTER field (to the Heartbeat Service) before the API will increment the field\\n* API-5111 : Bolt server displays incorrect port\\n* API-5112 : Remove REPLY-UNIQUE-ID from request and response messages\\n* API-5113 : Offer ability for users to acquire Message Specifications\\n* API-5114 : StringUtil::toJSON isn\\'t escaping newline characters\\n* API-5116 : Fix Open-Response example programs\\n* API-5118 : Build the SWIG-generated C# binding of the API under Windows\\n* API-5119 : Refactor Specification copy-constructor\\n* API-5120 : Generate a 32-bit distro of the API using Visual Studio 2017\\n* API-5121 : Create a Python 3.x binding for the GMSEC API\\n* API-5122 : Rename SWIG-generated C# libraries to match GMSEC API convention\\n* API-5123 : Build the GMSEC API using JDK 8\\n* API-5124 : Setting JVM stack size proves fatal with use of Java 8\\n* API-5125 : Add support for IBM MQ m/w wrapper to Environment Validator and GMSEC Help utilities\\n* API-5126 : InternalConnectionManager does not need to check for viable Connection object\\n* API-5127 : The GMSEC API crashes when handling validation of a MSG.AMSG message\\n* API-5133 : Custom validation example programs for the GMSEC API need updating\\n* API-5134 : Scan GMSEC API software using HP Fortify\\n* API-5135 : Embellish error message when error occurs processing an XML or JSON field definition\\n* API-5136 : Add API method so a user can add a list of Field objects to a Message\\n* API-5137 : Implement middleware wrapper that supports OpenDDS\\n* API-5138 : Update API build framework to check if 3rd-party tools are installed\\n* API-5140 : Fix comments in Java 4.x binding to follow JavaDoc conventions\\n* API-5142 : InternalConnectionManager should create MistMessage messages\\n* API-5143 : Fix package names for JNI related classes\\n* API-5144 : Generate Doxygen style documentation for the SWIG-generated C# binding\\n* API-5147 : Connection unsubscribe() documentation should indicate that an exception can be thrown if error occurs\\n* API-5148 : Create Project and Solution files for C# example programs\\n* API-5149 : Update C# (dotnet) example programs to build using .NET Framework 4\\n* API-5150 : ActiveMQ middleware wrapper calls on unsafe ActiveMQ C++ library method\\n* API-5151 : Enhance gmreq utility program to allow for configurable request republish timeout period\\n* API-5153 : Exception is not handled by setHeartbeatServiceField()\\n* API-5154 : Update Environment Validator script for OpenDDS\\n* API-5155 : Update GMSEC Help utility application for OpenDDS\\n* API-5156 : Heartbeat Service should publish only one message when PUB-RATE is 0 (zero)\\n* API-5158 : API should use CONNECTION-ENDPOINT tracking field for C2MS compliancy\\n* API-5163 : Fix example programs so that they compile and that they produce legal messages\\n* API-5164 : Late destruction of ConnectionManager affects Java applications relying on MistMessage setStandardFields()\\n* API-5165 : Memory leak occurs when a received message fails validation\\n* API-5166 : PUB-RATE cannot be changed when restarting Heartbeat Service\\n* API-5167 : Unable to run GMSEC API based Java application using Java 10\\n* API-5168 : ConfigFile should report more concise error messages\\n* API-5169 : A Field definition in XML representation that spans multiple lines cannot be parsed\\n* API-5170 : Drop support for IBM WebSphere 7.5\\n* API-5171 : Sorting fields in a Message leads to a core dump\\n* API-5173 : API is not identifying tracking fields as header fields\\n* API-5174 : Python bindings do not allow for bytearray to be used for representing raw binary data\\n* API-5175 : Provide Python applications with the means to convert a Field object into a more specialized field object\\n* API-5179 : Replace usage of javah with javac to build JNI header file\\n* API-5182 : Depending on middleware, API publish operations may not be thread-safe\\n* API-5184 : ActiveMQ m/w wrapper does not log error messages from client library\\n\\n\\n==============================================================================\\n= GMSEC API 4.4.2 release notes (2018 March 14)\\n==============================================================================\\n--- Change summary -----------------------------------------------------------\\n* API-4933 : Provide IPv6 support for Message Bus\\n* API-5044 : Address the sections of the API which are using the deprecated Mac system-level function OSAtomicAdd32()\\n* API-5057 : Add option to gmpub to allow it to publish forever\\n* API-5070 : GMSEC_TimeSpec overloaded operators should not be defined in a namespace\\n* API-5073 : Example programs should run when configured with the strictest level of message validation\\n* API-5074 : API Makefile should not attempt to build Perl and Python bindings if swig is unavailable\\n* API-5077 : The XML templates used by the API for message validation do not include tracking fields\\n* API-5078 : Product File Message class is not inserting properly named field\\n* API-5081 : Build 32-bit version of GMSEC API under RHEL (CentOS) 7\\n* API-5082 : Config object is not passed to internal API by MistMessage constructor\\n* API-5083 : JMSConnection constructor is not throwing an exception on error\\n* API-5087 : CONNECTION-ID tracking field should be a U32 field\\n\\n\\n==============================================================================\\n= GMSEC API 4.4.1 release notes (2018 February 6)\\n==============================================================================\\n--- Change summary -----------------------------------------------------------\\n* API-4731 : Add message validation to receiving operations and expand validation configuration options\\n* API-4760 : Support builds on Windows 10\\n* API-4787 : Provide utility app for publishing a canned GMSEC message supplied by the user\\n* API-4798 : Add support for certain Connection class methods within Connection Manager\\n* API-4963 : Add cautionary statements to API User\\'s Guide and code documentation regarding Callbacks\\n* API-5033 : PROCESS-ID in information bus header needs to be U32 rather than I16\\n* API-5039 : MnemonicMessage should readily support MVAL request-type messages\\n* API-5045 : Report the key value in the error when attempting to retrieve a non-existing value from a Config object\\n* API-5046 : Connection state should not be CONNECTED if a connection authentication failure occurs\\n* API-5047 : Fix logic error with Bolt\\'s TypeValidater\\n* API-5048 : Perl and Python bindings should not be built if SWIG_HOME is not defined\\n* API-5049 : Define PERL5_LIB and PYTHON_INC within API Linux configuration files\\n* API-5051 : Resource Info Generator should not be adding certain fields to Resource Message\\n* API-5053 : Use major.minor name convention for ActiveMQ CMS library wrapper file\\n* API-5056 : Provide an interface to set standard fields in MistMessage\\n* API-5058 : Further simplify API utility programs\\n* API-5059 : Exceptions should not be thrown from class destructors\\n* API-5060 : Allow for registration of Event Callback before initializing Connection Manager\\n* API-5062 : Certain string characters need to be escaped or converted when generating JSON and XML data\\n* API-5063 : Python example programs not handling error when user passes no args\\n* API-5064 : Define macro that contains hex representation of API version number\\n* API-5065 : Avoid assuming a synchronous request is done when a response has not been received following a timeout\\n* API-5066 : The default republish period when issuing request messages should be GMSEC_REQUEST_REPUBLISH_NEVER (-1)\\n* API-5069 : Define RPATH within all GMSEC libraries built for Linux systems\\n* API-5072 : ConfigFileIterator of the Java binding returns non-persistent references to objects\\n\\n\\n==============================================================================\\n= GMSEC API 4.4 release notes (2017 Dec 15)\\n==============================================================================\\no This release contains an additional interface for the Python language.\\n    This interface is based on the Python 2.x interface and is built and tested\\n    with CPython. Please refer to the example programs in\\n    GMSEC_API/examples/python and the code documentation in\\n    GMSEC_API_DOCS/manual/python_4x to learn more about the use of the new\\n    binding.\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4292 : Add subscription exclusion filters to subscription definitions in ConfigFile\\n* API-4446 : Consolidate API string constants, especially config keys, into a single header file\\n* API-4671 : Execute middleware performance testing and generate a performance test report for GMSEC API 4.2\\n* API-4740 : Add a tracking field which contains a list of unique subscription subjects in a process\\n* API-4741 : Add a tracking field which shows the server to which a GMSEC Connection is currently connected\\n* API-4758 : Simplify the example programs\\n* API-4791 : Fix inconsistencies in the values of the FIELD_CLASS attributes in XML template files used for message validation\\n* API-4792 : Change the UNSET type attribute to VARIABLE in the XML template files used for message validation\\n* API-4796 : Fix various minor issues with the XML template files used for message validation\\n* API-4837 : Fix Magic Bus so that it handles requests for resource information\\n* API-4869 : Provide support for ActiveMQ CMS 3.9.4 and OpenSSL 1.0.2\\n* API-4881 : Update MistMessage constructors so that the Schema ID does not require the version number of the ISD\\n* API-4898 : Allow applications to register an EventCallback before connecting to the bus\\n* API-4899 : Setting the field storage container type after a message has been created can truncate the message\\n* API-4900 : API Environment Validator script does not always report the version number of the API\\n* API-4901 : Allow users to set configuration options for messages that are received from the GMSEC Bus\\n* API-4921 : Field::getStringValue does not properly convert Binary, I8, or U8 Field values to strings\\n* API-4934 : Fix the message encoding and decoding logic which prevents Fields from being identified as a header\\n* API-4938 : Augment Field class to allow type of \"BINARY\" in addition to \"BLOB\" and \"BIN\"\\n* API-4953 : Rename Doxygen documentation directories for API 3.x\\n* API-4955 : Users shall be able to change the rate of publication of the Heartbeat Service\\n* API-4964 : JNI objects created within Callbacks need to be destroyed after use\\n* API-4969 : The JVM stack size should be specified by the Generic JMS wrapper\\n* API-4973 : The Java API\\'s registerHandler should reset to the default Java handler when invoked with null\\n* API-4997 : Fix the memory leak in the Java Message.getXXField() functions\\n* API-4999 : Add a getStandardFields function to the ConnectionManager\\n* API-5003 : ConnectionManager::request(no callback) and reply() should be able to validate messages\\n* API-5015 : MessagePopulator class doesn\\'t add MSG-ID field\\n* API-5027 : Fix the memory leak in the Java ConnectionManager requestSimpleService()\\n* API-5028 : Fix output from Message toXML() so that the message configuration, if any, is properly indented\\n* API-5032 : Fix the connectionManagerCreate() function in the C binding so that it does not enable message validation\\n* API-5034 : MIST ignoring lower level message templates when overriding addendum is applied\\n* API-5035 : Infinite loop occurring when addendum is added with no header listed in directory\\n* API-5050 : Mark all deprecated classes and functions of the API\\n\\n\\n==============================================================================\\n= GMSEC API 4.3 release notes (2017 May 1)\\n==============================================================================\\no For Mac OS X users, this release may break binary compatibility with previous\\n    API 4.x releases.  This is due to a change in how the API is built on OS X.\\n    Previously, libstdc++ was being used; now libc++ is used.  (See item 4462\\n    below).  This change was made to prevent a sporadic crash from occurring\\n    when using various API functions.\\n\\no Beginning with API 4.3, strict validation of subjects used within Messages\\n    and for subscriptions will be performed to ensure that they are GMSEC-\\n    compliant, as defined by the GMSEC Interface Specification Document.\\n    Users wishing to continue using non-strict validation may employ the use\\n    of the \"SEC-POLICY\" configuration option and set it equal to the value\\n    \"API3\".\\n\\no Beginning with API 4.3, support for Solaris 10 x86 and HP-UX has been dropped.\\n\\no Users of the Java language binding should be aware that the\\n    Connection.disconnect() and ConnectionManager.cleanup() functions now\\n    throw a GMSEC_Exception.  This means that any code which calls these\\n    functions must be encapsulated by a try..catch statement which handles\\n    a GMSEC_Exception, or else the application will not be able to compile.\\n    Previously compiled software will still be able to run, just as it did\\n    prior to this change.  This change was made in response to DR #4788 \\n    (See below) so that exceptional states can be handled more gracefully.\\n    \\no The Perl language binding of the API is being pared down to only include\\n    support for synchronous operations.  Features involving asynchronous\\n    operations (e.g. registration of callbacks, auto-dispatcher, etc.) have\\n    been removed from the binding.  Additionally, LogHandler subclassing has\\n    been removed.  The removal of these features is due to an instability\\n    discovered when executing subclass implementations in a multithreaded\\n    process, such as when using the AutoDispatcher or invoking a custom\\n    LogHandler from a non-main thread.\\n\\t\\no Configurable message bins (aggregated messages) now support subscription-type wildcards (e.g. \\'>\\', \\'*\\', and \\'+\\')\\n\\no The ConnectionManager, MISTMessage, DeviceMessage, ProductFileMessage,\\n    MnemonicMessage, and various iterator classes (Collectively referred to\\n    as the Messaging Interface Standardization Toolkit (MIST)) have been\\n    added to the 4.X Perl interface.\\n\\no The Config class can now ingest and output JSON data, as well as ingest\\n    strings of whitespace-delimited \"key=value\" pairs, much like command-line\\n    arguments used when running example programs such as gmpub and gmsub.\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4147 : Refactor implementation of the ActiveMQ Transport Listener\\n* API-4462 : Build API on Mac OS X using libc++ (not libstdc++)\\n* API-4512 : Improve efficiency of InternalSpecification\\n* API-4513 : Move InternalField::getValue() to StringUtil\\n* API-4514 : Add setSubject function to Message class\\n* API-4515 : Add support for custom Message validation\\n* API-4516 : Expand 4.X Perl Unit Testing Coverage\\n* API-4517 : Add support for MIST into the 4.X Perl interface\\n* API-4518 : Add Device, Mnemonic and Product File Iterator support in C binding\\n* API-4559 : Config class should be able to ingest alternative types of String data\\n* API-4561 : Update gmhelp to provide bind/connect connection examples for ZeroMQ\\n* API-4567 : Expand OS support for 4.X Perl interface\\n* API-4572 : Embellish error information included with exceptions thrown from Bolt\\n* API-4596 : Prevent MagicBus middleware wrapper from attempting to communicate with MB server once it is known that the connection has been lost\\n* API-4597 : Cannot create a BinaryField object with the Perl interface\\n* API-4598 : Comparison made against configuration string value should be case insensitive\\n* API-4600 : In the C++ and C example programs, unsubscribe should only be called on active subscriptions\\n* API-4601 : Error checking should by default be performed on subscription subjects with wildcard operators\\n* API-4610 : Create a Python language binding\\n* API-4615 : Create example programs and unit tests for Python language binding\\n* API-4616 : Incorrect cast types are used in Java binding when constructing C++ fields\\n* API-4618 : Users should be able to add or change Heartbeat Service fields\\n* API-4619 : Message template files do not use correct XML encoding syntax\\n* API-4657 : Field should be able to ingest JSON with numeric type syntax\\n* API-4662 : Implement a public toJSON() method for Config\\n* API-4669 : Config functions getFirst and getNext in Perl interface do not work\\n* API-4694 : Republish period for asynchronous request is not being properly handled\\n* API-4699 : Configurable Message Bin fails to handle message subject using wildcard element\\n* API-4707 : Replace use of deprecated OSAtomicAdd with alternative function for Mac OS X\\n* API-4717 : Remove references of API 3.x header files from API 4.x modules\\n* API-4721 : Address issues reported by HP Fortify\\n* API-4729 : Field.getString() should be Field.getStringValue() in the Java 4.X API\\n* API-4743 : Re-order field type and name when displaying Field XML information\\n* API-4744 : Issue Request-kind message when Connection Manager requestDirective() is called\\n* API-4751 : Make the decoding of a message packet more efficient\\n* API-4768 : Remove Field descriptions from validation error messages\\n* API-4788 : Update documentation and example programs to indicate that Connection::disconnect() can throw an exception in rare cases\\n* API-4803 : Non-ASCII Unicode characters cause String truncation when retrieving data from the Java language binding\\n* API-4804 : Improve error messages related to MnemonicMessage\\'s dependent field RAW-VALUE\\n* API-4807 : The getField functions should report the name of the requested field when unable to retrieve it from a message\\n* API-4809 : GMSEC_Exception does not initialize base class (Exception)\\n* API-4840 : Fix MistMessage to allow a value of 0 (zero) for F32 fields\\n* API-4853 : Client applications crash when using a Magic Bus fail-over configuration and yet only one broker is running\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.2.2 release notes (2016 December 16)\\n==============================================================================\\no This is a patch release that includes a couple of bug fixes.\\n\\no (Fastpath) Improvements to Configurable Message Binning that allow users to\\n    configure which messages, if any, should be excluded from being binned.\\n    Users can also employ the use of wild-cards when indicating which messages\\n    to bin or exclude from binning.\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4509 : Add ability to exclude messages, using subject, from being added to Configurable Message Bins\\n* API-4655 : Fixed bug related to how message tracking field options are processed by the Connection class\\n* API-4685 : Resolve dead-lock issue when call to publish() is made within a callback that was summoned by dispatch()\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.2.1 release notes (2016 November 9)\\n==============================================================================\\no This is a patch release to correct a bug in a method that is used by the Time Utility functions.\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4609 : Fix StringUtil::getTimeFromString() so that it can process time stamps that do not include fractional time\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.2 release notes (2016 October 31)\\n==============================================================================\\no This release contains an additional interface for the Perl language.\\n\\tThis new interface exists in parallel with the existing one, so all\\n\\texisting software that utilizes the GMSEC API will continue to function\\n\\twithout interruption. However, new software development should utilize\\n\\tthis new interface, as it is safer and leaner than the existing interface.\\n\\t\\no User-defined message validation has been added to the Messaging Interface\\n\\tStandardization Toolkit (MIST) portion of the API.  The ConnectionManager\\n\\tand Specification classes can now be configured to validate user-defined\\n\\tmessages, as would be defined in an addendum to the GMSEC ISD.  More\\n\\tinformation on this functionality is outlined in the GMSEC API User\\'s Guide\\n\\tin the section named, \"Addendums to the ISD.\"\\n\\t\\no (Fastpath) The API\\'s internal classes have been refactored to use Exceptions\\n\\tfor error handling rather than checking on a Status object returned from\\n\\teach step of an operation.  This has in turn improved the efficiency of\\n\\tthe API by reducing the number of instruction cycles the CPU must execute\\n\\tto perform operations such as publishing and receiving messages.\\n\\t\\n--- Change summary -----------------------------------------------------------\\n* API-3844 : Implement MIST Build 3\\n* API-3963 : Add source file name and line number to the Java LogEntry\\n* API-4122 : Add Perl interface for core (non-MIST) API 4.x\\n* API-4156 : Refactor Core API C++ Framework\\n* API-4334 : Fastpath: Streamline Connection publish() and receive() with seamless enhancements\\n* API-4392 : Add methods that allow users to easily get native value from a Field object\\n* API-4396 : Protect against concurrent access to the list of pending requests in RequestThread\\n* API-4399 : Prevent overwriting of EVENT-TIME field in MIST message field populator\\n* API-4400 : Java binding of API fails to catch exceptions thrown by the C++ layer\\n* API-4435 : Fastpath: Configurable Message Bins\\n* API-4444 : Catch exceptions in StdThread and remove include statements for API 3.x header files\\n* API-4481 : Stop Resource Message Service when ConnectionManager is destroyed\\n* API-4490 : Allow for XML or JSON data containing fields with an empty value to be parsed without error\\n* API-4501 : Add missing Time Utility functions to C binding of API\\n* API-4506 : Implement Mnemonic addSample() within JNI-layer of Java binding\\n* API-4519 : Increment COUNTER field in C2CX Heartbeat Messages\\n* API-4537 : Fix Java binding to return correct StatusClassification and StatusCode\\n* API-4570 : Prevent memory leak when Message class copy-constructor is called\\n* API-4571 : Bolt Server should report version information that correlates with the version of the API\\n* API-4582 : Fix Bolt middleware wrapper so that it does not throw an exception when a timeout occurs while attempting to receive a message\\n* API-4591 : Fix Message class to support U64 fields\\n* API-4595 : Make InternalConnection\\'s insertTrackingFields() method thread-safe\\n* API-4606 : Report correct message kind after ingesting XML or JSON data\\n* API-4607 : Mitigate thread-safe issue within AMQP middleware wrapper.\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.1 release notes (2016 July 25)\\n==============================================================================\\n\\no IMPORTANT: This version corrects a mild memory leak in the 4.X Java Callback\\n\\thandling system. All 4.X interface callbacks are now abstract classes,\\n\\tinstead of interfaces. This requires all code using these classes to \\n\\tchange their reference to Callback, ReplyCallback, and EventCallback to\\n\\tuse the keyword \"extends\" instead of \"implements\".\\n\\tThis change was necessary to ensure that callback memory is properly cleaned\\n\\tup.\\n\\tClient programs previously built with version 4.0 or 4.0.1 of the API should\\n\\tbe recompiled if the plan is to use API 4.1; otherwise undefined runtime\\n\\tbehavior can be expected.\\n\\no Corrected several coding defects identified by static code analysis\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4250 : Address Fortify issues within 4.0.1 (see important note above)\\n* API-4295 : Incorporate memory management corrections in C# binding\\n* API-4332 : Config class allows merging of config objects\\n* API-4353 : Correct logic in autodispatcher shutdown timing\\n* API-4357 : API will now warn users if log file cannot be opened\\t\\n* API-4358 : C2 NSS wrapper now frees resources (Compat C2 version only)\\n* API-4375 : ZeroMQ wrapper now employes the zero copy concept\\n* API-4385 : ConnectionManager requestDirective() method was not setting appropriate value for RESPONSE field\\n* API-4390 : WebSphere middleware wrapper asynchronous status check fails to dispatch event\\n* API-4391 : MagicBus fails to deliver messages to subscribers when two brokers are being used\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.0.1 release notes (2016 May 16)\\n==============================================================================\\n\\no Added new Connection and Publish-level Config options which expose the\\n\\tWebSphere MQ Asynchronous Put Response client option.  This new option\\n\\tallows users to toggle this functionality on or off and potentially\\n\\tachieve a higher throughput rate of message traffic when using\\n\\tthe WebSphere MQ middleware.  For more information on this topic,\\n\\tplease see the Installation and Configuration Guide Section 7.7\\n\\tas well as the current and legacy gmpub_wsmq_async example programs.\\n\\no Added the PublishWithConfig function to the 3.X C.v1 and C.v2 interfaces.\\n\\n--- Change summary -----------------------------------------------------------\\n* API-4274 : Exposure of WebSphere MQ Asynchronous Put Response Functionality\\n\\n\\n\\n==============================================================================\\n= GMSEC API 4.0 release notes (2016 April 04)\\n==============================================================================\\n\\no This release contains an additional interface for the Java, C, C++, and C#\\n \\tlanguages. This new interface exists in parallel with the existing one, \\n \\tso all existing software that utilizes the GMSEC API will continue to \\n \\tfunction without interruption. However, new software development should \\n \\tutilize this new interface, as it is safer and leaner than the existing\\n \\tinterface.\\n \\t\\no This release also contains an expanded set of functionality for the \\n\\tMessaging Interface Standardion Toolkit (MIST). Available for the new \\n\\tinterface only, this functionality includes validation of all GMSEC ISD\\n\\tdescribed messages, automatic production of resource messages, and \\n\\tsimplified production of Mnemonic, Device, and Product messages. For more\\n\\tinformation, please see the GMSEC API User\\'s Guide or the doxygen \\n\\tfor the ConnectionManager (available in GMSEC_API\\\\docs\\\\cpp_4x)\\n\\no New middleware supported with this release are Rabbit MQ via AMQP 1.0 and ZeroMQ. \\n\\t\\n--- Change summary -----------------------------------------------------------\\n* API-4171 : ZeroMQ wrapper drops messages due to interrupted function calls\\n* API-4146 : ActiveMQ Durable Subscription functionality has been corrected to work with separate Connection sessions.\\n* API-4136 : Added ability for users to provide a Config object to a Java Subscribe call\\n* API-4110 : Add support for anonymous classes in new Java interface\\n* API-4109 : Add automatic flushing of cached WebSphereMQ topic handles\\n* API-4069 : Add ability to disable request/reply operations for a Connection object, to simplify topic management\\n* API-4066 : Correct memory leak in open response reply delivery\\n* API-4047 : Conform new C# interface functions to industry standard\\n* API-4009 : Add support for U64 field to new interface\\n* API-4004 : Add support for ZeroMQ middleware, see API User\\'s Guide for details\\n* API-3971 : EventCallback reports the ConnectionEvent which resulted in the Callback invocation\\n* API-3959 : Repair validation error in InteralFieldDescriptor\\n* API-3924 : Add ability for JSON formatted GMSEC messages to be used as the middleware payload\\n* API-3905 : Add asynchronous message publishing\\n* API-3836 : Add JSON support to the Message and Field interfaces\\n* API-3811 : Add configurable TCP buffer to Bolt middleware client\\n* API-3777 : Add \"+\" operator to allowed subscription wildcard set\\n* API-3747 : Allow configuration of backlog queue size for the listen-socket used by Bolt server\\n* API-3737 : Add support for ActiveMQ CMS 3.8.4\\n* API-3639 : Add ability for API to report asynchronous middleware events\\n* API-3622 : Add support for U8\\n* API-3621 : Correct issue with Message Bus error class reporting\\n* API-3605 : Normalize middleware disconnect behavior\\n* API-3600 : Add support for current Mac versions\\n* API-3576 : Add support for AMQP 1.0, tested with Rabbit MQ\\n* API-3570 : Correct bug in Connection::Unsubscribe(), where subscription count was not correctly updated\\n* API-3550 : Correct issue in CMSConnection::GetMWINFO() that could product a segfault.\\n* API-3548 : Implement connection global unique identifier\\n* API-3451 : Correct issue with MIST loading templates on Linux\\n* API-3443 : Create new interface for Java, C, C++, C#\\n* API-3421 : Add new functional block for MIST\\n* API-3411 : Correct ConfigFile error handling\\n* API-3377 : Add exclusion filters to connection to allow simpler message filtering\\n* API-2604 : Add support for WebSphere MQ 8\\n* API-2291 : Add support for Component style configuration to ease XML configuration loading\\n\\n\\n[For change descriptions from prior releases of the GMSEC API, please send your request to gmsec-support@lists.nasa.gov]\\n'},\n",
       " {'repo': 'nasa/bingo',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '![Bingo Logo](media/logo.png)\\n\\nmaster: [![Build Status](https://travis-ci.com/nasa/bingo.svg?branch=master)](https://travis-ci.com/nasa/bingo) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingo/badge.svg?branch=master)](https://coveralls.io/github/nasa/bingo?branch=master)\\n\\ndevelop: [![Build Status](https://travis-ci.com/nasa/bingo.svg?branch=develop)](https://travis-ci.com/nasa/bingo) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingo/badge.svg?branch=develop)](https://coveralls.io/github/nasa/bingo?branch=develop) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/9fe09cffafe64032962a82f7f1588e9f)](https://www.codacy.com/app/bingo_developers/bingo?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=nasa/bingo&amp;utm_campaign=Badge_Grade)\\n\\n## General\\nBingo is an open source package for performing symbolic regression, Though it \\ncan be used as a general purpose evolutionary optimization package.  \\n\\n### Key Features\\n  * Integrated local optimization strategies\\n  * Parallel island evolution strategy implemented with mpi4py\\n  * Coevolution of fitness predictors\\n  \\n### Note\\nAt this point, the API is still in a state of flux. The current release has a \\nmuch more stable API but still lacks some of the features of older releases.\\n\\n## Getting Started\\n\\n### Cloning with git\\nThe Bingo repository uses git submodules so make sure to clone all the\\nsubmodules when cloning.  Git has an easy way to do this with:\\n```shell\\ngit clone --recurse-submodules ...\\n```\\n\\n### Dependencies\\nBingo is intended for use with Python 3.x.  Bingo requires installation of a \\nfew dependencies which are relatively common for data science work in python:\\n  - numpy\\n  - scipy\\n  - matplotlib\\n  - mpi4py (if parallel implementations are to be run)\\n  - pytest, pytest-mock (if the testing suite is to be run)\\n  \\nA `requirements.txt` file is included for easy installation of dependencies with \\n`pip` or `conda`.\\n\\nInstallation with pip:\\n```shell\\npip install -r requirements.txt\\n```\\n\\nInstallation with conda:\\n```shell\\nconda install --yes --file requirements.txt\\n```\\n\\n### BingoCpp\\nA section of bingo is written in c++ for increased performance.  In order to \\ntake advantage of this capability, the code must be compiled.  See the \\ndocumentation in the bingocpp submodule for more information.\\n\\nNote that bingo can be run without the bingocpp portion, it will just have lower \\nperformance.\\n\\nIf bingocpp has been properly installed, the following command should run \\nwithout error.\\n```shell\\npython -c \"from bingocpp\"\\n```\\n\\nA common error in the installation of bingocpp is that it must be built with \\nthe same version of python that will run your bingo scripts.  The easiest way \\nto ensure consistent python versioning is to build and run in a Python 3 \\nvirtual environment.\\n\\n### Documentation\\nSphynx is used for automatically generating API documentation for bingo. The \\nmost recent build of the documentation can be found in the repository at: \\n`doc/_build/html/index.html`\\n\\n\\n## Running Tests\\nAn extensive unit test suite is included with bingo to help ensure proper \\ninstallation. The tests can be run using pytest on the tests directory, e.g., \\nby running:\\n```shell\\npython -m pytest tests \\n```\\nfrom the root directory of the repository.\\n\\n## Usage Examples\\nThe best place to get started in bingo is by going through the jupyter notebook\\ntutorials in the [examples directory](examples/). They step through several of\\nthe most important aspects of running bingo with detailed explanations at each\\nstep.  The [examples directory](examples/) also contains several python scripts\\nwhich may act as a good base when starting to write your own custom bingo\\nscripts.\\n\\n\\n## Contributing\\n1. Fork it (<https://github.com/nasa/bingo/fork>)\\n2. Create your feature branch (`git checkout -b feature/fooBar`)\\n3. Commit your changes (`git commit -am \\'Add some fooBar\\'`)\\n4. Push to the branch (`git push origin feature/fooBar`)\\n5. Create a new Pull Request\\n\\n\\n## Versioning\\nWe use [SemVer](http://semver.org/) for versioning. For the versions available, \\nsee the [tags on this repository](https://github.com/nasa/bingo/tags). \\n\\n## Authors\\n  * Geoffrey Bomarito\\n  * Tyler Townsend\\n  * Jacob Hochhalter\\n  * Ethan Adams\\n  * Kathryn Esham\\n  * Diana Vera\\n  \\n## License \\nCopyright 2018 United States Government as represented by the Administrator of \\nthe National Aeronautics and Space Administration. No copyright is claimed in \\nthe United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\nThe Bingo Mini-app framework is licensed under the Apache License, Version 2.0 \\n(the \"License\"); you may not use this application except in compliance with the \\nLicense. You may obtain a copy of the License at \\nhttp://www.apache.org/licenses/LICENSE-2.0 .\\n\\nUnless required by applicable law or agreed to in writing, software distributed \\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR \\nCONDITIONS OF ANY KIND, either express or implied. See the License for the \\nspecific language governing permissions and limitations under the License.\\n'},\n",
       " {'repo': 'nasa/uam-apis', 'language': 'Shell', 'readme_contents': ''},\n",
       " {'repo': 'nasa/swSim',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# swSim\\n\\nProject Name: Solid Wave Sim (swSim)\\nVersion 1.0\\nAuthors: Erik Frankforter, Elizabeth Gregory, Bill Schneck\\nDescription:\\nSolid Wave Simulation (swSim) is software that solves heterogeneous, anisotropic elastodynamic equations for ultrasonic NDE simulation. A time-explicit staggered grid finite difference (FD) scheme is employed and solved on graphics processing units (GPUs). Parallelism via the Message Passing Interface (MPI) allows for deployment on a cluster, or on a single computer with one or more GPUs. Wavefield data is output using Visualization Toolkit (VTK) file formats for 3D rendering capabilities via open source tools, and a kernel composition module allows high-level registration of sequences of matrix operations, providing extensibility in equations and physics solved.\\n\\n# License: \\nCopyright 2020 United States Government as represented by the Administrator of the National \\nAeronautics and Space Administration. No copyright is claimed in the United States under \\nTitle 17, U.S. Code. All Other Rights Reserved. See Appendix A for 3rd party licenses.\\n\\nThe Solid-Wave Sim (swSIM) platform is licensed under the Apache License, Version 2.0 (the \\n\"License\"); you may not use this file except in compliance with the License. You may obtain \\na copy of the License at http://www.apache.org/licenses/LICENSE-2.0. \\n\\nUnless required by applicable law or agreed to in writing, software distributed under the \\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \\neither express or implied. See the License for the specific language governing permissions \\nand limitations under the License.\\n\\n# Notices:\\n\\nGoogletest is a product of Google Inc. and is subject to the following:\\n \\nCopyright 2008, Google Inc. All rights reserved.\\n           \\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\n# Documentation\\n\\nAvailable at https://nasa.github.io/swSim/\\n\\n# Building swSim\\n1. Download latest release\\n2. Ensure correct dependencies are installed, and are in the search path\\n  - MPI\\n  - VTK >= 9.0 (built and linked with the same MPI being used for PanNDE)\\n  - gcc >= 7.3\\n  - cmake >= 3.13.5\\n  - libxml2 >-2.9.10\\n  - cuda >= 10.1\\n3. Ensure Python3 is installed for some included post processing utilities\\n  - this is optional, if the utilities are not desired\\n4. cd to `swSimroot/directory`\\n5. generate compilation instructions using `cmake -B build -S .`\\n6. compile with `make -C build`Create build files using CMAKE 3.13 or higher\\n\\n\\n# Creating a Case\\n\\nThe testing directory contains four test cases:\\n* 30x30ElongatedQausiIsoCFRPTestPlate <- necessary for Code Testing\\n* 120x60CrossPlyCFRPTestPlate\\n* 120x60CrossPlyCFRPTestPlate_course \\n* 180x180ElongatedQausiIsoCFRPTestPlate\\n\\nEach of these subdirectories contain a .py file that can be ran to generate the required input files. We recommend creating scripts to generate the input files. \\n\\n\\n# Running a Case\\n\\nmpiexec -np <number_of_processes> ./bin/rsg_sim Path/to/Inputs.xml\\n\\n# Input File Requirements\\n * Envelope size in voxels (x, y, & z).\\n * Size of voxels in meters (x, y, & z).\\n * Root name of the geometry file (example: \\'Geometry\\' for Geometry.raw and Geometry_planCounts.raw)\\n * Name of the rotations file (example: \\'rotations.bin\\')\\n * Total number of materials \\n * Materials with number corresponding to the values in the geometry file:\\n    - density\\n    - 21 elements or the stiffness Matrix\\n * Total number of time steps to be simulated.\\n * Size of the time step in seconds\\n * Total number of excitations\\n * Excitations with unique ID number:\\n    - radius of the excitation\\n    - location of the excitation in meters\\n    - initial time step the excitation is active\\n    - the duration of the excitation in time steps \\n    - the orientation of the excitation (x, y, & z)\\n    - a scaling vector of the excitation (x, y, & z)\\n\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'nasa/api-docs',\n",
       "  'language': 'SCSS',\n",
       "  'readme_contents': '# api-docs: gh-pages\\n\\n### nasa/api-docs repository contains the front-end for http://api.nasa.gov/. This site is currently CNAME mapped to NASA domain, but in the event the page is down, you can access it through Github Pages here: http://nasa.github.io/api-docs/ .\\n\\nThis is the current iteration of api.nasa.gov.\\n\\nThis project contains API listings of available NASA public APIs and well as a GSA API key form to allow NASA branded API pages to be integrated with the GSA api.data.gov system. Our listing is currently **incomplete** as we are currently displaying the more simple API\\'s in our .json storage system.\\n\\nFor other US government agency API pages, see https://api.data.gov/\\n\\n## Features\\n\\n* Obtain an API key by filling out the form in the **Generate API Key** Section\\n* Information about the usage capabilities of your API in the **Authentication** Section\\n* A list of a few NASA publics API\\'s and how to use them in the **Browse APIs** Section\\n\\n\\n## Libraries and Software\\n\\napi.nasa.gov utilizes the following libraries and resourses:\\n\\n* NASA Web Design Service(CSS and JS): A style system created in order to help NASA websites have a more unified look. You can find their info here https://nasa.github.io/nasawds-site/\\n* JQuery (JS): A library that simplifies the javascript coding process\\n\\nThe API information that our site hosts is currently archived in our *apis.json* folder which is then read and generated into the page dynamically.\\n\\n## NOTICE: NASA does not host/develop these APIs\\nWe only map the orginal developer\\'s endpoint to one of our api.nasa.gov endpoints. Basically, api.nasa.gov acts as a passthrough service. Please do not contact us with technical questions about a particular API as <b> we do not have access to most API production environments</b>. You can follow links on our site to get more information about each API and use the contact information on those pages to reach the people who control each API.\\n\\n### If you are a NASA civil servant or contractor and wish to add an API to api.nasa.gov, please contact <a href=\"mailto.nasa-data@lists.arc.nasa.gov\">nasa-data@lists.arc.nasa.gov</a>.\\n\\n**Site Developer**: Jenna Horn\\n\\n**NASA Official**: <a href=\"mailto.Brian.a.Thomas@nasa.gov\">Brian Thomas</a>\\n'},\n",
       " {'repo': 'nasa/harmony-regression-tests',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Harmony Regression Tests\\n\\n# Running the Tests\\n\\nEach test suite is run in a separate Docker container using a temporary image built at test time.\\n`conda` is used for dependency management. The two steps for each test suite are building and\\nrunning the associated image.\\n\\n## Install Prerequisites\\n\\n* [Docker](https://www.docker.com/get-started)\\n\\n## Set Up Authentication\\nCreate a .netrc file in the `test` directory of this repository. It must contain\\ncredentials for on logging into [Earthdata Login production](https://urs.earthdata.nasa.gov)\\nfor tests run against Harmony production or\\n[Earthdata Login UAT](https://uat.urs.earthdata.nasa.gov) for Harmony SIT and\\nUAT environments.\\n\\nExample `test/.netrc` that can log into both environments:\\n\\n    machine urs.earthdata.nasa.gov login SOME_USER password SOME_PASSWORD\\n    machine uat.urs.earthdata.nasa.gov login SOME_UAT_USER password SOME_UAT_PASSWORD\\n\\nThis file will be copied to into the docker images and used when running the\\nnotebooks. The `.gitignore` file will prevent this from being committed to the\\ngit project, but we recommend providing a user account with minimal privileges\\nfor test purposes.\\n\\n## Build the Images\\n\\n    $ cd test\\n    $ make images\\n\\n`make -j images` can be used to make the images in parallel (faster), although this may lead to\\nDocker Desktop instabilities\\n\\n## Run the Notebooks\\n\\n    $ cd test\\n    $ export HARMONY_HOST_URL=<url of Harmony in the target environment>\\n    $ ./run_notebooks.sh\\n\\nOutputs will be in the `output` directory.\\n`HARMONY_HOST_URL` for SIT would be `https://harmony.sit.earthdata.nasa.gov`\\n\\n# Running the Tests in an AWS for Same-Region Data Access\\n\\nHarmony runs in the AWS us-west-2 region and offers additional access methods for\\nclients running in the same region.  We have provided a Terraform deployment to\\nease test execution in this region.\\n\\n## Create Terraform Autovars File\\nIn the `terraform` directory create a file called `key.auto.tfvars` and\\nadd a single line indicating the name of the ssh public key file that\\nshould be used for the EC2 instance that runs the notebooks.\\n\\nThis file name is the name of the S3 file created in the Harmony ssh key bucket as described in the Harmony project README.md.\\n\\nExample:\\n```\\nkey_name = \"harmony-sit-my-key-name\"\\n```\\n\\n## Execute the Tests\\n\\n**Important**: The following steps allocate resources in AWS. To ease repeated\\ntests and troubleshooting, they also don\\'t automatically clean up the instance\\nthey create.  See \"Clean Up Test Resources\" to ensure you are minimizing costs\\nby cleaning up resources.\\n\\nFirst create a `.env` file in the top level directory by copying in the `dot_env` file and filling\\nin the proper values. Then execute the following.\\n\\n    $ cd script\\n    $ export HARMONY_ENVIRONMENT=<uat|sit|sandbox|prod>\\n    $ ./test.sh\\n\\nOutput will be in the bucket specified with the `REGRESSION_TEST_OUTPUT_BUCKET`\\nenvironment variable with a folder for each notebook.\\n\\n## Clean Up Test Resources\\n\\nThe prior scripts do not clean up allocated resources.  To remove the resources\\nused to run the test, run.\\n\\n    $ terraform destroy\\n\\nTests outputs are not automatically deleted.\\n\\n# Notebook Development\\n\\nNotebooks and support files should be placed in a subdirectory of the `test` directory.\\n\\nFor example, in the `harmony` directory we have\\n\\n```\\n├── Harmony.ipynb\\n├── __init__.py\\n├── environment.yaml\\n└── util.py\\n```\\n\\n Notebook dependencies should be listed in file named `environment.yaml` at the top level of the\\n subdirectory. The `name` field in the file should be `papermill`. For example:\\n\\n ```yaml\\n name: papermill\\nchannels:\\n  - conda-forge\\n  - defaults\\ndependencies:\\n  - python=3.7\\n  - jupyter\\n  - requests\\n  - netcdf4\\n  - matplotlib\\n  - papermill\\n  - pytest\\n  - ipytest\\n```\\n\\n## Generating a Dependency Lockfile\\nTo increase runtime efficiency, the build relies on [conda-lock](https://pypi.org/project/conda-lock/). This is used to create a dependency lockfile that can be used\\nby conda to more efficiently load dependencies. The Docker build expects a lockfile\\nnamed `conda-linux-64.lock` to exist at the top level of a notebook directory (next to\\nthe `environment.yaml` file).\\n\\nTo build the lockflie install `conda-lock` by following the directions provided on its website. Then generate the lockfile for your notebook by running the following:\\n```\\nconda-lock -f environment.yaml -p linux-64\\n```\\n\\nTest notebooks should not rely on other forms of dependency management or expect user input.\\nThey _should_ utilize the `harmony_host_url` global variable to communicate with Harmony\\nor to determine the Harmony environment. This variable is set by `papermill` - see the\\n`Harmony.ipynb` for how to make use of this variable. More information can be found\\nin the [papermill](https://papermill.readthedocs.io/en/latest/usage-parameterize.html)\\ndocumentation on setting parameters.\\n\\nNew test suites must be added to the `Makefile`. A new `name-image` target (where name is the name of\\nthe test suite) should be added (see the `harmony-image` example), and the new image target\\nshould be added as a dependency of the `images` target. The docker image should have a name like\\n`harmony/regression-tests-<base_name>`, where `base_name` is the name of the test suite.\\n\\nFinally, add the image base name to the `images` array on line 6 of the `run_notebooks.sh` file.\\nFor instance, if the image is named `harmony/regression-tests-foo`, then we would add `foo` to the\\narray.\\n\\nThe `run_notebooks.sh` file can be used as described above to run the test suite. Notebooks are\\nexpected to exit with a non-zero exit code on failure when run from `papermill`.\\n'},\n",
       " {'repo': 'nasa/sch_lab',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/sch_lab/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/sch_lab/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : App : Scheduler Lab\\n\\nThis repository contains NASA\\'s Scheduler Lab (sch_lab), which is a framework component of the Core Flight System.\\n\\nThis lab application is a non-flight packet scheduler application for the cFS Bundle. It is intended to be located in the `apps/sch_lab` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes sch_lab as a submodule), which includes build and execution instructions.\\n\\nsch_lab is a simple packet scheduler application with a one second resoluton.\\n\\nTo change the list of packets that sch_lab sends out, edit the schedule table located in the platform include file: fsw/platform_inc/sch_lab_sched_tab.h\\n\\n## Version History\\n\\n### Development Build: v2.4.0-rc1+dev32\\n\\n- Removes app registration call, `CFE_ES_RegisterApp()` since applications do not need to register themselves.\\n- See <https://github.com/nasa/sch_lab/pull/76>\\n\\n### Development Build: v2.4.0-rc1+dev26\\n\\n- Fix #67, Update sequence count in transmitted messages\\n- Fix #69, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/sch_lab/pull/71>\\n\\n### Development Build: 2.4.0-rc1+dev12\\n\\n- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the \"raw\" message cmd/tlm types in definition\\n- See <https://github.com/nasa/sch_lab/pull/59>\\n\\n### Development Build: 2.4.0-rc1+dev9\\n\\n- Update to use MSG module. Replaces deprecated SB APIs with MSG\\n- See <https://github.com/nasa/sch_lab/pull/58>\\n\\n### Development Build: 2.4.0-rc1+dev6\\n\\n- Adds header guard (the other warning on the ticket was already resolved)\\n- See <https://github.com/nasa/sch_lab/pull/53>\\n\\n### Development Build: 2.4.0-rc1+dev2\\n\\n- Reorganize the sch_lab table so it matches the sample_app usage and format.\\n- See <https://github.com/nasa/sch_lab/pull/52>\\n\\n### Development Build: 2.3.0+dev37\\n\\n- Fixes schedule table documentation\\n- Add baseline and build number to version reporting\\n- See <https://github.com/nasa/sch_lab/pull/48>\\n\\n### Development Build: 2.3.7\\n\\n- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.\\n- See <https://github.com/nasa/sch_lab/pull/41>\\n\\n### Development Build: 2.3.6\\n\\n- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.\\n- See <https://github.com/nasa/sch_lab/pull/39>\\n\\n### Development Build: 2.3.5\\n\\n- Improved table handling\\n- sch_lab now builds on Raspbian OS\\n- Minor updates (see <https://github.com/nasa/sch_lab/pull/36>)\\n\\n### Development Build: 2.3.4\\n\\n- Fix for clean build with OMIT_DEPRECATED\\n- Minor updates (see <https://github.com/nasa/sch_lab/pull/35>)\\n\\n  ### Development Build: 2.3.3\\n\\n- Minor updates (see <https://github.com/nasa/sch_lab/pull/28>)\\n\\n  ### Development Build: 2.3.2\\n\\n- Table definition include update (see <https://github.com/nasa/sch_lab/pull/18>)\\n\\n  ### Development Build: 2.3.1\\n\\n- Minor updates (see <https://github.com/nasa/sch_lab/pull/16>)\\n\\n### _**OFFICIAL RELEASE: 2.3.0 - Aquila**_\\n\\n- Minor updates (see <https://github.com/nasa/sch_lab/pull/13>)\\n- Not backwards compatible with OSAL 4.2.1\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### _**OFFICIAL RELEASE: 2.2.0a**_\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nAs a lab application, extensive testing is not performed prior to release and only minimal functionality is included.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/ci_lab',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/ci_lab/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/ci_lab/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : App : Command Ingest Lab\\n\\nThis repository contains NASA\\'s Command Ingest Lab (ci_lab), which is a framework component of the Core Flight System.\\n\\nThis lab application is a non-flight utility to send commands to the cFS Bundle. It is intended to be located in the `apps/ci_lab` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes ci_lab as a submodule), which includes build and execution instructions.\\n\\nci_lab is a simple command uplink application that accepts CCSDS telecommand packets over a UDP/IP port. It does not provide a full CCSDS Telecommand stack implementation.\\n\\n## Version History\\n\\n### Development Build: v2.4.0-rc1+dev39\\n\\n- Removes unnecessary call to `CFE_ES_RegisterApp()` since app registration is done automatically.\\n- Demonstrates the use of the Zero Copy API. Adds a step that obtains a buffer prior to calling `OS_SocketRecvFrom` then transmits that same buffer directly rather than copying it.\\n- See <https://github.com/nasa/ci_lab/pull/85>\\n\\n### Development Build: v2.4.0-rc1+dev30\\n\\n- Use `cfe.h` instead of individual headers\\n- See <https://github.com/nasa/ci_lab/pull/78>\\n\\n### Development Build: v2.4.0-rc1+dev25\\n\\n- Fix #74, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/ci_lab/pull/76>\\n\\n### Development Build: v2.4.0-rc1+dev14\\n\\n- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the \"raw\" message cmd/tlm types in definition\\n- See <https://github.com/nasa/ci_lab/pull/65>\\n\\n### Development Build: v2.4.0-rc1+dev8\\n\\n- Replaces deprecated SB API\\'s with MSG\\n- No behavior impact, removes undesirable pattern use of `OS_PACK`\\n- See <https://github.com/nasa/ci_lab/pull/60>\\n\\n### Development Build: v2.4.0-rc1+dev2\\n\\n- Update the SocketID field to be `osal_id_t` instead of uint32\\n- Set Revision number to 99 for development version identifier in telemetry\\n- See <https://github.com/nasa/ci_lab/pull/56>\\n\\n\\n### Development Build: v2.3.0+dev36\\n\\n- Add build name and build number to version reporting\\n- See <https://github.com/nasa/ci_lab/pull/53>\\n\\n### Development Build: v2.3.5\\n\\n- Replace references to `ccsds.h` types with the `cfe_sb.h`-provided type.\\n- See <https://github.com/nasa/ci_lab/pull/50>\\n\\n### Development Build: v2.3.4\\n\\n- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.\\n- Minor changes, see <https://github.com/nasa/ci_lab/pull/47>\\n\\n### Development Build: v2.3.3\\n\\n- Offset UDP base port in multi-cpu configurations\\n- Minor changes, see <https://github.com/nasa/ci_lab/pull/44>\\n\\n### Development Build: v2.3.2\\n\\n- Use OSAL socket API instead of BSD sockets\\n- Remove PDU introspection code\\n- Update command and telemetry logic\\n- Collect globals as a single top-level global structure\\n- Minor changes, see <https://github.com/nasa/ci_lab/pull/38>\\n\\n### Development Build: v2.3.1\\n\\n- Code style and enforcement (see <https://github.com/nasa/ci_lab/pull/31>)\\n\\n### _**OFFICIAL RELEASE: v2.3.0 - Aquila**_\\n\\n- Minor updates (see <https://github.com/nasa/ci_lab/pull/12>)\\n- Not backwards compatible with OSAL 4.2.1\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### _**OFFICIAL RELEASE: v2.2.0a**_\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nDependence on cfe platform config header is undesirable, and the check is not endian safe. As a lab application, extensive testing is not performed prior to release and only minimal functionality is included.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/to_lab',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/to_lab/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/to_lab/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : App : Telemetry Output Lab\\n\\nThis repository contains NASA\\'s Telemetry Output Lab (to_lab), which is a framework component of the Core Flight System.\\n\\nThis lab application is a non-flight utility to downlink telemetry from the cFS Bundle. It is intended to be located in the `apps/to_lab` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes to_lab as a submodule), which includes build and execution instructions.\\n\\nto_lab is a simple telemetry downlink application that sends CCSDS telecommand packets over a UDP/IP port. The UDP port and IP address are specified in the \"Enable Telemetry\" command. It does not provide a full CCSDS Telecommand stack implementation.\\n\\nTo send telemtry to the \"ground\" or UDP/IP port, edit the subscription table in the platform include file: fsw/platform_inc/to_lab_sub_table.h. to_lab will subscribe to the packet IDs that are listed in this table and send the telemetry packets it receives to the UDP/IP port.\\n\\n## Version History\\n\\n### Development Build: v2.4.0-rc1+dev47\\n\\n- Removes app registration call, `CFE_ES_RegisterApp()` since applications do not need to register themselves.\\n- See <https://github.com/nasa/to_lab/pull/97>\\n\\n### Development Build: v2.4.0-rc1+dev41\\n\\n-  Use `cfe.h` header file\\n- See <https://github.com/nasa/to_lab/pull/91>\\n\\n### Development Build: v2.4.0-rc1+dev38\\n\\n- Fix #85, Remove numeric pipe ID from event printf\\n- Fix #87, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/to_lab/pull/89>\\n\\n### Development Build: 2.4.0-rc1+dev32\\n\\n- Removes end-of-function comments in `to_lab_app.c`\\n- Adds static analysis and code format check to continuous integration workflow. Updates workflow status badges in ReadMe\\n- Adds CodeQL analysis to continuous integration workflow\\n- See <https://github.com/nasa/to_lab/pull/84>\\n\\n### Development Build: 2.4.0-rc1+dev21\\n\\n- TO remains command-able after a \"remove all subscriptions\" command; the command now only removes all subscriptions to the Tlm_pipe\\n- See <https://github.com/nasa/to_lab/pull/75>\\n\\n### Development Build: 2.4.0-rc1+dev17\\n\\n- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the \"raw\" message cmd/tlm types in definition\\n- See <https://github.com/nasa/to_lab/pull/70>\\n\\n### Development Build: 2.4.0-rc1+dev13\\n\\n- Replaces deprecated SB API\\'s with MSG\\n- See <https://github.com/nasa/to_lab/pull/65>\\n\\n### Development Build: 2.4.0-rc1+dev9\\n\\n- Update the TLMsockid field to be `osal_id_t` instead of uint32\\n- Set revision number to 99 to indicate development status in telemetry\\n- See <https://github.com/nasa/to_lab/pull/59>\\n\\n### Development Build: 2.4.0-rc1+dev6\\n\\n- Adds header guard to `to_lab_sub_table.h`\\n- See <https://github.com/nasa/to_lab/pull/59>\\n\\n### Development Build: 2.4.0-rc1+dev3\\n\\n- Remove reference to deprecated `CFE_ES_SHELL_TLM_MID`.\\n- See <https://github.com/nasa/to_lab/pull/58>\\n\\n### Development Build: 2.3.0+dev45\\n\\n- Fixes bug where an unset address values caused subscriptions to MsgId 0 over 200 times. Added a `TO_UNUSED` entry at the end of the subscription list and a break in the subscription loop when `TO_UNUSED` found. No more subscriptions on the unused table slots (no MsgId 0 subscriptions).\\n- Corrects return value of `TO_LAB_init()` to be `int32` instead of `int`. Declaration now matches definition, and app builds without errors.\\n- Add build number and baseline to version reporting.\\n- See <https://github.com/nasa/to_lab/pull/53>\\n\\n### Development Build: 2.3.7\\n\\n- Makes the `TO_LAB_Subs` table into a CFE_TBL-managed table.\\n- See <https://github.com/nasa/to_lab/pull/46>\\n\\n\\n### Development Build: 2.3.6\\n\\n- Replace references to `ccsds.h` types with the `cfe_sb.h`-provided type.\\n- See <https://github.com/nasa/to_lab/pull/44>\\n\\n### Development Build: 2.3.5\\n\\n- Apply code style\\n- See <https://github.com/nasa/to_lab/pull/43>\\n\\n### Development Build: 2.3.4\\n\\n- Configure the maximum depth supported by OSAL, rather than a hard coded 64.\\n- See <https://github.com/nasa/to_lab/pull/39>\\n\\n### Development Build: 2.3.3\\n\\n- Apply the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.\\n- Deprecates shell tlm subscription\\n- Changes to documentation\\n- See <https://github.com/nasa/to_lab/pull/38>\\n\\n### Development Build: 2.3.2\\n\\n- Use OSAL socket API instead of BSD Sockets\\n\\n- Use global namespace to isolate variables\\n\\n- Minor updates (see <https://github.com/nasa/to_lab/pull/27>)\\n\\n### Development Build: 2.3.1\\n\\n- Fix for a clean build with OMIT_DEPRECATED\\n- Minor updates (see <https://github.com/nasa/to_lab/pull/26>)\\n\\n### _**OFFICIAL RELEASE: 2.3.0 - Aquila**_\\n\\n- Minor updates (see <https://github.com/nasa/to_lab/pull/13>)\\n\\n- Not backwards compatible with OSAL 4.2.1\\n\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### _**OFFICIAL RELEASE: 2.2.0a**_\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nAs a lab application, extensive testing is not performed prior to release and only minimal functionality is included.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/sample_app',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/sample_app/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/sample_app/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : App : Sample\\n\\nThis repository contains a sample application (sample_app), which is a framework component of the Core Flight System.\\n\\nThis sample application is a non-flight example application implementation for the cFS Bundle. It is intended to be located in the `apps/sample_app` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS> (which includes sample_app as a submodule), which includes build and execution instructions.\\n\\nsample_app is an example for how to build and link an application in cFS. See also the skeleton_app (<https://github.com/nasa/skeleton_app>) if you are looking for a bare-bones application to which to add your business logic.\\n\\n## Version History\\n\\n### Development Build: 1.2.0-rc1+dev62\\n\\n- Removes app registration call, `CFE_ES_RegisterApp()` since applications do not need to register themselves.\\n- Apply standard header guard on all header files by removing leading underscore. Convert file-scope block comments to doxygen format.\\n- See <https://github.com/nasa/sample_app/pull/145>\\n\\n\\n### Development Build: 1.2.0-rc1+dev56\\n\\n- Replaces <> with \" in local includes\\n- Adds CONTRIBUTIING.md that links to the main cFS contributing guide.\\n- Adds a description for the requirements of command and telemetry Message IDs to explain why the Msg IDs have those requirements in documentation.\\n- See <https://github.com/nasa/sample_app/pull/137>\\n\\n### Development Build: 1.2.0-rc1+dev48\\n\\n- Fix #126, simplify build to use wrappers and interface libs\\n- Fix #128, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/sample_app/pull/130>\\n\\n### Development Build: 1.2.0-rc1+dev37\\n\\n- Documentation: Add `Security.md` with instructions on reporting vulnerabilities\\n- Resolves bug where success code was reported as an error for `CFE_TBL_GetAddress`.\\n- Rename `UT_ClearForceFail` as `UT_ClearDefaultValue` given change from <https://github.com/nasa/osal/issues/724>\\n- See <https://github.com/nasa/sample_app/pull/121>\\n\\n### Development Build: 1.2.0-rc1+dev29\\n\\n- Aligns messages according to changes in cFE <https://github.com/nasa/cFE/issues/1009>. Uses the \"raw\" message cmd/tlm types in definition\\n- See <https://github.com/nasa/sample_app/pull/114>\\n\\n\\n### Development Build: 1.2.0-rc1+dev25\\n\\n- Rename `UT_SetForceFail` to `UT_SetDefaultReturnValue` since some functions that retain more than 1 value are not necessarily failing\\n- See <https://github.com/nasa/sample_app/pull/113>\\n\\n\\n### Development Build: 1.2.0-rc1+dev22\\n\\n- Replaces deprecated SB API\\'s with MSG\\n- No impact, removes undesirable pattern use of `OS_PACK`\\n- See <https://github.com/nasa/sample_app/pull/108>\\n\\n### Development Build: 1.2.0-rc1+dev18\\n\\n- No behavior changes. All identifiers now use the prefix `SAMPLE_APP_`. Changes the name of the main function from SAMPLE_AppMain to SAMPLE_APP_Main which affects the CFE startup script.\\n- Set REVISION to \"99\" to indicate development version status\\n- See <https://github.com/nasa/sample_app/pull/102>\\n\\n### Development Build: 1.2.0-rc1+dev13\\n\\n- Unit test MID string format now 32bit\\n- Installs unit test to target directory\\n- Checks only format string in UT event test\\n- See <https://github.com/nasa/sample_app/pull/98>\\n\\n### Development Build: 1.2.0-rc1+dev5\\n\\n- Applies standard coding style.\\n- Removes test code call of CFE_SB_InitMsg and sets the API/stub buffers directly.\\n- See <https://github.com/nasa/sample_app/pull/93>\\n\\n### Development Build: 1.1.0+dev65\\n\\n- Add build number and baseline to version report\\n- Install unit test as part of cmake recipe. Sample app test runner now shows up in expected install directory\\n- See <https://github.com/nasa/sample_app/pull/86>\\n\\n### Development Build: 1.1.11\\n\\n- Move the table to fsw/tables and renames \"sample_table\" to \"sample_app_table\\n- See <https://github.com/nasa/sample_app/pull/76>\\n\\n### Development Build: 1.1.10\\n\\n- Test cases now compare an expected event string with a string derived from the spec string and arguments that were output by the unit under test.\\n- Replace references to `ccsds.h` types with the `cfe_sb.h`-provided type.\\n- See <https://github.com/nasa/sample_app/pull/71>\\n\\n### Development Build: 1.1.9\\n\\n- Applies the CFE_SB_MsgIdToValue() and CFE_SB_ValueToMsgId() routines where compatibility with an integer MsgId is necessary - syslog prints, events, compile-time MID #define values.\\n- No more format conversion error in RTEMS build\\n- See <https://github.com/nasa/sample_app/pull/63>\\n\\n### Development Build: 1.1.8\\n\\n- Coverage data from make lcov includes the sample_app code\\n- See <https://github.com/nasa/sample_app/pull/62>\\n\\n### Development Build: 1.1.7\\n\\n- Fix bug where table is not released after being used\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/52>)\\n\\n### Development Build: 1.1.6\\n\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/49>)\\n\\n### Development Build: 1.1.5\\n\\n- Fix to build on RASPBIAN OS\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/47>)\\n\\n### Development Build: 1.1.4\\n\\n- Fix for a clean build with OMIT_DEPRECATED\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/44>)\\n\\n### Development Build: 1.1.3\\n\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/34>)\\n\\n### Development Build: 1.1.2\\n\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/20>)\\n\\n### Development Build: 1.1.1\\n\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/15>)\\n\\n### _**OFFICIAL RELEASE: 1.1.0 - Aquila**_\\n\\n- Minor updates (see <https://github.com/nasa/sample_app/pull/11>)\\n- Not backwards compatible with OSAL 4.2.1\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### _**OFFICIAL RELEASE: 1.0.0a**_\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\n## Known issues\\n\\nAs a sample application, extensive testing is not performed prior to release and only minimal functionality is included. Note discrepancies likely exist between this application and the example detailed in the application developer guide.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/svs',\n",
       "  'language': None,\n",
       "  'readme_contents': \"# svs\\nNASA's Scientific Visualization Studio\\n\\n[NASA's Scientific Visualization Studio](http://svs.gsfc.nasa.gov) The Scientific Visualization Studio works closely with scientists \\nin the creation of visualizations, animations,\\nand images in order to promote a greater understanding of Earth and Space Science research activities at NASA and within the \\nacademic research community supported by NASA.\\n\"},\n",
       " {'repo': 'nasa/Lightkurve',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"Lightkurve\\n==========\\n\\n**A friendly package for Kepler & TESS time series analysis in Python.**\\n\\n**Documentation: https://docs.lightkurve.org**\\n\\n|pypi-badge| |conda-badge| |azure-badge| |cov-badge| |doi-badge| |astropy-badge|\\n\\n.. |pypi-badge| image:: https://img.shields.io/pypi/v/lightkurve.svg\\n                :target: https://pypi.python.org/pypi/lightkurve\\n.. |conda-badge| image:: https://img.shields.io/conda/vn/conda-forge/lightkurve.svg\\n                 :target: https://anaconda.org/conda-forge/lightkurve\\n.. |azure-badge| image:: https://dev.azure.com/KeplerGO/Lightkurve/_apis/build/status/Lightkurve-PyTest?branchName=master\\n                 :target: https://dev.azure.com/KeplerGO/Lightkurve/_build/latest?definitionId=1&branchName=master\\n.. |cov-badge| image:: https://codecov.io/gh/KeplerGO/lightkurve/branch/master/graph/badge.svg\\n              :target: https://codecov.io/gh/KeplerGO/lightkurve\\n.. |astropy-badge| image:: https://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat\\n                   :target: http://www.astropy.org\\n.. |doi-badge| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1181928.svg\\n              :target: https://doi.org/10.5281/zenodo.1181928\\n\\n\\n**Lightkurve** is a community-developed, open-source Python package which offers a beautiful and user-friendly way\\nto analyze astronomical flux time series data,\\nin particular the pixels and lightcurves obtained by\\n**NASA's Kepler and TESS exoplanet missions**.\\n\\n.. image:: https://raw.githubusercontent.com/KeplerGO/lightkurve/master/docs/source/_static/images/lightkurve-teaser.gif\\n\\nThis package aims to lower the barrier for students, astronomers,\\nand citizen scientists interested in analyzing Kepler and TESS space telescope data.\\nIt does this by providing **high-quality building blocks and tutorials**\\nwhich enable both hand-tailored data analyses and advanced automated pipelines.\\n\\n\\nDocumentation\\n-------------\\n\\nRead the documentation at `https://docs.lightkurve.org <https://docs.lightkurve.org>`_.\\n\\n\\nQuickstart\\n----------\\n\\nPlease visit our quickstart guide at `https://docs.lightkurve.org/quickstart.html <https://docs.lightkurve.org/quickstart.html>`_.\\n\\n\\nContributing\\n------------\\n\\nWe welcome community contributions!\\nPlease read the  guidelines at `https://docs.lightkurve.org/about/contributing.html <https://docs.lightkurve.org/about/contributing.html>`_.\\n\\n\\nCiting\\n------\\n\\nIf you find Lightkurve useful in your research, please cite it and give us a GitHub star!\\nPlease read the citation instructions at `https://docs.lightkurve.org/about/citing.html <https://docs.lightkurve.org/about/citing.html>`_.\\n\\n\\nContact\\n-------\\nLightkurve is an open source community project created by `the authors <AUTHORS.rst>`_.\\nYou can contact several of its main contributors via keplergo@mail.arc.nasa.gov.\\n\"},\n",
       " {'repo': 'nasa/DTNME',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': 'DTN Marshal Enterprise Implementation\\r\\n============================\\r\\n\\r\\nThis is the Delay Tolerant Networking reference implementation managed \\r\\nby Marshal Space Flight Center. Please bear with us as we get this repo\\r\\nup and running. We have worked hard to clean up the code to remove \\r\\nunused or half finsihed classes from the original DTN2 implementation\\r\\nin an attempt to stabilize the code. It is expected that with v0.1\\r\\nthere will be some things that we missed, but should be caught by the\\r\\ntime we reach v1.0. See the file STATUS for an overview of the state\\r\\nof the code, particularly in reference to the various internet drafts\\r\\nand (someday) RFCs that describe the bundling protocol. Also, the file\\r\\nRELEASE-NOTES describes major changes in each release. This implementation\\r\\nis based on DTN2, but also includes all the changes that Marshal Space\\r\\nFlight Center has made over the years. As such intermediary version DTNME\\r\\nv0.1 will include some things that will not be in the final version DTNME v1.0\\r\\n\\r\\nCompilation / Installation Instructions\\r\\n---------------------------------------\\r\\n\\r\\nDependencies\\r\\nlibdb-dev g++ automake autotools-dev tk tk-dev tcl tcl-dev libssl-dev\\r\\n\\r\\n<core> can be replaced with the number of cores you would like to use to in \\r\\nconjunction with the make -j option to try to speed up the build process.\\r\\n\\r\\nFirst Time Building\\r\\n./init_dtnme.sh <cores>\\r\\n\\r\\nSubsequent Times Building\\r\\n./make_dtnme.sh <cores>\\r\\n\\r\\nInstalling\\r\\n./install_dtnme.sh <Desired Installation Directory>\\r\\n\\r\\nNote that by default, the configure script will also run configure\\r\\ninside the oasys/ subdirectory.\\r\\n\\r\\nNote that if you need to make changes to the configure.ac script or\\r\\none of the helper scripts in aclocal/*.ac, run make_dtnme.sh to\\r\\nrecreate configure and then check if both your changes as well as the\\r\\nnewly generated configure script are ready.\\r\\n\\r\\nReporting Bugs / Other Help\\r\\n---------------------------\\r\\nA bug tracking system is in place. Please direct bug reports to \\r\\nhttps://github.com/nasa/DTNME.\\r\\n\\r\\nDTNME Directory Structure\\r\\n------------------------\\r\\n\\r\\napplib/\\t\\t\\tapplication interface library and ipc layer\\r\\napps/\\t\\t\\texample dtn applications\\r\\ndaemon/\\t\\t\\tdtn router daemon sources\\r\\nservlib/\\t\\tdtn router internals\\r\\n|-- bundling\\t\\tbundle management and forwarding logic\\r\\n|-- cmd\\t\\t\\ttcl based command interface\\r\\n|-- contacts\\r\\n|-- conv_layers\\t\\tconvergence layers\\r\\n|-- discovery\\r\\n|-- gcm\\r\\n|-- naming\\t\\tendpoint identifier schemes\\r\\n|-- reg\\t\\t\\tlocal registrations\\r\\n|-- routing\\t\\tbundle routing logic\\r\\n|-- security\\t\\tbundle security protocol\\r\\n|-- session\\r\\n`-- storage\\t\\tpersistent storage management\\r\\nsim/\\t\\t\\tsimulation framework\\r\\ntest/\\t\\t\\tunit tests and other test files\\r\\ntest-utils/\\t\\ttest scripts and utilities\\r\\n\\r\\nExternal Requirements\\r\\n---------------------\\r\\noasys-1.3.0+ (see Note - Oasys)\\r\\ngcc/g++\\r\\n\\r\\nOptional External Packages\\r\\n--------------------------\\r\\nbonjour\\r\\n\\r\\nOptional Internal Packages\\r\\n--------------------------\\r\\nNORM convergence layer support\\r\\nBundle Security Protocol support (see Note BSP)\\r\\nLTP convergence layer support via TCD\\'s LTPlib\\r\\nInternet Draft Compliant IP Neighbor Discovery (see README.IPND)\\r\\n\\r\\n\\r\\nNote - Oasys\\r\\n--------------------------\\r\\nBefore compiling DTNME please compile Oasys-1.3.0+ (must be \\r\\ninstalled or located in DTNME or ../). Support for the\\r\\nfollowing DTNME options should be specified when configuring\\r\\noasys\\r\\n\\r\\nspecify location / support of:\\r\\n    \\r\\n    Python \\r\\n    tcl                  \\r\\n    google perftools\\r\\n    expat\\r\\n    xerces-c\\r\\n    xsd tool\\r\\n    Berkeley DB                  \\r\\n    mysql \\r\\n    postgres             \\r\\n\\r\\ncompile with or without support for:\\r\\n\\r\\n    bluetooth \\r\\n    zlib \\r\\n    tclreadline\\r\\n    profiling\\r\\n    google profiling\\r\\n    assembly-based atomic functions\\r\\n\\r\\nenable or disable:\\r\\n\\r\\n    oasys debugging\\r\\n    oasys memory debugging\\r\\n    oasys lock debugging\\r\\n    oasys optimization\\r\\n\\r\\n\\r\\nNote - BSP\\r\\n--------------------------\\r\\nThe standard ciphersuites require, amongst other things, \\r\\nan implementation of sha-256 message digest algorithm.\\r\\n\\r\\nThe DTN reference code uses OpenSSL for cryptographic\\r\\nand related functions. Unfortunately, some versions of\\r\\nOpenSSL do not include sha-256.\\r\\n\\r\\nThe \"configure\" process checks for the availability of\\r\\nsha-256 and provides an error if it is not found.\\r\\n\\r\\nIf your system\\'s OpenSSL does not have sha-256 then you \\r\\ncan either upgrade it or build and use a local  version \\r\\nof OpenSSL. OpenSSL can be obtained from\\r\\nhttp://www.openssl.org\\r\\n\\r\\nOpenSSL 0.9.8 version include sha-256 by default. If your\\r\\nsystem uses version 0.9.7 and you do not wish to upgrade\\r\\nthen you can enable sha-256 in later versions of 0.9.7,\\r\\nsuch as 0.9.7l and 0.9.7m. To enable sha-256, specify \"fips\"\\r\\nwhen running \"Configure\".\\r\\n\\r\\nIf you wish to leave you system installation untouched and\\r\\nbuild against a local version, then configure dtn using\\r\\n./configure --with-bsp --with-openssl=/path/to/openssl\\r\\n\\r\\nMac OS X note: for Mac OS X users ONLY. If you build dtn\\r\\nagainst a local OpenSSL using \"--with-openssl=/path/to/openssl\"\\r\\nyou MUST also specify with it LDFLAGS=\"-Wl,-search_paths_first\". \\r\\nThe configuration for OS X users would then be \\r\\n./configure --with-bsp --with-openssl=/path/to/openssl LDFLAGS=\"-Wl,-search_paths_first\"\\r\\nNote that the quotes are required for the LDFLAGS argument.\\r\\n\\r\\n'},\n",
       " {'repo': 'nasa/eefs',\n",
       "  'language': 'C',\n",
       "  'readme_contents': 'eefs\\n====\\n\\nEEPROM File System\\n\\nThis is the EEPROM File System Project (EEFS). It is a simple file system for memory devices such as EEPROM, RAM, ROM, etc. Currently it is not intended for block oriented devices such as disks and flash devices.\\n\\nIt can be used as a simple file system to boot an embedded system running vxWorks, RTEMS, or even no operating system. An EEFS image can be created on the development host, providing a single file to burn into an image that is loaded on a target. The file system is easy to understand, debug, and dump. \\n\\nThere are drivers for RTEMS, vxWorks, and there is a standalone API for systems that do not have a file system. \\n\\nThere is even a \"microeefs\" interface that allows the lookup of a file from a single function. This allows the bootloader to locate an image in EEPROM by the file name with a minimal amount of code. \\nFuture releases will include the ability to allow multiple EEFS volumes ( volumes in RAM and EEPROM at the same time ) \\n\\n\\n'},\n",
       " {'repo': 'nasa/GeneLab_Data_Processing',\n",
       "  'language': 'R',\n",
       "  'readme_contents': '<img src=\"NASA_GeneLab_logo-2019.png\" align=\"left\" alt=\"\"/>\\n\\n\\n# GeneLab_Data_Processing\\n\\n## About\\nThe [NASA GeneLab](https://genelab.nasa.gov/) Data Processing team and [Analysis Working Group](https://genelab.nasa.gov/awg/charter) members have created standard pipelines for processing omics data from spaceflight and space-relevant experiments. This repository contains the processing pipelines that have been standardized to date for the assay types indicated below. Each subdirectory in this repository holds current and previous pipeline versions for the respective assay type, including detailed descriptions and processing instructions as well as the exact processing commands used to generate processed data for datasets hosted in the [GeneLab Data Repository](https://genelab-data.ndc.nasa.gov/genelab/projects).\\n\\n---\\n## Assay Types\\nClick on an assay type below for data processing information.\\n- [Amplicon](Amplicon)  \\n- [RNAseq](RNAseq)  \\n\\n---\\n## Usage\\nWe encourage all investigators working with space-relevant omics data to process their data using the standard pipelines described here when possible. Anyone planning to publish analyses derived from [GeneLab processed data](https://genelab-data.ndc.nasa.gov/genelab/projects) may refer to this repository for data processing methods. If you have omics data from a spaceflight or space-relevant experiment, you can submit your data to GeneLab through our [submission portal](https://genelab-data.ndc.nasa.gov/geode-sso-login/).\\n\\n---\\n### Contact\\nFor any questions, comments, and/or issues please [contact GeneLab](https://genelab.nasa.gov/help/contact).\\n'},\n",
       " {'repo': 'nasa/cumulus-process-py',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# cumulus-process-py\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-process-py.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-process-py)\\n[![PyPI version](https://badge.fury.io/py/cumulus-process.svg)](https://badge.fury.io/py/cumulus-process)\\n\\ncumulus-process-py is a collection of python utilities for the NASA Cumulus project.\\n\\n## The Use Case\\n\\nThis library provides various utilities and helper functions for python tasks developed to work with the Cumulus framework.\\n\\nThe utilities help writing tasks that involve metadata extraction from input files, thumbnail creation, or even more complex data processing such as running custom science code to create higher level products from an input file.\\n\\n## Installation\\n\\n    $ pip install cumulus-process\\n\\n## Development\\n\\n    $ pip install -r requirements.txt\\n    $ pip install -r requirements-dev.txt\\n\\n## Testing\\n\\nTesting requires [localstack](https://github.com/localstack/localstack). Follow the instruction for localstack and install it on your machine then:\\n\\n    $ nosetests -v\\n\\n## Usage\\n\\nTo use the library, subclass `Process` class from `cumulus_process` and implement:\\n1. the `process` method,\\n2. a `default_keys` property (needed for functionality such as `self.fetch()` unless you are overriding input_keys in config)\\n\\n## Example:\\n\\nSee the [example](example) folder.'},\n",
       " {'repo': 'nasa/GSAP',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# GSAP\\n\\n<font size=\"+3\"><div align=\"center\"> **[For complete documentation see the wiki\\nhere](https://github.com/nasa/GSAP/wiki)** </div></font>\\n\\n## About\\nThe Generic Software Architecture for Prognostics (GSAP) is a framework for\\napplying prognostics. It makes applying prognostics easier by implementing many\\nof the common elements across prognostic applications. The standard interface\\nenables reuse of prognostic algorithms and models across using the GSAP\\nframework.\\n\\nThere are two operational modes for GSAP: \\'async\\' and \\'simple\\'. These are both described below:\\n* \\'async\\': Asynchronous GSAP. This takes advantage of parallization, including automatic thread management to perform calculations on multiple threads. \\n* \\'simple\\': Simple single-threaded GSAP. Resulting prognostics application is smaller and simplier, but does not take advantage of multi-threading. Ideal for resource constrained hardware. \\n\\n![GSAP Layers](images/Layers.png)\\n\\nThe GSAP framework is used through the creation of communicators, prognosers, or\\nmodels (the deployment layer). The elements of the deployment layer plugs into\\nthe framework and use the tools of the support layer. These elements are\\ndescribed further below:\\n\\n* **Communicators**:\\n\\nCommunicators are used to communicate data with the outside world. These\\nfunction as interfaces with various data sources and sinks. Some examples could\\nbe a playback agent that reads from a file, a GUI for displaying prognostic\\nresults, an automated report generator, or a client that connects into a network\\nmessaging system (for example: SCADA). These systems can receive data which will\\nbe used by prognosers or communicate the results with operators.\\n\\n* **Prognosers**:\\n\\n  This is the core of the GSAP system. Inside the prognosers is the core logic for performing prognostics. A new prognoser is created to support a new method for performing prognostics. Many prognostics systems follow a common model-based structure. Those systems do not require the creation of a new prognoser, only the creation of a new model that will be used by the modelBasedPrognoser. For more information on this see the section Extending.\\n\\n* **Models**:\\n\\n  Models are a method of representing the behavior of a component. A common way of performing prognostics is using a model that describes both the healthy and damaged behavior of the components. The modelBasedPrognoser uses the models to perform prognostics.\\n\\nEach of these components is configured through the use of configuration files.\\nThis allows for a GSAP deployment to be configured to a new configuration or\\nsystem without any software changes.\\n\\n## Using\\n[See the Getting Started page for\\ninformation](https://github.com/nasa/GSAP/wiki/Getting-Started)\\n\\n## Extending\\nGSAP is designed to be easy to extend to fit your use. Extending GSAP is done by\\nadding Prognosers, Models, Algorithms, or Communicators. When the behavior of the component\\nbeing prognosed is represented by a model, users can create a new model and use\\nthe supplied modelBasedPrognoser for prognostics. This is done instead of adding\\na new prognoser. [For more information on extending see the wiki\\nhere](https://github.com/nasa/GSAP/wiki)\\n\\n## Contact\\nIf you have questions, please contact Chris Teubert\\n(christopher.a.teubert@nasa.gov)\\n\\n## Contributing\\nAll contributions are welcome! If you are having problems with the plugin, please open an issue on GitHub or email Chris Teubert. If you would like to contribute directly, please feel free to open a pull request against the \"develop\" branch. Pull requests will be evaluated and integrated into the next official release.\\n\\n## Notices\\n\\nCopyright ©2016, 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/MOSAIC',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# MOSAIC: Mars On Site Shared Analytics, Information, and Computing\\n\\n![The MOSAIC scheduler in action](images/preview.gif)\\n\\nThis repository contains MOSAIC schedulers and the Pluggable Distributed Resource Allocator (PDRA).\\n\\nTogether, the tools in this repository enable heterogeneous multi-robot systems to share computational tasks with complex dependencies among agents with heterogeneous computation capabilities over time-varying communication links.\\n\\nMaintainers:\\n\\n- [Federico Rossi](https://github.com/federico3) `federico.rossi@jpl.nasa.gov`\\n- [Tiago Stegun Vaquero](https://github.com/tvaquero) `tiago.stegun.vaquero@jpl.nasa.gov`\\n- [Marc Sanchez Net](https://github.com/msancheznet) `marc.sanchez.net@jpl.nasa.gov`\\n- [Joshua Vander Hook](https://github.com/jodavaho/) `hook@jpl.nasa.gov`\\n\\n![MOSAIC live demo](images/MOSAIC_demo.png)\\n\\n## [MOSAIC schedulers](schedulers)\\n\\n![MOSAIC schedules](images/examples.png)\\n\\nWe propose scheduling and task-allocation algorithms to share computational tasks among heterogeneous agents over time-varying communication links.\\n\\nSpecifically, we propose:\\n\\n- A mixed-integer programming algorithm for scheduling tasks in heterogeneous robotic networks with time-varying communication links. The scheduler can accommodate any non-cyclical dependencies between tasks and arbitrary time-varying communication links, handle optional tasks with associated rewards, and optimize cost functions including rewards for optional tasks, makespan, and energy usage. The scheduler is presented in \\\\[1\\\\].\\n\\n- A mixed-integer programming algorithm for task allocation in  heterogeneous robotic networks with *periodic* communication links. The task allocation algorithm also accommodates any non-cyclical dependencies between tasks and handles optional tasks with associated rewards and maximum latency requirements; it can maximize reward from optional tasks or minimize energy use. The task allocation algorithm is presented in \\\\[2\\\\].\\n\\n<!-- [![Alt text](https://img.youtube.com/vi/VID/0.jpg)](https://www.youtube.com/watch?v=VID) -->\\n\\n## [Pluggable Distributed Resource Allocator](distributed_resource_allocator)\\n\\n![Animation of the Pluggable Distributed Resource Allocator in action](images/pdra.gif)\\n\\nThe Pluggable Distributed Resource Allocator (PDRA) is a middleware for distributed computing in heterogeneous mobile robotic networks. It allows the MOSAIC schedulers to be easily \"plugged\" in existing autonomy executives with minimal software changes. PDRA sits between an existing single-agent planner/executor and existing computational resources (e.g. ROS packages). It intercepts the executor’s requests and, if needed, transparently routes them to other nodes for execution.\\nSimulation results show that PDRA can reduce energy and CPU usage by over 50\\\\% in representative multi-robot scenarios compared to a naive scheduler; runs on embedded platforms; and performs well in delay- and disruption-tolerant networks (DTNs). PDRA is available to the community under an open-source license.\\n\\n<!-- [![Alt text](https://img.youtube.com/vi/VID/0.jpg)](https://www.youtube.com/watch?v=VID) -->\\n\\n## References\\n\\n\\\\[1\\\\] Joshua Vander Hook, Tiago Vaquero, Federico Rossi, Martina Troesch, Marc Sanchez Net, Joshua Schoolcraft, Jean-Pierre de la Croix, and Steve Chien, [\"Mars On-Site Shared Analytics Information and Computing,\"](https://aaai.org/ojs/index.php/ICAPS/article/view/3556) in Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling, vol. 29, no. 1, pp. 707-715, July 2019.\\n\\n\\\\[2\\\\] Federico Rossi\\\\*, Tiago Stegun Vaquero\\\\*, Marc Sanchez Net, Maíra Saboia da Silva, and Joshua Vander Hook, [\"The Pluggable Distributed Resource Allocator (PDRA):a Middleware for Distributed Computing in Mobile Robotic Networks\"](https://arxiv.org/abs/2003.13813), under review.\\n\\n## Copyright\\n\\nCopyright 2019 by California Institute of Technology.  ALL RIGHTS RESERVED.\\nUnited  States  Government  sponsorship  acknowledged.   Any commercial use\\nmust   be  negotiated  with  the  Office  of  Technology  Transfer  at  the\\nCalifornia Institute of Technology.\\n\\nThis software may be subject to  U.S. export control laws  and regulations.\\nBy accepting this document,  the user agrees to comply  with all applicable\\nU.S. export laws and regulations.  User  has the responsibility  to  obtain\\nexport  licenses,  or  other  export  authority  as may be required  before\\nexporting  such  information  to  foreign  countries or providing access to\\nforeign persons.\\n\\nThis  software  is a copy  and  may not be current.  The latest  version is\\nmaintained by and may be obtained from the Mobility  and  Robotics  Sytstem\\nSection (347) at the Jet  Propulsion  Laboratory.   Suggestions and patches\\nare welcome and should be sent to the software\\'s maintainer.\\n'},\n",
       " {'repo': 'nasa/concept-tagging-api',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# OCIO STI Concept Tagging Service\\n\\nAn API for exposing models created with [STI concept training](https://github.com/nasa/concept-tagging-training). This project was written about [here](https://strategy.data.gov/proof-points/2019/05/28/improving-data-access-and-data-management-artificial-intelligence-generated-metadata-tags-at-nasa/) for the Federal Data Strategy Incubator Project. A running version of this API may be found [here](http://ec2-100-25-26-114.compute-1.amazonaws.com:5001/), however, this is a temporary instance for demos purposes. It may not be available long-term. Please do not use it in production or at scale.\\n\\n### What is Concept Tagging\\nBy concept tagging, we mean you can supply text, for example:\\n`Volcanic activity, or volcanism, has played a significant role in the geologic evolution of Mars.[2] Scientists have known since the Mariner 9 mission in 1972 that volcanic features cover large portions of the Martian surface.` and get back predicted keywords, like `volcanology, mars surface, and structural properties`, as well as topics, like `space sciences, geosciences`, from a standardized list of several thousand NASA concepts with a probability score for each prediction.\\n\\n## Index\\n1. [Using Endpoint](#using-endpoint)\\n    1. [Request](#request)\\n    2. [Response](#response)\\n2. [Running Your Own Instance](#running-your-own-instance)\\n    1. [Installation](#installation)\\n        1. [Pull Docker Image](#pull-docker-image)\\n        2. [Build Docker Image](#build-docker-image)\\n        3. [With Local Python](#with-local-python)\\n    2. [Download Models](#downloading-models)\\n    3. [Running Service](#running-service)\\n        1. [Using Docker](#using-docker)\\n        2. [Using Local Python](#using-local-python)\\n\\n## Using Endpoint\\n### Request\\nThe endpoint accepts a few fields, shown in this example:\\n```json\\n{\\n    \"text\": [\\n        \"Astronauts go on space walks.\",\\n        \"Basalt rocks and minerals are on earth.\"\\n    ], \\n    \"probability_threshold\":\"0.5\",\\n    \"topic_threshold\":\"0.9\", \\n    \"request_id\":\"example_id10\"\\n}\\n```\\n- **text** *(string or list of strings)* -- The text(s) to be tagged.\\n- **probability_threshold** *(float in [0, 1])* -- a threshold under which a concept tag will not be returned by the API. For example, if the threshold is set to 0.8 and a concept only scores 0.5, the concept will be omitted from the response. Setting to 1 will yield no results. Setting to 0 will yield all of the classifiers and their scores, no matter how low.\\n- **topic_threshold** *(float in [0, 1])* -- A probability threshold for categories. If a category falls under this threshold, its respective suite of models will not be utilized for prediction. If you set this value to 1, only the generalized concept models will be used for tagging, yielding significant speed gains.\\n- **request_id** *(string)* -- an optional ID for your request.  \\n\\nYou might send this request using curl. In the command below:\\n1. Substitute `example_payload_multiple.json` with the path to your json request.\\n2. Substitute `http://0.0.0.0:5000/` with the address of the API instance.\\n```\\ncurl -X POST -H \"Content-Type: application/json\" -d @example_payload_multiple.json http://0.0.0.0:5000/findterms/\\n```\\n### Response\\nYou will then receive a response like that [here](docs/multiple_response.json). In the `payload`, you will see multiple fields, including:\\n- **features** -- words and phrases directly extracted from the document. \\n- **sti_keywords** -- concepts and their prediction scores. \\n- **topic_probability** -- model scores for all of the categories.\\n\\n## Running Your Own Instance\\n### Installation\\nFor most people, the simplest installation entails [building the docker image](#build-docker-image), [downloading the models](#downloading-models), and [running the docker container](#using-docker).\\n\\n\\n#### Build Docker Image\\nFirst, clone this repository and enter its root.\\nNow, you can build the image with:\\n```\\ndocker build -t concept_tagging_api:example .\\n```\\n\\\\* Developers should look at the `make build` command in the [Makefile](Makefile). It has an automated process for tagging the image with useful metadata.\\n\\n#### With Local Python\\n\\\\* tested with python:3.7  \\nFirst, clone this repository and enter its root.  \\nNow, create a virtual environment. For example, using [venv](https://docs.python.org/3/library/venv.html):\\n```\\npython -m venv venv\\nsource venv/bin/activate\\n```\\nNow install the requirements with:\\n```\\nmake requirements\\n```\\n\\n### Downloading Models\\nThen, you need to download the machine learning models upon which the service relies. \\n\\nYou can find zipped file which contains all of the models [here](https://data.nasa.gov/docs/datasets/public/concept_tagging_models/10_23_2019.zip). Now, to get the models in the right place and unzip:\\n```bash\\nmkdir models\\nmv <YOUR_ZIPPED_MODELS_NAME>.zip models\\ncd models\\nunzip <YOUR_ZIPPED_MODELS_NAME>.zip\\n```\\nAlternatively, the models can also be downloaded from data.nasa.gov where they are named <a href=\\'https://data.nasa.gov/Software/STI-Tagging-Models/jd6d-mr3p\\'>STI Tagging Models</a>. However, they download slower from that location.\\n\\n### Running Service\\n\\n#### Using Docker\\nWith the docker image and model files in place, you can now run the service with a simple docker command. In the below command be sure to:\\n 1. Substitute `concept_tagging_api:example` for the name of your image.\\n 2. Substitute `$(pwd)/models/10_23_2019` to the path to your models directory. \\n 3. Substitute `5001` with the port on your local machine from which you wish to access the API.\\n```\\ndocker run -it \\\\\\n    -p 5001:5000 \\\\\\n    -v $(pwd)/models/10_23_2019:/home/service/models/experiment \\\\\\n    concept_tagging_api:example\\n```\\n\\nNote that you you may experience permission errors when you start the container. To resolve this issue, set the user and group of your `models` directory to 999. This is the uid for the user \\n\\n**optional**\\nThe entrypoint to the docker image is [gunicorn](https://docs.gunicorn.org/en/stable/index.html), a python WSGI HTTP Server which runs our flask app. You can optionally pass additionally arguments to gunicorn. For example:\\n```bash\\ndocker run -it \\\\\\n    -p 5001:5000 \\\\\\n    -v $(pwd)/models/10_23_2019:/home/service/models/experiment \\\\\\n    concept_tagging_api:example --timeout 9000 \\n```\\nSee [here](https://docs.gunicorn.org/en/stable/design.html#async-workers) for more information about design considerations for these gunicorn settings.\\n\\n#### Pitfalls & Gotchas to Remeber\\n- If you run this on a cloud service and run an upgrade on everything out of date for security reasons, you may need to run `sudo service docker stop`\\nand then `sudo service docker start` to get docker going again. You\\'ll also have to find the docker container that you had last running and restart it.\\n- If you run the docker container as described above, remember to try the URL of your service with the proper port at the end of the URL. \\n\\n#### Using Local Python\\nWith the requirements installed and the model files in place, you can now run the service with python locally. \\nIn the command below, substitute `models/test` with the path to your models directory. For example, if you followed the example from [With Bucket Access](#with-bucket-access), it will be `models/10_23_2019`.\\n```\\nexport MODELS_DIR=models/test; \\\\\\npython service/app.py\\n```\\n'},\n",
       " {'repo': 'nasa/CompDam_DGD',\n",
       "  'language': 'Fortran',\n",
       "  'readme_contents': '# CompDam - Deformation Gradient Decomposition (DGD)\\nThis code is a continuum damage mechanics (CDM) material model intended for use with the Abaqus finite element code. This is a research code which aims to provide an accurate representation of mesoscale damage modes in fiber-reinforced polymer composite materials in finite element models in which each ply is discretely represented.\\n\\nThe CDM material model is implemented as an Abaqus/Explicit user subroutine (VUMAT) for the simulation of matrix cracks formed under tensile, compressive, and shear loading conditions and fiber fracture under tensile and compressive loading. Within CompDam, the emphasis of many recent developments has been on accurately representing the kinematics of composite damage. The kinematics of matrix cracks are represented by treating them as cohesive surfaces embedded in a deformable bulk material in accordance with the Deformation Gradient Decomposition (DGD) approach. Fiber tensile damage is modeled using conventional CDM strain-softening.\\n\\nThis software may be used, reproduced, and provided to others only as permitted under the terms of the agreement under which it was acquired from the U.S. Government. Neither title to, nor ownership of, the software is hereby transferred. This notice shall remain on all copies of the software.\\n\\nCopyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\nPublications that describe the theories used in this code:\\n- Andrew C. Bergan, [\"A Three-Dimensional Mesoscale Model for In-Plane and Out-of-Plane Fiber Kinking\"](https://doi.org/10.2514/6.2019-1548) *AIAA SciTech Forum*, San Diego, California, 7-11 January 2019.\\n- Carlos Dávila, [\"From S-N to the Paris Law with a New Mixed-Mode Cohesive Fatigue Model\"](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20180004395.pdf) NASA/TP-2018-219838, June 2018.\\n- Andrew C. Bergan, et al., [\"Development of a Mesoscale Finite Element Constitutive Model for Fiber Kinking\"](https://doi.org/10.2514/6.2018-1221) *AIAA SciTech Forum*, Kissimmee, Florida, 8-12 January 2018.\\n- Frank A. Leone Jr. [\"Deformation gradient tensor decomposition for representing matrix cracks in fiber-reinforced materials\"](https://dx.doi.org/10.1016/j.compositesa.2015.06.014) *Composites Part A* (2015) **76**:334-341.\\n- Frank A. Leone Jr. [\"Representing matrix cracks through decomposition of the deformation gradient tensor in continuum damage mechanics methods\"](https://iccm20.org/fullpapers/file?f=Abk7n4gkWV) *Proceedings of the 20th International Conference on Composite Materials*, Copenhagen, Denmark, 19-24 July 2015.\\n- Cheryl A. Rose, et al. [\"Analysis Methods for Progressive Damage of Composite Structures\"](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20140001002.pdf) NASA/TM-2013-218024, July 2013.\\n\\nExamples of this code being applied can be found in the following publications:\\n- Andrew C. Bergan and Wade C. Jackson, [\"Validation of a Mesoscale Fiber Kinking Model through Test and Analysis of Double Edge Notch Compression Specimens\"](https://doi.org/10.12783/asc33/26003) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.\\n- Imran Hyder, et al., [\"Implementation of a Matrix Crack Spacing Parameter in a Continuum Damage Mechanics Finite Element Model\"](https://doi.org/10.12783/asc33/26052) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.\\n- Frank Leone, et al., [\"Benchmarking Mixed Mode Matrix Failure in Progressive Damage and Failure Analysis Methods\"](https://doi.org/10.12783/asc33/26030) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.\\n- Brian Justusson, et al., et al., [\"Quantification of Error Associated with Using Misaligned Meshes in Continuum Damage Mechanics Material Models for Matrix Crack Growth Predictions in Composites\"](https://doi.org/10.12783/asc33/26097) *33rd American Society for Composites (ASC) Annual Technical Conference*, Seattle, Washington, 24-27 September 2018.\\n- Kyongchan Song, et al. [\"Continuum Damage Mechanics Models for the Analysis of Progressive Damage in Cross-Ply and Quasi-Isotropic Panels Subjected to Static Indentation\"](https://doi.org/10.2514/6.2018-1466) *AIAA SciTech Forum*, Kissimmee, Florida, 8-12 January 2018.\\n- Imran Hyder, et al. [\"Assessment of Intralaminar Progressive Damage and Failure Analysis Using an Efficient Evaluation Framework\"](https://doi.org/10.12783/asc2017/15405) *32nd American Society for Composites (ASC) Annual Technical Conference*, West Lafayette, Indiana, 22-25 October 2017.\\n- Frank A. Leone, et al. [\"Fracture-Based Mesh Size Requirements for Matrix Cracks in Continuum Damage Mechanics Models\"](https://doi.org/10.2514/6.2017-0198) *AIAA SciTech Forum*, Grapevine, Texas, 9-13 January 2017.\\n- Mark McElroy, et al. [\"Simulation of delamination-migration and core crushing in a CFRP sandwich structure\"](https://doi.org/10.1016/j.compositesa.2015.08.026) *Composites Part A* (2015) **79**:192-202.\\n\\nFor any questions, please contact the developers:\\n- Frank Leone   | [frank.a.leone@nasa.gov](mailto:frank.a.leone@nasa.gov)     | (W) 757-864-3050\\n- Andrew Bergan | [andrew.c.bergan@nasa.gov](mailto:andrew.c.bergan@nasa.gov) | (W) 757-864-3744\\n- Carlos Dávila | [carlos.g.davila@nasa.gov](mailto:carlos.g.davila@nasa.gov) | (W) 757-864-9130\\n\\n\\n## Table of contents\\n- [Getting started](#getting-started)\\n- [Model features](#model-features)\\n- [Elements](#elements)\\n- [Material properties](#material-properties)\\n- [State variables](#state-variables)\\n- [Fatigue analyses](#fatigue-analyses)\\n- [Implicit solver compatibility](#implicit-solver-compatibility)\\n- [Example problems](#example-problems)\\n- [Advanced debugging](#advanced-debugging)\\n- [Python extension module](#python-extension-module)\\n- [Summary of tests classes](#summary-of-tests-classes)\\n- [Contributing](#contributing)\\n- [Citing CompDam](#citing-compdam)\\n\\n\\n## Getting started\\n\\n### Source code\\nThe user subroutine source code is located in the `for` directory. The main entry point is `CompDam_DGD.for`.\\n\\n### Prerequisites\\n[Intel Fortran Compiler](https://software.intel.com/en-us/fortran-compilers) version 11.1 or newer is required to compile the code ([more information about compiler versions](usersubroutine-prerequisites.md)). MPI must be installed and configured properly so that the MPI libraries can be linked by CompDam. It is recommended that Abaqus 2016 or newer is used with this code. Current developments and testing are conducted with Abaqus 2019. Python supporting files require Python 2.7.\\n\\n### Initial setup\\nAfter cloning the CompDam_DGD git repository, it is necessary to run the setup script file `setup.py` located in the repository root directory:\\n```\\n$ python setup.py\\n```\\n\\nThe main purpose of the setup.py script is to 1) set the `for/version.for` file and 2) add git-hooks that automatically update the `for/version.for`.\\n\\nIn the event that you do not have access to python, rename `for/version.for.nogit` to `for/version.for` manually. The additional configuration done by `setup.py` is not strictly required.\\n\\n### Abaqus environment file settings\\nThe `abaqus_v6.env` file must have [`/fpp`](https://software.intel.com/en-us/fortran-compiler-developer-guide-and-reference-fpp), [`/Qmkl:sequential`](https://software.intel.com/en-us/fortran-compiler-developer-guide-and-reference-mkl-qmkl), and [`/free`](https://software.intel.com/en-us/fortran-compiler-developer-guide-and-reference-free) in the `ifort` command where the format for Windows is used. The corresponding Linux format is: `-fpp`, `-free`, and `-mkl=sequential`. The `/fpp` option enables the Fortran preprocessor, which is required for the code to compile correctly. The `/free` option sets the compiler to free-formatting for the source code files. The `/Qmkl:sequential` enables the [Intel Math Kernel Library (MKL)](https://software.intel.com/en-us/mkl), which provides optimized and verified functions for many mathematical operations. The MKL is used in this code for calculating eigenvalues and eigenvectors.\\n\\nA sample environment file is provided in the `tests` directory for Windows and Linux systems.\\n\\n### Submitting a job\\nThis code is an Abaqus/Explicit VUMAT. Please refer to the Abaqus documentation for the general instructions on how to submit finite element analyses using user subroutines. Please see the [example input file statements](#example-input-file-statements) for details on how to interface with this particular VUMAT subroutine.\\n\\nAnalyses with this code **must** be run in double precision. Some of the code has double precision statements and variables hard-coded, so if Abaqus/Explicit is run in single precision, compile-time errors will arise. When submitting an Abaqus/Explicit job from the command line, double precision is specified by including the command line argument `double=both`.\\n\\nFor example, run the test model `test_C3D8R_elastic_fiberTension` in the `tests` directory with the following command:\\n```\\n$ abaqus job=test_C3D8R_elastic_fiberTension user=../for/CompDam_DGD.for double=both\\n```\\n\\n### Example input file statements\\nExample 1, using an [external material properties file](#defining-the-material-properties-in-a-props-file):\\n\\n    *Section controls, name=control_name, distortion control=YES\\n    **\\n    *Material, name=IM7-8552\\n    *Density\\n     1.57e-09,\\n    *Depvar, delete=11\\n    ** the above delete statement is optional\\n      19,\\n      1, CDM_d2\\n      2, CDM_Fb1\\n      3, CDM_Fb2\\n      4, CDM_Fb3\\n      5, CDM_B\\n      6, CDM_Lc1\\n      7, CDM_Lc2\\n      8, CDM_Lc3\\n      9, CDM_FIm\\n     10, CDM_alpha\\n     11, CDM_STATUS\\n     12, CDM_Plas12\\n     13, CDM_Inel12\\n     14, CDM_FIfT\\n     15, CDM_slide1\\n     16, CDM_slide2\\n     17, CDM_FIfC\\n     18, CDM_d1T\\n     19, CDM_d1C\\n    *Characteristic Length, definition=USER, components=3\\n    *User material, constants=3\\n    ** 1              2  3\\n    ** feature flags,  , thickness\\n              100001,  ,       0.1\\n    **\\n    *Initial Conditions, type=SOLUTION\\n     elset_name,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,\\n     0.d0,  0.d0,  -999,     1,  0.d0,  0.d0,  0.d0, 0.d0,\\n     0.d0,  0.d0,  0.d0,  0.d0\\n    ** In each step, NLGEOM=YES must be used. This is the default setting.\\n\\nExample 2, using an [input deck command](#defining-the-material-properties-in-the-input-deck):\\n\\n    *Section controls, name=control_name, distortion control=YES\\n    **\\n    *Material, name=IM7-8552\\n    *Density\\n     1.57e-09,\\n    *Depvar, delete=11\\n    ** the above delete statement is optional\\n      19,\\n      1, CDM_d2\\n      2, CDM_Fb1\\n      3, CDM_Fb2\\n      4, CDM_Fb3\\n      5, CDM_B\\n      6, CDM_Lc1\\n      7, CDM_Lc2\\n      8, CDM_Lc3\\n      9, CDM_FIm\\n     10, CDM_alpha\\n     11, CDM_STATUS\\n     12, CDM_Plas12\\n     13, CDM_Inel12\\n     14, CDM_FIfT\\n     15, CDM_slide1\\n     16, CDM_slide2\\n     17, CDM_FIfC\\n     18, CDM_d1T\\n     19, CDM_d1C\\n    *Characteristic Length, definition=USER, components=3\\n    *User material, constants=40\\n    ** 1              2  3          4  5  6  7  8\\n    ** feature flags,  , thickness, 4, 5, 6, 7, 8\\n              100001,  ,       0.1,  ,  ,  ,  ,  ,\\n    **\\n    **  9         10        11        12        13        14        15        16\\n    **  E1,       E2,       G12,      nu12,     nu23,     YT,       SL        GYT,\\n        171420.0, 9080.0,   5290.0,   0.32,     0.52,     62.3,     92.30,    0.277,\\n    **\\n    **  17        18        19        20        21        22        23        24\\n    **  GSL,      eta_BK,   YC,       alpha0    E3,       G13,      G23,      nu13,\\n        0.788,    1.634,    199.8,    0.925,      ,          ,         ,          ,\\n    **\\n    **  25        26        27        28        29        30        31        32\\n    **  alpha11,  alpha22,  alpha_PL, n_PL,     XT,       fXT,      GXT,      fGXT,\\n        -5.5d-6,  2.58d-5,          ,     ,     2326.2,   0.2,      133.3,    0.5,\\n    **\\n    **  33        34        35        36        37        38        39        40\\n    **  XC,       fXC,      GXC,      fGXC,       cl,     w_kb,     None,     mu\\n        1200.1,      ,         ,          ,         ,     0.1,          ,     0.3\\n    ** For spacing below a6=schaefer_a6, b2=schaefer_b2, n=schaefer_n and A=schaefer_A\\n    **  41        42        43        44\\n    **  a6,       b2,       n,        A\\n    **\\n    *Initial Conditions, type=SOLUTION\\n     elset_name,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,\\n     0.d0,  0.d0,  -999,     1,  0.d0,  0.d0,  0.d0, 0.d0,\\n     0.d0,  0.d0,  0.d0,  0.d0\\n    ** In each step, NLGEOM=YES must be used. This is the default setting.\\n\\n### Running tests\\nTest cases are available in the `tests` directory. The tests are useful for demonstrating the capabilities of the VUMAT as well as to verify that the code performs as intended. Try running some of the test cases to see how the code works. The test cases can be submitted as a typical Abaqus job using the Abaqus command line arguments.\\n\\n### Building a shared library\\nCompDam_DGD can be built into a shared library file. Follow these steps:\\n1. Place a copy of the Abaqus environment file (with the compiler flags specified) in the `for` directory\\n2. In Linux, and when using Abaqus versions prior to 2017, rename `CompDam_DGD.for` to `CompDam_DGD.f`\\n3. From the `for` directory, run:\\n```\\n$ abaqus make library=CompDam_DGD\\n```\\nThis command will create shared libraries for the operating system it is executed on (`.dll` for Windows and `.so` for Linux).\\n\\nWhen using a pre-compiled shared library, it is only necessary to specify the location of the shared library files in the environment file (the compiler options are not required). To run an analysis using a shared library, add `usub_lib_dir = <full path to shared library file>` to the Abaqus environment file in the Abaqus working directory.\\n\\n## Model features\\nThe CompDam_DGD material model implements a variety of features that can be enabled or disabled by the user. An overview of these features is provided in this section. The material properties required for each feature are listed. References are provided to more detailed discussions of the theoretical framework for each feature.\\n\\n### Fully orthotropic elasticity\\nThe composite materials modeled with CompDam_DGD can be defined assuming either [transverse isotropy](https://www.efunda.com/formulae/solid_mechanics/mat_mechanics/hooke_iso_transverse.cfm) or [orthotropy](https://www.efunda.com/formulae/solid_mechanics/mat_mechanics/hooke_orthotropic.cfm). For a transversely isotropic material definition, the following properties must be defined: E1, E2, G12, v12, and v23. For an orthotropic material definition, the following additional properties must be defined: E2, G13, G23, and nu13.\\n\\n### Matrix damage\\nTensile and compressive matrix damage is modeled by embedding cohesive laws to represent cracks in the material according to the deformation gradient decomposition method of [Leone (2015)](https://doi.org/10.1016/j.compositesa.2015.06.014). The matrix crack normals can have any orientation in the 2-3 plane, defined by the angle `CDM_alpha`. The mixed-mode behavior of matrix damage initiation and evolution is defined according to the Benzeggagh-Kenane law. The initiation of compressive matrix cracks accounts for friction on the potential crack surface according to the LaRC04 failure criteria. In the notation of the [paper](https://doi.org/10.1016/j.compositesa.2015.06.014), `Q` defines the material direction that is severed by the crack. In this implementation `Q=2` except when `CDM_alpha = 90`.\\n\\nThe following material properties are required for the prediction of matrix damage: YT, SL, GYT, GSL, eta_BK, YC, and alpha0. The state variables related to matrix damage are `CDM_d2`, `CDM_FIm`, `CDM_B`, `CDM_alpha`, `CDM_Fb1`, `CDM_Fb2`, and `CDM_Fb3`.\\n\\n### Thermal strains\\nThe thermal strains are calculated by multiplying the 1-, 2-, and 3-direction coefficients of thermal expansion by the current &Delta;T, as provided by the Abaqus solver. The thermal strains are subtracted from the current total strain.\\n\\nThe required material properties are the coefficients of thermal expansion in the 1 and 2 directions. It is assumed that the 2- and 3-direction coefficients of thermal expansion are equal.\\n\\nHygroscopic strains are not accounted for. If the effects of hygroscopic expansion are to be modeled, it is recommended to smear the hygroscopic and thermal expansion coefficients to approximate the response using the solver-provided &Delta;T.\\n\\n### Shear nonlinearity\\nThree approaches to modeling the matrix nonlinearity are available: Ramberg-Osgood plasticity, Schapery theory, and Schaefer plasticity. These three methods are mutually exclusive and optional.\\n\\n#### Ramberg-Osgood plasticity\\nShear nonlinearity in the 1-2 and/or the 1-3 plane can be modeled using the [Ramberg-Osgood equation](https://en.wikipedia.org/wiki/Ramberg%E2%80%93Osgood_relationship), with its parameters selected to fit experimental data. As applied herein, the Ramberg-Ogsood equation is written in the following form for the 1-2 plane:\\n\\n*&gamma;*<sub>12</sub> = [*&tau;*<sub>12</sub> + *&alpha;*<sub>PL</sub>sign(*&tau;*<sub>12</sub>)|*&tau;*<sub>12</sub>|<sup>*n*<sub>PL</sub></sup>]/*G*<sub>12</sub>\\n\\nwhere *&gamma;*<sub>12</sub> is the shear strain and *&tau;*<sub>12</sub> is the shear stress. Likewise, the expression for the 1-3 plane is\\n\\n*&gamma;*<sub>13</sub> = [*&tau;*<sub>13</sub> + *&alpha;*<sub>PL</sub>sign(*&tau;*<sub>13</sub>)|*&tau;*<sub>13</sub>|<sup>*n*<sub>PL</sub></sup>]/*G*<sub>13</sub>\\n\\nPrior to the initiation of matrix damage (i.e., `CDM_d2 = 0`), the nonlinear shear response due to the above equation is plastic, and the unloading/reloading slope is unchanged. No pre-peak nonlinearity is applied to the matrix tensile or compressive responses (i.e., *&sigma;<sub>22</sub>*).\\n\\nThe required material inputs are the two parameters in the above equation: *&alpha;*<sub>PL</sub> and *n*<sub>PL</sub>. Note that the same constants are used for the 1-2 and 1-3 planes under the assumption of transverse isotropy (see [Seon et al. (2017)](https://doi.org/10.12783/asc2017/15267)). For the 1-2 plane, the state variables `CDM_Plas12` and `CDM_Inel12` are used to track the current plastic shear strain and the total amount of inelastic plastic shear strain that has occurred through the local deformation history, respectively. For cases of monotonic loading, `CDM_Plas12` and `CDM_Inel12` should have the same magnitude. Likewise, the state variables `CDM_Plas13` and `CDM_Inel13` are utilized for the 1-3 plane. The [feature flags](#controlling-which-features-are-enabled) can be used to enable this Ramberg-Osgood model in the 1-2 plane, 1-3 plane, or both planes.\\n\\n#### Schapery micro-damage\\nMatrix nonlinearity in the 1-2 plane can also be modeled using Schapery theory, in which all pre-peak matrix nonlinearity is attributed to the initiation and development of micro-scale matrix damage. With this approach, the local stress/strain curves will unload to the origin, and not develop plastic strain. A simplified version of the approach of [Pineda and Waas](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20120000914.pdf) is here applied. The micro-damage functions *e<sub>s</sub>* and *g<sub>s</sub>* are limited to third degree polynomials for ease of implementation. As such, four fitting parameters are required for each of *e<sub>s</sub>* and *g<sub>s</sub>* to define the softening of the matrix normal and shear responses to micro-damage development.\\n\\n*e<sub>s</sub>*(*S<sub>r</sub>*) = *e<sub>s0</sub>* + *e<sub>s1</sub>S<sub>r</sub>* + *e<sub>s2</sub>S<sub>r</sub>*<sup>2</sup> + *e<sub>s3</sub>S<sub>r</sub>*<sup>3</sup>\\n\\n*g<sub>s</sub>*(*S<sub>r</sub>*) = *g<sub>s0</sub>* + *g<sub>s1</sub>S<sub>r</sub>* + *g<sub>s2</sub>S<sub>r</sub>*<sup>2</sup> + *g<sub>s3</sub>S<sub>r</sub>*<sup>3</sup>\\n\\nwhere *S<sub>r</sub>* is the micro-damage reduced internal state variable. These eight material properties must be defined in an [external material properties file](#defining-the-material-properties-in-a-props-file). *S<sub>r</sub>* is stored in the 12<sup>th</sup> state variable slot, replacing `CDM_Plas12`, when Schapery theory is used in a model. The 13<sup>th</sup> state variable slot is not used when Schapery micro-damage is used.\\n\\n#### Schaefer\\nShear nonlinearity in the 1-2 plane can be modeled using the Schaefer prepeak model. In this model, effective plastic strain is related to effective plastic stress through the power law:\\n\\n*&epsilon;*<sub>plastic</sub> = *A &sigma;*<sup>*n*</sup>\\n\\nAdditionally, a yield criterion function (effective stress) is defined as:\\n\\n*f* = *&sigma;* = (*S*<sub>22</sub> + *a*<sub>6</sub> *S*<sub>12</sub><sup>2</sup>)<sup>1/2</sup> + *b*<sub>2</sub> *S*<sub>22</sub>\\n\\nwhere *a*<sub>6</sub>, *b*<sub>2</sub>, *A* and *n* are material constants needed for the Schaefer prepeak material model. These four material properties must be defined in an [external material properties file](#defining-the-material-properties-in-a-props-file). Additionally, when defining  *a*<sub>6</sub>, *b*<sub>2</sub>, *A* and *n* in the external material properties file the variables are prefixed with schaefer_ (to disambiguate the otherwise nondescript material property names and symbols).\\n\\nThe above two equations are used in concert to determine plastic strain through the relationship:\\n\\n*&epsilon;* <sub>plastic</sub> = *n A f* <sup>*n* - 1</sup> &part;*f* / &part;*S*<sub>*i*</sub> &part;*f* / &part;*S*<sub>j</sub>\\n\\n*f* (i.e., schaefer_f) and the tensorial plastic strain determined by the nonlinearity model are stored as state variables (27 through 32 for plastic strain and 33 for *f*)\\n\\n### Fiber tensile damage\\nA continuum damage mechanics model similar to the work of [Maimí et al. (2007)](https://doi.org/10.1016/j.mechmat.2007.03.005) is used to model tensile fiber damage evolution. The model utilizes a non-interacting maximum strain failure criterion, and bilinear softening after the initiation of failure. The area under the stress-strain curve is equal to the fracture toughness divided by the element length normal to fracture, i.e., `CDM_Lc1`. The required material properties are: XT, fXT, GXT, and fGXT, where fXT and fGXT are ratios of strength and fracture toughness for bilinear softening, defined as the *n* and *m* terms in equations (25) and (26) of [Dávila et al. (2009)](https://doi.org/10.1007/s10704-009-9366-z). To model a linear softening response, both fXT and fGXT should be set equal to 0.5.\\n\\n### Fiber compression damage\\n\\n#### Model 1: Max strain, bilinear softening (BL)\\nSame model as in tension, but for compression. Assumes maximum strain failure criterion and bilinear softening. The required material properties are: XC, fXC, GXC, and fGXC.\\n\\nLoad reversal assumptions from [Maimí et al. (2007)](https://doi.org/10.1016/j.mechmat.2007.03.006).\\n\\n#### Model 2: Placeholder\\n\\n#### Model 3, 4, 5: Fiber kinking theory (FKT)\\nA model based on Budiansky\\'s fiber kinking theory from [Budiansky (1983)](https://doi.org/10.1016/0045-7949(83)90141-4), [Budiansky and Fleck (1993)](https://doi.org/10.1016/0022-5096(93)90068-Q), and [Budiansky et al. (1998)](https://doi.org/10.1016/S0022-5096(97)00042-2) implemented using the DGD framework to model in-plane (1-2) and/or out-of-plane (1-3) fiber kinking. The model is described in detail in [Bergan et al. (2018)](https://doi.org/10.2514/6.2018-1221), [Bergan and Jackson (2018)](https://doi.org/10.12783/asc33/26003), and [Bergan (2019)](https://doi.org/10.2514/6.2019-1548). The model accounts for fiber kinking due to shear instability by considering an initial fiber misalignment, nonlinear shear stress-strain behavior via Ramberg-Osgood, and geometric nonlinearity. Fiber failure can be introduced by specifying a critical fiber rotation angle.\\n\\nThe required material properties are: XC, YC, wkb, alpha0, alpha_PL, and n_PL. The [feature flag](#controlling-which-features-are-enabled) for fiber compression should be set to \\'3\\', \\'4\\', or \\'5\\' to activate this model feature. Model \\'3\\' enables in-plane (1-2 plane) fiber kinking. Model \\'4\\' enables out-of-plane (1-3 plane) fiber kinking. Model \\'5\\' enables fiber kinking in both planes (uncoupled). This feature requires 25 state variables to be defined and initialized. The relevant state variables are:\\n- `CDM_phi0_12`: initial fiber misalignment (radians) in the 1-2 plane.\\n- `CDM_phi0_13`: initial fiber misalignment (radians) in the 1-3 plane.\\n- `CDM_gamma_12`: rotation of the fibers due to loading (radians) in the 1-2 plane.\\n- `CDM_gamma_13`: rotation of the fibers due to loading (radians) in the 1-3 plane.\\n- `CDM_Fbi`: the components of the first column of `Fm` used for decomposing the element where `i=1,2,3`.\\n\\nThe current fiber misalignment is `CDM_phi0_1i + CDM_gamma_1i` where `i=2 or 3`.\\n\\nThe initial conditions for the state variable `CDM_phi0_12` and `CDM_phi0_13` determine the initial fiber misalignments as described in [initial conditions](#initial-conditions).\\n\\nA fiber failure criterion described in [Bergan and Jackson (2018)](https://doi.org/10.12783/asc33/26003) is implemented to represent the material behavior in confined conditions under large strains (post failure). The fiber failure criterion is satisfied when\\n\\n*&phi;* &ge; *&phi;*<sub>ff,c</sub>\\n\\nwhere *&phi;* is the current fiber rotation. Once the fiber failure criterion is satisfied, the plastic shear strain is held constant. The value for *&phi;*<sub>ff,c</sub> is defined as the parameter `fkt_fiber_failure_angle` since it is not a well-defined material property. The fiber failure criterion is disabled when *&phi;*<sub>ff,c</sub> < 0. The same angle is used for in-plane and out-of-plane kinking.\\n\\nThe fiber kinking theory model implemented here is preliminary and has some known shortcomings and caveats:\\n- The model has only been tested for C3D8R. Limited application with C3D6 demonstrated issues. No testing has been completed for other element types.\\n- The interaction of this model with matrix cracking has not been fully tested and verified.\\n- No effort has been made to model progressive crushing.\\n- Other mechanisms of fiber compressive failure (e.g., shear driven fiber breaks) are not accounted for. An outcome of this is that the model predicts the material does not fail if shear deformation is fully constrained.\\n- No special consideration for load reversal has been included.\\n\\nRelevant single element tests are named starting with `test_C3D8R_fiberCompression_FKT`.\\n\\n### Friction\\nFriction is modeled on the damaged fraction of the cross-sectional area of DGD cracks using the approach of [Alfano and Sacco (2006)](https://doi.org/10.1002/nme.1728). The coefficient of friction *&mu;* must be defined to account for friction on the failed crack surface.\\n\\nThe amount of sliding which has taken place in the longitudinal and transverse directions are stored in state variables `CDM_slide1` and `CDM_slide2`, respectively.\\n\\n### Strain definition\\nThe strain is calculated using the deformation gradient tensor provided by the Abaqus solver. The default strain definition used is the Green-Lagrange strain:\\n\\n*E* = (*F*<sup>T</sup>*F* - *I*)/2\\n\\nHooke\\'s law is applied using the Green-Lagrange strain to calculate the 2<sup>nd</sup> Piola-Kirchhoff stress *S*.\\n\\n### Fiber nonlinearity\\nNonlinear elastic behavior in the fiber direction can be introduced with the material property c<sub>*l*</sub>. The expression used follows [Kowalski (1988)](https://doi.org/10.1520/STP26136S):\\n\\n*E<sub>1</sub>* = *E<sub>1</sub>*(1 + c<sub>*l*</sub>*&epsilon;*<sub>11</sub>)\\n\\nBy default, fiber nonlinearity is disabled by setting c<sub>*l*</sub> = 0.\\n\\n\\n## Elements\\nCompDam_DGD has been developed and tested using the Abaqus three-dimensional, reduced-integration `C3D8R` solid elements. Limited testing has been performed using the `CPS4R` plane stress element, the `SC8R` continuum shell element, the fully-integrated `C3D8` solid element, and the `COH3D8` cohesive element.\\n\\nBecause CompDam_DGD is a material model, it is expected to be compatible with structural elements generally. However, users are advised to perform tests with any previously untested element types before proceeding to use CompDam_DGD in larger structural models.\\n\\n\\n## Material properties\\nA set of material properties must be defined for the material of interest. This section describes how to specify the material properties.\\n\\n### Defining material properties\\nMaterial properties can be defined in the input deck or in a separate `.props` file. Definition of the material properties in a `.props` file is more convenient and generally preferred since it isolates the material properties from the structural model definition.\\n\\n#### Defining the material properties in a `.props` file\\nUsing a `.props` file is a versatile means of defining material properties. The subroutine looks for a file named as `jobName_materialName` where the job name is the name of the Abaqus job (default is input deck name) and the material is name assigned as `*Material, name=materialName` in the input deck. If no file is found, then the subroutine looks for `materialName.props`. The `.props` file must be located in the Abaqus working directory.\\n\\nThe `.props` should contain one property per line with the format `[NAME] = [VALUE]` where the name is symbolic name for the property and the value is assigned value for the property. Blank lines or commented text (denoted by `//`) is ignored. See the [table of material properties](#table-of-material-properties) for a complete list of material property symbolic names, acceptable values, and recommended test methods for characterizing the properties.\\n\\nWhen a `.props` is used to define the material properties, the feature flags and thickness still must be defined in the input deck.\\n\\n#### Defining the material properties in the input deck\\nMaterial properties can be defined in the input deck. Any optional material property can be left blank and the corresponding feature(s) will be disabled. The ordering of the material properties for the input deck definition is given in the first (#) column of the [table of material properties](#table-of-material-properties).\\n\\n#### Table of material properties\\n| #  |         Symbol         |   Name   |                  Description                  |                       Units                    |            Admissible values             | Recommended Test |\\n|----|------------------------|----------|-----------------------------------------------|------------------------------------------------|------------------------------------------|-----------------|\\n|  9 | *E<sub>1</sub>*        | E1       | Longitudinal Young\\'s modulus                  | F/L<sup>2</sup>                                | 0 < *E<sub>1</sub>* < &infin;            | ASTM D3039      |\\n| 10 | *E<sub>2</sub>*        | E2       | Transverse Young\\'s modulus                    | F/L<sup>2</sup>                                | 0 < *E<sub>2</sub>* < &infin;            | ASTM D3039      |\\n| 11 | *G<sub>12</sub>*       | G12      | In-plane Shear modulus                        | F/L<sup>2</sup>                                | 0 < *G<sub>12</sub>* < &infin;           | ASTM D3039      |\\n| 12 | *&nu;<sub>12</sub>*    | nu12     | Major Poisson\\'s ratio                         | -                                              | 0 &le; *&nu;<sub>12</sub>* &le; 1        | ASTM D3039      |\\n| 13 | *&nu;<sub>23</sub>*    | nu23     | Minor Poisson\\'s ratio                         | -                                              | 0 &le; *&nu;<sub>23</sub>* &le; 1        |                 |\\n|    | ===                    |          |                                               |                                                |                                          |                 |\\n| 14 | *Y<sub>T</sub>*        | YT       | Transverse tensile strength                   | F/L<sup>2</sup>                                | 0 < *Y<sub>T</sub>* < &infin;            | ASTM D3039      |\\n| 15 | *S<sub>L</sub>*        | SL       | Shear strength                                | F/L<sup>2</sup>                                | 0 < *S<sub>L</sub>* < &infin;            |                 |\\n| 16 | *G<sub>Ic</sub>*       | GYT      | Mode I fracture toughness                     | F/L                                            | 0 < *G<sub>Ic</sub>* < &infin;           | ASTM D5528      |\\n| 17 | *G<sub>IIc</sub>*      | GSL      | Mode II fracture toughness                    | F/L                                            | 0 < *G<sub>IIc</sub>* < &infin;          | ASTM D7905      |\\n| 18 | *&eta;*                | eta_BK   | BK exponent for mode-mixity                   | -                                              | 0 < *&eta;* < &infin;                 |                 |\\n| 19 | *Y<sub>C</sub>*        | YC       | Transverse compressive strength               | F/L<sup>2</sup>                                | 0 < *Y<sub>C</sub>* < &infin;            | ASTM D3410      |\\n| 20 | *&alpha;<sub>0</sub>*  | alpha0   | Fracture plane angle for pure trans. comp.    | Radians                                        | 0 &le; *&alpha;<sub>0</sub>* &le; &pi;/2 |                 |\\n|    | ------                 |          |                                               |                                                |                                          |                 |\\n| 21 | *E<sub>3</sub>*        | E3       | 3-direction Young\\'s modulus                   | F/L<sup>2</sup>                                | 0 < *E<sub>3</sub>* < &infin;            |                 |\\n| 22 | *G<sub>13</sub>*       | G13      | Shear modulus in 1-3 plane                    | F/L<sup>2</sup>                                | 0 < *G<sub>13</sub>* < &infin;           |                 |\\n| 23 | *G<sub>23</sub>*       | G23      | Shear modulus in 1-2 plane                    | F/L<sup>2</sup>                                | 0 < *G<sub>23</sub>* < &infin;           |                 |\\n| 24 | *&nu;<sub>13</sub>*    | nu13     | Poisson\\'s ratio in 2-3 plane                  | -                                              | 0 &le; *&nu;<sub>13</sub>* &le; 1        |                 |\\n|    | ------                 |          |                                               |                                                |                                          |                 |\\n| 25 | *&alpha;<sub>11</sub>* | alpha11  | Coefficient of long. thermal expansion        | /&deg;                                         | -1 &le; *&alpha;<sub>11</sub>* &le; 1    |                 |\\n| 26 | *&alpha;<sub>22</sub>* | alpha22  | Coefficient of tran. thermal expansion        | /&deg;                                         | -1 &le; *&alpha;<sub>22</sub>* &le; 1    |                 |\\n|    | ------                 |          |                                               |                                                |                                          |                 |\\n| 27 | *&alpha;<sub>PL</sub>* | alpha_PL | Nonlinear shear parameter                     | (F/L<sup>2</sup>)<sup>1-*n*<sub>PL</sub></sup> | 0 &le; *&alpha;<sub>PL</sub>* < &infin;  |                 |\\n| 28 | *n<sub>PL</sub>*       | n_PL     | Nonlinear shear parameter                     | -                                              | 0 &le; *n<sub>PL</sub>* < &infin;        |                 |\\n|    | ------                 |          |                                               |                                                |                                          |                 |\\n| 29 | *X<sub>T</sub>*        | XT       | Long. tensile strength                        | F/L<sup>2</sup>                                | 0 < *X<sub>T</sub>* < &infin;            | ASTM D3039      |\\n| 30 | *f<sub>XT</sub>*       | fXT      | Long. tensile strength ratio                  | -                                              | 0 &le; *f<sub>XT</sub>* &le; 1           |                 |\\n| 31 | *G<sub>XT</sub>*       | GXT      | Long. tensile fracture toughness              | F/L                                            | 0 < *G<sub>XT</sub>* < &infin;           |                 |\\n| 32 | *f<sub>GXT</sub>*      | fGXT     | Long. tensile fracture toughness ratio        | -                                              | 0 &le; *f<sub>GXT</sub>* &le; 1          |                 |\\n|    | ------                 |          |                                               |                                                |                                          |                 |\\n| 33 | *X<sub>C</sub>*        | XC       | Long. compressive strength                    | F/L<sup>2</sup>                                | 0 < *X<sub>C</sub>* < &infin;            | ASTM D3410      |\\n| 34 | *f<sub>XC</sub>*       | fXC      | Long. compression strength ratio              | -                                              | 0 &le; *f<sub>XC</sub>* &le; 1           |                 |\\n| 35 | *G<sub>XC</sub>*       | GXC      | Long. compression fracture toughness          | F/L                                            | 0 < *G<sub>XC</sub>* < &infin;           |                 |\\n| 36 | *f<sub>GXC</sub>*      | fGXC     | Long. compression fracture toughness ratio    | -                                              | 0 &le; *f<sub>GXC</sub>* &le; 1          |                 |\\n|    | ------                 |          |                                               |                                                |                                          |                 |\\n| 37 | *c<sub>l</sub>*        | cl       | Fiber nonlinearity coefficient                | -                                              | 0 &le; *c<sub>l</sub>* 33                |                 |\\n| 38 | *w<sub>kb</sub>*       | w_kb     | Width of the kink band                        | L                                              | 0 &le; *w<sub>kb</sub>* &infin;          |                 |\\n| 40 | *&mu;*                 | mu       | Coefficient of friction                       | -                                              | 0 &le; *&mu;* &le; 1                     |                 ||\\n\\nNotes:\\n- The first five inputs (above the ===) are required\\n- Properties for each feature are grouped and separated by ------\\n- The number in the first column corresponds to the property order when defined in the input deck\\n- Properties not listed in the table above (see next section):\\n  1. [feature flags](#controlling-which-features-are-enabled)\\n  2. *reserved*\\n  3. [thickness](#definition-of-thickness)\\n  4. *reserved*\\n  5. *reserved*\\n  6. *reserved*\\n  7. *reserved*\\n  8. *reserved*\\n- &infin; is calculated with the Fortran intrinsic `Huge` for double precision\\n- In the event that both a `.props` file is found and material properties are specified in the input deck (`nprops > 8`), then the material properties from the input deck are used and a warning is used.\\n\\n### Required inputs for the `*Material` data lines in the input deck\\nThe feature flags and thickness are defined in the input deck on the material property data lines. These properties must be defined in the input deck whether the other material properties are defined via the .props file or via the input deck. While feature flags and thickness are not material properties per se, they are used in controlling the behavior of the material model.\\n\\n#### Controlling which features are enabled\\nModel features can be enabled or disabled by two methods. The first method is specifying only the material properties required for the features you would like to enable. CompDam_DGD disables any feature for which all of the required material properties have not been assigned. If an incomplete set of material properties are defined for a feature, a warning is issued.\\n\\nThe second method is by specifying the status of each feature directly as a material property in the input deck. Each feature of the subroutine is controlled by a position in an integer, where 0 is disabled and 1 is enabled. In cases where mutually exclusive options are available, numbers greater than 1 are used to specify the particular option to use.\\n\\nThe positions correspond to the features as follows:\\n- Position 1: Matrix damage (1=intra-laminar cracking in solid elements, 2=interlaminar cracking in cohesive elements)\\n- Position 2: Shear nonlinearity (1=Ramberg-Osgood 1-2 plane, 2=Schapery, 3=Ramberg-Osgood 3-D, 4=Ramberg-Osgood 1-3 plane, 5=Schaefer || more information [here](#shear-nonlinearity))\\n- Position 3: Fiber tensile damage\\n- Position 4: Fiber compression damage (1=max strain, 2=N/A, 3=FKT-12, 4=FKT-13, 5=FKT-3D || more information [here](#fiber-compression-damage))\\n- Position 5: Energy output contribution (0=all mechanisms, 1=only fracture energy, 2=only plastic energy)\\n- Position 6: Friction\\n\\nFor example, `101000` indicates that the model will run with matrix damage and fiber tension damage enabled; `120001` indicates that the model will run with matrix damage, in-plane shear nonlinearity using Schapery theory, and friction; and `200000` indicates that the model is being applied to cohesive elements.\\n\\n#### Definition of thickness\\nLength along the thickness-direction associated with the current integration point.\\n\\n## State variables\\nThe table below lists all of the state variables in the model. The model requires a minimum of 18 state variables. Additional state variables are defined depending on which (if any) shear nonlinearity and fiber compression features are enabled. For fiber compression model 1: nstatev = 19 and for model 3: nstatev = 25. For shear nonlinearity models 3 or 4: nstatev = 21.\\n\\n| # | Name             | Description                                                           |\\n|---|------------------|-----------------------------------------------------------------------|\\n|  1| `CDM_d2`         | d2, Matrix cohesive damage variable                                   |\\n|  2| `CDM_Fb1`        | Bulk material deformation gradient tensor component 12                |\\n|  3| `CDM_Fb2`        | Bulk material deformation gradient tensor component 22                |\\n|  4| `CDM_Fb3`        | Bulk material deformation gradient tensor component 32                |\\n|  5| `CDM_B`          | Mode Mixity (*G*<sub>II</sub> / (*G*<sub>I</sub> + *G*<sub>II</sub>)) |\\n|  6| `CDM_Lc1`        | Characteristic element length along 1-direction                       |\\n|  7| `CDM_Lc2`        | Characteristic element length along 2-direction                       |\\n|  8| `CDM_Lc3`        | Characteristic element length along 3-direction                       |\\n|  9| `CDM_FIm`        | Matrix cohesive failure criterion (del/del_0)                         |\\n| 10| `CDM_alpha`      | alpha, the cohesive surface normal [degrees, integer]                 |\\n| 11| `CDM_STATUS`     | STATUS (for element deletion)                                         |\\n| 12| `CDM_Plas12`     | 1-2 plastic strain                                                    |\\n| 13| `CDM_Inel12`     | 1-2 inelastic strain                                                  |\\n| 14| `CDM_FIfT`       | Failure index for fiber tension                                       |\\n| 15| `CDM_slide1`     | Cohesive sliding displacement, fiber direction                        |\\n| 16| `CDM_slide2`     | Cohesive sliding displacement, transverse direction                   |\\n| 17| `CDM_FIfC`       | Failure index for fiber compression                                   |\\n| 18| `CDM_d1T`        | Fiber tension damage variable                                         |\\n|---|------------------|-----------------------------------------------------------------------|\\n| 19| `CDM_d1C`        | Fiber compression damage variable                                     |\\n|---|------------------|-----------------------------------------------------------------------|\\n| 20| `CDM_Plas13`     | 1-3 plastic strain                                                    |\\n| 21| `CDM_Inel13`     | 1-3 inelastic strain                                                  |\\n|---|------------------|-----------------------------------------------------------------------|\\n| 22| `CDM_phi0_12`    | Initial fiber misalignment, 1-2 plane (radians)                       |\\n| 23| `CDM_gamma_12`   | Current rotation of the fibers due to loading, 1-2 plane (radians)    |\\n| 24| `CDM_phi0_13`    | Initial fiber misalignment, 1-3 plane (radians)                       |\\n| 25| `CDM_gamma_13`   | Current rotation of the fibers due to loading, 1-3 plane (radians)    |\\n| 26| `CDM_reserve`    | Reserved                                                              |\\n|---|------------------|-----------------------------------------------------------------------|\\n| 27| `CDM_Ep1`        | Plastic strain in 11 direction calculated using Schaefer Theory       |\\n| 28| `CDM_Ep2`        | Plastic strain in 22 direction calculated using Schaefer Theory       |\\n| 29| `CDM_Ep3`        | Plastic strain in 33 direction calculated using Schaefer Theory       |\\n| 30| `CDM_Ep4`        | Plastic strain in 12 direction calculated using Schaefer Theory       |\\n| 31| `CDM_Ep5`        | Plastic strain in 23 direction calculated using Schaefer Theory       |\\n| 32| `CDM_Ep6`        | Plastic strain in 31 direction calculated using Schaefer Theory       |\\n| 33| `CDM_fp1`        | Yield criterion (effective stress) calculated using Schaefer Theory   |\\n\\nWhen using the material model with cohesive elements, a different set of state variables are used. Cohesive state variables with a similar continuum damage mechanics counterpart utilize the same state variable number. The cohesive element material model requires fewer state variables, however, resulting in \"gaps\" in the cohesive state variable numbering.\\n\\n| # | Name             | Description                                                           |\\n|---|------------------|-----------------------------------------------------------------------|\\n|  1| `COH_dmg`        | Cohesive damage variable                                              |\\n|  2| `COH_delta_s1`   | Displacement-jump in the first shear direction                        |\\n|  3| `COH_delta_n`    | Displacement-jump in the normal direction                             |\\n|  4| `COH_delta_s2`   | Displacement-jump in the second shear direction                       |\\n|  5| `COH_B`          | Mode Mixity (*G*<sub>II</sub> / (*G*<sub>I</sub> + *G*<sub>II</sub>)) |\\n|  9| `COH_FI`         | Cohesive failure criterion (del/del_0)                                |\\n| 15| `COH_slide1`     | Cohesive sliding displacement, fiber direction                        |\\n| 16| `COH_slide2`     | Cohesive sliding displacement, transverse direction                   |\\n\\n### Initial conditions\\nAll state variables should be initialized using the `*Initial conditions` command. As a default, all state variables should be initialized as zero, except `CDM_alpha`, `CDM_STATUS`, `CDM_phi0_12`, and `CDM_phi0_13`.\\n\\nThe initial condition for `CDM_alpha` can be used to specify a predefined angle for the cohesive surface normal. To specify a predefined `CDM_alpha`, set the initial condition for `CDM_alpha` to an integer (degrees). The range of valid values for `CDM_alpha` depends on the aspect ratio of the element, but values in the range of 0 to 90 degrees are always valid. Setting `CDM_alpha` to -999 will make the subroutine evaluate cracks every 10 degrees in the 2-3 plane to find the correct crack initiation angle. Note that `CDM_alpha` is measured from the 2-axis rotating about the 1-direction. The amount by which alpha is incremented when evaluating matrix crack initiation can be changed from the default of 10 degrees by modifying `alpha_inc` in the `CompDam.parameters` file. Note that `CDM_alpha = 90` only occurs when `CDM_alpha` is initialized as 90; when `CDM_alpha` is initialized to -999, the value of 90 is ignored in the search to find the correct initiation angle since it is assumed that delaminations are handled elsewhere in the finite element model (e.g., using cohesive interface elements).\\n\\nSince `CDM_STATUS` is used for element deletion, always initialize `CDM_STATUS` to 1.\\n\\nThe initial condition for `CDM_phi0_12` and `CDM_phi0_13` are used to specify the initial fiber misalignment. One of the followings options is used depending on the initial condition specified for `CDM_phi0_12` and `CDM_phi0_13` as follows:\\n- *&phi;<sub>0</sub>* = 0 :: The value for *&phi;<sub>0</sub>* is calculated for shear instability. For 3-D kinking, *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>* is required.\\n- *&phi;<sub>0</sub>* &le; 0.5 :: The value provided in the initial condition is used as the initial fiber misalignment.\\n- *&phi;<sub>0</sub>* = 1 :: A pseudo random uniform distribution varying spatially in the 1-direction is used. The spatial distribution algorithm relies on an uniform element size and fiber aligned mesh. The random number generator can be set to generate the same realizations or different realizations on multiple nominally identical analyses using the Boolean parameter `fkt_random_seed`. When using the random distribution for *&phi;<sub>0</sub>*, the characteristic length must be set to include 6 components: `*Characteristic Length, definition=USER, components=6`. For 3-D kinking, *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>* is required.\\n- *&phi;<sub>0</sub>* = 2 :: Identical to *&phi;<sub>0</sub>* = 1, with the exception that a different realization is calculated for each ply. For 3-D kinking, *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>* is required.\\n- *&phi;<sub>0</sub>* = 3 :: (Intended for use with 3-D FKT only) A pseudo random distribution varying spatially in the 1-direction is used with a 2-parameter lognormal distribution for the polar angle and a normal distribution for the azimuthal angle. The parameters starting with `fkt_init_misalignment` are used to control the polar and azimuthal distributions. Requires *&phi;<sub>0,12</sub>* = *&phi;<sub>0,13</sub>*.\\n\\nPre-existing damage can be modeled by creating an element set for the damaged region and specifying different initial conditions for this element set. For example, to create an intraply matrix crack with no out-of-plane orientation, the following initial conditions could be specified for the cracked elements:\\n\\n    *Initial Conditions, type=SOLUTION\\n     damaged_elset,  1.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,\\n              0.d0,  0.d0,     0,     1,  0.d0,  0.d0,  0.d0,  0.d0,\\n              0.d0,  0.d0,  0.d0,  0.d0\\n\\n\\n## Fatigue analyses\\nThe cohesive fatigue constitutive model in CompDam can predict the initiation and the propagation of matrix cracks and delaminations as a function of fatigue cycles. The analyses are conducted such that the applied load (or displacement) corresponds to the maximum load of a fatigue cycle. The intended use is that the maximum load (or displacement) is held constant while fatigue damage develops with increasing step time. The constitutive model uses a specified load ratio *R*<sub>min</sub>/*R*<sub>max</sub>, the solution increment, and an automatically-calculated cycles-per-increment ratio to accumulate the damage due to fatigue loading. The cohesive fatigue model response is based on engineering approximations of the endurance limit as well as the Goodman diagram. No additional material inputs must be defined or state variables requested beyond those required for a quasi-static analysis step. This approach can predict the stress-life diagrams for crack initiation, the Paris law regime, as well as the transient effects of crack initiation and stable tearing.\\n\\nA detailed description of the cohesive fatigue implemented herein is available in a [2018 NASA technical paper by Carlos Dávila](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20180004395.pdf).\\n\\n### Usage\\nThe fatigue capability of CompDam is disabled by default. To run a fatigue analysis, one of the analysis steps must be identified as a fatigue step. A step is identified as a fatigue step by setting the `fatigue_step` parameter to the target step number, e.g., `fatigue_step = 2` for the second analysis step to be a fatigue step. The first analysis step cannot be a fatigue step, as the model is assumed to be unloaded at that point.\\n\\nThe load ratio *R*<sub>min</sub>/*R*<sub>max</sub> has a default value of 0.1, and can be changed using the parameter `fatigue_R_ratio`.\\n\\nAn example of a double cantilever beam subjected to fatigue under displacement-control is included in the `examples/` directory. The geometry and conditions of this example problem correspond to the results presented in Figure 20 of [Dávila (2018)](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20180004395.pdf).\\n\\n### Interpreting the results of a fatigue analysis\\nWithin a fatigue step, each solution increment represents either a number of fatigue cycles or a fractional part of a single fatigue cycle. During the solution, the number of fatigue cycles per solution increment changes based on the maximum amount of energy dissipation in any single element. If the rate of energy dissipation is too high (as defined by the parameter `fatigue_damage_max_threshold`), the increments-to-cycles ratio is decreased. If the rate of energy dissipation is too low (as defined by the parameter `fatigue_damage_min_threshold`), the increments-to-cycles ratio is increased. The parameter `cycles_per_increment_init` defines the initial ratio of fatigue cycles per solution increment. Any changes to increments-to-cycles ratio are logged in an additional output file ending in `_inc2cycles.log`, with columns for the fatigue step solution increment, the updated increments-to-cycles ratio, and the accumulated fatigue cycles.\\n\\n\\n## Implicit solver compatibility\\nThe repository includes a developmental capability to run the CompDam VUMAT in an Abaqus/Standard analysis using a wrapper, `for/vumatWrapper.for`, which translates between the UMAT and VUMAT user subroutine interfaces. The intended usage is for Abaqus/Standard runs with little or no damage.\\n\\n### Usage\\nTo run an analysis with CompDam in Abaqus/Standard, the following input deck template is provided. Note that 9 additional state variables are required.\\n\\n    *Section controls, name=control_name, hourglass=ENHANCED\\n    **\\n    *Material, name=IM7-8552\\n    *Density\\n     1.57e-09,\\n    *User material, constants=40\\n    ** 1              2  3          4  5  6  7  8\\n    ** feature flags,  , thickness, 4, 5, 6, 7, 8\\n              100001,  ,       0.1,  ,  ,  ,  ,  ,\\n    **\\n    **  9         10        11        12        13        14        15        16\\n    **  E1,       E2,       G12,      nu12,     nu23,     YT,       SL        GYT,\\n        171420.0, 9080.0,   5290.0,   0.32,     0.52,     62.3,     92.30,    0.277,\\n    **\\n    **  17        18        19        20        21        22        23        24\\n    **  GSL,      eta_BK,   YC,       alpha0    E3,       G13,      G23,      nu13,\\n        0.788,    1.634,    199.8,    0.925,      ,          ,         ,          ,\\n    **\\n    **  25        26        27        28        29        30        31        32\\n    **  alpha11,  alpha22,  alpha_PL, n_PL,     XT,       fXT,      GXT,      fGXT,\\n        -5.5d-6,  2.58d-5,          ,     ,     2326.2,   0.2,      133.3,    0.5,\\n    **\\n    **  33        34        35        36        37        38        39        40\\n    **  XC,       fXC,      GXC,      fGXC,       cl,     w_kb,     None,     mu\\n        1200.1,      ,         ,          ,         ,     0.1,          ,     0.3\\n    **\\n    *Depvar, delete=11\\n      28,\\n      1, CDM_d2\\n      2, CDM_Fb1\\n      3, CDM_Fb2\\n      4, CDM_Fb3\\n      5, CDM_B\\n      6, CDM_Lc1\\n      7, CDM_Lc2\\n      8, CDM_Lc3\\n      9, CDM_FIm\\n     10, CDM_alpha\\n     11, CDM_STATUS\\n     12, CDM_Plas12\\n     13, CDM_Inel12\\n     14, CDM_FIfT\\n     15, CDM_slide1\\n     16, CDM_slide2\\n     17, CDM_FIfC\\n     18, CDM_d1T\\n     19, CDM_d1C\\n     20, CDM_DIRECT11\\n     21, CDM_DIRECT21\\n     22, CDM_DIRECT31\\n     23, CDM_DIRECT12\\n     24, CDM_DIRECT22\\n     25, CDM_DIRECT32\\n     26, CDM_DIRECT13\\n     27, CDM_DIRECT23\\n     28, CDM_DIRECT33\\n    *User defined field\\n    **\\n    ** INITIAL CONDITIONS\\n    **\\n    *Initial Conditions, Type=Solution\\n    ALL_ELEMS,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,\\n    0.d0,  0.d0,  -999,     1,  0.d0,  0.d0,  0.d0,  0.d0,\\n    0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,  0.d0,\\n    0.d0,  0.d0,  0.d0,  0.d0,  0.d0\\n    *Initial Conditions, Type=Field, Variable=1\\n    GLOBAL,  0.d0\\n    ** GLOBAL is an nset with all nodes attached to CompDam-enabled elements\\n    ** In each step, NLGEOM=YES must be used. This is NOT the default setting.\\n\\n### Current limitations\\nAs the `vumatWrapper` is a developmental capability, several important limitations exist at present:\\n1. The material Jacobian tensor is hard-coded in `for/vumatWrapper.for` for IM7/8552 elastic stiffnesses. A more general Jacobian is needed.\\n2. The material response can become inaccurate for large increments in rotations. If large rotations occur, small increments must be used. A cut-back scheme based on rotation increment size is needed.\\n3. Testing has been conducted on the C3D8R element type only.\\n\\n\\n## Example problems\\nThe directory `examples/` includes example models that use CompDam along with corresponding files that defined the expected results (for use with Abaverify, following the same pattern as the test models in the `tests/` directory. The file `example_runner.py` can be used to automate submission of several models and/or for automatically post-processing the model results to verify that they match the expected results.\\n\\n\\n## Advanced debugging\\nUsing an interactive debugger helps to identify issues in the Fortran code. Abaqus knowledge base article QA00000007986 describes the details involved. The following is a quick-start guide for direct application to CompDam.\\n\\nSeveral statements for debugging need to be uncommented in the environment file. Follow these steps:\\n1. Copy your system environment file to your local working directory. For the example below, copy the environment file to the `tests` directory.\\n2. Edit the local environment file: uncomment lines that end with `# <-- Debugging`, `# <-- Debug symbols`, and `# <-- Optimization Debugging`\\n\\nRun the job with the `-debug` and `-explicit` arguments. For example:\\n```\\n$ abaqus -j test_C3D8R_fiberTension -user ../for/CompDam_DGD.for -double both -debug -explicit\\n```\\n\\nThis command should open the [Visual Studio debugging software](https://msdn.microsoft.com/en-us/library/sc65sadd.aspx) automatically. Open the source file(s) to debug. At a minimum, open the file with the subroutine entry point `for/CompDam_DGD.for`. Set a break point by clicking in the shaded column on the left edge of the viewer. The break point will halt execution. Press <kbd>F5</kbd> to start the solver. When the break point is reached, a yellow arrow will appear and code execution will pause. Press <kbd>F5</kbd> to continue to the next break point, press <kbd>F11</kbd> to execute the next line of code following execution into function calls (Step Into), or press <kbd>F10</kbd> to execute the next line of code but not follow execution into function calls (Step Over).\\n\\nTo stop execution, close the Visual Studio window. Choose stop debugging and do not save your changes.\\n\\n[More tips on debugging Fortran programs from Intel](https://software.intel.com/en-us/articles/tips-for-debugging-run-time-failures-in-intel-fortran-applications).\\n\\n## Python extension module\\nCompDam can be compiled into a [Python extension module](https://docs.python.org/2/extending/extending.html), which allows many of the Fortran subroutines and functions in the `for` directory to be called from Python. The Python package [`f90wrap`](https://github.com/jameskermode/f90wrap) is used to automatically generate the Python extension modules that interface with the Fortran code. This Python extension module functionality is useful for development and debugging.\\n\\n### Dependencies and setup\\nThe python extension module requires some additional dependencies. First, the procedure only works on Linux using the bash shell. Windows users can use the [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/about). In addition, `gfortran` 4.6+ is required. Type `gfortran --version` to check if you have this available. The remaining dependencies are python packages and can be installed as follows. The Python extension module works with Python 2 and 3; Python 2.7 is used for consistency with Abaqus in the following description.\\n\\nUsing [Conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) significantly simplifies the setup process, so it is assumed that you have a recent version of Conda available (see the [Conda installation guide](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)). Further, the bash scripts described below include calls to Conda, so they will not work correctly without installing and configuring Conda as follows. Add the Conda-Forge channel by typing:\\n```\\n$ conda config --add channels conda-forge\\n```\\n\\nConda stores Python packages in containers called environments. Create a new environment:\\n```\\n$ conda create --name compdam python=2.7\\n```\\nand switch to your new environment:\\n```\\n$ source activate compdam\\n```\\nwhich will add `(compdam)` to the prompt. Install `numpy`, `matplotlib`, and `f90wrap` by typing:\\n```\\n(compdam) $ conda install numpy matplotlib f90wrap\\n```\\nAfter typing \\'y\\' in response to the prompt asking if you would like to proceed, Conda will install `f90wrap` and all of its dependencies. This completes the setup process. These steps only need to be executed once.\\n\\nNote, you can exit the Conda environment by typing `source deactivate`. When you open a new session, you will need to activate the Conda environment by typing `source activate compdam`.\\n\\n### Example usage\\nThis section describes how to compile CompDam into a Python extension module and run a simple example.\\n\\nThe relevant files are in the `pyextmod` directory, so set `pyextmod` as your current working directory.\\n\\nThe bash shell is required. Type `bash` to open a bash shell session if you are using a different shell. Activate the environment in which you have installed the dependencies listed above, e.g. `source activate compdam`.\\n\\nNext compile the python extension module by executing `make` in the `pyextmod` directory as follows:\\n```\\n(compdam) CompDam_DGD/pyextmod> make\\n```\\n\\nWhen you execute `make`, the Fortran modules in the `for` directory are compiled to object files, the shared library `_CompDam_DGD.so` is built, and the Python extension module interface `CompDam_DGD.py` is created. A large amount of output is given in the terminal. After the module is created, most of the functionality of CompDam is available in python with `import CompDam_DGD`.\\n\\nThe file `test_pyextmod_dgdevolve.py` shows an example of how the python extension module can be used. Just as when CompDam_DGD is used in Abaqus, `CompDam_DGD.py` expects `CompDam.parameters` and `props` files (if provided) are located in the working directory. Note the `test_pyextmod_dgdevolve.py` loads a visualization tool that shows how the element deforms as equilibrium is sought by the DGD algorithm.\\n\\nIt is necessary to recompile the CompDam after making changes to the Fortran code. Recompile with the `make` command. It is a good idea to run `make clean` before rerunning `make` to remove old build files.\\n\\nNote that portions of CompDam that are specific to Abaqus are hidden from `f90wrap` using the preprocessor directive `#ifndef PYEXT`.\\n\\n### Associated scripts\\nIn the `tests` directory the shell scripts `pyextmod_compile.sh` and `pyextmod_run.sh` are available to help streamline execution of the python extension module. These two scripts assume that Conda environment called `compdam` is available with `abaverify` and `f90wrap`. Both must be executed with the `-i` option. The script `pyextmod_run.sh` loads a debug file and executes the specified DGD routine. The DGD routine and the debug file are specified as arguments as follows:\\n```\\n$ bash -i pyextmod_run.sh dgdevolve <job-name>\\n```\\nThe `<job-name>` is the Abaqus job name where it is assumed that the debug file resides in the testOutput folder with the name `job-name-1-debug-0.py`\\n\\nIn the `pyextmod` directory, the `helpers.py` file includes logic to load debug.py files.\\n\\n\\n## Summary of tests classes\\nThis section includes a brief summary of each test implemented in the `tests` folder. The input deck file names briefly describe the test. All of the input decks start with `test_<elementType>_` and end with a few words describing the test. A more detailed description for each is given below:\\n- *elastic_fiberTension*: Demonstrates the elastic response in the 1-direction under prescribed extension. The 1-direction stress-strain curve has the modulus E1.\\n- *elastic_matrixTension*: Demonstrates the elastic response in the 2-direction under prescribed extension. The 2-direction stress-strain curve has the modulus E2.\\n- *elastic_simpeShear12*: Demonstrates the elastic response in the 1-2 plane. The 1-2 plane stress-strain curve has the module G12.\\n- *elementSize*: Verifies that the characteristic element lengths Lc1, Lc2, and Lc3 are being properly calculated.\\n- *error*: Verifies that analyses can cleanly terminate upon encountering an error within the user subroutine.\\n- *failureEnvelope_sig11sig22*: A parametric model in which *&sigma;<sub>11</sub>* is swept from *-X<sub>C</sub>* to *X<sub>T</sub>* and *&sigma;<sub>22</sub>* is swept from *-Y<sub>C</sub>* to *Y<sub>T</sub>* in order to re-create the corresponding failure envelope.\\n- *failureEnvelope_sig12sig22*: A parametric model in which *&tau;<sub>12</sub>* is swept from *0* to *S<sub>L</sub>* and *&sigma;<sub>22</sub>* is swept from *-Y<sub>C</sub>* to *Y<sub>T</sub>* in order to re-create the corresponding failure envelope.\\n- *failureEnvelope_sig12sig23*: A parametric model in which *&tau;<sub>12</sub>* is swept from *0* to *S<sub>L</sub>* and *&tau;<sub>23</sub>* is swept from *0* to *S<sub>T</sub>* in order to re-create the corresponding failure envelope.\\n- *fatigue_normal*: Demonstrates the traction-displacement curve of a cohesive law subjected to mode I fatigue loading.\\n- *fatigue_shear13*: Demonstrates the traction-displacement curve of a cohesive law subjected to shear fatigue loading in the 1-3 plane.\\n- *fiberCompression_BL*: Demonstrates the constitutive response in the 1-direction under prescribed shortening. The 1-direction stress-strain curve has a bilinear softening law. A conventional CDM approach to material degradation is used.\\n- *fiberCompression_FKT*: Demonstrates the constitutive response in the 1-direction under prescribed shortening. The fiber kink band model is used. `FF` indicates that the fiber failure criterion is enabled. `FN` indicates that fiber nonlinearity is enabled.\\n- *fiberLoadReversal*: Demonstrates the constitutive response in the 1-direction under prescribed extension and shortening reversals. The 1-direction stress-strain curve shows the intended behavior under load reversal.\\n- *fiberTension*: Demonstrates the constitutive response in the 1-direction under prescribed extension. The 1-direction stress-strain curve is trilinear.\\n- *matrixCompression*: Demonstrates the constitutive response in the 2-direction under prescribed compression displacement.\\n- *matrixTension*: Demonstrates the constitutive response in the 2-direction under prescribed extension. The 2-direction stress-strain curve is bilinear.\\n- *mixedModeMatrix*: A parametric model used to stress the internal DGD convergence loop. The crack angle *&alpha;* and the direction of loading are varied. Large tensile and compressive displacements are prescribed to ensure the DGD method is able to find converged solutions under a wide variety of deformations.\\n- *nonlinearShear12*: Demonstrates the nonlinear Ramberg-Osgood constitutive response under prescribed simple shear deformation in the 1-2 plane with matrix damage enabled. Several cycles of loading and unloading are applied, with increasing peak displacements in each cycle.\\n- *nonlinearShear12_loadReversal*: Demonstrates the response of the Ramberg-Osgood model under load reversal in the 1-2 plane. Several cycles of loading and unloading are applied, with inelastic strain accumulated throughout the load history. Damage is disabled.\\n- *nonlinearShear13*: Demonstrates the nonlinear Ramberg-Osgood constitutive response under prescribed simple shear deformation in the 1-3 plane with matrix damage enabled. Several cycles of loading and unloading are applied, with increasing peak displacements in each cycle.\\n- *nonlinearShear13_loadReversal*: Demonstrates the response of the Ramberg-Osgood model under load reversal in the 1-3 plane. Several cycles of loading and unloading are applied, with inelastic strain accumulated throughout the load history. Damage is disabled.\\n- *schapery12*: Demonstrates the in-plane response to prescribed simple shear deformation for the Schapery micro-damage model. Several cycles of loading and unloading are applied, with increasing peak shear displacements in each cycle.\\n- *simpleShear12*: Demonstrates the constitutive response under prescribed simple shear. The shear stress-strain curve is bilinear.\\n- *simpleShear12friction*: Demonstrates the constitutive response under prescribed simple shear with friction enabled. The element is loaded under transverse compression and then sheared. Shows the friction-induced stresses.\\n\\n## Contributing\\nWe invite your contributions to CompDam_DGD! Please submit contributions (including a test case) with pull requests so that we can reproduce the behavior of interest. Commits history should be clean. Please contact the developers if you would like to make a major contribution to this repository. Here is a [checklist](contributing-checklist.md) that we use for contributions.\\n\\n## Citing CompDam\\nIf you use CompDam, please cite using the following BibTex entry:\\n\\n    @misc{CompDam,\\n    title={Comp{D}am - {D}eformation {G}radient {D}ecomposition ({DGD}), v2.5.0},\\n    author={Frank A. Leone and Andrew C. Bergan and Carlos G. D\\\\\\'{a}vila},\\n    note={https://github.com/nasa/CompDam\\\\_DGD},\\n    year={2019}\\n    }\\n'},\n",
       " {'repo': 'nasa/astrobot',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# astrobot\\nA Slack/Discord bot integration with NASA data.\\n\\nThis is a Slack/Discord bot that is designed to use the NASA APOD API to allow users to query the API through Slack or Discord. \\n\\n## Contributing\\nWe do accept pull requests from the public. Please note that we can be slow to respond. Please be patient. Pull requests should not impact previous functionality. \\n\\n## Feedback\\nStar this repo if you found it useful. Use the github issue tracker to give feedback on this repo.\\n'},\n",
       " {'repo': 'nasa/irg_open',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': 'Notices:\\n--------\\nCopyright © 2020 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nDisclaimers\\n-----------\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n\\nirg_open\\n==============================\\nThis is a repository of tools to be shared across projects. It is open source,\\nso do not add anything private or EAR/ITAR restricted. It originated in the\\nIntelligent Robotics Group (IRG) at NASA Ames Research Center. Hence the name.\\n\\n'},\n",
       " {'repo': 'nasa/tblCRCTool',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/tblCRCTool/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/tblCRCTool/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : Tool : Table CRC Generator\\n\\nThis repository contains NASA\\'s Table CRC Generator Tool (tblCRCTool), which is a framework component of the Core Flight System.\\n\\nThis lab application is a ground utility to generate binary table CRCs for cFS. It is intended to be located in the `tools/tblCRCTool` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.\\n\\n## Version Notes\\n\\n### Development Build: 1.2.0-rc1+dev25\\n\\n- Fix #43, Add Testing Tools to the Security Policy\\n- Fix #36 #38 #40 #41, Check lseek return and exit/error processing updates\\n- See <https://github.com/nasa/tblCRCTool/pull/89>\\n\\n### Development Build: 1.2.0-rc1+dev19\\n\\n- Changes CLI \"help\" option to use two dashes: `--help`\\n- Adds static analysis and format check to continuous integration workflow. Adds workflow status badges to ReadMe.\\n- Adds CodeQL Analysis to continuous integration workflow.\\n- See <https://github.com/nasa/tblCRCTool/pull/35>\\n\\n### Development Build: 1.2.0-rc1+dev12\\n\\n- Documentation: Add `Security.md` with instructions on reporting vulnerabilities\\n- Removes unimplemented CRC cases to eliminate static analysis warnings\\n- See <https://github.com/nasa/tblCRCTool/pull/29>\\n\\n### Development Build: 1.2.0-rc1+dev3\\n\\n- Use `sizeof()` instead of a hard coded value for the table file header size to keep this tool in sync if the size of the cFE file or table header should ever change.\\n- Update version baseline to v1.2.0-rc1\\n- Set REVISION number to 99 to indicate development version\\n- See <https://github.com/nasa/tblCRCTool/pull/25>\\n\\n### Development Build: 1.1.0+dev7\\n\\n- Create a version header file\\n- Report version when responding to `-help` command\\n- See <https://github.com/nasa/tblCRCTool/pull/22>\\n\\n### Development Build: 1.1.1\\n\\n- Apply Code Style\\n- See <https://github.com/nasa/tblCRCTool/pull/18>\\n\\n### **_OFFICIAL RELEASE: 1.1.0 - Aquila_**\\n\\n- Minor updates (see https://github.com/nasa/tblCRCTool/pull/12)\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### **_OFFICIAL RELEASE: 1.0.0a_**\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\nNOTE - there are other parameter set management schemes used with the cFS (JSON, csv, etc) which may be more applicable for modern missions.  Contact the community as detailed below for more information.\\n\\n## Known issues\\n\\nThis ground utility was developed for a specific mission/configuration, and may not be applicable for general use.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n'},\n",
       " {'repo': 'nasa/elf2cfetbl',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '![Static Analysis](https://github.com/nasa/elf2cfetbl/workflows/Static%20Analysis/badge.svg)\\n![Format Check](https://github.com/nasa/elf2cfetbl/workflows/Format%20Check/badge.svg)\\n\\n# Core Flight System : Framework : Tool : ELF to cFE Table Converter\\n\\nThis repository contains NASA\\'s ELF to cFE Table Converter Tool (elf2cfetbl), which is a framework component of the Core Flight System.\\n\\nThis lab application is a ground utility to convert ELF to cFE binary tables for cFS. It is intended to be located in the `tools/elf2cfetbl` subdirectory of a cFS Mission Tree. The Core Flight System is bundled at <https://github.com/nasa/cFS>, which includes this tool as a submodule, and includes build and execution instructions.\\n\\nSee README.txt for more information.\\n\\n## Version History\\n\\n### Development Build: v3.2.0-rc1+dev24\\n- Fix #73, Add Testing Tools to the Security Policy\\n- See <https://github.com/nasa/elf2cfetbl/pull/75>\\n\\n### Development Build: v3.2.0+dev20\\n- Changes cpp-styled comments to c-style to meet coding standard\\n- See <https://github.com/nasa/elf2cfetbl/pull/72>\\n\\n### Development Build: v3.2.0+dev9\\n- Restricts destination file permissions\\n- Squash int comparison warning\\n- Replace ctime (which generates LGTM warning) with ctime_r\\n- Deconflicts global/local parameters\\n- See <https://github.com/nasa/elf2cfetbl/pull/62>\\n\\n### Development Build: v3.1.0+dev39\\n\\n- Adds a null to the end of SrcFilename and DstFilename when using strncpy.\\n- Support ELF files that have all strings, including ELF section names, in one single \".strtab\" section in the ELF file.\\n- Version reporting now uses the version numbers defined in elf_version.h and reports build number.\\n- See  <https://github.com/nasa/elf2cfetbl/pull/47>\\n\\n### Development Build: 3.1.5\\n\\n- Apply code style\\n- See <https://github.com/nasa/elf2cfetbl/pull/44>\\n\\n### Development Build: 3.1.4\\n\\n- Fix string termination warnings in GCC9\\n- See <https://github.com/nasa/elf2cfetbl/pull/41>\\n\\n### Development Build: 3.1.3\\n\\n- Builds for vxworks w/ 32-bit host\\n- See <https://github.com/nasa/elf2cfetbl/pull/40>\\n\\n### Development Build: 3.1.2\\n\\n- Minor bug fixes and documentation (see <https://github.com/nasa/elf2cfetbl/pull/25>)\\n\\n### Development Build: 3.1.1\\n\\n- Minor updates (see <https://github.com/nasa/elf2cfetbl/pull/19>)\\n\\n### **_OFFICIAL RELEASE: 3.1.0 - Aquila_**\\n\\n- Minor updates (see <https://github.com/nasa/elf2cfetbl/pull/13>)\\n- Not backwards compatible with OSAL 4.2.1\\n- Released as part of cFE 6.7.0, Apache 2.0\\n\\n### **_OFFICIAL RELEASE: 3.0a_**\\n\\n- Released as part of cFE 6.6.0a, Apache 2.0\\n\\nNOTE - there are other parameter set management schemes used with the cFS (JSON, csv, etc) which may be more applicable for modern missions. Contact the community as detailed below for more information.\\n\\n## Known issues\\n\\nThis ground utility was developed for a specific mission/configuration, and may not be applicable for general use. The Makefile and for_build/Makefile are no longer supported or tested.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at <https://github.com/nasa/cFS>.\\n\\nOfficial cFS page: <http://cfs.gsfc.nasa.gov>\\n'},\n",
       " {'repo': 'nasa/nasa',\n",
       "  'language': None,\n",
       "  'readme_contents': '[<img align=\"left\" alt=\"https://nasa.gov/\" width=\"22px\" src=\"https://raw.githubusercontent.com/iconic/open-iconic/master/svg/globe.svg\" />](http://justingosses.com/)\\n[<img align=\"left\" alt=\"nasa | Twitter\" width=\"22px\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/twitter.svg\" />](https://twitter.com/nasa?lang=en)\\n[<img align=\"left\" alt=\"nasa | LinkedIn\" width=\"22px\" src=\"https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg\" />](https://www.linkedin.com/in/nasa/)\\n\\n\\n<br />\\n<br />\\n\\nHi :wave:, welcome to the NASA org on github.com! \\n\\nGithub.com/nasa has one of the largest collections of NASA open-source code repositories. Members of the NASA org can find instructions for github.com/nasa in <a href=\"http://nasa.github.io/\">http://nasa.github.io/</a>. \\n\\n🔭 Additional open-source code repositories resides in a variety of locations other than github.com/nasa. To discover code across all of these locations, we suggest you use code.nasa.gov & software.nasa.gov. These are two different sites holding metadata that describe code projects. Any code released through the NASA Software Release Authority process should be cataloged on those sites.\\n\\n### [`Code.nasa.gov`](https://code.nasa.gov) &mdash; \\nIs a page with short descriptions of all of NASA\\'s open-source code. Code.nasa.gov feeds into code.gov, which covers open-source and government-source code from many different U.S. governmental agencies. To assist in discovery, code projects described on code.nasa.gov have both human and A.I.-generated tags. These can be useful for finding related code projects.\\n### [`software.nasa.gov`](https://software.nasa.gov) &mdash; \\nContains metadata descriptions for all code projects in code.nasa.gov as well as government-source code projects only sharable with other government agencies. It is part of the large [`https://technology.nasa.gov/`](https://technology.nasa.gov/) that also includes patents and spinoffs. To help discoverability, software.nasa.gov puts each code project into one fo the following categories: <small>Business Systems and Project Management, System Testing, Operations, Design and Integration Tools, Vehicle Management (Space/Air/Ground), Data Servers Processing and Handling, Propulsion, Structures and Mechanisms, Crew and Life Support, Data and Image Processing, Materials and Processes, Electronics and Electrical Power, Environmental Science (Earth, Air, Space, Exoplanet), Autonomous Systems, and Aeronautics</small>.\\n\\n<br />\\n<br />\\n\\n# NOTE - PROFILE READMES CURRENTLY DON\\'T WORK FOR ORG PROFILES ONLY USER PROFILES :( \\nhttps://github.community/t/readme-for-organization-front-page/2920\\n \\n'},\n",
       " {'repo': 'nasa/digital-strategy',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': \"NASA Digital Government Strategy\\n===========================\\n\\nThis repository serves as a source for NASA's response to the president's digital strategy. Citizen developers are encouraged to use this information to build applications and tools.\\n\\nAPI\\n---\\n\\nThe files contained in this repository are available as a psuedo-service using the following syntax:\\n\\n`https://raw.github.com/NASA/digital-strategy/master/{file}.{format}`\\n\\nExamples:\\n\\n[`https://raw.github.com/NASA/digital-strategy/master/digitalstrategy.xml`](https://raw.github.com/NASA/digital-strategy/master/digitalstrategy.xml)\\n\\nFiles\\n-----\\n\\n* `digitalstrategy.json` and `digitalstrategy.xml` - machine-readable representation of the action items from the digital strategy\\n* `digitalstrategy.html` - embeddable HTML version of the action items from the digital strategy\\n\\nQuestions\\n----\\n\\nImplementation of the Digital Strategy at NASA is coordinated by the NASA's Open Government Team, located in the Office of the Chief Information Officer. For general information, contact [Beth Beck](mailto:beth.beck@nasa.gov).  \\nQuestions about the website, APIs, data or code can be directed to [Jason Duley](jason.duley@nasa.gov).\\n\"},\n",
       " {'repo': 'nasa/GeneLab-sampleProcessing',\n",
       "  'language': None,\n",
       "  'readme_contents': ' <img src=\"NASA_GeneLab_logo-2019.png\" align=\"middle\" alt=\"\"/>\\n\\n# GeneLab-sampleProcessing\\n\\n### About\\nThe NASA GeneLab Sample Processing Laboratory generates open science data from spaceflight missions. Over the years, it has developed a set of standard operating procedures (SOPs) that it utilizes to generate high-quality standardized data. This repository houses these SOPs.\\n\\n\\n## Tissue Storage and Cutting ##\\n#### [1.1 Sample aliquoting, labeling and storage](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/1.1_sample_archiving_v1.0.md) ####\\nThis SOP describes in detail how GeneLab SPL handles sample storage, aliquoting, labeling and the consensus acronyms we use.\\n\\n#### [1.2 Frozen tissue cutting](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/1.2_tissue_cutting_v1.0.md) ####\\nThis SOP describes the steps required to safely section a tissue prior the extraction of nucleic acids. The procedure, if followed correctly will allow portioning a piece of tissue without thawing and compromising the original biological sample.\\n\\n## Homogenization ##\\n#### [2.1 Tissue homogenization using Bullet Blender Gold Bead Beater](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/2.1_homogenization_bead_beater_v1.0.md) ####\\nThis SOP describes the steps required to lyse and homogenize biological material using a Bullet Blender 24 Gold bead beater. This procedure was validated for downstream RNA/DNA extraction using the Qiagen AllPrep kits (SOP#3.1), it is also possible to use the procedure with downstream RNA extraction using Trizol (SOP#3.2).\\n\\n#### [2.2 Tissue homogenization using Polytron Rotor Stator Homogenizer](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/2.2_homogenization_polytron_v1.0.md) ####\\nThis SOP describes the steps required to homogenize biological samples using the handheld rotor stator homogenizer Polytron. This type of homogenator allow for a larger lysis buffer volume and is used mainly for samples that require larger yield and/or not yet optimized for bead homogenization. This procedure is currently routinely used for mouse skin RNA extraction that requires a downstream Trizol extraction (SOP#3.2).\\n\\n## Extraction ##\\n#### [3.1 QIAGEN AllPrep DNA/RNA Mini (Cat#80204) with QIAGEN RNase-Free DNase Set. (Cat#79254)](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/3.1_qiagen_allprep_rna_dna_v1.0.md) ####\\nThis SOP describes the steps required to extract both RNA and DNA from mammalian tissues. The extraction kit used in this procedure is Qiagen AllPrep DNA/RNA Mini. In addition, in this procedure we describe steps for depleting the isolated RNA from DNA using QIAgen RNase-Free DNase set. This step is required for RNA that will be used for sequencing. It is strongly advised to read the AllPrep DNA/RNA Mini-Handbook in full.\\n\\n#### [3.2 TRIzol RNA Extraction with QIAGEN RNase-Free DNase Set and QIAGEN Allprep](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/3.2_trizol_rna_with_qiagen_cleanup_v1.0.md) ####\\nThis SOP describes the steps required to extract RNA from mammalian tissue using the TRIzol reagent for isolation and Qiagen Allprep mini kit for clean-up. In addition, in this procedure we describe steps for depleting the isolated RNA from DNA using QIAgen RNase-Free DNase set. This step is required for RNA that will be used for sequencing. It is strongly advised to read the AllPrep DNA/RNA Mini-Handbook in full.\\n\\n#### [3.3 Feces DNA Extraction using Maxwell RSC instrument with Purefood GMO kit](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/3.3_feces_dna_extraction_v1.0.md) ####\\nThis SOP describes the steps required to isolate DNA from mouse fecal pellets using Maxwell RSC instrument with Purefood GMO kit.\\n\\n## DNA/RNA QC and Troubleshooting ##\\n#### [4.1 RNA/DNA/miRNA/cDNA quantification using Qubit Fluorimeter](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/4.1_dna_rna_quant_qubit_v1.0.md) ####\\nThis SOP describes the steps required to perform quantification of RNA/DNA or cDNA using the Invitrogen (Thermo Fisher Scientific) fluorimeter – Qubit and the Qubit kits. Flourimetric methods are advantageous over spectrophotometric methods since they are more precise and specific to the molecule being measured.\\n\\n#### [4.2 QC genomic DNA](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/4.2_genomic_dna_tapestation_v1.0.md) ####\\nThis SOP describes the steps required to perform automated electrophoresis of genomic DNA to assess DNA quality using the Agilent 4200 TapeStation System and Agilent Genomic DNA TapeStation reagents. Any number of samples can be analyzed between 1 and 96.\\n\\n#### [4.3 Quality analysis of RNA using Agilent Bioanalyzer 2100 System](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/4.3_rna_bioanalyzer_v1.0.md) ####\\nThis SOP describes the steps required to perform automated electrophoresis of RNA samples. This procedure is using the Agilent 2100 Bioanalyzer System and Agilent RNA 6000 Nano and Pico kits. This procedure will generate a gel image of the RNA as well as RIN and DV200 values, those are recorded and used to track sample quality.\\n\\n## Library Preparation ##\\n##### [5.1 Illumina TruSeq Stranded Total RNA Library Prep using EpMotion](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/5.1_truseq_stranded_total_rna_epmotion_v1.0.md) ####\\nThis SOP describes the steps used to automate library preparation using the Illumina TruSeq Stranded Total RNA Gold kit on an EpMotion 5073/5075.\\n\\n#### [5.2 Use of ERCC spike in mixes and UMRR/UHRR controls for Total RNA-Sequencing](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/5.2_controls_and_spike_ins_v1.0.md) ####\\nAs described in detail on GeneLab.nasa.gov webpage titled “GeneLab Sequencing Standards and Services” we encourage the researchers to use two levels of control in the Total RNA sequencing workflow. This SOP describes the suggested protocol.\\n\\n#### [5.3 Illumina Nextera DNA Flex Manual Library Preparation](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/5.3_nextera_flex_manual_v1.0.md) ####\\nThis SOP follows the Illumina Nextera DNA Flex Library Prep Guide, Doc# 1000000025416 v07. It is strongly advised to read the guide in full before using this SOP.\\n\\n## Library QC and Troubleshooting ##\\n#### [6.1 qPCR quantification of Illumina sequencing libraries](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.1_qiagility_lib_quant_v1.0.md) ####\\nThis SOP describes the steps for qPCR quantification of Illumina Sequencing libraries using a QIAgility.\\n\\n#### [6.2 Quant-iT PicoGreen dsDNA quantification of Illumina sequencing libraries](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.2_picogreen_v1.0.md) ####\\nThis SOP lists the steps for dsDNA quantification of sequencing libraries using the Quant-iT™ PicoGreen™ dsDNA Assay Kit.\\n\\n#### [6.3 QC cDNA](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.3_d1000_dna_tapestation_v1.0.md) ####\\nThis SOP lists the steps for DNA quantification of sequencing libraries using an Agilent D1000 TapeStation.\\n\\n#### [6.4 Normalizing TruSeq Stranded Total RNA Library](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.4_normalize_truseq_lib_v1.0.md) ####\\nThis SOP describes the steps to normalize TruSeq Stranded Total RNA libraries.\\n\\n#### [6.5 TruSeq Total RNA library pooling, normalization and QC](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.5_truseq_total_rna_library_pooling_normalization_qc_v1.0.md) ####\\nThis SOP describes the steps to pool, normalize, and check the quality of TruSeq Total RNA libraries using an Agilent Tapestation and an Illumina iSeq.\\n\\n#### [6.6 Manual Illumina TruSeq total RNA (Ribo Gold) library clean-up from adapter dimers](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/6.6_library_manual_cleanup.md) ####\\nThis SOP describes the steps for manually cleaning Illumina TruSeq Total RNA libraries of adapter dimers.\\n\\n## Sequencing Parameters ##\\n#### [7.1 Setting up NovaSeq 6000 and iSeq 100 sequencers](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/7.1_sequencer_setup_v1.0.md) ####\\nThis SOP describes the setup of Illumina NovaSeq 6000 and iSeq 100 sequences used by NASA GeneLab.\\n\\n## Sequencing QC and Troubleshooting ##\\n#### [8.1 GeneLab SOP for Generating iSeq QC complete report from HTStream on MMOC](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/8.1_generate_htstream_iseq_qc_report_on_mmoc_v1.0.md) ####\\nThis SOP describes the steps for generating an iSeq quality control report using the high throughput genomic data pre-processing tool HTStream.\\n\\n#### [8.2 GeneLab SOP for Generating iSeq HTStream output to calculate library pooling values](https://github.com/nasa/GeneLab-sampleProcessing/blob/master/SOP_text/8.1_generate_htstream_iseq_qc_report_on_mmoc_v1.0.md) ####\\nThis SOP describes the steps required to calculate library pooling volumes after the first iSeq (pool by volume) run and instructions on how to pool libraries when there is not enough volume.\\n\\n**_If you have any questions, please contact us at ARC-DL-GeneLab-sequencing-group@mail.nasa.gov._**\\n'},\n",
       " {'repo': 'nasa/RtRetrievalFramework',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': \"======================\\nRT Retrieval Framework\\n======================\\n\\nJet Propulsion Laboratory, California Institute of Technology. \\nCopyright 2017 California Institute of Technology. \\nU.S. Government sponsorship acknowledged.\\n\\nThis software (NASA NTR-49044) retrieves a set of atmospheric/surface/instrument\\nparameters from a simultaneous fit to spectra from multiple absorption bands.\\nThe software uses an iterative, non-linear retrieval technique (optimal\\nestimation). After the retrieval process has converged, the software performs an\\nerror analysis. The products of the software include all quantities needed to\\nunderstand the information content of the measurement, its uncertainty, and its\\ndependence on interfering atmospheric properties.\\n\\nThe software provides a flexible, efficient, and accurate tool to retrieve the\\natmospheric composition from near-infrared spectra. Its unique features are:\\n\\n* Spectra from ground-based or space-based measurement with arbitrary\\nobservation geometry can be analyzed.\\n* The retrieved parameters can be chosen from a large set of atmospheric (e.g.,\\nvolume mixing ratio of gases or aerosol optical depth), surface (e.g.,\\nLambertian reflection), and instrument (e.g., spectral shift or instrument line\\nshape parameters) parameters.\\n* The software uses an accurate, state-of-the-art, multiple-scattering radiative\\ntransfer code combined with an efficient polarization approximation to simulate\\nmeasured spectra.\\n* The software enables fast and highly accurate simulations of broad spectral\\nranges by an optional parallelization of the frequency processing in the\\nradiative transfer model.\\n\\nThe software was originally created for JPL's OCO, ACOS and OCO-2 projects for\\nLevel-2 processing.\\n\\nDocumentation\\n-------------\\n\\nDocumentation on setup and compiling of the software can be found in the\\ndoc/users_guide.pdf PDF file, or online in HTML form at:\\nhttp://nasa.github.io/RtRetrievalFrameworkDoc/ \\n\\nFor information on the algorithms used by the software the OCO2 L2 Algorithm\\nTheoretical Basis Document can be found at the Goddard DAAC: \\nhttp://disc.sci.gsfc.nasa.gov/OCO-2/documentation/oco-2-v6/OCO2_L2_ATBD.V6.pdf\\n\\nFor further information on the methods used by OCO, see the OCO-2 project\\npublications page: \\nhttp://oco.jpl.nasa.gov/science/publications/\\n\"},\n",
       " {'repo': 'nasa/PanNDE',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Welcome to PanNDE!\\n\\nPanNDE is an attempt at building a modular, developable infrastructure for doing field simulations (initially elastodynamic) for Non-Destructive Evaluation applications. Performance as well as low-level access is a key concern to enable rapid, adaptive modelling for model inversion or for MAPOD like applications. As with all research, work begets work, and development will hopefully continue. For now, if you find use for this, great, and if not, hopefully future development will bring features that are useful, or that the code base provides a jumping-off point for other development by other researchers. For now, good luck, and have fun! Science is a grand adventure!\\n\\n# Notices:\\n\\nCopyright 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nGoogletest is a product of Google Inc. and is subject to the following:\\n \\nCopyright 2008, Google Inc. All rights reserved.\\n           \\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n* Neither the name of Google Inc. nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n \\n# Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n# Building PanNDE\\n\\n1. Download latest release\\n2. Ensure correct dependencies are installed, and are in the search path\\n  - MPI\\n  - VTK >= 9.0 (built and linked with the same MPI being used for PanNDE)\\n  - Metis\\n  - gcc >= 7.3\\n  - cmake >= 3.13.5\\n3. Ensure Python3 is installed for some included post processing utilities\\n  - this is optional, if the utilities are not desired\\n4. cd to `PanNDE/root/directory`\\n5. generate compilation instructions using `cmake -B build -S .`\\n6. compile with `make -C build`\\n\\n# Running the Test Suites\\n\\n1. Ensure PanNDE is built on the system\\n2. Run serial tests:\\n  - `./bin/HostDataTests > HostDataTestResults.log`\\n3. Run parallel tests:\\n  - `mpirun -n 4 -outfile-pattern=NetMPITestResult_%r.log ./bin/NetMPITests`\\n  - `mpirun -n 4 -outfile-pattern=VTKIOTestResult_%r.log ./bin/VTKIOTests`\\n  - `mpirun -n 4 -outfile-pattern=HostSolverTestResult_%r.log ./bin/HostSolverTests`\\n\\n\\n# Creating a Demonstration Case\\n\\n1. Ensure PanNDE is built on the system\\n2. There are three demonstration case builders provided:\\n  - Aluminum angle stock with single transducer:\\n    1. Run `./bin/DemoCaseBuilder`\\n    2. The resulting case file will be `./data/demo_case_0.vtu`\\n  - Quasi-isotropic 8-ply IM7 CFRP layup with single transducer:\\n    1. Run `./bin/PlateCaseBuilder -o $JOB_FILENAME_NO_EXTENSION`\\n    2. The resulting case file will be named with the filename provided\\n  - Parametric plate with two transducers:\\n    1. Run `./bin/ParameterizedDemoPlateCase -p $PARAMETER_FILE_YAML`\\n    2. The resulting case file will be generated based on the parameters contained in the provided parameter file\\n    3. A sample parameter file that produces the previous quasi-isotropic 8-ply CFRP case is provided in `demo_quasi_iso_cfrp.yaml`. The case can be changed to suit user\\'s use cases, however stability criteria are the user\\'s responsibility\\n\\n# Running a Case\\n\\n1. Ensure PanNDE is built on the system\\n2. Produce a `*.vtu` file that provides all required input data\\n  - For required data, see Job File Requirements, below\\n3. Run `mpirun -n $NPROCS ./bin/DemoModelNxd -f $JOB_FILE -o $RESULT_FILE_NAME_ROOT`\\n  - results will be written to a `*.pvtu`/`*.vtu` set with appropriate file numbering schema for reconstruction in scientific visualization software\\n\\n# Job File Requirements\\n\\n1. Elastodynamic parameter fields:\\n  - cell centered stiffness coefficients named:\\n    - `C11 C12 C13 C14 C15 C16 C22 C23 C24 C25 C26 C33 C34 C35 C36 C44 C45 C46 C55 C56 C66`\\n  - cell centered density values named: \\n    - `density`\\n2. Time step and file write frequency information:\\n  - simulation time step named:\\n    - `dt`\\n  - simulation write times array named:\\n    - `write_times`\\n3. Hanning windowed Ncycle sine transducer excitation information:\\n  - Number of transducers named:\\n    - `NTransducers`\\n  - Transducer parameters:\\n    - `XD<transducer index>/XCenter`\\n    - `XD<transducer index>/YCenter`\\n    - `XD<transducer index>/ZCenter`\\n    - `XD<transducer index>/Frequency`\\n    - `XD<transducer index>/NCycle`\\n    - `XD<transducer index>/Phase`\\n    - `XD<transducer index>/Radius`\\n\\n# Future Work/Features\\n\\n1. Generic transducers, or more transducer type options\\n2. PZT model\\n3. Thermal conduction model\\n4. GPU data allocator and solver\\n\\n'},\n",
       " {'repo': 'nasa/harmony-qgis',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Harmony QGIS\\n\\nThis plugin allows the user to select areas on the map and upload them as queries (along with other parameters) to the [Harmony](https://harmony.earthdata.nasa.gov) Earth science data/service broker, showing the results on the map.\\n\\n## Requirements\\nQGIS version 3.12 or higher\\n\\n## Installation\\n1. Clone this repository under the plugins directory for QGIS (/Users/USER_NAME/Library/Application Support/QGIS/QGIS3/profiles/default/python/plugins on a Mac)\\n```\\n$ git clone https://github.com/nasa/harmony-qgis.git\\n```\\n3. Restart QGIS\\n4. Select Plugins->Manage and Install Plugins from the menu\\n5. Click the check box next to the Harmony plugin\\n\\n## Usage\\n\\nThe basic workflow begins as labeled in red numbers on figure 1:\\n\\n1. (optional) select a layer in the layers panel. This will preselect the layer in the main dialog, but is not strictly necessary.\\n2. Click the Harmony plugin icon in the toolbar.\\n\\n<figure>\\n<img src=\"qgis1_labeled.png\">\\n<figcaption>Figure 1 - Using the plugin</figcaption>\\n</figure>\\n\\nThis will open the main dialog shown in figure 2.\\nHere you can configure a Harmony query by entering parameters in the relevant fields. At a minimum you must fill in the `Collection`, `Version`, and `Variable` fields in order to execute a query.\\n\\nAs labeled in figure 2, these fields are\\n\\n1. `collection` (required) Used to indicate the Common Metadata Repository (CMR) collection id to use\\n2. `version` (required) Used to indicate which version of the Harmony API to use\\n3. `variable` (required) Used to indicate which variable to use\\n4. `download directory` Where to store the images downloaded from Harmony\\n5. `harmony url` The URL of the Harmony service to use (for accessing various environments such as UAT or Prod)\\n6. `shape layer` A selection box to choose a layer to be sent as a shapefile to indicate a sub-setting boundary with the Harmony query. This will be preselected if the layer was selected when the plugin icon was selected. **Note** it is not necessary to provide a layer to execute a query.\\n7. `additional parameters` This table can be used to add additional query parameters as key/value pairs. Consult the Harmony documentation for available parameters.\\n8. `session selector` This selector can be used to fill in the other fields from a previous session (see the \\'Sessions\\' section below).\\n9. `session management` This button will bring up the sessions management dialog (Figure 3) to allow the user to delete, import, and export sessions.\\n\\n<figure>\\n<img src=\"dialog.png\" height=735 width=728/>\\n<figcaption>Figure 2 - The main dialog</figcaption>\\n</figure>\\n\\nAfter filling in the query parameter fields and clicking `OK`, the plugin will send the query to Harmony and begin downloading any results (or present error messages in the case of failure). Image results are downloaded in the background to preserve responsiveness in the main UI. Results are downloaded as they become available (rather than all at once) and are rendered as raster layers. A progress bar is rendered in the status window at the bottom and updated as new results are downloaded.\\n\\nFigure 3 shows the results of a query that generated 20 new layers. The layers are shown in the main map (1) with the new layers added to the layer panel (2). A text message indicating the number of new layers is shown in the status bar (3).\\n\\n<figure>\\n<img src=\"qgis2_labeled.png\">\\n<figcaption>Figure 3 - Displaying Harmony results</figcaption>\\n</figure>\\n\\n### Sessions\\n\\nSessions are a way to save settings from the main dialog for later reuse. This is particularly useful when showing demos.\\n\\nWhen the main dialog first opens, the `Session` selector is set to `<NEW SESSION>`. If the user fills in the dialog, doesn\\'t change the `Session` selector, and then selects `OK`, a pop-up dialog will appear asking to save the settings as a session. If the user creates the new session, the name will appear in the `Session` selector in subsequent invocations. Choosing a session name from the selector populates the other fields in the dialog with the last saved settings.\\n\\nThe user can click on the session management button, (9) in figure 2, to bring up the sessions management dialog shown in figure 5.\\n\\n<figure>\\n<img src=\"sessions_dialog.png\" height=480 width=569/>\\n<figcaption>Figure 5 - The sessions management dialog</figcaption>\\n</figure>\\n\\nAll the saved sessions are shown in the list widget (1). Selecting one ore more (shift select) saved sessions will enable the `Delete` button (2). As the name suggests, this will delete the selected sessions. Selecting sessions will also enable the `Export` button (4)\\nwhich can be used to write out the data for one or more sessions as a JSON file. This can be shared with other users to allow them to reproduce the Harmony queries defined by sessions. Conversely, the `Import` button (3) can be used to load sessions from an exported file.\\n\\n**Note** the selected layer (if any) is saved as part of the session data. If an imported session contains a layer name that does not exist in the current project, this information is discarded.\\n'},\n",
       " {'repo': 'nasa/T-MATS',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'T-MATS\\n==========\\n\\nToolbox for the Modeling and Analysis of Thermodynamic Systems <br>\\n<meta name=\"keywords\" content=\"T-MATS, TMATS, Control System, Numerical Methods, Newton-Raphson, Jacobian Calculation, Propulsion, Aircraft Engine, Jet, Turbofan, Turbojet, Compressor, Turbine, Nozzle, Inlet, open source, simulation, modeling, NASA, thermodynamics, turbomachinery, MATLAB, Simulink, jet, engine,  etc.\">\\n<meta name=\"google-site-verification\" content=\"nqMigEmX-6lqKvj4sewxDamtZEXHEvE1VfzjVRZoJ40\" />\\n<b> <a href= \"https://github.com/nasa/T-MATS/releases\" >Click Here</a> for stable release download</b> <br>\\nFor questions, comments, and general support see the <b> <a href= \"https://groups.google.com/forum/#!forum/t-mats-user-group\" >T-MATS user\\'s forum</a></b>. <br>\\nPlease take a moment and fill out our <b> <a href= \"https://docs.google.com/forms/d/1cjcCyOKZpV49-gsdGGaKHUsZ2c2fKX5mwkKrntt60Eo/viewform?usp=send_form\" >Survey</a></b>, user interest and feedback allows us to continue working on this package.<br><br>\\n\\n<b>Introduction</b> <br>\\nThe Toolbox for the Modeling and Analysis of Thermodynamic Systems (T-MATS) \\nis a Simulink toolbox intended for use in the modeling and simulation of thermodynamic \\nsystems and their controls. T-MATS contains generic thermodynamic and controls \\ncomponents that may be combined with a variable input iterative solver and optimization \\nalgorithm to create complex systems to meet the needs of a developer. Development of this tool\\nwas initiated on behalf of the NASA Aviation Safety Program\\'s Vehicle Systems Safety Technologies\\n(VSST) project.\\n<br><br>\\n<b>Description</b> <br>\\nThe T-MATS software provides a toolbox for the development of thermodynamic \\nsystem models; it contains a simulation framework, multi-loop solver techniques, and modular \\nthermodynamic simulation blocks. While much of the capability in T-MATS is in transient \\nthermodynamic simulation, the developers\\' main interests are in aero-thermal applications; \\nas such, one highlight of the T-MATS software package is the turbomachinery block set. This \\nset of Simulink blocks gives a developer the tools required to create virtually any steady \\nstate or dynamic turbomachinery simulation, e.g., a gas turbine simulation. In systems where \\nthe control or other related systems are modeled in MATLAB/Simulink, the T-MATS developer has \\nthe ability to create the complete system in a single tool.\\n<br><br>\\nT-MATS is written in MATLAB/Simulink (The Mathworks, Inc.), is open source, \\nand is intended for use by industry, government, and academia. All T-MATS equations \\nwere developed from public sources and all default maps and constants provided in the \\nT-MATS software package are nonproprietary and available to the public. The software \\nis released under the Apache V2.0 license agreement. \\n<br><br>\\n<b>Getting Started</b> <br>\\nStable releases of T-MATS are located under the <a href= \"https://github.com/nasa/T-MATS/releases\" >releases tab</a>. It is encouraged that a user\\ndownload the most up to date version using the appropriate software download button (green button). \\nInstallation instructions are detailed in the user\\'s manual which is included within the package. \\n<br><br>\\nT-MATS encourages open collaboration and if a user wishes to become a developer the software \\nmay be forked at any time via the main page link.\\n'},\n",
       " {'repo': 'nasa/bplib',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# bplib\\n\\n[1. Overview](#1-overview)  \\n[2. Build with Make](#2-build-with-make)  \\n[3. Application Design](#3-application-design)  \\n[4. Application Programming Interface](#4-application-programming-interface)  \\n[5. Storage Service](#5-storage-service)  \\n\\n[Note #1 - Bundle Protocol Version 6](doc/bpv6_notes.md)  \\n[Note #2 - Library Development Guidelines](doc/dev_notes.md)  \\n[Note #3 - Configuration Parameter Trades](doc/parm_notes.md)  \\n[Note #4 - Bundle Flow Analysis for Intermittent Communication](doc/perf_analysis_ic.md)  \\n\\n----------------------------------------------------------------------\\n## 1. Overview\\n----------------------------------------------------------------------\\n\\nThe Bundle Protocol library (bplib) implements a subset of the RFC5050 Bundle Protocol and targets embedded space flight applications. The library uses the concept of a bundle channel to manage the process of encapsulating application data in bundles, and extracting application data out of bundles.  A channel specifies how the bundles are created (e.g. primary header block fields), and how bundles are processed (e.g. payloads extracted from payload block). Bplib contains no threads and relies entirely on the calling application for its execution context and implements a thread-safe blocking I/O model where requested operations will either block according to the provided timeout, or return an error code immediately if the operation cannot be performed.\\n\\nBplib assumes the availability of a persistent queued storage system for managing the rate buffering that must occur between data and bundle processing. This storage system is provided at run-time by the application, which can either use its own or can use one of the included storage services. In addition to the storage service, bplib needs an operating system interface provided at compile-time. By default a POSIX compliant operating systems interface is built with the included makefile - see below for further instructions on changing the operating system interface.\\n\\n----------------------------------------------------------------------\\n## 2. Build with Make\\n----------------------------------------------------------------------\\n\\n#### Prerequisites\\n\\n1. To build the static and shared libraries, the only prerequisites are the __make__ build system and a compiler toolchain (by default __gcc__).\\n\\n2. Tailoring the build to provide unique system prerequisites is accomplished by providing a custom configuration makefile.  See `posix.mk` as an example. If your custom file is called `{my_config_makefile}` then the following commands would be used:\\n   * `make CONFIG={my_config_makefile}`\\n   * `sudo make CONFIG={my_config_makefile} install`\\n\\n3. To build the optional Lua extension used for unit testing (and useful for any user implemented Lua applications), then you need to have Lua installed on your system.  Given the various versions and configurations Lua can be found in for different systems, the default behavior of the makefile is to look for Lua 5.3 in `/opt/lua5.3` which assumes you\\'ve downloaded and installed Lua yourself.  This can be accomplished via the followings steps:\\n   * Download Lua 5.3 from `www.lua.org`\\n   * `tar -xvzf lua-5.3.X.tar.gz` where X is whatever the latest stable version of Lau 5.3 is.\\n   * `cd lua-5.3.X`\\n   * `make MYCFLAGS=\"-fpic\" linux`\\n   * `sudo make install INSTALL_TOP=/opt/lua5.3`\\n   * `alias bplua=\"LUA_CPATH=/opt/lua5.3/lib/lua/5.3/?.so LUA_PATH=/opt/lua5.3/lib/lua/5.3/?.lua /opt/lua5.3/bin/lua\"`\\n\\n4. If you want to use a different Lua installation, you must run the Makefile found in `binding/lua` directly.  From the command line: the `PREFIX` variable can be set to the installed version of Lua you want to use (e.g. __/usr__), and the `LIBDIR` variable can be set to where the bplib.so extension module should be installed (e.g. __/usr/lib64/lua/5.1__).  Managing multiple Lua distributions on a single system can be a little tricky - the biggest problem being that as of 5.3 there is still a global search path for shared objects and Lua files that is independent of the location of the binary Lua interpreter.  To get around this problem you can either set the `package.cpath` and `package.path` variables within your Lua scripts or you can alias the call to start the Lua interpreter with the paths called out in the command (as shown above).\\n\\n#### Building\\n\\nTo build only the static and shared libraries (which is recommended), use the following commands:\\n* `make`\\n* `sudo make install`\\n\\nTo build everything, including the language bindings and example application, go to repository root directory and execute the following commands:\\n* `make dev`\\n* `sudo make install-dev`\\n\\nThe dev target of the makefile produces the following binaries:\\n* `build/libbp.so.<version>` - shared library\\n* `build/libbp.a` - static library\\n* `bindings/lua/build/bplib.so` - lua extension module\\n* `app/build/bpsend` - example application that sends bundles\\n* `app/build/bprecv` - example program that receives bundles\\n\\nAnd performs the following installations:\\n* `/usr/local/lib`: bplib libraries\\n* `/usr/local/inc`: bplib includes\\n* `/usr/local/lib/lua/5.3`: lua extensions and helper scripts\\n* `/opt/bpio/bin`: example application binaries\\n\\nAdditional make commands are as follows:\\n* `make clean` will remove all generated files and directories\\n* `make testmem` will call valgrind for detecting memory leaks\\n* `make testcpu` will call valgrind/callgrind for detecting cpu bottlenecks\\n* `make testheap` will call valgrind/massif for detecting sources of memory bloat\\n* `make testcov` will generate a line coverage report (if built and run with gcov, which is enabled by default)\\n\\nOn CentOS you may need to create a file with the conf extension in /etc/ld.so.conf.d that contains the line \\'/usr/local/lib\\'.\\n* `sudo echo \"/usr/local/lib\" > /etc/ld.so.conf.d/local.conf`\\n* `sudo ldconfig`\\n\\n#### Example Application\\n\\nFor those that learn better through examples, an example application is provided in the `apps` directory.  This example program is not intended to be complete, but provides a quick way to see how to use the library.  After building and installing bplib on your system, you can do a simple test to see if the application will run by doing the following:\\n\\n* `cd apps`\\n* `make`\\n* `./test_run.sh`\\n\\nThis will create two windows, the first executing the **bprecv** program, and the second executing the **bpsend** program.  Any line you type in the **bpsend** window is bundled and sent over UDP to the **bprecv** program.  Custody transfer is employed and the **bpsend** program will keep track of the number of messages it has sent vs. the number of messages that have been acknowledged.\\n\\n#### Unit Tests\\n\\nTo manually run the unit test suite:\\n* `lua5.3 binding/lua/test/test_runner.lua`\\n\\nTo run a specific unit test using one of the test targets provided in the makefile:\\n* make test{mem|cpu|heap|cov} testcase=binding/lua/test/ut_{test}.lua\\n\\nNote that getting the lua extension to compile for your specific linux distribution can be difficult as they often come with different versions and named in different ways.  Please see the prerequisites section above and the makefile in `bindline/lua` for hints to how to get your version of Lua working with this library.\\n\\n#### Releases\\n\\nThe default `posix.mk` configuration makefile is for development and builds additional C unit tests, code coverage profiling, stack protector, and uses minimum compiler optimizations. When releasing the code, the library should be built with `release.mk` as follows:\\n* `make CONFIG=release.mk`\\n\\n----------------------------------------------------------------------\\n## 3. Application Design\\n----------------------------------------------------------------------\\n\\n![Figure 1](doc/bp_api_architecture.png \"BP Library API (Architecture)\")\\n\\nBplib is written in \"vanilla C\" and is intended to be linked in as either a shared or static library into an application with an API for reading/writing application data and reading/writing bundles.\\n\\nConceptually, the library is meant to exist inside a board support package for an operating system and be presented to the application as a service.  In such a design only the interface for reading/writing data would be provided to the application, and the interface for reading/writing bundles would be kept inside the board support package.  This use case would look a lot like a typical socket application where a bundle channel (socket) is opened, data is read/written, and then at some later time the channel is closed.  Underneath, the operating system would take care of sending and receiving bundles.\\n\\nIn order to support bplib being used directly by the application, both the data and the bundle interfaces are provided in the API. In these cases, the application is also responsible for sending and receiving the bundles.\\n\\nAn example application design that manages both the data and bundle interfaces could look as follows:\\n1. A __bundle reader__ thread that receives bundles from a convergence layer and calls bplib to _process_ them\\n2. A __data writer__ thread that _accepts_ application data from bplib\\n3. A __bundle writer__ thread that _loads_ bundles from bplib and sends bundles over a convergence layer\\n4. A __data reader__ thread that _stores_ application data to bplib\\n\\nThe stream of bundles received by the application is handled by the bundle reader and data writer threads. The __bundle reader__ uses the `bplib_process` function to pass bundles read from the convergence layer into the library.  If those bundles contain payload data bound for the application, that data is pulled out of the bundles and queued in storage until the __data writer__ thread calls the `bplib_accept` function to dequeue the data out of storage and write it to the application.\\n\\nConversely, the stream of bundles sent by the application is handled by the data reader and bundler writer threads. The __data reader__ thread calls `bplib_store` to pass data from the application into the library to be bundled.  Those bundles are queued in storage until the __bundle writer__ threads calls the `bplib_load` function to dequeue them out of storage and write them to the convergence layer.\\n\\n----------------------------------------------------------------------\\n## 4. Application Programming Interface\\n----------------------------------------------------------------------\\n\\n#### 4.1 Functions\\n\\n| Function        | Purpose |\\n| --------------- | ------- |\\n| [bplib_init](#initialize)                | Initialize the BP library - called once at program start |\\n| [bplib_open](#open-channel)              | Open a channel - provides handle to channel for future channel operations |\\n| [bplib_close](#close-channel)            | Close a channel |\\n| [bplib_flush](#flush-channel)            | Flush active bundles on a channel |\\n| [bplib_config](#config-channel)          | Change and retrieve channel settings |\\n| [bplib_latchstats](#latch-statistics)    | Read out bundle statistics for a channel |\\n| [bplib_store](#store-payload)            | Create a bundle from application data and queue in storage for transmission |\\n| [bplib_load](#load-bundle)               | Retrieve the next available bundle from storage to transmit |\\n| [bplib_process](#process-bundle)         | Process a bundle for data extraction, custody acceptance, and/or forwarding |\\n| [bplib_accept](#accept-payload)          | Retrieve the next available data payload from a received bundle |\\n| [bplib_ackbundle](#acknowledge-bundle)   | Release bundle memory pointer for reuse (needed after bplib_load) |\\n| [bplib_ackpayload](#acknowledge-payload) | Release payload memory pointer for reuse (needed after bplib_accept) |\\n| [bplib_routeinfo](#route-information)    | Parse bundle and return routing information |\\n| [bplib_display](#display-bundle)         | Parse bundle and log a break-down of the bundle elements |\\n| [bplib_eid2ipn](#eid-to-ipn)             | Utility function to translate an EID string into node and service numbers |\\n| [bplib_ipn2eid](#ipn-to-eid)             | Utility function to translate node and service numbers into an EID string |\\n| [bplib_attrinit](#attr-init)             | Utility to initialize a channel attribute structure with default values.  Useful if the calling application only wants to change a few attributes without setting them all. |\\n\\n__Note__: _functions that operate on a channel are thread-safe with other functions that operate on channels, but they are not thread-safe with the open and close functions.  A channel can only be closed when no other operations are being performed on it._\\n\\n\\n----------------------------------------------------------------------\\n##### Initialize\\n\\n`void bplib_init (void)`\\n\\nInitializes the BP library.  This must be called before any other call to the library is made.  It calls the operating system layer initialization routine.\\n\\n----------------------------------------------------------------------\\n##### Open Channel\\n\\n`bp_desc_t* bplib_open (bp_route_t route, bp_store_t store, bp_attr_t* attributes)`\\n\\nOpens a bundle channel that uses the provided endpoint IDs, storage service, and attributes.\\n\\nThis function returns a channel handle that is used for all future operations on the channel.  The open and close calls are mutex\\'ed against other open and close calls, but once a channel is created, operations on that channel are only mutex\\'ed against other operations on the same channel.  A channel persists until it is closed.\\n\\n`route` - a set of endpoing IDs defining the source, destination, and report to endpoints\\n\\n* __local node__: The {node} number of the ipn:{node}.{service} endpoint ID used for the source and custody endpoints of bundles generated on the channel.\\n\\n* __local service__: The {service} number of the ipn:{node}.{service} endpoint ID used for the source and custody enpoints of bundles generated on the channel.\\n\\n* __destination node__: The {node} number of the ipn:{node}.{service} endpoint ID used for the destination enpoint of bundles generated on the channel.\\n\\n* __destination service__: The {service} number of the ipn:{node}.{service} endpoint ID used for the destination enpoint of bundles generated on the channel.\\n\\n* __report to node__: The {node} number of the ipn:{node}.{service} endpoint ID used for the report to enpoint of bundles generated on the channel.\\n\\n* __report to service__: The {service} number of the ipn:{node}.{service} endpoint ID used for the report to enpoint of bundles generated on the channel.\\n\\n`store` - a set of callbacks that provide access to the desired storage service.  See [Storage Service](#storage-service) section for more details.\\n\\n`attributes` - set of characteristics and settings for the channel that trade memory usage and performance\\n\\n* __lifetime__: Bundle generation parameter - the number of seconds since its creation that the bundle is valid.  Once the lifetime of a bundle expires, the bundle can be deleted by the bundle agent.\\n\\n* __request_custody__: Bundle generation parameter - if set then the bundle request custody transfer and includes a CTEB extension block.\\n\\n* __admin_record__: Bundle generation parameter - if set then the bundle is set as an adminstrative record.  The library handles this setting automatically for Aggregate Custody Signals that it generates; but if the user wants to create their own adminstrative record, then this attribute provides that option.\\n\\n* __integrity_check__: Bundle generation parameter - if set then the bundle includes a BIB extension block.\\n\\n* __allow_fragmentation__: Bundle generation parameter - if set then any generated or forwarded bundles on the channel will be fragmented if the size of the bundle exceeds the __max_length__ attribute of the channel; if not set, then any bundle generated or forwarded that exceeds the __max_length__ will be dropped.\\n\\n* __cipher_suite__: Bundle generation parameter - provides the CRC type used inside the BIB extension block.  If the __integrity_check__ attribute is not set, then this setting is ignored.  If the __integrity_check__ attribute is set and this attribute is set to BP_BIB_NONE, then a BIB is included but the cipher result length is zero (this provide unambigous indication that no integrity check is included). Currently supported cipher suites are: BP_BIB_CRC16_X25, and BP_BIB_CRC32_CASTAGNOLI.\\n\\n* __timeout__: The number of seconds the library waits before re-loading an unacknowledged bundle.\\n\\n* __max_length__: The maximum size in bytes that a bundle can be, both on receipt and on transmission.\\n\\n* __cid_reuse__: The library\\'s behavior when a bundle times-out - if set, bundles that are retransmitted use the original Custody ID of the bundle when it was originally sent; if not set, then a new Custody ID is used when the bundle is retransmitted.  Re-using the Custody ID bounds the size of the Aggregrate Custody Signal coming back (worse-case gaps).  Using a new Custody ID makes the average size of the Aggregate Custody Signal smaller.\\n\\n* __dacs_rate__: The maximum number of seconds to wait before an Aggregate Custody Signal which has accumulated acknowledgments is sent.  Every time a call to `bplib_load` is made, the code checks to see if there is an Aggregate Custody Signal which exists in memory but has not been sent for at least __dacs_rate__ seconds.\\n\\n* __protocol_version__: Which version of the bundle protocol to use; currently the library only supports version 6.\\n\\n* __retransmit_order__: The order in which bundles that have timed-out are retransmitted. There are currently two retransmission orders supported: BP_RETX_OLDEST_BUNDLE, and BP_RETX_SMALLEST_CID.\\n\\n* __active_table_size__:  The number of unacknowledged bundles to keep track of. The larger this number, the more bundles can be sent before a \"wrap\" occurs (see BP_OPT_WRAP_RESPONSE).  But every unacknowledged bundle consumes 8 bytes of CPU memory making this attribute the primary driver for a channel\\'s memory usage.\\n\\n* __max_fills_per_dacs__: The maximum number of fills in the Aggregate Custody Signal.  An Aggregate Custody Signal is sent when the maximum fills are reached or the __dacs_rate__ period has expired (see BP_OPT_DACS_RATE).\\n\\n* __max_gaps_per_dacs__: The maximum number of Custody ID gaps a channel can keep track up when receiving bundles requesting custody transfer.  If this gap limit is reached, the Aggregate Custody Signal is sent and a new one immediately begins to accumulate acknowledgments.\\n\\n* recover_storage: Instructs the storage service to attempt to recover the bundles and payloads assocaited with a previous channel with the same local node and service.\\n\\n* __storage_service_parm__: A pass through to the storage service `create` function.\\n\\n`returns` - pointer to a channel descriptor.  On error, NULL is returned.\\n\\n----------------------------------------------------------------------\\n##### Close Channel\\n\\n`void bplib_close (bp_desc_t* desc)`\\n\\nCloses the specified bundle channel and releases all run-time resources associated with it; this does not include the bundles stored in the storage service; nor does it include bundles that have been transmitted but not yet acknowledged (active bundles).  The close call is not mutex\\'ed against other channel operations - it is the caller\\'s responsibility that the close call is made non-concurrently with any other library function call on that channel.\\n\\n`desc` - a descriptor for which channel to close\\n\\n----------------------------------------------------------------------\\n##### Flush Channel\\n\\n`int bplib_flush (bp_desc_t* desc)`\\n\\nFlushes all active bundles on a channel; this treats each bundle that has been transmitted but not yet acknowledged as if it was immediately acknowledged.  This function is separate from the bplib_close function because it is possible that a storage service supports resuming where it left off after a channel is closed.  In such a case, closing the channel would occur without flushing the data since the next time the channel was opened, the data that had not yet been relinquished would resume being sent.\\n\\n`channel` - a descriptor for which channel to flush\\n\\n----------------------------------------------------------------------\\n##### Config Channel\\n\\n`int bplib_config (bp_desc_t* desc, int mode, int opt, void* val, int len)`\\n\\nConfigures or retrieves an attribute on a channel.\\n\\n`desc` - a descriptor for which channel to configure or retrieve attribute\\n\\n`mode` - whether to read or write the attribute\\n\\n* BP_OPT_MODE_READ: the attribute is read and placed into the memory localtion pointed to by _val_\\n\\n* BP_OPT_MODE_WRITE: the attribute is written with the value stored at the memory location pointed to by _val_\\n\\n`opt` - the attribute to perform the operation on, as described in the table below.  The different attributes that can be changed or read are further described in the [Open Channel](#open-channel) section.\\n\\n| Option                 | Units    | Default | Description |\\n| ---------------------- | -------- | ------- | ----------- |\\n| BP_OPT_LIFETIME        | int      | 0 | Amount of time in seconds added to creation time specifying duration of time bundle is considered valid, 0: infinite |\\n| BP_OPT_REQUEST_CUSTODY | int      | 1 | Sets whether transmitted bundles request custody transfer, 0: false, 1: true |\\n| BP_OPT_ADMIN_RECORD    | int      | 0 | Sets whether generated bundles are administrative records, 0: false, 1: true |\\n| BP_OPT_INTEGRITY_CHECK | int      | 1 | Sets whether transmitted bundles include a BIB extension block, 0: false, 1: true |\\n| BP_OPT_ALLOW_FRAGMENTATION | int  | 1 | Sets whether transmitted bundles are allowed to be fragmented, 0: false, 1: true |\\n| BP_OPT_CIPHER_SUITE    | int      | BP_BIB_CRC16_X25 | The type of Cyclic Redundancy Check used in the BIB extension block - BP_BIB_NONE, BP_BIB_CRC16_X25, BP_BIB_CRC32_CASTAGNOLI |\\n| BP_OPT_TIMEOUT         | int      | 10 | Amount of time in seconds to wait for positive acknowledgment of transmitted bundles before retransmitting, 0: infinite |\\n| BP_OPT_MAX_LENGTH      | int      | 4096 | Maximum length of the transmitetd bundles |\\nBP_WRAP_BLOCK, BP_WRAP_DROP |\\n| BP_OPT_CID_REUSE       | int      | 0 | Sets whether retransmitted bundles reuse their original custody ID, 0: false, 1: true |\\n| BP_OPT_DACS_RATE       | int      | 5 | Sets minimum rate of ACS generation |\\n\\n__NOTE__: _transmitted_ bundles include both bundles generated on the channel from local data that is stored, as well as bundles that are received and forwarded by the channel.\\n\\n`val` - the value set or returned\\n\\n`len` - the length in bytes of the memory pointed to by _val_\\n\\n`returns` - [return code](#4-2-return-codes).\\n\\n----------------------------------------------------------------------\\n##### Latch Statistics\\n\\n`int bplib_latchstats (bp_desc_t* desc, bp_stats_t* stats)`\\n\\nRetrieve channel statistics populated in the structure pointed to by _stats_.\\n\\n`desc` - a descriptor for channel to retrieve statistics on\\n\\n`stats` - pointer to the statistics structure to be populated\\n\\n* __lost__: number of deleted bundles due to: storage failure, and memory copy failure\\n\\n* __expired__: number of deleted bundles due to their lifetime expiring\\n\\n* __unrecognized__: number of bundles that were attempted to be processed but either could not be parsed or were of an unsupported type\\n\\n* __transmitted_bundles__: number of bundles returned by the `bplib_load` function for the first time (does not include retransmissions)\\n\\n* __transmitted_dacs__: number of dacs returned by the `bplib_load` function\\n\\n* __retransmitted_bundles__: number of bundles returned by the `bplib_load` function because the bundle timed-out and is being resent\\n\\n* __delivered_payloads__: number of bundle payloads delivered to the application via the `bplib_accept` function\\n\\n* __received_bundles__: number of bundles destined for the local node that were successfully processed by the `bplib_process` function; the payload was successfully stored by the storage service and is awaiting acceptance\\n\\n* __forwarded_bundles__: number of bundles destined for the another node that were successfully processed by the `bplib_process` function; this does not indicated that the forwarded bundle was transmitted, only that it was successfully stored by the storage service and is awaiting transmission.\\n\\n* __received_dacs__: number of DACS destined for the local node that were successfully processed by the `bplib_process` function; this only counts the DACS bundles received by the local node, not the bundles acknowledged by the DACS - that is represented in the acknowledged_bundles statistic.\\n\\n* __stored_bundles__: number of data bundles currently in storage\\n\\n* __stored_payloads__: number of payloads currently in storage\\n\\n* __stored_dacs__: number of aggregate custody signal bundles currently in storage\\n\\n* __acknowledged_bundles__: number of locally stored bundles positively acknowleged and deleted due to a custody signal acknowledgment\\n\\n* __active_bundles__: number of bundles that have been loaded for which no acknowledgment has been received\\n\\n----------------------------------------------------------------------\\n##### Store Payload\\n\\n`int bplib_store (bp_desc_t* desc, void* payload, int size, int timeout, uint32_t* flags)`\\n\\nInitiates sending the data pointed to by _payload_ as a bundle. The data will be encapsulated in a bundle (or many bundles if the channel allows fragmentation and the payload exceeds the maximum bundle length) and queued in storage for later retrieval and transmission.\\n\\n`desc` - a descriptor for channel to create bundle on\\n\\n`payload` - pointer to data to be bundled\\n\\n`size` - size of payload in bytes\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`flags` - flags that provide additional information on the result of the store operation (see [flags](#6-3-flag-definitions)).  The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.\\n\\n`returns` - size of bundle created in bytes, or [return code](#4-2-return-codes) on error.\\n\\n----------------------------------------------------------------------\\n##### Load Bundle\\n\\n`int bplib_load (bp_desc_t* desc, void** bundle,  int* size, int timeout, uint32_t* flags)`\\n\\nReads the next bundle from storage to be sent by the application over the convergence layer.  From the perspective of the library, once a bundle is loaded to the application, it is as good as sent.  Any failure of the application to send the bundle is treated no differently that a failure downstream in the bundle reaching its destination.  On the other hand, the memory containing the bundle returned by the library is kept valid until the `bplib_ackbundle` function is called, which must be called once for every returned bundle.  So while subsequent calls to `bplib_load` will continue to provide the next bundle the library determines should be sent, the application is free to hold onto the bundle buffer and keep trying to send it until it acknowledges the bundle to the library.\\n\\n`desc` - a descriptor for channel to retrieve bundle from\\n\\n`bundle` - pointer to a bundle buffer pointer; on success, the library will populate this pointer with the address of a buffer containing the bundle that is loaded.\\n\\n`size` - pointer to a variable holding the size in bytes of the bundle buffer being returned, populated on success.\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`flags` - flags that provide additional information on the result of the load operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.\\n\\n`returns` - the bundle reference, the size of the bundle, and [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Process Bundle\\n\\n`int bplib_process (bp_desc_t* desc, void* bundle,  int size, int timeout, uint32_t* flags)`\\n\\nProcesses the provided bundle.\\n\\nThere are three types of bundles processed by this function:\\n(1) If the bundle is an aggregate custody signal, then any acknowledged bundles will be freed from storage.\\n(2) If the bundle is destined for the local node, then the payload data will be extracted and queued for retrieval by the application; and if custody is requested, then the current aggregate custody signal will be updated and queued for transmission if necessary.\\n(3) If the bundle is not destined for the local node, then the bundle will be queued for transmission as a forwarded bundle; and if custody is requested, then the current aggregate custody signal will be updated and queued for transmission if necessary.\\n\\n`desc` - a descriptor for channel to process bundle on\\n\\n`bundle` - pointer to a bundle\\n\\n`size` - size of the bundle in bytes\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`flags` - flags that provide additional information on the result of the process operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.\\n\\n`returns` - [return code](#4-2-return-codes).\\n\\n----------------------------------------------------------------------\\n##### Accept Payload\\n\\n`int bplib_accept (bp_desc_t* desc, void** payload, int* size, int timeout, uint32_t* flags)`\\n\\nReturns the next available bundle payload (from bundles that have been received and processed via the `bplib_process` function) to the application. The memory containing the payload returned by the library is kept valid until the `bplib_ackpayload` function is called, which must be called once for every returned payload.  So while subsequent calls to `bplib_accept` will continue to provide the next payload the library determines should be accepted, the payload will not be deleted from the library\\'s storage service until it is acknowledged by the application.\\n\\n`desc` - a descriptor for channel to accept payload from\\n\\n`payload` - pointer to a payload buffer pointer; on success, the library will populate this pointer with the address of a buffer containing the payload that is accepted.\\n\\n`size` - pointer to a variable holding the size in bytes of the payload buffer being returned, populated on success.\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`flags` - flags that provide additional information on the result of the accept operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.\\n\\n`returns` - the payload reference, the size of the payload, and [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Acknowledge Bundle\\n\\n`int bplib_ackbundle (bp_desc_t* desc, void* bundle)`\\n\\nInforms the library that the memory and storage used for the payload can be freed.  The memory will be immediately freed, the storage will be freed immediately only if the bundle is not requesting custody transfer (otherwise, if the bundle is requesting custody transfer, then the ACS acknowledgment frees the storage).  This must be called at some point after every bundle that is loaded.\\n\\n`desc` - a descriptor for channel to acknowlwedge bundle\\n\\n`bundle` - pointer to the bundle buffer to be acknowledged\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Acknowledge Payload\\n\\n`int bplib_ackpayload (bp_desc_t* desc, void* payload)`\\n\\nInforms the library that the memory and storage used for the payload can be freed.  This must be called at some point after every payload that is accepted.\\n\\n`desc` - a descriptor for channel to acknowlwedge payload\\n\\n`payload` - pointer to the payload buffer to be acknowledged\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Route Information\\n\\n`int bplib_routeinfo (void* bundle, int size, bp_route_t* route)`\\n\\nParses the provided bundle and supplies its endpoint ID node and service numbers.  Used to route a received bundle to the appropriate channel by looking up its destination endpoint ID prior to making other library calls that require a channel identifier.\\n\\n`bundle` - pointer to a buffer of memory containing a properly formatted bundle\\n\\n`size` - size of the bundle\\n\\n`route` - pointer to a route structure that is populated by the function (see [Open Channel](#open-channel) for more details on the structure contents).\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Display Bundle\\n\\n`int bplib_display (void* bundle, int size, uint32_t* flags)`\\n\\nParses the provided bundle (transversing the primary block, extension blocks, and payload block), and logs debug information about the bundle.\\n\\n`bundle` - pointer to a buffer of memory containing a properly formatted bundle\\n\\n`size` - size of the bundle\\n\\n`flags` - flags that provide additional information on the result of the accept operation (see [flags](#6-3-flag-definitions)). The flags variable is not initialized inside the function, so any value it has prior to the function call will be retained.\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### EID to IPN\\n\\n`int bplib_eid2ipn (const char* eid, int len, bp_ipn_t* node, bp_ipn_t* service)`\\n\\nConvert a enpoint ID string into the IPN node and service numbers\\n\\n`eid` - string containing the endpoint ID\\n\\n`len` - length of the __eid__ string\\n\\n`node` - pointer to variable that will be populated with the node number\\n\\n`service` - pointer to the variable that will be populated with the service number\\n\\n----------------------------------------------------------------------\\n##### IPN to EID\\n\\n`int bplib_ipn2eid (char* eid, int len, bp_ipn_t node, bp_ipn_t service)`\\n\\nConvert an IPN node and service number to an enpoint ID string\\n\\n`eid` - pointer to a buffer that will be populated with the endpoint ID string\\n\\n`len` - length of the __eid__ buffer\\n\\n`node` - node number used to populate the endpoint ID string\\n\\n`service` - service number used to populate the endpoint ID string\\n\\n----------------------------------------------------------------------\\n##### Initialize Attributes\\n\\n`int bplib_attrinit (bp_attr_t* attr)`\\n\\nInitialize an attribute structure with the library default values.  This is useful when creating a channel where only a few attributes need to be changed.\\n\\n`attr` - pointer to attributes structure populated by the function (see [Open Channel](#open-channel) for more details on the attributes structure contents).\\n\\n----------------------------------------------------------------------\\n#### 4.2 Return Codes\\n\\n| Code                    | Value | Description |\\n| ----------------------- | ----- | ----------- |\\n| BP_SUCCESS              | 0     | Operation successfully performed |\\n| BP_ERROR                | -1    | Generic error occurred; further information provided in flags to determine root cause |\\n| BP_TIMEOUT              | -2    | A timeout occurred when a blocking operation was performed |\\n\\n----------------------------------------------------------------------\\n#### 4.3 Flag Definitions\\n\\n| Flag                           | Value | Description |\\n| ------------------------------ | ----- | ----------- |\\n| BP_FLAG_DIAGNOSTIC             | 0x00000000 | No event issued - diagnostic message only |\\n| BP_FLAG_NONCOMPLIANT           | 0x00000001 | Valid bundle but the library was not able to comply with the standard |\\n| BP_FLAG_INCOMPLETE             | 0x00000002 | At least one block in bundle was not recognized |\\n| BP_FLAG_UNRELIABLE_TIME        | 0x00000004 | The time returned by the O.S. preceded the January 2000 epoch, or went backwards |\\n| BP_FLAG_DROPPED                | 0x00000008 | A bundle was dropped because a required extension block could not be processed |\\n| BP_FLAG_FAILED_INTEGRITY_CHECK | 0x00000010 | A bundle with a BIB failed the integrity check on the payload |\\n| BP_FLAG_BUNDLE_TOO_LARGE       | 0x00000020 | The size of a bundle exceeds the capacity allowed by library |\\n| BP_FLAG_ROUTE_NEEDED           | 0x00000040 | A bundle needs to be routed before transmission |\\n| BP_FLAG_STORE_FAILURE          | 0x00000080 | Storage service failed to deliver data |\\n| BP_FLAG_UNKNOWN_CID            | 0x00000100 | An ACS bundle acknowledged a CID for which no bundle was found |\\n| BP_FLAG_SDNV_OVERFLOW          | 0x00000200 | The local variable used to read/write and the value was of insufficient width |\\n| BP_FLAG_SDNV_INCOMPLETE        | 0x00000400 | There was insufficient room in block to read/write value |\\n| BP_FLAG_ACTIVE_TABLE_WRAP      | 0x00000800 | The active table wrapped; see BP_OPT_WRAP_RESPONSE |\\n| BP_FLAG_DUPLICATES             | 0x00001000 | The custody ID was already acknowledged |\\n| BP_FLAG_CUSTODY_FULL           | 0x00002000 | An aggregate custody signal was generated due the number of custody ID gaps exceeded the maximum allowed |\\n| BP_FLAG_UNKNOWNREC             | 0x00004000 | A bundle contained unknown adminstrative record |\\n| BP_FLAG_INVALID_CIPHER_SUITEID | 0x00008000 | An invalid cipher suite ID was found in a BIB |\\n| BP_FLAG_INVALID_BIB_RESULT_TYPE| 0x00010000 | An invalid result type was found in a BIB |\\n| BP_FLAG_INVALID_BIB_TARGET_TYPE| 0x00020000 | An invalid target type was found in a BIB |\\n| BP_FLAG_FAILED_TO_PARSE        | 0x00040000 | Unable to parse a bundle due to internal inconsistencies in bundle |\\n| BP_FLAG_API_ERROR              | 0x00080000 | Calling program incorrectly used a library function, e.g. passing in invalid parameter |\\n\\n----------------------------------------------------------------------\\n## 5. Storage Service\\n----------------------------------------------------------------------\\n\\nThe application is responsible for providing the storage service to the library at run-time through call-backs passed to the `bplib_open` function.\\n\\n----------------------------------------------------------------------\\n##### Create Storage Service\\n\\n`int create (int type, bp_ipn_t node, bp_ipn_t service, bool recover, void* parm)`\\n\\nCreates a storage service.\\n\\n`type` - the type of bundle being stored, will be one of the following (defined in bplib.h): BP_STORE_DATA_TYPE, BP_STORE_DACS_TYPE, BP_STORE_PAYLOAD_TYPE\\n\\n`node` - the {node} number of the ipn:{node}.{service} endpoint ID used for the source of bundles generated on the channel\\n\\n`service` - the {service} number of the ipn:{node}.{service} endpoint ID used for the source of bundles generated on the channel\\n\\n`recover` - _true_: attempt to recover bundles from existing storage that matches the type, node, and service; _false_: do not recover any bundles. Note that a storage service does not need to provide a recovery capability, in which case this parameter is ignored and bundles are never recovered.\\n\\n`parm` - service specific parameters pass through library to this function.  See the storage_service_parm of the attributes structure passed to the `bplib_open` function.\\n\\n`returns` - handle for storage service used in subsequence calls.\\n\\n----------------------------------------------------------------------\\n##### Destroy Storage Service\\n\\n`int destroy (int handle)`\\n\\nDestroys a storage service.  This does not mean that the data stored in the service is freed - that is service specific.\\n\\n`handle` - handle to the storage service\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Enqueue Storage Service\\n\\n`int enqueue (int handle, void* data1, int data1_size, void* data2, int data2_size, int timeout)`\\n\\nStores the pointed to data into the storage service.\\n\\n`handle` - handle to the storage service\\n\\n`data1` - pointer to first block of memory to store.  This must be concatenated with __data2__ by the function into one continuous block of data.\\n\\n`data1_size` - size of first block of memory to store.\\n\\n`data2` - pointer to second block of memory to store.  This must be concatenated with __data1__ by the function into one continuous block of data.\\n\\n`data2_size` - size of the second block of memory to store.\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Dequeue Storage Service\\n\\n`int dequeue (int handle, void** data, int* size, bp_sid_t* sid, int timeout)`\\n\\nRetrieves the oldest data block stored in the storage service that has not yet been dequeued, and returns a _Storage ID_ that can be used to retrieve the data block in the future.\\n\\n`handle` - handle to the storage service\\n\\n`data` - the pointer that will be updated to point to the retrieved block of memory.  This function returns the data block via a pointer and performs no copy.  The data is still owned by the storage service and is only valid until the next dequeue or relinquish call.\\n\\n`size` - size of data block being retrieved.\\n\\n`sid` - pointer to a _Storage ID_ variable populated by the function.  The sid variable is used in future storage service functions to identify the retrieved data block.\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Retrieve Storage Service\\n\\n`int retrieve (int handle, void** data, int* size, bp_sid_t sid, int timeout)`\\n\\nRetrieves the data block stored in the storage service identified by the _Storage ID_ sid parameter.\\n\\n`handle` - handle to the storage service\\n\\n`data` - the pointer that will be updated to point to the retrieved block of memory.  This function returns the data block via a pointer and performs no copy.  The data is still owned by the storage service and is only valid until the next dequeue or relinquish call.\\n\\n`size` - size of data block being retrieved.\\n\\n`sid` - the _Storage ID_ that identifies which data block to retrieve from the storage service\\n\\n`timeout` - 0: check, -1: pend, 1 and above: timeout in milliseconds\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Release Storage Service\\n\\n`int release (int handle, bp_sid_t sid)`\\n\\nReleases any in-memory resources associated with the dequeueing or retrieval of a bundle.\\n\\n`handle` - handle to the storage service\\n\\n`sid` - the _Storage ID_ that identifies the data block for which memory resources are released.\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Relinquish Storage Service\\n\\n`int relinquish (int handle, bp_sid_t sid)`\\n\\nDeletes the stored data block identified by the _Storage ID_ sid parameter.\\n\\n`handle` - handle to the storage service\\n\\n`sid` - the _Storage ID_ that identifies which data block to delete from storage\\n\\n`returns` - [return code](#4-2-return-codes)\\n\\n----------------------------------------------------------------------\\n##### Get Count Storage Service\\n\\n`int getcount (int handle)`\\n\\nReturns the number of data blocks currently stored in the storage service.\\n\\n`handle` - handle to the storage service\\n\\n`returns` - number of data blocks\\n\\n----------------------------------------------------------------------\\nThe storage service call-backs must have the following characteristics:\\n* `enqueue`, `dequeue`, `retrieve`, and `relinquish` are expected to be thread safe against each other.\\n* `create` and `destroy` do not need to be thread safe against each other or any other function call - the application is responsible for calling them when it can complete atomically with respect to any other storage service call\\n* The memory returned by the dequeue and retrieve function is valid until the release function call.  Every dequeue and retrieve issued by the library will be followed by a release.\\n* The _Storage ID (SID)_ returned by the storage service cannot be zero since that is marked as a _VACANT_ SID\\n\\n\\n'},\n",
       " {'repo': 'nasa/simupy-flight',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'SimuPy Flight Vehicle Toolkit\\n=============================\\n\\nVehicle flight simulation is an important part of the innovation of aerospace vehicle technology. The NASA Engineering Safety Center (NESC) has identified and addressed the need to verify flight vehicle simulations through their work on the `“Six degree-of-freedom (6-DOF) Flight Simulation Test Cases.” <https://nescacademy.nasa.gov/flightsim/>`_ The author was prompted to develop tools that would allow for the rapid implementation of simulations for novel vehicle concepts including hypersonic re-entry vehicles and urban air mobility vehicles.\\n\\nThis software library leverages open source scientific computing tools to implement an efficient simulation framework for flight vehicles in Python. Equations of motion are composed in blocks using the SimuPy library, an open source Python alternative to Simulink, and integrated using SciPy’s wrappers for standard Fortran implementations of ordinary differential equation solvers. Dynamics equations of the inertial state variables for the position, orientation, and their corresponding rates for integration are developed using the SymPy symbolic library and implemented using code generation. Kinematics equations are implemented through symbolic definition and code generation as well as leveraging other open source software that implements useful functions, such as the solutions to the inverse geodesy problem.\\n\\n\\nNESC Test Cases\\n---------------\\n\\nA number of the NESC Atmospheric test cases have been implemented to verify the implementation and derivation of the equations of motion. These are located in the `nesc_test_cases` directory. To run, simply execute any of `nesc_case##.py` files or the `run_nesc_cases.py` which will iterate through test cases that have been implemented. These scripts will attempt to load the NESC reference results from the parent directory and plot the results along with the results from the SimuPy implemntation. To include the NESC results in the comparison plots, download the `Atmospheric trajectory data <https://nescacademy.nasa.gov/src/flightsim/Datasets/Atmospheric_checkcases.zip>`_ and unzip the `Atmospheric_checkcases` directory to the root `simupy_flight` directory. You can place the `Atmospheric_checkcases` directory in different location by changing the `data_relative_path` variable in the `nesc_testcase_helper.py` script.\\n\\nLicense\\n-------\\n\\nThis software is released under the `NASA Open Source Agreement Version 1.3 <https://github.com/nasa/simupy-flight/raw/master/license.pdf>`_.\\n\\n\\nNotices\\n-------\\n\\nCopyright © 2021 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nDisclaimers\\n-----------\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.'},\n",
       " {'repo': 'nasa/europa',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# EUROPA \\n\\n[![Build Status](https://travis-ci.org/nasa/europa.svg?branch=master)](https://travis-ci.org/nasa/europa) <a href=\"https://scan.coverity.com/projects/3615\">\\n  <img alt=\"Coverity Scan Build Status\"\\n       src=\"https://scan.coverity.com/projects/3615/badge.svg\"/>\\n</a>\\n[![Coverage Status](https://coveralls.io/repos/nasa/europa/badge.png)](https://coveralls.io/r/nasa/europa)\\n\\n**EUROPA is available under [NASA\\'s Open Source Agreement (NOSA)](https://ti.arc.nasa.gov/opensource/nosa)**\\n\\nWelcome!  EUROPA is a framework to model and tackle problems in Planning, Scheduling and Constraint Programming. EUROPA is typically embedded in a host application. It is designed to be expressive, efficient, extendable and configurable. It includes:\\n \\n- **A Plan Database:** The technology cornerstone of EUROPA for storage and manipulation of plans as they are initialized and refined. The EUROPA Plan Database integrates a rich representation for actions, states, objects and constraints with powerful algorithms for automated reasoning, propagation, querying and manipulation.\\n- **A Problem Solver:** A core solver to automatically find and fix flaws in the plan database. It can be configured to plan, schedule or both. It can be easily customized to integrate specialized heuristics and resolution operations.\\n- **A Tool Box:** Europa includes a debugger for instrumentation and visualization of applications. It also includes a very high-level, declarative modeling language for describing problem domains and partial-plans.\\n\\n[Learn more...](//github.com/nasa/europa/wiki/What-Is-Europa)\\n\\nEUROPA was developed at NASA\\'s Ames Research Center and is available under NASA\\'s open source agreement [(NOSA)](https://ti.arc.nasa.gov/opensource/nosa) \\n\\n|**Getting Started**|**Documentation**|**Development**|\\n|-------------------|-----------------|---------------|\\n|[Download](https://github.com/nasa/europa/wiki/Europa-Download)|[Background/Overview](https://github.com/nasa/europa/wiki/Europa-Background)|[Building EUROPA](https://github.com/nasa/europa/wiki/Building-Europa)|\\n|[Installation](https://github.com/nasa/europa/wiki/Europa-Installation)|[User Documentation](https://github.com/nasa/europa/wiki/Europa-Docs)|[Automated Builds](https://github.com/nasa/europa/wiki/Nightly-Builds)|\\n|[Quick Start](https://github.com/nasa/europa/wiki/Quick-Start)|[Examples](https://github.com/nasa/europa/wiki/Europa-Examples)|[Developer Notes](https://github.com/nasa/europa/wiki/Misc-Development)|\\n||[Publications](https://github.com/nasa/europa/wiki/Europa-Publications)|[Product Roadmap](https://github.com/nasa/europa/wiki/Europa-Roadmap)|\\n|||[People](https://github.com/nasa/europa/wiki/Europa-Team)|\\n\\nFor questions, please use the [europa-users](http://groups.google.com/group/europa-users) google group.\\n'},\n",
       " {'repo': 'nasa/SBN',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Software Bus Network\\nNASA Core Flight System (cFS) Software Bus Network (SBN) Application\\n\\n## Description\\nThe SBN is a cFS application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\nThe SBN application connects the cFE Software Bus (SB) to other buses, bridging the publish/subscribe messaging service to separate cFS instances in separate partitions, processes, processors, and/or networks.\\n\\n## License\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/atd2-fuser',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Overview\\n---\\n\\nThe Fuser aggregates flight data from multiple FAA sources, Airline data, and \\n3rd party data into a unified source. Flight information is organized by \\nindividual flights (one take-off and one landing) using the Globally Unique \\nFlight Identifier (GUFI). As new messages are received, the flight file is \\nupdated. Clean and accurate data is assured through the use of transformation \\nand mediation processes which enforce business rules on the received data. For \\nadditional details see the [NASA ATD-2 Industry Workshop Fuser Architecture Overview](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Architecture-Overview_84377881.html) \\n  \\n>**Fuser Archicture Diagram**  \\n>![Fuser Architecture Diagram](images/Fuser_Architecture_Overview.png)\\n\\n\\n## Data Mappings\\n---\\n\\nData mapping representing the set of available flight fields produced by the \\nFuser and how those fields map to SWIM data can be found at \\n[NASA ATD-2 Industry Workshop Data Dictionary](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Data-Dictionary_92471950.html)\\nand\\n[NASA ATD-2 Industry Workshop Fuser Database Input Mapping Table](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Database-Input-Mapping-Table_85328219.html)\\n\\n\\n## Configuration\\n---\\n\\nThe Fuser is highly configurable allowing for customization of attribute\\nfilters, mediation rules, updaters, and message endpoints.  All property files \\nare located under *src/main/resources/config/fuser* directory.\\n\\n  \\n| Configuration | Location | Description |\\n|---|---|---|\\n| Default | properties.props | Default properties file containing definitions which affect the Fuser behavior. Definitions are store in *key=value* pairings. These definitions include but are not limited to: broker URL\\'s,  logging behavior, active updaters, which data sources are active, etc. |\\n| Attribute Filters | /attributes/attributes.xml | XML file containing sets of *attributeFilter* elements grouped by arrival/departure category. Each *attributeFilter* element contains attributes defining the source and type (exclusion/inclusion) of filter applied to the nested set of flight elements. Additional information about attribute filters can be found at [NASA ATD-2 Industry Workshop Fuser Filtering Overview](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Fuser-Filtering-Overview_85328206.html) |\\n| Mediation Rules | rules.properties.props beans.rules.xml | The *beans.rules.xml* Spring context defines the set of mediation rules which may be applied to  messages received by the Fuser.  Each bean definition represents a mediation rule determining how  specific flight fields are handled given a list of allowable sources.  The *rules.properties.props* file defines *key=value* pairings for controlling which mediation rules  are active and the priority for which mediation rules should be interpreted.  Additional information about mediation rules can be found at [NASA ATD-2 Industry Workshop Data Mediation Overview](https://aviationsystems.arc.nasa.gov/atd2-industry-workshop/fuser/Data-Mediation-Overview_85328193.html) |\\n| Messaging | topics.properties.props | Collection of *key=value* pairings defining the JMS routing of data through the Fuser.  Notable  properties in this file are:   - fuser.endpoint.fromFuser.fused  - fuser.endpoint.fromFuser.envelope.fused  - fuser.endpoint.fromFuser.remove  The above listed properties control the JMS topics used for distributed Fused data to downstream  applications. |\\n| Environment | src/main/resources/env | Environment property files provide support for easily running the Fuser in different environments  (e.g. production, development, test) without having to repeatedly reconfigure several different files.  An environment file may contain a subset of properties defined in *key=value* pairings which will  override other properties contained in: properties.props, rules.properties.props, and topics.properties.props. |\\n\\n## Installation\\n---\\n\\nThe Fuser depends on Apache Maven 3.5.4 or higher and requires Java 8 to compile\\nall sources. As part of the Maven build process a deploy ready zip artifact will \\nbe generated.\\n  \\nFrom the root directory of the fuser-nasa-atd2-opensource project execute: *mvn clean install*\\n\\nThis step is necessary to install all Mosaic ATM libraries.\\n  \\nThe following artifacts should be produced in the fuser-nasa-atd2-opensource/FuserCoreParent/Fuser/target/ folder:\\n  \\n - Fuser.jar: JAR file containing the compiled binary representation of the Fuser\\n - Fuser-bin.zip: Compressed artifact containing all of the dependencies, configurations, and\\nscripts required to run the Fuser as a stand-alone application\\n - Fuser-resources.zip: Compressed artifact containing only the Fuser configuration files\\n\\n\\n## Execution\\n---\\n\\nComplete the [Installation](#Installation) steps as execution requires the \\nFuser-bin.zip artifact generated as part of the installation.  Execution\\nrequires that a Java 8 JRE be installed on the target system.  Steps to execute \\nthe Fuser as a stand-alone application are:\\n\\n1.  Unzip the Fuser-bin.zip artifact.  This should result in a directory structure\\nsimilar to:\\n  \\n>```\\n>Fuser\\n>   |--/config        : contains all property and configuration files\\n>   |--/env           : contains environment property overrride files\\n>   |--/lib           : contains all required dependencies\\n>   |--/plugins       : contains plugins for consuming data sources\\n>   |--redis-utils\\n>   |--runFuser\\n>   |--runFuser.bat\\n>```\\n  \\n2. Make any modifications to the Fuser configuration as outlined in the \\n[Configuration](#Configuration) section.  Note that all configuration files\\nare available under the */config* directory.\\n  \\n3. Execute the *./runFuser [environment property name]* where \\n*environment property name* refers to the name of a specific property file \\nlocated in the */env* directory which will be used to override any default\\nconfigurations.  If no environment property name is\\nprovided the default configurations will be used.\\n\\n\\n## License\\n---\\nThis software is released under the [NASA Open Source Agreement Version 1.3](license.pdf)\\n\\n## Notices\\n---\\nCopyright � 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\n  \\n## Disclaimers\\n---\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n  \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n\\n'},\n",
       " {'repo': 'nasa/utm-apis',\n",
       "  'language': None,\n",
       "  'readme_contents': '# UTM APIs\\n\\nThis repository contains the collection of OpenAPI specification APIs within the NASA\\'s research version of the UTM System.  Many of UTM\\'s data models are common and these are maintained using Swagger\\'s \"Domain\" references.\\n\\n## References\\n\\n- [UTM Home Page](https://utm.arc.nasa.gov/)\\n- [UTM Publications](https://utm.arc.nasa.gov/documents.shtml)\\n- [UTM Swaggerhub](https://app.swaggerhub.com/organizations/utm)\\n\\n\\n## Sandbox and RELEASE branch\\n\\nutm-apis commits pinned to our Sandbox are represented in the RELEASE branch, and the master branch is our development branch.  You will generally find the master branch to be ahead of the Sandbox.  The tags in RELEASE correspond with our sandbox releases.\\n\\nFor codegen you generally will point to RELEASE.  \\n\\n\\n## Viewing Local Swagger Spec files\\n\\nYou have choices to edit/view local swagger files. IMO option 1 is better for viewing all files.  A local swagger editor is good if you want rendering.\\n\\n1. Use a text editor that supports YAML:\\n  - Sublime Text 2 highlights YAML well\\n  - [Atom](https://atom.io/) works well and has a [Swagger Lint Plugin](https://atom.io/packages/linter-swagger) that provides a good first cut at valid OpenAPI 2.0 correctness\\n\\n2. Install local [Swagger Editor](https://swagger.io/swagger-editor/)\\n\\n3. Bring up an [online swagger editor](https://editor.swagger.io/) and copy or import the source swagger.\\n\\n\\n## Codegen\\n\\nYou can generate code from OpenAPI Specifications (swagger).  \\n\\nSwagger Hub has a feature where the site will generate code into a zip file which is downloaded.  This is a great first checkout, however, it uses the default language-specific configurations.\\n\\nOne option (which we use) is to generate all the data models into a library.\\nNote that codegen parses only from API Specifications, not directly from Swagger Domains.  \\nAn approach to creating a model-only library is to codegen against all the APIs whereby the generation\\noutput is filtered for model-only.\\nThe argument to the input spec can be a local file or internet\\n\\n````````\\nGENERATE=\"java  -Dmodels -DmodelDocs=false -DapiDocs=false -jar $CODEGEN generate  -l spring --config config.json\"\\n\\n$GENERATE -i ${SWREPO}/fims-uss-api/swagger.yaml   #localfile input\\nor\\n$GENERATE -i https://raw.githubusercontent.com/nasa/utm-apis/master/uss-api/swagger.yaml\\n\\n````````\\n\\nYour language-specific configurations can also generate model-specific data validations.\\nFor example using swagger-codegen\\'s \\'Spring server\\' language you can codegen bean validations\\nand java8\\'s OffsetDateTime class for date-time using this language-specific config.\\n\\n````````\\n\\n{\\n  \"library\": \"spring-boot\",\\n  \"java8\": \"true\",\\n  \"dateLibrary\":\"java8\",\\n  \"modelPackage\": \"gov.nasa.utm.utmcommons.model\",\\n  \"useBeanValidation\":\"true\"\\n}\\n\\n````````\\n'},\n",
       " {'repo': 'nasa/aladynpi',\n",
       "  'language': None,\n",
       "  'readme_contents': '-----------------------------------------------------------------------\\n04-26-2019\\n\\nALADYN_PI mini-app is a simple molecular dynamics (MD) code for performing \\nconstant energy atomistic simulations using either a straight \\nArtificial Neural Network (ANN) interatomic potential, or\\nPhysically Informed Neural Network (PINN) interatomic potential. \\n\\nReferences:\\n 1. Vesselin I. Yamakov, Edward H. Glaessgen, NASA/TM-2019-220431\\n    (https://ntrs.nasa.gov/citations/20200000069)\\n 2. G.P.Purja Pun, V.Yamakov, J.Hickman, E.H.Glaessgen, Y.Mishin, \\n    Physical Review Materials, 4, 113807 (2020).\\n    (https://link.aps.org/doi/10.1103/PhysRevMaterials.4.113807)\\n\\nThe trained ANN was produced and offered by Ganga P. Pun and Yuri Mishin \\nfrom George Mason University.\\n\\n=======================================================================\\n\\n COMPILATION\\n\\n-----------------------------------------------------------------------\\n* FORTRAN compiler (Intel 2003 or newer, or PGI FORTRAN with OpenACC)\\n-----------------------------------------------------------------------\\nSource directory: ALADYN_PI.source\\n-----------------------------------------------------------------------\\nCompilation: use the provided makefiles with the following options:\\n\\nIntel compiler:\\n> make -f makefile.intel            ! compiles with -O3 optimization on \\n> make -f makefile.intel DEBUG=TRUE ! check and warning flags on\\n> make -f makefile.intel OMP=TRUE   ! compile with OpenMP direectives\\n\\nPGI compiler:\\n> make -f makefile.pgi              ! compiles with -O3 optimization on \\n> make -f makefile.pgi DEBUG=TRUE   ! check and warning flags on\\n> make -f makefile.pgi OMP=TRUE     ! compile with OpenMP direectives\\n> make -f makefile.pgi ACC=TRUE  ! compile with OpenMP+OpenACC direectives\\n\\nEdit the provided makefiles for specific compiler option of your choice\\n=======================================================================\\n\\n EXECUTION\\n\\n-----------------------------------------------------------------------\\nRun from the example (test) directory: ALADYN_PI.test\\n\\nRunning a tets case:\\n> aladyn_pi              ! executes 10 MD steps report at each step\\n> aladyn_pi -n 100 -m 10 ! executes 100 MD steps, report at each 10-th step\\n\\nAvailable PBS scripts used for NASA/LaRC K3 cluster:\\nALADYN_PI_intel.job      - run the intel version\\nALADYN_PI_pgi_V100.job   - run the pgi version with OpneACC for V100 gpu.\\n\\nScreen output is riderected to aladyn_pi.out\\n\\nExample outputs are saved in: aladyn_pi_intel.out and aladyn_pi_pgi.out\\n=======================================================================\\n\\nINPUT FILES:\\n\\n-----------------------------------------------------------------------\\nPINN.dat - Neural network potential file\\nstructure.plt - input atomic structure file\\n\\n-----------------------------------------------------------------------\\n--- Available potential files in directory POT:\\n(to be linked or copied as PINN.dat in the working directory of aladyn_pi)\\n\\nANN_Si.dat - Straight Artificial Neural Network potential for Si\\nPINN_Si.dat - Physically Informed Neural Network potential for Si\\nPINN_Al.dat - Physically Informed Neural Network potential for Al (ref.2)\\n\\n-----------------------------------------------------------------------\\n--- Available Si single crystal test structures in directory STR_Si:\\n\\nSi_N4000.plt      -   4000 atoms Si crystal\\nSi_N8000.plt      -   8000 atoms Si crystal\\nSi_N16000.plt     -  16000 atoms Si crystal\\nSi_N32000.plt     -  32000 atoms Si crystal\\nSi_N64000.plt     -  64000 atoms Si crystal\\nSi_N128000.plt    - 128000 atoms Si crystal\\nSi_N192000.plt    - 192000 atoms Si crystal\\nSi_N256000.plt    - 256000 atoms Si crystal\\nSi_N512000.plt    - 512000 atoms Si crystal\\n\\n-----------------------------------------------------------------------\\n--- Available Al single crystal test structures in directory STR_Al:\\n\\nAl_N4000.plt      -   4000 atoms Al crystal\\nAl_N8000.plt      -   8000 atoms Al crystal\\nAl_N16000.plt     -  16000 atoms Al crystal\\nAl_N32000.plt     -  32000 atoms Al crystal\\nAl_N64000.plt     -  64000 atoms Al crystal\\nAl_N128000.plt    - 128000 atoms Al crystal\\nAl_N192000.plt    - 192000 atoms Al crystal\\nAl_N256000.plt    - 256000 atoms Al crystal\\n\\nUse any of the above structures by linking them to structure.plt, e.g.,\\n> ln -s STR/Al_N4000.plt structure.plt\\n\\n=======================================================================\\n\\nSOURCE FILES: \\n\\nFile: ALADYN_PI.source.zip contains Full Double PRECISION ANN calculation and BOP calculations.\\n\\nFile: ALADYN_PIs.source.zip contains SINGLE PRECISION ( rela(kind=4) ) ANN calculation and Double PRECISION BOP calculations.\\n\\n-----------------------\\n\\nFiles in ALADYN_PI.source.zip\\n\\n    aladyn_pi.f       - Main program\\n    aladyn_pi_sys.f   - system modul\\n    aladyn_pi_sys_OMP.f    - system modul for OpneMP compilation\\n    aladyn_pi_sys_NO_OMP.f - system modul without OpneMP compilation\\n    aladyn_pi_sys_ACC.f    - system modul for OpneACC compilation\\n    aladyn_pi_mods.f  - contains general purpose modules\\n    aladyn_pi_IO.f    - I/O operations\\n\\n    aladyn_pi_ANN_OMP.f  - Artificial Neural Network OpneMP code\\n    contains:\\n     subroutine Frc_ANN_OMP  ! ANN force & energy (OpenMP version)\\n\\n    aladyn_pi_ANN_ACC.f  - Artificial Neural Network OpneACC code\\n     subroutine Frc_ANN_ACC  ! ANN force & energy (OpenACC version)\\n\\n    aladyn_pi_PINN_OMP.f - Physically Informed NN OpneMP code\\n     subroutine Frc_PINN_OMP ! PINN force & energy (OpenMP version)\\n\\n    aladyn_pi_PINN_ACC.f - Physically Informed NN OpneACC code\\n     subroutine Frc_PINN_ACC ! PINN force & energy (OpenACC version)\\n\\n    aladyn_pi_MD.f    - molecular dynamics module\\n     contains:\\n      subroutine get_T     ! Calculates current system temperature\\n      subroutine predict_atoms ! Gear predictor call !\\n      subroutine correct_atoms ! Gear corrector call !\\n      \\n-----------------------\\n\\nFiles in ALADYN_PIs.source.zip\\n\\n    aladyn_pi.f       - Main program\\n    aladyn_pi_sys.f   - system modul\\n    aladyn_pi_sys_OMP.f    - system modul for OpneMP compilation\\n    aladyn_pi_sys_NO_OMP.f - system modul without OpneMP compilation\\n    aladyn_pi_sys_ACC.f    - system modul for OpneACC compilation\\n    aladyn_pi_mods.f  - contains general purpose modules\\n    aladyn_pi_IO.f    - I/O operations\\n\\n    aladyn_pi_ANN_OMPs.f  - Single Precision Artificial Neural Network OpneMP code\\n    contains:\\n     subroutine Frc_ANN_OMP  ! ANN force & energy (OpenMP version)\\n\\n    aladyn_pi_ANN_ACCs.f  - Single Precision Artificial Neural Network OpneACC code\\n     subroutine Frc_ANN_ACC  ! ANN force & energy (OpenACC version)\\n\\n    aladyn_pi_PINN_OMP.f - Physically Informed NN OpneMP code\\n     subroutine Frc_PINN_OMP ! PINN force & energy (OpenMP version)\\n\\n    aladyn_pi_PINN_ACC.f - Physically Informed NN OpneACC code\\n     subroutine Frc_PINN_ACC ! PINN force & energy (OpenACC version)\\n\\n    aladyn_pi_MD.f    - molecular dynamics module\\n     contains:\\n      subroutine get_T     ! Calculates current system temperature\\n      subroutine predict_atoms ! Gear predictor call !\\n      subroutine correct_atoms ! Gear corrector call !\\n\\n\\n-----------------------------------------------------------------------\\nSuggested subroutines for optimization: \\nFrc_ANN_OMP and Frc_ANN_ACC (Optimized versions from ALADYN miniapp)\\n\\nFrc_PINN_OMP and Frc_PINN_ACC\\n(Optimized versions from ALADYN miniapp)\\n\\n-----------------------------------------------------------------------\\n For further information contact:\\n\\n Vesselin Yamakov\\n National Institute of Aerospace\\n 100 Exploration Way,\\n Hampton, VA 23666\\n phone: (757)-864-2850\\n fax:   (757)-864-8911\\n e-mail: yamakov@nianet.org\\n\\n=======================================================================\\n Notices:\\n Copyright 2020 United States Government as represented by the \\n Administrator of the National Aeronautics and Space Administration. \\n All Rights Reserved.\\n \\n Disclaimers:\\n No Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY \\n WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, \\n INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE \\n WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF \\n MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM \\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR \\n FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM \\n TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, \\n CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT \\n OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY \\n OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  \\n FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES \\n REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, \\n AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n\\n Waiver and Indemnity:  \\n RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES \\n GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR \\n RECIPIENT. IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY \\n LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH \\n USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, \\n RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND \\n HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND \\n SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED \\n BY LAW. RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE \\n IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n=======================================================================\\n\\n'},\n",
       " {'repo': 'nasa/astrobee_media',\n",
       "  'language': 'CMake',\n",
       "  'readme_contents': \"# Astrobee Robot Software - Multimedia\\n\\n## About\\n\\nAstrobee is a free-flying robot that is designed to operate as a payload inside\\nthe International Space Station (ISS). The Astrobee Robot Software consists of\\nembedded (on-board) software, supporting tools and a simulator. The Astrobee\\nRobot Software operates on Astrobee's three internal single board computers and\\nuses the open-source Robot Operating System (ROS) framework as message-passing\\nmiddleware. The Astrobee Robot Software performs vision-based localization,\\nprovides autonomous navigation, docking and perching, manages various sensors\\nand actuators, and supports user interaction via screen-based displays, light\\nsignaling, and sound. The Astrobee Robot Software enables Astrobee to be\\noperated in multiple modes: plan-based task execution (command sequencing),\\nteleoperation, or autonomously through execution of hosted code uploaded by\\nproject partners (guest science). The software simulator enables Astrobee Robot\\nSoftware to be evaluated without the need for robot hardware.\\n\\nThis repository provides multimedia used by Astrobee Robot Software, for both\\nvisualizing experiments and simulating free-flyers. It includes meshes and\\ntextures from the following two external sources:\\n\\n* [IGOAL](https://nasa3d.arc.nasa.gov/detail/iss-internal) - ISS Interior.\\n* [RoboNaut2](https://gitlab.com/nasa-jsc-robotics/r2_gazebo) - ISS Handrails.\\n\\n## Change Log\\n\\nv0.0.4 : Improved handrail meshes and perching arm textures.\\nv0.0.3 : Restructured media to allow spawning certain models from world file.\\nv0.0.2 : Added granite lab mesh.\\nv0.0.1 : Added dock, space station, free-flyer and perching arm meshes.\\n\\n## Usage Guidelines\\n\\nThe repository contains original multimedia created in the Astrobee project and\\nredistributed multimedia from other NASA projects. All multimedia is covered by\\nNASAs [Media Usage Guidelines](https://www.nasa.gov/multimedia/guidelines/index.html).\\n\\n## Instructions\\n\\nSetup instructions for the Astrobee Robot Software may be found on the\\n[official respository](https://github.com/nasa/astrobee).\\n\"},\n",
       " {'repo': 'nasa/nasa-latex-docs',\n",
       "  'language': 'TeX',\n",
       "  'readme_contents': 'NASA-LaTeX-Docs\\n================\\n\\nPlease refer to the official documentation: [NASA-LaTeX-Docs Documentation](https://nasa.github.io/nasa-latex-docs/html)\\n\\nUsage:\\n-------\\n\\nWith this LaTeX package, the formatting is all handled internally so that report developers only need to create their own content following a provided template (or even create their own). If not using a template (example: AIAA or IEEE submission), this package also comes with an incredibly versatile, OS independent, Python build script and Latexmk configuration file for easy document type-setting and warning/error detection. \\n\\nThe build script is located in the following directory and can be used to build **any LaTeX document**:\\n\\n    nasa-latex-docs/buildPDF.py\\n\\nPlease see [NASA-LaTeX-Docs Documentation](https://nasa.github.io/nasa-latex-docs/html) for more details, examples, and sample outputs!\\n\\nSystem Requirements:\\n-------\\n\\n- TeX Live Distribution 2015+ (with latest updates)\\n- Python 2.6+, Python 3+'},\n",
       " {'repo': 'nasa/cumulus-message-adapter-java',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# cumulus-message-adapter-java\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter-java.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-message-adapter-java)\\n\\nThis repository contains a client library for integrating Lambda task code written in Java with the core [Cumulus Message Adapter](#about-the-cumulus-message-adapter-cma).\\n\\n## About Cumulus\\n\\nCumulus is a cloud-based data ingest, archive, distribution and management\\nprototype for NASA\\'s future Earth science data streams.\\n\\nRead the [Cumulus Documentation](https://cumulus-nasa.github.io/)\\n\\n## About the Cumulus Message Adapter (CMA)\\n\\nThe [Cumulus Message Adapter (CMA)](https://github.com/cumulus-nasa/cumulus-message-adapter) is a library that adapts incoming messages in the\\nCumulus protocol to a format more easily consumable by Cumulus tasks, invokes\\nthe tasks, and then adapts their response back to the Cumulus message protocol\\nto be sent to the next task.\\n\\n## Installation\\n\\nAdd the CMA Java client library as a dependency in your project. The current version and dependency installation code can be found [here](https://clojars.org/gov.nasa.earthdata/cumulus-message-adapter).\\n\\n## Task definition\\n\\nIn order to use the CMA Java client library, you will need to create two\\nmethods in your task module: a handler function and a business logic function.\\n\\nThe handler function is a standard Lambda handler function.\\n\\nThe business logic function is where the actual work of your task occurs. The class containing this work should implement the `ITask` interface and the `String PerformFunction(String input, Context context);` function. `input` is the simplified JSON from the message adapter and `context` is the AWS Lambda Context.\\n\\n## Cumulus Message Adapter interface\\n\\nCreate an instance of `MessageParser` and call\\n\\n```java\\nRunCumulusTask(String input, Context context, ITask task)\\n```\\n\\nor\\n\\n```java\\nRunCumulusTask(String input, Context context, ITask task, String inputSchemaLocation, String outputSchemaLocation, String configSchemaLocation)\\n```\\n\\nwith the following parameters:\\n\\n* `input` - the input to the Lambda function\\n* `context` - the Lambda context\\n* `task` - an instance of the class that implements `ITask`\\n\\nAnd optionally:\\n\\n* `inputSchemaLocation` - file location of the input JSON schema, can be null\\n* `outputSchemaLocation` - file location of the output JSON schema, can be null\\n* `configSchemaLocation` - file location of the config JSON schema, can be null\\n\\nIf the schema locations are not specified, the message adapter will look for schemas in a schemas directory at the root level for the files: input.json, output.json, or config.json. If the schema is not specified or missing, schema validation will not be peformed.\\n\\n `RunCumulusTask` throws a `MessageAdapterException` when there is an error.\\n\\n## Example Cumulus task\\n\\nFor a full example see the [task folder](./task).\\n\\n## Creating a deployment package\\n\\nThe compiled task code, the message parser uber-jar, the cumulus message adapter zip, and any other dependencies should all be included in a zip file and uploaded to lambda. Information on the zip file folder structure is located [here](https://docs.aws.amazon.com/lambda/latest/dg/create-deployment-pkg-zip-java.html).\\n\\n## Usage in Cumulus Deployment\\n\\nFor documenation on how to utilize this package in a Cumulus Deployment, please view the [Cumulus Workflow Lambda Documentation](https://nasa.github.io/cumulus/docs/workflows/lambda#deploy-a-lambda) and the [Cumulus Workflow Input/Output Documentation](https://nasa.github.io/cumulus/docs/workflows/input_output).\\n\\n## Logging\\n\\nThe message adapter library contains a logging class `AdapterLogger` that standardizes the log format for Cumulus. Static functions are provided to log error, fatal, warning, debug, info, and trace.\\n\\nFor example, to log an error, call:\\n\\n```java\\nAdapterLogger.LogError(\"Error message\");\\n```\\n\\n## Development\\n\\n### Prerequisites\\n\\n* [Apache Maven](https://maven.apache.org/install.html)\\n\\n### Building\\n\\nTo build a new version of the CMA Java client library:\\n\\n```shell\\ncd message_parser\\nmvn -B package\\n```\\n\\n#### Using locally built package with task code\\n\\nThese instructions assume that your task is using Maven for dependency management.\\n\\n1. Get the current version number of the CMA Java client library from [`message_parser/pom.xml`](./message_parser/pom.xml):\\n\\n    ```xml\\n    <version>1.2.12</version>\\n    ```\\n\\n2. Make sure the `pom.xml` for your task includes a `dependency` referencing the correct version:\\n\\n    ```xml\\n    <dependency>\\n      <groupId>gov.nasa.earthdata</groupId>\\n      <artifactId>cumulus-message-adapter</artifactId>\\n      <version>1.2.12</version>\\n    </dependency>\\n    ```\\n\\n3. From the task project root, install the locally built package as a dependency and copy it into place (making sure that the version number is correct in `-Dfile` and `-Dversion`):\\n\\n    ```shell\\n    mvn install:install-file \\\\\\n      -Dfile=/path/to/cumulus-message-adapter-java/message_parser/target/cumulus-message-adapter-1.2.12.jar \\\\\\n      -DgroupId=gov.nasa.earthdata \\\\\\n      -DartifactId=cumulus-message-adapter \\\\\\n      -Dversion=1.2.12 \\\\\\n      -Dpackaging=jar \\\\\\n      -DgeneratePom=true\\n    mvn clean dependency:copy-dependencies\\n    ```\\n\\nRunning your task code (locally or when packaged and deployed) should now use the locally built CMA Java client package.\\n\\n### Integration Tests\\n\\nIntegration tests are located in the test folder in `MessageParserTest.java`. By default, the latest [Cumulus Message Adapter](https://github.com/cumulus-nasa/cumulus-message-adapter) will be downloaded and used for the tests.\\n\\nTo build and run the tests using the latest version of Cumulus Message Adapter, run:\\n\\n```shell\\ncd message_parser\\nmvn -B test\\n```\\n\\nTo build and run the tests using a different version of the Cumulus Message Adapter, run\\n\\n```shell\\nMESSAGE_ADAPTER_VERSION=vx.x.xx mvn -B test\\n```\\n\\n### Running the example task\\n\\nFollow the installation instructions above for the example task.\\n\\nIf updating the version of the message parser, make sure to update the `pom.xml` in the task code. To build the task with this dependency, run:\\n\\n```shell\\ncd task/\\nmvn clean install -U\\nmvn -B package\\n```\\n\\n## Benefits of the Cumulus Message Adapter\\n\\nThis approach has a few major advantages:\\n\\n1. It explicitly prevents tasks from making assumptions about data structures\\n   like `meta` and `cumulus_meta` that are owned internally and may therefore\\n   be broken in future updates. To gain access to fields in these structures,\\n   tasks must be passed the data explicitly in the workflow configuration.\\n2. It provides clearer ownership of the various data structures. Operators own\\n   `meta`. Cumulus owns `cumulus_meta`. Tasks define their own `config`,\\n   `input`, and `output` formats.\\n3. The Cumulus Message Adapter greatly simplifies running Lambda functions not\\n   explicitly created for Cumulus.\\n4. The approach greatly simplifies testing for tasks, as tasks don\\'t need to\\n   set up cumbersome structures to emulate the message protocol and can just\\n   test their business function.\\n'},\n",
       " {'repo': 'nasa/webgs',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# WebGS\\n\\nWebGS is a web-based ground control station that is compatible with [ICAROUS](https://github.com/nasa/icarous) (versions greater than 2.1.19) and capable of multi-aircraft simulations.\\n\\n<a href=\"screenshots/screenshot1_webgs.png\"><img src=\"screenshots/screenshot1_webgs_small.png\"><a href=\"screenshots/screenshot2_webgs.png\"><img src=\"screenshots/screenshot2_webgs_small.png\">\\n\\n<a href=\"screenshots/screenshot4_webgs.png\"><img src=\"screenshots/screenshot4_webgs_small.png\"><a href=\"screenshots/screenshot3_webgs.png\"><img src=\"screenshots/screenshot3_webgs_small.png\">\\n\\n## Installation\\n\\nClone the repository.\\nMake sure you have installed node.js and [npm](https://www.npmjs.com/get-npm).\\n[Python3](https://www.python.org/downloads/) is also required.\\n\\nThe install script will download the required python and node packages, update the submodules to the latest versions, and build daa-displays.\\nMove into the webgs directory and run:\\n\\n    ./install.sh\\n\\nIf updating WebGS and having trouble with submodules not loading. You may need to delete WebGS and start from scratch.\\n\\nWebgs is setup by default to connect to Open Street Maps. Webgs is also configured to use mapbox for the background display. To get an authorization key go to <https://www.mapbox.com/> and create an account. After receiving an authorization token open `/webgs/MainJS/settings/MapSettings.js` in a text editor, and follow the instructions to update.\\n\\n[DAA-Displays](https://github.com/nasa/daa-displays) is installed as a submodule in the applications folder `/DAA/daa-displays` by default. The installation script will build the daa-displays automatically. This behavior can be skipped by running `install.sh -D`. [DAA-Displays](https://github.com/nasa/daa-displays) can be launched from WebGS after connecting to an aircraft (no need to follow the instructions on <https://github.com/nasa/daa-displays>).  \\n\\nWINDOWS USERS: SITL is disabled since ICAROUS cannot be built and run on Windows. Use \\'Connect to Hardware\\' instead. DAA-Displays are also not able to be built on Windows. Run the install script with the `-D` option. Efforts have been made to make WebGS usable on Windows, but there may still be issues. Please notify the developers of any issues. We will happily fix what we can.  \\n\\n## Startup\\nBy default WebGS uses https. This assumes the proper ssl certs have been generated and loaded into the `/certs` directory. Webgs can also be run in developer mode which uses http and does not require certs. Detailed instructions on creating self signed certificates are located in `/certs.README.md`.\\n\\nTo start Webgs:\\n\\n    python3 start_webgs.py -HOST {name or localhost} -CERT {filename}.crt -KEY {filename}.key\\n\\nUpdate `-HOST`, `-CERT` and `-KEY` as needed, `-CERT` and `-KEY` default to `localhost.crt` and `localhost.key` respectively, so they are not needed if that is the names you chose.\\n\\n    python3 start_webgs.py -HOST {name or localhost}\\n\\nOr, start with http:\\n\\n    python3 start_webgs.py -DEV True\\n\\nThis script starts a local http server, starts the webgs socket server, and opens a web browser (chrome if it can be found otherwise the default browser) to `{hostname}:8082`.\\n\\nThere are potentially some compatibility issues with browsers other than Chrome and Firefox. These issues are mainly just styling. There may be some weird colors, or things may be slightly out of place.\\n\\n### To connect to the server from another device (only if on the same local network)\\n\\nIf the web server and socket server are on another device on your local network. The server is not public facing, and will not be seen by anyone outside of the local network. Enter the ip address of the machine running the web server into the browser address bar in format `- {https or http}://<hostname>:8082`\\n\\n### Connect WebGS over UDP\\n\\nAssuming [ICAROUS](https://github.com/nasa/icarous) is configured properly, ensure you are on the same network as the device running [ICAROUS](https://github.com/nasa/icarous). Typically this will involve changing the IP address of your machine. Start the web server and the socket server. Ensure the Web page is connected to the socket server. In the settings panel, set:\\n\\n    GCS Mode -> \\'Connect to Hardware\\'\\n    Select Input Type -> IP\\n    IP Address -> {the same IP address ICAROUS is configured to output to}\\n    Component Id -> 5 (Default is 5. This is the standard ICAROUS Config. 0 will connect to Autopilot in most configurations.)\\n\\nEnsure the Port and Baud Rate are correct. Press connect to aircraft.\\n\\n### Connect WebGS via Serial USB Device\\n\\nAssuming [ICAROUS](https://github.com/nasa/icarous) is configured properly, ensure you are on the same network as the device running [ICAROUS](https://github.com/nasa/icarous). Typically this will involve changing the IP address of your machine. Start the web server and the socket server. Ensure the Web page is connected to the socket server. In the settings panel, set:\\n\\n    GCS Mode -> \\'Connect to Hardware\\'\\n    Select Input Type -> USB\\n    IP Address -> {the same IP address ICAROUS is configured to output to}\\n    Component Id -> 5 (Default is 5. This is the standard ICAROUS Config. 0 will connect to Autopilot in most configurations.)\\n\\nEnsure the Port and Baud Rate are correct. Press connect to aircraft.\\n\\n### To run simulations\\n\\n[ICAROUS](https://github.com/nasa/icarous) must be installed and properly built. On the settings page ensure\\n\\n    GCS Mode is set to \\'SITL\\'\\n    Path to ICAROUS is set correctly\\n    Path to Ardupilot is set correctly (if needed)\\n    SIM TYPE -> ArduCopter (Spelling and Capitalization Matter)\\n\\nThen either right click on the map or click on the Aircraft button and select \\'New Aircraft\\'. The parameters for [ICAROUS](https://github.com/nasa/icarous) in version 2 are auto loaded and they may need to be changed. This can be done once the aircraft is started.\\n\\n### To view own-ship perspective flight instruments\\n\\nAfter the aircraft has started, click Open DAA Display. This will open the display in a new tab. Currently this display only works on port 8082. If the server was launched on another port the map will not be displayed.\\n\\n### Playback\\n\\nWebgs uses the MAVProxy format for creating `.tlog` files for each flight. These files along with the Server logs, [ICAROUS](https://github.com/nasa/icarous) outputs, ardupilot outputs, and a text file containing all of the received mavlink messages are stored in the LogFiles directory. To playback a file, change the GCS Mode to Playback. Enter the file name in the text box. (It assumes files will be located in the LogFiles directory.) Click Start Playback. It may take a few seconds to load the file.\\n\\nNote: I would not recommend fast forwarding at the beginning of the file. If you miss the flight plan messages, a flight plan will not show up on the map.\\n\\n### Merging .tlog files for multi-aircraft playback\\n\\nA Python3 script has been included for creating a `.mlog` file that webgs is capable of playing. It is located in webgs/utils/\\n\\n    python3 mergeTlogs.py -h or --help for instructions on how to use it.\\n\\n### Fly By File\\n\\nWebgs is capable of flying scripted scenarios that are repeatable and adjustable. Functionality is still limited but it has been tested with four simulated aircraft flying simultaneously, each with multiple intruders and a geofence, repeated 50 times, adjusting parameters, flight plans, and intruders after 25 flights. Examples and instructions on building a script are located in `/webgs/Examples/TestScripts`.\\n\\n## Current version\\n\\nWebGS v1.0.10\\n\\n## Notices\\n\\nCopyright 2019 United States Government as represented by the Administrator of the National Aeronautics\\nand Space Administration. All Rights Reserved.\\n\\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED\\xa0\"AS IS\" WITHOUT ANY WARRANTY OF ANY\\nKIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY\\nWARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED\\nWARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY\\nWARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE.\\nTHIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT\\nAGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE,\\nSOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT\\nSOFTWARE.\\xa0 FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\nREGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND\\nDISTRIBUTES IT\\xa0\"AS IS.\"\\n\\nWaiver and Indemnity:\\nRECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT.\\xa0 IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES,\\nDEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY\\nDAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT\\nSOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES\\nGOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT,\\nTO THE EXTENT PERMITTED BY LAW.\\xa0 RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL\\nBE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n### Contact\\n\\nAndrew Peters andrew.peters@nianet.org  \\nCesar Munoz cesar.a.munoz@nasa.gov\\n'},\n",
       " {'repo': 'nasa/NASAaccess',\n",
       "  'language': 'R',\n",
       "  'readme_contents': 'NASAaccess\\n=====================\\nIbrahim N. Mohammed (Ibrahim.mohammed@nasa.gov)\\n\\nWhat is NASAaccess?\\nNASAaccess is a software application in the form of a R package (https://www.r-project.org/about.html) and a web application.  NASAaccess software can generate gridded ascii tables of climate (CIMP5) and weather data (GPM, TRMM, GLDAS) needed to drive various hydrological models (e.g., SWAT, VIC, RHESSys, ...etc.).\\n\\nHow to cite the NASAaccess software?\\nMohammed, I. N., 2019, NASAaccess: Downloading and reformatting tool for NASA earth observation data products [software]. National Aeronautics and Space Administration, Goddard Space Flight Center, Greenbelt, Maryland. https://github.com/nasa/NASAaccess\\n\\nWhere to find the NASAaccess software?\\nThe NASAaccess R package is an open source software package under NASA Open Source Agreement v1.3 (https://opensource.org/licenses/NASA-1.3) and can be downloaded from Github at https://github.com/nasa/NASAaccess.\\n\\nNASAaccess is also available as a web-based application that can be installed on servers for multiple usage suited for agencies and centers.  Full details on installing the web-based version of NASAaccess at local servers can be found at https://github.com/BYU-Hydroinformatics/SWATOnline/blob/master/Documentation/nasaaccess%20-%20Installation%20Guide.pdf\\n\\nWhat is needed to run the NASAaccess software on my local machine?\\nOn a local machine the user should have installed the following programs as well as setting up a user account.  The list below gives a summary of what is needed to be done prior to work with NASAaccess software on any local machine:\\n\\t•\\tInstalling R software — https://cloud.r-project.org/\\n\\t•\\tInstalling Rstudio software — https://rstudio.com/ (OPTIONAL)\\n\\t•\\tNASAaccess R package needs a user registration access with Earthdata (https://earthdata.nasa.gov/).  Users should set up a registration account(s) with Earthdata login as well as well as authorizing NASA GESDISC data access.  Please refer to https://disc.gsfc.nasa.gov/data-access for further details.\\n\\t•\\tAfter registration with Earthdata the NASAaccess software package users should create a reference file (‘netrc’) with Earthdata credentials stored in it to streamline the retrieval access to NASA servers.\\n\\t\\t\\to\\tCreating the ‘.netrc’ file at the user machine \\'Home\\' directory and storing the user NASA GESDISC logging information in it is needed to execute the package commands is explained at https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+cURL+And+Wget\\n\\t\\t\\to\\tFor Windows users the NASA GESDISC logging information should be saved in a file ‘_netrc’ beside the ‘.netrc’ file explained above.\\n\\t•\\tInstalling ‘curl’ software (https://curl.haxx.se/).  Since Mac users have ‘curl’ as part of macOS build, Windows users should make sure that their local machines build have \\'curl\\' installed properly.\\n\\t•\\tChecking if you can run curl from your command prompt.  Type curl --help and you should see the help pages for the curl program once everything is defined correctly.\\n\\t•\\tWithin Rstudio or R terminal run the following commands to install NASAaccess:\\n\\t\\t\\to\\tlibrary(devtools)\\n\\t\\t\\to\\tinstall_github(\"nasa/NASAaccess\")\\n\\t\\t\\to\\tlibrary(NASAaccess)\\n\\nWithin the Rstudio help tab the user can verify that the package has been installed and browse the help pages of the various functions of NASAaccess.\\n'},\n",
       " {'repo': 'nasa/daidalus',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': '![](docs/DAIDALUS.jpeg \"\")\\n\\nDetect and AvoID Alerting Logic for Unmanned Systems\\n========\\n\\nDetect and Avoid Alerting Logic for Unmanned Systems (DAIDALUS) is a\\nsoftware library that implements a configurable detect and avoid (DAA)\\nconcept intended to support the integration of Unmanned Aircraft\\nSystems into civil airspace.  DAIDALUS serves as a reference\\nimplementation of the functional requirements specified in DO-365, the\\nMinimum Operational Performance Standards (MOPS) for Unmanned Aircraft\\nSystems (UAS) developed by RTCA Special Committee 228 (SC-228).\\n\\nThis repository includes a prototype\\nimplementations written in Java and C++.  The repository also includes\\nthe formal specification of DAIDALUS core algorithms witten the Prototype\\nVerification System ([PVS](http://pvs.csl.sri.com)).\\n\\nThe core functionalities implemented by DAIDALUS include\\n\\n*  detection logic,\\n*  alerting logic based on hazard and non-hazard volumes, and\\n*  instantaneous and kinematic maneuver guidance in the form of\\nsuggestive guidance (i.e., bands) and directive guidance (i.e.,\\npreferred direction and velocity vector).\\n\\n### Documentation\\n\\nA draft of user guide is available at https://nasa.github.io/daidalus. \\n\\n### Software Library\\nThe library can be compiled using the Unix utility `make` with the\\nprovided `Makefile` in both the [Java](Java/Makefile) and\\n[C++](C++/Makefile) directories. From the directory Java,\\nthe `make` command produces a jar file in the directory `Java/lib`.\\nFrom the directory C++, the `make` command will generate a static library\\nin `C++/lib`.\\n\\nThe sample application `DaidalusExample`, which is available in\\n[Java](Java/src/DaidalusExample.java) and\\n[C++](C++/src/DaidalusExample.cpp), illustrates the main\\nfunctionalities provided by DAIDALUS including reading/writing\\nconfiguration files, detection logic, alerting logic, maneuver\\nguidance logic, and computation of loss of well-clear contours.  This\\napplication can be compiled using the provided `Makefile`.\\nTo run the example application in a Unix environment, type from the\\ndirectory Java (or C++):\\n\\n```\\n$ ./DaidalusExample\\n```\\n\\nOther sample applications that illustrate DAIDALUS functionalities on\\nencounter files are provided in [Java](Java/README.md) and\\n[C++](C++/README.md).\\n\\n### More information about DAIDALUS\\nFor technical information about the definitions and algorithms in this\\nrepository, visit https://shemesh.larc.nasa.gov/fm/DAIDALUS.\\n\\n### Current Release\\n\\nv2.0.2, December-11-2020\\n\\n### License\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. \\n\\n### Contact\\n\\n[Cesar A. Munoz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.\\n\\n### Logo\\n\\nThe DAIDALUS logo was designed by \\n[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC), NASA Langley Research Center.\\n\\n### Copyright Notice\\n\\nCopyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/EMTG',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': 'Welcome to EMTGv9 Open Source Release\\nREADME.opensource file compiled by Jacob Englander 4-20-2020\\n\\nThis package contains all of the code that NASA GSFC is releasing for the Evolutionary Mission Trajectory Generator, version 9. No third-party code is included in this release. You will need to download the third-party components yourself. This information is detailed in documents that may be found in the docs/build_system folder.\\n\\nIn particular, you will need a license for SNOPT. This is not free. Many of you probably already have SNOPT, and the rest may have to purchase it. EMTGv9 is known to work with SNOPT 7.5 and 7.6. It probably also works with 7.7 but I can\\'t verify this because I don\\'t have a license for 7.7. Older versions *might* work.\\n\\nYou will want to download SPICE ephemeris files for the bodies that you are visiting. I recommend downloading the full 3+ GB set from http://naif.jpl.nasa.gov/naif/ and placing it in your Universe/ephemeris_files folder\\n\\nNote that regardless of whether you download all of the various SPICE ephemerides for the solar system, you will need to download de430.bsp from https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de430.bsp and place it in your Universe/ephemeris_files folder. I could not supply this file because it is too large for Github.\\n\\nYou must edit the EMTG-Config.cmake file as follows:\\n\\n1) Change the CSPICE_DIR to wherever you have placed your CSPICE package. CSPICE_DIR should point to the root CSPICE directory.\\n\\n2) Change the SNOPT_ROOT_DIR to be wherever you have placed the unzipped SNOPT folder.  From this directory, it will seek the traditional /lib and /interface folders (as appropriate to your SNOPT version, which should be auto detected).  Report any problems to the EMTG lead developers.\\n\\tExample: This is currently usually a directory called \"snopt7\"\\n\\n3) Change the \\'BOOST_ROOT\\' to the root of your boost installer.  This presupposes you have already built boost (run its bootstrapper)\\n\\t\\t3a) Optionally, if during the run phase of CMAKE boost is not being found, uncomment the next two lines and specifically specify the BOOST_INCLUDEDIR and BOOST_LIBRARYDIR.  The default options are traditional variants that are sometimes chosen by boost installers, but yours may vary.\\n\\nOnce all the above is in place, open CMake, and point the source directory at the EMTG root directory.  Point the build directory to the directory of your choice (may be the same). Run configure, then Generate, as per the usual CMake Process. This will create either  makefile for Linux or a Visual Studio project for Windows, or an Xcode project for Mac.\\n\\nEMTGv9 is provided \"as is\" in its imperfect but very capable state. The US Government, NASA, and the developers cannot guarantee that the results you produce with EMTG are correct. EMTG is intended for use as a trade study tool and an initial guess generator for a flight navigation tool. We recommend that you use it in those contexts and only those contexts.\\n'},\n",
       " {'repo': 'nasa/HDTN-BPCodec',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '## BPCodec ##\\n\\nBPCodec is a stand-alone library designed to support encoding and decoding the bundle protocol format.  Both version 6 and version 7 of the bundle protocol are supported.  The version 6 encode and decode functions have been tested with JPL\\'s ION [1] and GSFC\\'s bplib [2], [3]. The version 7 decode functions have been tested on bundles from uPCN [4]. The version 7 encode functions will be released in the near term. The version 7 implemention is in early stages (pre-alpha) but is being released for research and collaborative purposes in the overall DTN community.\\n\\nNote that the use of this library will produce results that are compatible with the Bundle Protocol from a wire perspective.  It does not, however, intend to offer a complete implementation of either version 6 or version 7 of the Bundle Protocol - semantics related to receipt, forwarding, and custody must be independently implemented and observed by a system that wishes to advertise compatibility with such.\\n\\n## Build Environment ##\\n* BPCodec build environment requires CMake version 3.13\\n* Testing has been done with gcc version 8.3.0 (Debian 8.3.0-6) \\n* Target: x86_64-linux-gnu \\n* Tested on Debian 10\\n* Known issues:\\n* Ubuntu distributions may install an older CMake version that is not compatible\\n* Mac OS may not support recvmmsg and sendmmsg functions, recvmsg and sendmsg could be used\\n\\n## Overview ##\\n\\nThe core of the library is found in bpv6.h and bpv7.h - each header file is responsible for the corresponding protocol version.  \\n\\nThe library is geared around two primary operations:\\n\\n* Taking a byte string and properly decoding it into a C structure, and\\n* Taking a C structure and properly encoding it into a byte string\\n\\nThe library expects some familiarity with the bundle protocol: for example, one should know that the primary block should come first, and that the primary block is followed by additional canonical blocks.  The library also does not presently enforce constraints (e.g. \"the payload block shall always be the last block in a bundle\").\\n\\nThe \"test\" directory includes a small collection of bundles used to validate that encoding / decoding work as expected.  These bundles are utilized by a small collection of benchmark utilities included with the library.  \\n\\nNote that this library does not yet support the generation of bundle security headers.\\n\\n## Applications ##\\nThe \"apps\" directory includes several tools built upon the basic bpcodec functions.\\n* The encode-benchmark simply tests the CBOR (BP v7) and SDNV (BP v6) functions on generated values.\\n* The bundle-benchmark reads bundles from files in the \"test\" directory. These bundles were generated from ION (JPL), bplib (GSFC) and uPCN in order to test the bundle decoding on a variety of implementations. The ION and bplib bundles are BP v6 and the uPCN bundle is BP v7. In the current release not all bundle extension blocks are supported, although this functionality will be added to the release in the near term.\\n* The bpgen tool will generate BP v6 bundles of a specified size as well as an approximate data rate. Source and destination node numbers can also be specified. Endpoint IDs currently follow the IPN style of addressing (node_number.service_number). This tool has been used with both ION and bplib in previous testing. The BP v7 functions have not been integrated into this tool yet, although this is planned in the near term. It will generate a csv file of statistics from the bundles sent. \\n* The bpsink tool will listen for and receive bundles. It will generate a csv file of statistics from the bundles received. \\n\\n## Quick Start ##\\n* git clone https://github.com/nasa/HDTN-BPCodec.git\\n* cd ./HDTN-BPCodec\\n* mkdir ./build\\n* cd ./build\\n* cmake ..\\n* make\\n\\nThe simplest test you can start with is to run bpgen with bpsink. Bpgen will generate bundles and send them out as UDP packets on 127.0.0.1 port 4556.\\n\\nIn one terminal:\\n* cd ./HDTN-BPCodec/build/apps\\n* ./bpgen    (there are command line parameters you can set as well but you don\\'t need to)\\n\\n\\nIn another terminal:\\n* cd ./HDTN-BPCodec/build/apps\\n* ./bpsink\\n\\n## Future Work ##\\nThere are several areas currently under development with functionality that will be released in the near term.\\n* BP v7 primary and canonical block encode functions are being developed.\\n* BP v6 extension blocks are in the process of being tested with current BP v6 implementations of ION and BPlib.\\n* CRC validation is planned to be added to the BP v7 functions.\\n\\n## References ##\\n1. https://sourceforge.net/projects/ion-dtn/\\n2. https://software.nasa.gov/software/GSC-18318-1\\n3. Hylton, A., Raible, D., Clark, G.A., Dudukovich, R., Tomko, B., Burke, L. (2019). Rising Above the Cloud- Toward High-Rate Delay-Tolerant Networking in Low- Earth Orbit.\\n4. https://upcn.eu/\\n'},\n",
       " {'repo': 'nasa/LHASA',\n",
       "  'language': 'R',\n",
       "  'readme_contents': \"# LHASA\\nLandslide Hazard Analysis for Situational Awareness\\n\\nLHASA was created at Goddard Space Flight Center to identify the potential for landslides in nearly real time. The model is currently running, and it's output can be seen at https://pmm.nasa.gov/precip-apps. It can also be accessed as an ArcGIS web map at https://landslides.nasa.gov/viewer. For more detail about the model, please see the published article at http://dx.doi.org/10.1002/2017ef000715.\\n\\nThe R scripts here are written to be easily understood, executed, and modified by potential users of this research.\\n\\nFull operational code is available in python at https://github.com/vightel/ojo-bot. \\n\"},\n",
       " {'repo': 'nasa/System_Monitor_for_Radiation_Testing',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# System Monitor for Radiation Testing (SMRT)\\n\\nSMRT is a Python®-based software package intended to monitor computer system performance during radiation testing.\\n\\n----\\n\\n**Table of Contents**\\n\\n[[_TOC_]]\\n\\n----\\n\\n# Purpose\\n\\nThis tool is intended to provide a comprehensive, cross-platform (Windows®/Linux®, Intel®/ARM®, etc.) test suite geared towards assessing computer performance as they go through radiation testing.  It\\'s hoped that this will be widely adopted by the aerospace avionics community, creating a common standard for radiation survivability of computers (especially commercial-off-the-shelf single board computers) and that the community will continually improve this tool.\\n\\n# Questions/Comments/Concerns?\\n\\nThe best way to get these things addressed is to open an issue on GitHub® and/or email the developer (samuel.m.pedrotty@nasa.gov).  Don\\'t hesitate to reach out-- it\\'s likely others also have your questions/comments/concerns.  We encourage users to add capability and fix bugs and then submit merge/pull requests to have their improvements brought back to the repository for all to benefit from.\\n\\n# Basic Use\\n\\n1. move the repository to the target computer\\n2. install Python 3\\n3. run the install script in the \"setup\" directory\\n4. run the start_tests.py script in the py_src directory\\n    * the test scripts will start assessing the system and logging data to a folder named with the start time in the \"data\" folder\\n5. stop the script or shutdown the SBC to terminate the software\\n\\n# Data Analysis\\n1. after running the tool, run the \"plot_results.py\" script in the \"py_src\" folder to load, concatenate, and plot the data.\\n    * in some cases, it may be best to transfer the data to a faster computer for analysis and plotting\\n\\n# Detailed Use\\n\\nIt\\'s assumed that all of the interfaces of the computer under test will be connected appropriately to allow their monitoring (e.g. displays, ethernet, USB devices, etc.).\\n\\n## Pre-test\\n\\n1. appropriately install the Operating System on the computer (this software should support Linux and Windows)\\n2. move the repository to the computer\\n3. install Python 3\\n4. run the install script in the \"setup\" directory\\n5. update the user input section of the \"start_tests.py\" script in the \"py_src\" directory\\n    * set the *ram_pct_to_use*, the RAM usage parameter with units of percents (< 95% is recommended to provide the OS some headroom)\\n    * set the *data_save_interval*, the  parameter that determines the time between data file saving with units of seconds (if a high particle flux is to be used, an interval of < 3 seconds is recommended)\\n    * set the *test_cycle_time*, the parameter that sets the run rate of the test scripts with units of seconds (0.1 is recommended.  Note that some tests will run slower than the rate as their functions take a set time that may be larger than some user-input cycle times)\\n6. configure the computer as it will be when under test (software and hardware)\\n7. run the \"start_tests.py\" script in the \"py_src\" directory for a couple minutes to verify functionality\\n    * it\\'s recommended to run the script as \\'sudo\\' on Linux and as administrator on Windows\\n8. transfer the data to a faster computer that also has Python 3 installed and run the \"plot_results.py\" script to verify the tool (and computer) functioned as intended\\n9. delete the pre-test data from the computer to save disk space, keep the set that was moved onto the faster computer as a pre-test artifact\\n\\n## Test use\\n\\n10. appropriately deploy the computer in the test environment, connecting all interfaces as briefly mentioned above\\n11. run the \"start_tests.py\" script in the \"py_src\" directory and wait until you see the terminal print stating RAM allocation is complete, and that the cpu, disk, and network monitoring functions are operating\\n12. start the radiation test (e.g., start the particle flow and other test support equipment)\\n13. stop the test and/or make note when any off nominal behavior is observed, including unresponsive interfaces, crashed software, or prints stating off-nominal conditions were detected (including RAM changes, disk disconnections, and network adapter disconnections)\\n    * noting the flux/fluence and failure type will provide insight into the MTBF of that computer subsystem and will inform fault recovery and mitigation design\\n14. when the test is complete, assuming the device is still functional, transfer the data off and save it as a test artifact\\n15. plot the data with \"plot_results.py\" script in the \"py_src\" directory and review the data for other indicators of faults to inform MTBF analysis\\n\\n\\n# Detailed Function Description\\n\\n## install_tool.py\\n\\nThis script automatically installs all of the necessary software requirements for Windows and Linux platforms after Python 3 is installed by the user.  The packages required for data plotting are optional and user-prompted as they can be memory-intensive for small, cheap, computers with limited RAM.\\n\\n## start_tests.py\\n\\nThis script starts all of the test scripts and provides a regular, 1 Hz print to the terminal to provide a \"heartbeat\" to indicate if the computer/interface is still functioning.  This script also contains the aforementioned user inputs.\\n\\n## test_ram.py\\n\\nThis script starts writing 1s to a struct until a user-defined amount of RAM is consumed.  Each cycle, the script will check the struct to make sure it\\'s consistent.  If it\\'s not, it will log an \\'upset\\' and will print *RAM STATE CHANGE DETECTED*.  The RAM will not be overwritten to reset it.  Sometimes restarting the script can reset upsets, but sometimes the whole device must be restarted.  At other times, especially after the unit under test has been subject to a considerable particle flux, upsets may trigger after restarting even when the particle beam is off.  The script also records the percentage of RAM used.\\n\\n## test_cpu.py\\n\\nThis script records the percentage of CPU used, current frequency in MHz, and CPU temperature in C (temperature is Linux-only).\\n\\n## test_disks.py\\n\\nThis script records the number of disks/drives detected and will print *NUMBER OF DISKS HAS CHANGED!* if a change in the number of drives is detected.  This script also records some information regarding the drives.\\n\\n## test_networks.py\\n\\nThis script records the number of network adapters detected and will print *NUMBER OF NETWORK ADAPTERS HAS CHANGED!* if a change in the number of adapters is detected.  This script also records some information regarding the adapters.\\n\\n## plot_results.py\\n\\nThis script concatenates and loads all data in the most recently created data folder.  It then plots the data and, depending on a user-set flag, can also save the plots and pngs.\\n\\n# Known Issues and Forward Work\\n1. The CPU temperature code only works on Linux operating systems.  Windows CPU temp values are hard-coded to 9999\\n2. The test_disks and test_networks scripts leave much to the user to parse in post-processing.  This should be intelligently handled by the tool.\\n3. The CPU frequency code only works on Linux operating systems.  Windows CPU frequency values are fixed to the system\\'s base clock speed.\\n\\n'},\n",
       " {'repo': 'nasa/NASA-3D-Resources',\n",
       "  'language': 'Mathematica',\n",
       "  'readme_contents': \"NASA-3D-Resources\\n=================\\n\\nWelcome to the 3D Resources github site. Here you'll find a growing collection of 3D models, textures, and images from inside NASA. All of these resources are free to download and use. \\n\\nPlease read the Usage Guidelines here: \\n\\nhttp://www.nasa.gov/audience/formedia/features/MP_Photo_Guidelines.html\\n\\nOur team's goal is to provide a one-stop shop for 3D models, images, textures, and visualizations. We offer these assets for your use, free and without copyright. We recognize that the site is only as good as its contributors and thank them kindly.\\n\\nWe welcome feedback and comments. Tell us how you're using our models, and let us know what you think: arc-special-proj@lists.nasa.gov\\n\\nWe acknowledge and thank the many authors who have contributed to the 3D Resources collections.\\n\\nNASA Jet Propulsion Laboratory - Solar System Simulator\\t\\n\\nMichael Carbajal. NASA Headquarters\\t\\n\\nNASA Ames Research Center\\t\\n\\nBrian Kumanchik, Christian Lopez. NASA/JPL-Caltech\\t\\n\\nNASA Johnson Space Center\\t\\n\\nDigitalSpace Corporation\\t\\n\\nEyes on the Solar System. NASA/JPL-Caltech\\t\\n\\nDoug Ellison / NASA-JPL\\t\\n\\nChristopher M. Garcia. NASA/JPL-Caltech\\t\\n\\nChristian A. Lopez. NASA/JPL-Caltech\\t\\n\\nKevin Lane. NASA/JPL-Caltech\\t\\n\\nNASA Jet Propulsion Laboratory\\t\\n\\nNASA Goddard Space Flight Center\\n\\t\\nNASA Johnson Space Center - Space Educators' Handbook\\t\\n\\nChris Meaney. NASA\\t\\n\\nBall Aerospace\\t\\n\\nDana Berry. NASA/Kepler Mission\\t\\n\\nNASA/JPL-Caltech\\t\\n\\nChristopher M. Garcia, Christian A. Lopez. NASA/JPL-Caltech\\t\\n\\nCarlo Conti\\t\\n\\n\\n\\n## Other Resources for NASA 3D Models\\n- https://nasa3d.arc.nasa.gov/\\n\"},\n",
       " {'repo': 'nasa/CTF',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Table of Contents\\n   * [cFS Test Framework](#cfs-test-framework)\\n      * [Getting Started](#getting-started)\\n         * [CTF Prerequisites](#ctf-prerequisites)\\n         * [CTF Directory Structure](#ctf-directory-structure)\\n         * [Configuration](#configuration)\\n         * [Sample cFS Workspace](#sample-cfs-workspace)\\n            * [Running the Sample cFS Workspace Scripts](#running-the-sample-cfs-workspace-scripts)\\n      * [Using CTF on a New cFS Project](#using-ctf-on-a-new-cfs-project)\\n         * [CCSDS Message Definitions](#ccsds-message-definitions)\\n            * [CCDD Auto Export](#ccdd-auto-export)\\n            * [Other](#other)\\n         * [New cFS Project Configuration](#new-cfs-project-configuration)\\n         * [Developing New Test Scripts](#developing-new-test-scripts)\\n      * [Developing or Extending CTF Plugins](#developing-or-extending-ctf-plugins)\\n      * [Updating CTF](#updating-ctf)\\n      * [Release Notes](#release-notes)\\n      * [License](#license)\\n\\n# cFS Test Framework\\n\\nThe cFS Test Framework (CTF) provides cFS projects with the capability to develop and run automated test and verification scripts. The CTF tool parses and executes JSON-based test scripts containing test instructions, while logging and reporting the results. CTF utilizes a plugin-based architecture to allow developers to extend CTF with new test instructions, external interfaces, and custom functionality.\\n\\nCTF is currently in active development, and is available under the NASA Open Source Agreement license (NOSA). See the [license](#license) section for more information.\\n\\n## Getting Started\\n\\nTo get started, clone the CTF repository using the following:\\n\\n`git clone https://github.com/nasa/CTF`\\n\\n### CTF Prerequisites\\n\\nCTF has been developed and tested on Linux (CentOS 7) and requires Python 3.x. The CTF Editor requires an installation of NodeJS/NPM.\\n\\nThere are two methods to install dependencies. \\n\\n1. An Anaconda environment setup script is provided to install all OS/Python dependencies into a self contained Anaconda environment. This method does not require sudo privileges. However, the CTF Python 3 environment must be activated prior to running CTF.\\n\\n2. a PIP `requirements.txt` file is provided to install all Python 3 CTF dependencies. OS dependencies need to be installed manually and may require sudo privileges. This method provides the most light-weight dependency installation, but is more involved than the Anaconda setup.\\n\\n#### CTF Prerequisites: Satisfied with Anaconda Environment \\n\\nThe `setup_ctf_env.sh` script will setup an Anaconda 3 environment, which contains Python3, along with the python components identified in `requirements.txt` along with NodeJS/NPM.\\n\\nTo set up the CTF environment execute `source setup_ctf_env.sh`. Note: this may take several minutes depending on your network connection.\\n\\nAfter the initial setup to activate the ctf environment execute `source activate_ctf_env.sh`.\\n\\nIf the Anaconda environment is corrupted, the environment can be reinstalled by executing `source setup_ctf_env.sh -u`.\\n\\n#### CTF Prerequisites: Satisfied Without Anaconda Environment\\n\\nInstall Python3 (on CentOS run `sudo yum install python3-devel python3 python3-pip`)\\n\\nInstall NodeJS/NPM, visit https://nodejs.org/en/ and install Node version >= 10.12.0 (tested with v12.5.0). NPM will be included in the installation.\\n\\nWith Python 3 installed, PIP dependencies can be installed using `pip install -r requirements.txt`. Note: ensure that dependencies are installed to a PIP venv in order to easily update/reinstall packages, or install using  `--user` in the above pip command.\\n\\n#### CTF Prerequisites: Other applications needed\\n\\n`xterm` is required in order to run cFS in a separate terminal. Install `xterm` by using `sudo yum install xterm` or `sudo apt install xterm` if on Debian.\\n\\n`cmake3` is required in order to build cFS projects. Install cmake3 using `sudo yum install cmake3` if needed.\\n\\nA working cFS project is also needed. Note that CTF provides the [sample_cfs_workspace](#sample-cfs-workspace) if no existing cFS project is available.\\n\\n### CTF Directory Structure\\n```\\n├── activate_ctf_env.sh\\n├── ctf (executable)\\n├── README.md\\n├── requirements.txt\\n├── run_editor.sh\\n├── setup_ctf_env.sh\\n├── configs/\\n├── external/\\n├── lib/\\n├── plugins/\\n├── schemas/\\n└── tools/\\n    └── ctf_ui/\\n    └── schema_validator/\\n```\\n\\n### Configuration\\n\\nThe configuration file contains configuration options for each CTF plugin. To run CTF with a selected configuration, add the `--config_file <path_to_config_file>` command line argument when running CTF. Each plugin can define one (or more) sections with specific configuration values as follows\\n\\n```\\n[some_plugin_config]\\nsome_field = true\\nsome_other_field = xyz\\n```\\n\\nPlease refer to each plugin\\'s `README` document for information on the required configuration fields for each plugin. In addition, example configuration files are provided under `configs/examples`.\\n\\n###### Note: More variants of the config can be created, and loaded via the ctf command-line argument (`--config_file`). It is recommended to create different configurations for different testing use-cases such as a CI config, a Test Authoring/Debugging config, etc..\\n\\n###### Note: Environment variables can be used as a value using similar syntax as follows `${env_variable}`\\n\\n###### Note: Other INI field values can be referenced using the following syntax `field2 = ${section:field1}`. \\n\\n### Sample cFS Workspace\\n\\nBy default, The CTF tool contains a minimal cFS workspace (`external/sample_cfs_workspace.tgz`) for CTF testing and evaluation purposes. It also provides a reference cFS/CTF workspace layout that is applicable to other cFS projects. \\n\\nThe `sample_cfs_workspace` contains a set of cFS apps that receive CCSDS commands and output CCSDS telemetry messages through the CI/TO apps. It also contains a set of sample CTF test scripts that can be executed against the compiled cFS system from `sample_cfs_workspace`.\\n\\nTo get started using the `sample_cfs_workspace`, follow the documentation under [external/README.md](external/README.md). After completing the instructions, CTF will be set up as a cFS tool *within* the `sample_cfs_workspace` and can be used to execute test scripts against that cFS workspace. A similar approach can be followed to add CTF to an existing cFS project.\\n\\n###### Note: If cFS does not build, you may have missing dependencies. Install of `gcc-multilib` in order to build cFS projects successfully. On CentOS 7, run the following command `sudo yum install libgcc.i686 glibc-devel.i686`\\n\\n#### Running the Sample cFS Workspace Scripts\\nFirst, review the contents of `configs/default_config.ini`. Update any fields as needed. Specifically, the `[cfs]: workspace_dir` may need to be update with the appropriate path according to where `sample_cfs_workspace` is extracted.\\n\\nTo run the provided example scripts, simply run the `ctf` executable (ensure the CTF environment is activated) with the configuration file and the script to run as follows.\\n\\n```\\ncd ~/sample_cfs_workspace/ctf_tests\\nsource activate_ctf_env.sh\\nctf --config_file configs/default_config.ini --script_dir test_scripts/sample_test_suite\\n```\\n\\n###### Note: Ensure `build_cfs` is set to `true` in the `configs/default_config.ini` file, if the cFS executable has not been built manually.\\n\\nCTF will proceed to build and start the cFS project and execute the test script. An `xterm` window showing the output of the cFS instance will launch, after which the test script begins execution.\\n\\n###### Note: run `sudo sysctl -w fs.mqueue.msg_max=1024` if you see an error from cFS regarding the msg_max queue size.\\n\\nSample CTF output is shown below.\\n\\n```\\n[13:00:47.209] ctf                             (42 ) *** INFO: Status_Manager created\\n[13:00:47.209] ctf                             (48 ) *** INFO: Loading Plugins - Start\\n[13:00:47.209] plugin_manager                  (202) *** INFO: Looking for plugins under package: plugins\\n[13:00:47.214] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.ccsds_plugin.ccsds_plugin.CCSDSPlugin\\n[13:00:47.214] ccsds_plugin                    (35 ) *** WARNING: CFS Plugin not yet loaded... \\n[13:00:47.345] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.cfs.cfs_plugin.CfsPlugin\\n[13:00:47.346] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.example_plugin.example_plugin.ExamplePlugin\\n[13:00:47.348] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.sp0_plugin.sp0_plugin.sp0Plugin\\n[13:00:47.348] plugin_manager                  (235) *** INFO:     Found plugin class: plugins.ssh.ssh_plugin.SshPlugin\\n[13:00:47.349] plugin_manager                  (211) *** INFO: From package: plugins  - Loaded the following plugins: [\\'CCSDS Plugin\\', \\'CFS Plugin\\', \\'ExamplePlugin\\', \\'sp0_plugin\\', \\'SshPlugin\\']\\n[13:00:47.349] ctf                             (50 ) *** INFO: Loading Plugins - End\\n[13:00:47.349] ctf                             (58 ) *** INFO: Status_Manager created\\n[13:00:47.349] ctf                             (62 ) *** INFO: Script_Manager created\\n[13:00:47.349] ctf                             (64 ) *** INFO: Reading Test Scripts\\n[13:00:47.350] ctf                             (95 ) *** INFO: Loaded Script: <lib.test_script.TestScript object at 0x7f9d731e7bb0>\\n[13:00:47.350] ctf                             (108) *** INFO: Completed reading in the json script file/files passed in the arguments\\n[13:00:47.350] ctf                             (111) *** INFO: Script Manager - Start\\n[13:00:47.350] ccsds_plugin                    (38 ) *** INFO: Initialized CCSDS Plugin\\n[13:00:47.350] cfs_time_manager                (45 ) *** INFO: CfsTimeManager Initialized. Verification Poll Period = 0.05.\\n[13:00:47.350] cfs_plugin                      (152) *** INFO: Initialized CfsPlugin\\n[13:00:47.350] sp0_plugin                      (64 ) *** INFO: Initialized SP0 plugin\\n[13:00:47.351] test_script                     (92 ) *** INFO: Verification Test Name: CFS CI Functions Test\\n[13:00:47.351] test_script                     (93 ) *** INFO: Verification Test Number: CFS-CI-Functions-Test\\n[13:00:47.351] test_script                     (94 ) *** INFO: Test Conductor: ashehata\\n[13:00:47.351] test_script                     (95 ) *** INFO: Run Date/Time: 10/08/2020 / 13:00:47\\n[13:00:47.351] test_script                     (96 ) *** INFO: Platform: #1 SMP Tue Mar 31 23:36:51 UTC 2020\\n[13:00:47.351] test_script                     (97 ) *** INFO: Requirement Verification Targets: MyRequirement\\n[13:00:47.351] test_script                     (98 ) *** INFO: Test Description : Testing CI Functions\\n[13:00:47.351] test_script                     (99 ) *** INFO: Input file utilized : CiFunctionTests.json\\n[13:00:47.352] test                            (199) *** INFO: Test CI-Function-Test-001: Starting\\n[13:00:47.352] test                            (200) *** INFO: Test CI Enable TO command\\n[13:00:47.352] test                            (157) *** INFO: Waiting 1 time-units before executing RegisterCfs\\n[13:00:48.374] cfs_plugin                      (161) *** INFO: RegisterCfs: Name cfs_LX1\\n[13:00:48.379] cfs_config                      (93 ) *** WARNING: Config Value cfs_LX1:evs_messages_clear_after_time does not exist or is not the right type. Attempting to load from base section [cfs].\\n[13:00:48.379] cfs_controllers                 (56 ) *** INFO: Creating MID Map from CCDD Data at /home/ashehata/sample_cfs_workspace/ccdd/json\\n[13:00:48.396] ccsds_packet_interface          (69 ) *** INFO: Importing CCSDS header definitions from /home/ashehata/sample_cfs_workspace/tools/ctf/plugins/ccsds_plugin/cfe/ccsds_v2/ccsds_v2.py\\n[13:00:48.397] cfs_controllers                 (71 ) *** INFO: Starting Local CFS Interface\\n[13:00:48.397] cfs_controllers                 (85 ) *** WARNING: Not starting CFS executable... Expecting \"StartCfs\" in test script...\\n[13:00:48.397] cfs_controllers                 (88 ) *** INFO: CfsController Initialized\\n[13:00:48.397] cfs_plugin                      (197) *** INFO: Register for cfs_LX1 finished.\\n[13:00:48.397] test                            (74 ) *** TEST_PASS: Instruction RegisterCfs: {\\'name\\': \\'cfs_LX1\\'}\\n[13:00:48.397] test                            (157) *** INFO: Waiting 1 time-units before executing StartCfs\\n[13:00:49.412] cfs_plugin                      (254) *** INFO: StartCfs: Name cfs_LX1\\n[13:00:49.412] cfs_controllers                 (96 ) *** INFO: Starting CFS on cfs_LX1\\n[13:00:49.413] local_cfs_interface             (100) *** INFO: Starting CFS Executable\\nxterm: cannot load font \\'-misc-fixed-medium-r-semicondensed--13-120-75-75-c-60-iso10646-1\\'\\n[13:00:52.518] cfs_controllers                 (105) *** INFO: Skipping enable output...\\n[13:00:52.519] test                            (74 ) *** TEST_PASS: Instruction StartCfs: {\\'name\\': \\'cfs_LX1\\'}\\n[13:00:52.519] test                            (157) *** INFO: Waiting 1 time-units before executing SendCfsCommand\\n[13:00:53.584] cfs_controllers                 (123) *** INFO: Sending CFS Command to target: cfs_LX1, CI_CMD_MID:CI_ENABLE_TO_CC with Args: {\"cDestIp\": \"127.0.0.1\", \"usDestPort\": 5011, \"usRouteMask\": 0, \"iFileDesc\": 0}\\n[13:00:53.585] test                            (74 ) *** TEST_PASS: Instruction SendCfsCommand: {\\'name\\': \\'\\', \\'mid\\': \\'CI_CMD_MID\\', \\'cc\\': \\'CI_ENABLE_TO_CC\\', \\'args\\': {\\'cDestIp\\': \\'127.0.0.1\\', \\'usDestPort\\': 5011, \\'usRouteMask\\': 0, \\'iFileDesc\\': 0}}\\n[13:00:53.585] test                            (157) *** INFO: Waiting 0 time-units before executing CheckTlmValue\\n[13:00:53.585] test                            (96 ) *** INFO: Waiting up to 2.0 time-units for verification of CheckTlmValue: {\\'name\\': \\'\\', \\'mid\\': \\'CI_HK_TLM_MID\\', \\'args\\': [{\\'variable\\': \\'usCmdCnt\\', \\'value\\': [1], \\'compare\\': \\'==\\'}, {\\'variable\\': \\'usCmdErrCnt\\', \\'value\\': [0], \\'compare\\': \\'==\\'}]}\\n[13:00:53.585] cfs_plugin                      (289) *** INFO: CheckTlmValue: CFS Target: , MID CI_HK_TLM_MID, Args [{\"variable\": \"usCmdCnt\", \"value\": [1], \"compare\": \"==\"}, {\"variable\": \"usCmdErrCnt\", \"value\": [0], \"compare\": \"==\"}]\\n[13:00:53.636] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_SB_HousekeepingTlm_t with MID: 0x2008\\n[13:00:53.636] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_EVS_LongEventTlm_t with MID: 0x2006\\n[13:00:53.842] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_TIME_HousekeepingTlm_t with MID: 0x200e\\n[13:00:54.047] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_TBL_HousekeepingTlm_t with MID: 0x200c\\n[13:00:54.047] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CFE_ES_HousekeepingTlm_t with MID: 0x2001\\n[13:00:54.253] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: SCH_HkPacket_t with MID: 0x240b\\n[13:00:54.357] cfs_interface                   (225) *** INFO: Receiving Telemetry Packets for Data Type: CI_HkTlm_t with MID: 0x2404\\n[13:00:54.408] cfs_interface                   (503) *** INFO: PASSED Intermediate Check - usCmdCnt: Actual: 1, Expected: 1, Comparison: ==, Tol: +0, -0\\n[13:00:54.408] cfs_interface                   (503) *** INFO: PASSED Intermediate Check - usCmdErrCnt: Actual: 0, Expected: 0, Comparison: ==, Tol: +0, -0\\n[13:00:54.408] cfs_controllers                 (319) *** INFO: PASSED Final Check for MID:{\\'MID\\': 9220, \\'name\\': \\'CI_HkTlm_t\\', \\'PARAM_CLASS\\': <class \\'plugins.cfs.ccsds.ccdd_export_reader.CI_HkTlm_t\\'>}, Args:[{\\'variable\\': \\'usCmdCnt\\', \\'value\\': [1], \\'compare\\': \\'==\\'}, {\\'variable\\': \\'usCmdErrCnt\\', \\'value\\': [0], \\'compare\\': \\'==\\'}]\\n[13:00:54.408] test                            (134) *** TEST_PASS: Verification Passed CheckTlmValue: {\\'name\\': \\'\\', \\'mid\\': \\'CI_HK_TLM_MID\\', \\'args\\': [{\\'variable\\': \\'usCmdCnt\\', \\'value\\': [1], \\'compare\\': \\'==\\'}, {\\'variable\\': \\'usCmdErrCnt\\', \\'value\\': [0], \\'compare\\': \\'==\\'}]}\\n```\\n\\n## Using CTF on a New cFS Project\\n\\n### CCSDS Message Definitions\\n\\nTo use CTF for a new cFS project, message definition files must first be generated. The message information files allow CTF to create the command/telemetry C structures in python in order to build CCSDS commands and process CCSDS telemetry.\\n\\n#### CCDD Auto Export\\n\\nCurrently, CTF obtains the CCSDS message definitions by parsing a set of files that adhere to a CTF-supported JSON schema to define command/telemetry message structures. These files can be created manually for testing, or created automatically using a tool such as CCDD.\\n\\nFor the `sample_cfs_workspace` project, the CCSDS message definition JSONs are automatically generated by the [CCDD](https://github.com/nasa/CCDD) tool.\\n\\nCTF utilizes a CCDD export reader that parses and stores the message definitions JSON into a format usable by CTF.\\n\\n###### Note: Ensure that the CCSDS JSON files directory is correctly set in the INI configuration file under `[cfs]: CCSDS_data_dir` in order for CTF to correctly process these files.\\n\\n#### Other\\n\\nProjects that do not use CCDD to maintain their command and telemetry dictionary can utilize the same current JSON schema to define message information. This can be done automatically (generated from some external tool), or manually. In addition, more complex projects may implement their own `Custom CCSDS Reader` using the provided CCSDS Interface calls to set up the message information in CTF. An example of that is shown under `plugins/cfs/ccsds/ccdd_export_reader.py`.\\n\\nEssentially, each reader will need to read the message information files from some source, and relay that information to CTF using the following functions\\n\\n```python\\nadd_telem_msg(self, mid_name, mid, name, parameters, parameter_enums = None)\\n\\nadd_cmd_msg(self, mid_name, mid, command_code_map, command_enums = None)\\n\\nadd_enumeration(self, key, value)\\n```\\n\\nExample usage can be found in the `plugins/cfs/ccsds/ccsds_export_reader.py`\\n\\n### New cFS Project Configuration\\n\\nTo configure CTF for a new project, simply create a configuration INI file that is specific to the project\\'s testing needs. `config_template.ini` is provided under the `configs/` directory as a template for new projects to use.\\n\\nIt is recommended to start with simple tests that start/stop cFS to verify that the build and run capability is working. Once that is achieved, more complicated test scripts can be created.\\n\\n### Developing New Test Scripts\\n\\nThe CTF tool provides a [CTF Editor](tools/ctf_ui) to assist in the creation, modification, and running of scripts. Currently, the editor can be obtained from the repository above. Please refer to the [CTF Editor User\\'s Guide](docs/ctf_editor/usage_guide.md) for information on how to run and use the editor.\\n\\n\\nRefer to the following [guide](docs/ctf/ctf_json_test_script_guide.md) for information on the JSON input script file description. This is useful when scripts are to be written manually without the aid of the editor.\\n\\nPlugin READMEs include documentation of their own instructions and configuration fields:\\n\\n* [CCSDS Plugin](plugins/ccsds_plugin/README.md)\\n* [CFS Plugin](plugins/cfs/README.md)\\n* [Example Plugin](plugins/example_plugin/README.md)\\n* [SSH Plugin](plugins/ssh/README.md)\\n\\n## Developing or Extending CTF Plugins\\n\\nRefer to the [plugin guide](docs/ctf/ctf_plugin_guide.md) for information on creating custom CTF plugins.\\n\\n## Updating CTF\\n\\nCTF versions can be checked out by running `git checkout <version_tag>`. This updates the CTF submodule/repository to the selected version.\\n\\nIn addition, the configuration (and test script) files may need to be updated with new configuration fields, or test script additions/changes.\\n\\nThe sections below describe the changes needed to quickly update to a specific version. Note that the release notes contain a more complete set of changes.\\n\\n### Updating to v1.X\\n\\nCTF v1.0 introduces major additions to the configuration file, as well as changes to the test script schema and instructions.\\n\\n* Existing CTF Test Scripts\\n    * Using a find & replace tool, update all references within a test script as follows\\n        * All Instances\\n            * `\"commands\"` -> `\"instructions\"`\\n            * `\"command`   -> `\"instruction\"`\\n        * CFS Plugin Instructions\\n            * `\"name\"`     -> `\"target\"` \\n            * `\"set_length\"` to `\"payload_length\"`\\n            \\n* Config changes (reference `configs/default_config.ini` for descriptions and examples)\\n    * Add fields\\n        * core:additional_plugin_path\\n        * core:ignored_instructions\\n        * core:delay_between_scripts\\n        * cfs:CCSDS_target\\n        * cfs:evs_event_mid_name\\n    * Delete fields\\n        * cfs:evs_event_msg_mid\\n\\n* CCSDS Message Definitions\\n    * **Update to CCSDS Definition Schema (JSON) to support multiple MID values for the same message definitions**\\n        * For command structure definitions\\n            * Rename `command_id_name` to `cmd_mid_name`\\n            * Remove `command_message_id`. Now maintained in a separate MID map file\\n            * Add `cmd_description`\\n            * Rename `command_codes` to `cmd_codes`\\n                * Rename `name` to `cc_name`\\n                * Rename `code` to `cc_value`\\n                * Rename `description` to `cc_description`\\n                * Add `cc_data_type`\\n                * Rename `args` to `cc_parameters`\\n                    * No changes needed within `cc_parameters` (previously `args`)\\n        \\n        * For telemetry structure definitions\\n            * Rename `name` to `tlm_mid_name`\\n            * Remove `mid`. Now maintained in a separate MID map file\\n            * Rename `mid_name` to `tlm_data_type`\\n            * Add `tlm_description`\\n            * Rename `parameters` to `tlm_parameters`\\n                * No changes to `name`, `array_size`, `description`, `bit_length`, `parameters`\\n\\n        * For alias and constant definitions\\n            * Rename `cfs_type_name` to `alias_name`\\n            * Rename `c_type` to `actual_name`\\n            * Rename `cfs_macro_name` to `constant_name`\\n            * Rename `c_macro` to `constant_value`\\n\\n        * MID Map JSON\\n            * Move the \"mid\" field from each message definition to a separate MIDs map file, mapping between MID names and raw MID values per cFS target.\\n            \\n            * An array of objects mapping target to MID value as follows\\n                ```\\n                [\\n                    {\\n                        \"target\": \"my_target\",\\n                        \"mids\": [ \\n                                    {\"mid_name\": \"CFS_MID_NAME\", \"mid_value\": \"0x1234\"}\\n                                    ...\\n                                ]\\n                    }\\n                ]\\n                ```\\n    * Refer to the `sample_cfs_workspace/ccdd/json` for reference (after extracting the workspace from `external/`)\\n\\n## Release Notes\\n### v1.1\\n12/18/2020\\n\\n* CTF Core Changes\\n    * Add support for a `disable` field to CTF instructions within a test script to temporarily disable that instruction.\\n    \\n    * Add support for a `description` field to CTF instructions within a test script to capture comments.\\n    \\n    * Add `end_test_on_fail_commands` attribute to the `Plugin` class, allowing plugins to define a list of critical commands that end the current test script on failure.\\n    \\n    * Minor improvements and bug fixes.\\n\\n* CFS Plugin Changes\\n    * Add the `RegisterCfs` and `StartCfs` instructions to the `end_test_on_fail_commands` such that test execution is halted if registering or starting cFS fails.\\n    \\n    * Add support for Short Event Messages by setting the `cfs:evs_short_event_mid_name` field in the config to match the MID name of the `CFE_EVS_ShortEventTlm_t` within the CCSDS JSON definitions.\\n    \\n    * Add the CheckNoEvent instruction to check that an event message was *not* sent during the verification timeout.\\n    \\n    * Ensure args do not get malformed while sending a command to multiple cFS targets\\n    \\n    * Resolve CheckTlmValue passing if one or more variables fails to be evaluated from the telemetry packet.\\n    \\n    * Resolve connection and deployment issues with cFS targets running with the SP0 protocol (WIP)\\n    \\n    * Minor improvements and bug fixes.\\n\\n    \\n* CTF Editor Changes\\n    * Add support for disabling/enabling test instructions or test cases\\n    \\n    * Add support for viewing/editing descriptions (comments) within the test script\\n    \\n    * Add the ability to rename folders or test scripts within the editor\\n    \\n    * Minor improvements and bug fixes.\\n     \\n### v1.0\\n10/30/2020\\n\\nMajor Release\\n\\nThis is the first open source release for CTF. Note that CTF remains in active development.\\n\\nExisting users should review the change logs below and ensure current configurations and scripts are updated.\\n\\n* Release Additions\\n    * Add `external/` directory with a self contained example CFS project. This project can be configured to utilize CTF out of the box. Refer to `external/README.md` for more information.\\n\\n* CTF Core Changes\\n    \\n    * **Update test script JSON schema with more accurate field names**\\n        * Update `commands` array to `instructions`\\n        * Update `command` object to `instruction`\\n        * Refer to [Updating to v1.0](updating-to-v10) \\n        \\n    * Add an option to provide an additional plugin path within the configuration file. Custom plugins are loaded from that directory, in addition to CTF default plugins.\\n        * Add the `additional_plugin_path = <path_to_custom_plugins_dir>` field to the config ini.\\n        * Note - Give the custom plugin directory a unique name (do not use `plugins`, `lib`, etc...), so as to not shadow any modules within the CTF repo.\\n    \\n    * Add an option to ignore specific CTF instructions within the configuration file. This is useful for CI or specific-configurations that may not have the ability to run certain instructions.\\n        * Add the `ignored_instructions = <instruction_1>, <instruction_2>, ...` field to the config ini. Specify the instructions to ignore (comma-seperated).\\n    \\n    * Minor improvements and bug-fixes.\\n        \\n* CCSDS Plugin Changes\\n    * **Update to CCSDS Definition Schema (JSON) to support multiple MID values for the same message definitions**\\n        * Refer to [Updating to v1.0](updating-to-v10) \\n        \\n    * **Change config field `evs_event_msg_mid` to `evs_event_mid_name` which should now be set to the MID name for the Event Messages (for example `CFE_EVS_LONG_EVENT_MSG_MID`).** \\n        * The actual MID value will be retrieved from the CCSDS message definitions.\\n\\n* CFS Plugin Changes\\n    * **Update `name` field to `target`**\\n        * The `name` field is used for *all* cFS Plugin Instructions. This field is now renamed to `target` for clarity, and specifies the cFS target to apply the instruction to.\\n            * **Please change all instances of `\"name\":` to `\"target\"` within existing test scripts. Example scripts have been updated as part of the release.**\\n    \\n    * No longer validate `cfs_run_dir` on registration. Previously, if the cFS instance was not built (i.e the run directory does not exist), validation would fail.\\n    * Rename `set_length` to `payload_length` for the `SendInvalidLengthCfsCommand` test instruction.\\n    \\n* User IO Plugin (New)\\n    * Add support for `WaitForUserInput` test instruction (with a prompt message shown to the user). This is useful when testing safety-critical functionality, or need to pause the test until the user confirms to proceed.\\n\\n* Example Plugin Changes\\n    * Add an example of loading and utilizing a C shared library within a CTF plugin.\\n\\n* CTF Editor Changes\\n    * Improvements to the Run Status View\\n        * Instructions can now be expanded while/after a test run to inspect instruction arguments and data.\\n    \\n    * Updates to support CTF and CCSDS definition changes.\\n\\n### v0.6\\n2020-09-14\\n\\n* CCSDS Plugin Changes\\n    * Configurable CCSDS Headers\\n        * Allows users to define CCSDS header structures for Command and Telemetry packets\\n            * Reference CCSDS Plugin README for documentation on creating custom headers and their associated implementations\\n        * Migrate CCSDS V1 and V2 definitions to utilize new configurable header functionality\\n        * Added a new config field `CCSDS_header_path` to set the appropriate python header module for CTF to use\\n\\n* cFS Plugin Changes\\n    * Allow \"RegisterCfs\" instruction to be used without specifying a name. This uses the default `[cfs]` configuration section. \\n    * Add new field to \"StartCfs\" test instruction to support additional command-line arguments when starting the cFS instance.\\n        * Example\\n            ```json\\n                {\\n                    \"command\": \"StartCfs\",\\n                    \"data\": {\\n                        \"name\": \"\",\\n                        \"run_args\": \"-R PO\"\\n                    },\\n                    \"wait\": 1\\n                }\\n            ```\\n        * Note: The StartCfs run_args value will append arguments to the `cfs_run_args` field defined in the config\\n    * Fix \"SendInvalidLengthCfsCommand\" instruction to send the actual length specified in the instruction as the payload length\\n        * Note: Total length received by FSW will be equal to the specified `payload_length` + length of the CCSDS header(s)\\n\\n    * Fix command/telemetry sockets not sending/receiving packets when cFS restarts within a single script\\n        * CTF to re-initialize command/telemetry sockets after restart\\n        \\n    * Write cFS output log files in append mode such that logs are not overwritten when cFS restarts\\n    \\n* CTF Editor Changes\\n\\n    * Fix rendering bug causing last script to be hidden in the file-pane behind sidebar collapse button\\n\\n### v0.5.1\\n\\nDisable implicit padding of telemetry and command payload structures (assumes explicit padding in the C data structures, or disabled C compiler implicit padding).\\n\\nMinor improvements and bug-fixes.\\n\\n### v0.5\\n2020-08-06\\n\\nMajor backend updates to improve reliability/maintainability of CTF.\\n\\n* cFS Plugin Changes\\n    * Major architecture rework to support multiple and remote cfs instances\\n    * cFS Plugin\\n        * Telemetry Reader\\n            * Add time-tags to telemetry packets for more accurate telemetry verifications.\\n            * Log begin time of a verification, and clear all packets received for the selected MID prior to verification time.\\n            * For event messages, the config field `evs_message_clear_after_time` is added to keep events available in CTF telemetry\\n          up to a specific time. This is needed since EVS messages are only output once.\\n        * CheckEvent \\n            * Now support regular expressions via the `is_regex` parameter.\\n            * `msg` parameter is now optional. Leaving it blank will only verify the APP and ID fields of EVS messages. \\n            * Note: Ensure that the TO subscription table for the FSW project is configured to output a large number of EVS messages in order to\\n            avoid dropped packets.\\n    \\n* Logging\\n    * Substantially clean up logging when running in INFO or ERROR log level\\n        * Polling instruction logs should be reduced to only show actual vs. expected and if no packets are received in that poll...\\n    * cFS plugin will directly write the following to the script log folder\\n        * cFS Build/Output Logs\\n        * cFS EVS Logs\\n        * cFS Telemetry Logs (Experimental, does not output array elements)\\n\\n* Config INI Changes\\n    * Reference configs/default_config.ini or config_template.ini for descriptions of fields\\n    * Core\\n        * Rename `telem_verify_timeout` to `ctf_verification_timeout`\\n        * Rename `telem_verify_poll_period` to `ctf_verification_poll_period`\\n    * CCSDS\\n        * Remove `CCSDS_reader_script_path`, `CCSDS_reader_class`, `CCSDS_module`\\n    * cFS\\n        * Remove `evs_tlm_list_depth`\\n        * Rename `ip` to `cfs_target_ip`\\n        * Add `cfs_output_file`, `log_ccsds_imports`, `evs_messages_clear_after_time`\\n\\n* CTF Editor\\n    * Update to allow saving commands or function instructions with empty arguments.\\n        * Empty arguments will be saved to the test script with an empty string value.\\n        * CTF Backend will zero-out any command argument set to an empty string.\\n    * Add `is_regex` field to CheckEvent instruction. \\n        * Refer to `scripts/example/test_advanced_example.json` for reference.\\n        \\n### v0.3.2\\n2020-08-05\\n\\n* Log MID values of unknown packets\\n* Resolve cfs_output file not being written to results when no event messages are received over telemetry\\n* Add initial support for command array arguments\\n    \\n### v0.3.1\\n2020-07-08\\n\\n* cFS Plugin Changes\\n    * Add new instruction \"ArchiveCfsFiles\", which accepts a FSW artifacts and moves any files created during the test run to the test results directory.\\n        * Note: Currently, this feature is supported for Linux cFS targets only. SSH and SP0 support for archiving FSW artifacts is planned for a later version.\\n    * Redesign multi-cfs architecture to be purely configuration based (remove script-specific overrides to allow scripts to be platform independent.) \\n        * Each cFS Target should have its own configuration section in the config INI file.\\n        * RegisterCfs receives a name, and attempts to load target configuration (including protocol) from the loaded configuration.\\n        * A PASS/FAIL for that instruction will be set after validating the necessary config fields for cFS and the specific protocol (Linux, SSH, SP0)\\n    * Rework macro replacement logic such that \\n        * Macro replacement occurs for command argument values, telemetry field values, and array indices.\\n        * All macros in a test script should start with the # prefix (example: \"#SOME_MACRO_IN_MACRO_MAP\").\\n    * Resolve CheckEvent is a polling instruction, to allow for a timeout/polling (similar to typical CheckTlmValue).\\n    * Resolve missing \"build cfs\" implementation for local cfs targets.\\n    \\n* General\\n    * Rework setup_ctf_env.sh and activate_ctf_env.sh such that \\n        * Setup only needs to be run once (within the CTF install directory) and will install the anaconda environment to the user\\'s home directory, or a custom path.\\n        * Activate *can* be placed at the project-specific CTF test directory\\n        * Multiple projects using CTF on the same platform can share the user-specific anaconda environment.\\n\\n* CTF Editor\\n    * Resolve rendering bug when attempting to select an array element within a telemetry field.\\n    * Ensure arrays are left with \"index-placeholders\" `[]` where the test author can insert either a hard-coded index, or a macro from the macro map.\\n        * Note: macros should be preceded with the # token to identify that a replacement needs to take place before evaluating.\\n\\n* CTF Core\\n    * Update Test Pass/Fail log message with string-based payload to mirror the data within the test script during test run.\\n    \\n### v0.3\\n2020-05-21\\n\\n* cFS Plugin Changes\\n    * Serialize Telemetry Receiving Logic (and accompanying lessons learned about the cFS SCH configuration, and how it should be set up…) -> This resolves the timing issues and dropped packets we were experiencing\\n    * Continuous Telemetry Verification Capability -> CTF now has the ability to “continually” verify a piece of telemetry against a condition defined in the test script. If the verification fails at any point during the test run, the failure is logged and the test will fail\\n\\n* CTF Core\\n    * Configuration/Path improvements to resolve the relative/absolute path issues we were seeing when CTF is configured outside of its repository\\n\\n* CTF Editor\\n    * Update editor to support new instruction (continuous verifications)\\n    * Fixe bug in editor launching backend in pre-defined working directory (as opposed to dynamically).\\n    \\n* Other minor bug fixes and improvements\\n\\n### v0.2\\n2020-04-24\\n\\n* cFS Plugin Changes\\n    * More Generic CCSDS/CCDD Interfaces\\n        * Add CCSDS Reader Interface (CFE 6.6/CFE 6.7, CCSDS V1/V2)\\n        * Add CCDD JSON Export Reader\\n    * Multi/Remote cFS Support\\n        * Allow the cFS Plugin to execute/manage multiple instances of cFS running on local or remote targets\\n        * Remote Targets Support: SSH, SP0, Local (linux). More targets can be added as needed.\\n    * Change command argument structure in test script JSON. Arguments are now encoded in the test script as a JSON dictionary.   \\n\\n* Initial Plugin Implementations For\\n    * SSH Plugin\\n    * SP0 Plugin\\n    * CCSDS Plugin\\n           \\n* Re-haul CTF Editor to support generic CCSDS JSON Exports, and multi/remote cFS.\\n\\n* Other bug-fixes and improvements\\n    \\n### v0.1\\n2019-11-22\\n\\n* Initial CTF Release\\n\\n* Initial Plugin Implementations For\\n    * cFS Plugin\\n    * Example Plugin\\n    * PLATO Plugin (Deprecated)\\n    * Remote Execution Plugin (Deprecated)\\n    * Trick cFS (Removed for later release)\\n\\n* Initial CTF Editor Release\\n\\n## License\\n\\nMSC-26646-1, \"Core Flight System Test Framework (CTF)\"\\n\\nCopyright (c) 2019-2020 United States Government as represented by the\\nAdministrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nThis software is governed by the NASA Open Source Agreement (NOSA) License and may be used,\\ndistributed and modified only pursuant to the terms of that agreement.\\nSee the License for the specific language governing permissions and limitations under the\\nLicense at https://software.nasa.gov/.\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the\\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\\neither expressed or implied.\\n\\n\\n'},\n",
       " {'repo': 'nasa/cumulus-ecs-task',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"# cumulus-ecs-task\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-ecs-task.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-ecs-task)\\n[![npm version](https://badge.fury.io/js/%40cumulus%2Fcumulus-ecs-task.svg)](https://badge.fury.io/js/%40cumulus%2Fcumulus-ecs-task)\\n\\nUse this Docker image to run a Node.js Lambda function in AWS\\n[ECS](https://aws.amazon.com/ecs/).\\n\\n## About\\n\\ncumulus-ecs-task is a Docker image that can run Lambda functions as ECS\\nservices.\\n\\nWhen included in a Cumulus workflow and deployed to AWS, it will download a\\nspecified Lambda function, and act as an activity in a Step Functions workflow.\\n\\n## Compatibility\\n\\nThis only works with Node.js Lambda functions, and requires that the Lambda\\nfunction it is running has a dependency of at least v1.0.1 of\\n[cumulus-message-adapter-js](https://github.com/cumulus-nasa/cumulus-message-adapter-js).\\n\\n## Usage\\n\\nLike other Cumulus libraries, cumulus-ecs-task is designed to be deployed to AWS\\nusing [kes](https://github.com/developmentseed/kes) to manage Cloudformation\\nconfig. This documentation assumes you're working with a Cumulus deployment and\\nthat you have files and directory structure similar to what's found in the\\n[cumulus template repository](https://github.com/cumulus-nasa/template-deploy).\\n\\n### Options\\n\\nThis library has two options:\\n\\n- `activityArn` **required**\\n  - The arn of the activity in a step functions workflow. Used to receive\\n    messages for that activity and send success/failure responses.\\n- `lambdaArn` **required**\\n  - The arn of the lambda function you want to run in ECS.\\n\\n### Workflow config\\n\\nFor examples of how to integrate this image with Cumulus, please see the\\n[documentation](https://nasa.github.io/cumulus/docs/workflows/developing-workflow-tasks#ecs-activities)\\nand our\\n[example workflow](https://github.com/nasa/cumulus/blob/master/example/cumulus-tf/ecs_hello_world_workflow.tf)\\nin source.\\n\\n## Development\\n\\nTo run locally:\\n\\n```bash\\nnpm start -- --activityArn <your-activity-arn> --lambdaArn <your-lambda-arn>\\n```\\n\\nTo build the docker image:\\n\\n```bash\\nnpm run build\\n```\\n\\nTo run in Docker locally:\\n\\n```bash\\ndocker run -e AWS_ACCESS_KEY_ID='<aws-access-key>' \\\\\\n  -e AWS_SECRET_ACCESS_KEY='<aws-secret-key>' \\\\\\n  cumuluss/cumulus-ecs-task \\\\\\n  --activityArn <your-activity-arn> \\\\\\n  --lambdaArn <your-lambda-arn>\\n```\\n\\n### To test a workflow while developing locally\\n\\nYou can execute workflows on AWS that test the version of cumulus-ecs-task that\\nyou're developing on locally.\\n\\nFirst, make sure that the ECS cluster for your deployment has zero tasks running\\nthat might respond to a workflow's requests.\\n\\nThat way only your local version will respond to your workflow.\\n\\nNext, start ecs-cumulus-task locally.\\n\\nEither with node:\\n\\n```bash\\nnpm start -- --activityArn <your-activity-arn> --lambdaArn <your-lambda-arn>\\n```\\n\\nOr with docker:\\n\\n```bash\\n# build the image\\nnpm run build\\n\\n# run the image\\ndocker run -e AWS_ACCESS_KEY_ID='<aws-access-key>' \\\\\\n  -e AWS_SECRET_ACCESS_KEY='<aws-secret-key>' \\\\\\n  cumuluss/cumulus-ecs-task \\\\\\n  --activityArn <your-activity-arn> \\\\\\n  --lambdaArn <your-lambda-arn>\\n```\\n\\nFinally, trigger a workflow. You can do this from the Cumulus dashboard, the\\nCumulus API, or with the AWS Console.\\n\\n## Troubleshooting\\n\\nSSH into the ECS container instance.\\n\\nMake sure the EC2 instance has internet access and is able to pull the image\\nfrom docker hub by doing:\\n\\n```bash\\ndocker pull cumuluss/cumulus-ecs-task:1.7.0\\n```\\n\\n`cat` the ecs config file to make sure credentials are correct:\\n\\n```bash\\ncat /etc/ecs/ecs.config\\n```\\n\\nCheck if there's multiple entries of the config.\\n\\nIf there is, there are two things to try:\\n\\n- Delete the ec2 instance and redeploy\\n- Delete the incorrect config and restart the ecs agent (I haven't tested this\\n  much but I expect it to work. You'll still want to update the docker\\n  credentials in the deployment's app directory). Restart the agent by doing:\\n\\n```bash\\nsudo stop ecs\\nsource /etc/ecs/ecs.config\\nsudo start ecs\\n```\\n\\n## Create a release\\n\\nTo create a release, first make sure the [CHANGELOG.md](CHANGELOG.md) file is\\nupdated with all the changes made.\\n\\nNext, bump the version and the changes will automatically be released upon merge\\nto master.\\n\\n```bash\\nnpm version <major|minor|patch|specific version>\\n```\\n\\nCreate the build\\n\\n```bash\\nnpm run build\\n```\\n\\nRelease to Docker Hub\\n\\n```bash\\nnpm run release\\n```\\n\\n## Contributing\\n\\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) file.\\n\\n## License\\n\\n[Apache-2.0](LICENSE)\\n\"},\n",
       " {'repo': 'nasa/dawn-grand-toolbox',\n",
       "  'language': 'IDL',\n",
       "  'readme_contents': \"# dawn-grand-toolbox\\n\\nSoftware to access and process data acquired by the NASA Dawn mission's Gamma-Ray and Neutron Detector (GRaND). The software consists of routines written in the Interactive Data Language (IDL), compatible with IDL version 8.6.1+. The GRaND archive consists entirely of character and binary tables. Two routines are provided to ingest the data for processing. These are found in the grd4-IDL subdirectory:\\n\\n## grd4_read_character_table\\nFunction that reads character (ASCII) tables, including most raw files and all calibrated and derived data files.\\n\\n## grd4_read_binary_table\\nFunction that reads binary data found in raw data files that contain event-mode gamma-ray and neutron data (-EMG and -EMN files).\\n\\nThe routines parse the label, extracting metadata useful for analysis, and read the data.\\xa0For time series data files (raw and calibrated), the metadata includes mission phase and target infomation instrument settings (STATE, TELREADOUT, and TELSOH) and observing conditions.\\n\\n## sample_labels\\nThis directory contains sample labels for evaluation of the software. The accompanying data files can be downloaded from the Planetary Data System Small Bodies Node: https://sbn.psi.edu/pds/resource/dawn/dawngrandPDS4.html. The GRD_STATE_TABLE.TAB and label are included in the dawn-grand-ancillary/miscellaneous/ directory.\"},\n",
       " {'repo': 'nasa/PRECiSA',\n",
       "  'language': 'Haskell',\n",
       "  'readme_contents': 'PRECiSA\\n=======\\n\\nPRECiSA (Program Round-off Error Certifier via Static Analysis) is a fully automatic static analyzer for floating-point programs.\\n\\nThe main functionality of PRECiSA is the [round-off error estimation](#floating-point-round-off-error-estimation).\\nGiven a floating-point program, PRECiSA computes a sound overapproximation of the round-off error that may occur together with PVS certificates ensuring its correctness.\\n[Here](http://precisa.nianet.org/) you can try the PRECiSA web-interface and find more information on the tool.\\n\\nIn addition, PRECiSA implements a [program transformation](#automatic-generation-of-stable-C-code-from-PVS) from a PVS real-valued program to floating-point C code.\\nThis transformation corrects unstable tests by over-approximating the conditional guards in the if-then-else statements.\\nThe resulting transformed program emits a warning when an unstable test is detected (i.e., the floating-point computational flow diverges with respect to the ideal real number one).\\nThe generated C code is annotated with ACSL contracts that relate the floating-point implementation with the real-valued program specification.\\n\\n\\n\\n# Installation\\n\\nPRECiSA runs on Linux and Mac OS X operating systems.\\n\\n\\n## Prerequisites\\n\\n\\nTo build and install PRECiSA you need:\\n\\n* The [Glorious Haskell Compiler](https://www.haskell.org/ghc/) and its package manager Cabal, both available as part of the [Haskell platform](https://www.haskell.org/platform/),\\n* [CMake](https://cmake.org/),\\n* the [NASA Kodiak library](https://github.com/nasa/Kodiak) to compute numerical error bounds.\\n\\nTo verify certificates generated by PRECiSA you need:\\n\\n* [PVS version 6.0](http://pvs.csl.sri.com/),\\n* the [NASA PVS library](https://github.com/nasa/pvslib).\\n\\nIf you want to use the SMT optimization you need [FPRock](https://github.com/nasa/FPRoCK).\\n\\n\\n## Build\\n\\n1. Install [Kodiak](https://github.com/nasa/Kodiak) producing a dynamic library `libKodiakDynamic.so` (for Linux) or `libKodiakDynamic.dylib` (for MacOS).\\n\\n2. Make accessible the *Kodiak* dynamic library to your linker.\\n   Normally, this is done by updating the LD_LIBRARY_PATH environment variable with the directory in which the `libKodiakDynamic.so` (or `libKodiakDynamic.dylib`) file is present.\\n   Let `<kodiak-directory>` represent this directory.\\n   In a bourne shell that environment variable can be set like this:\\n   ```\\n   $ export LD_LIBRARY_PATH=\"<directory-containing-libKodiakDynamic.so>:$LD_LIBRARY_PATH\"\\n   ```\\n   You can add the previous line to your shell initialization script (for example, `~/.profile` or `~/.bashrc`) to make the `LD_LIBRARY_PATH` change permanent.\\n\\n3. Go to the `PRECiSA` sub-directory of the repository.\\n   ```\\n   $ cd <repository-root>/PRECiSA\\n   ```\\n\\n4. Use the following commands to create a `cabal` *sandbox* in this folder and install the executable in `<repository-root>/.cabal-sandbox/bin/precisa`\\n   ```\\n   $ cabal v1-sandbox init    # Creates a new sandbox.\\n                              # Old cabal versions may need the command `cabal sandbox init` instead\\n\\n   $ cabal v1-install --enable-optimization --extra-lib-dirs=<kodiak-directory>\\n   ```\\n\\n5. Now, the `precisa` executable can be run with:\\n   ```\\n   $ .cabal-sandbox/bin/precisa\\n   ```\\n\\n\\n\\n# Floating-Point Round-Off Error Estimation\\n\\nThe input to the PRECiSA round-off error estimator are the following files:\\n\\n* A program `example.pvs` composed of floating-point valued functions. In its current version, PRECiSA accepts a subset of the language of the Prototype Verification System (PVS), including LET-IN expressions, IF-THEN-ELSE constructions, function calls, and floating-point values and operations such as: addition, multiplication, division, subtraction, floor, square root, trigonometric functions, logarithm, and exponential. For example:\\n   ```\\n   example: THEORY\\n   BEGIN\\n   IMPORTING float@aerr754dp\\n\\n\\texample (X,Y: unb_double) : unb_double =\\n\\t\\tIF (X >= 0 AND Y < 4)\\n\\t\\tTHEN IF (Y > 0)\\n\\t\\t\\t THEN X+Y\\n\\t\\t\\t ELSE X*Y\\n\\t\\t\\t ENDIF\\n        ELSE X-Y ENDIF\\n\\n   END example\\n   ```\\n\\n* A file `example.input` containing initial ranges for the input variables of the program. For example:\\n   ```\\n   example(X,Y): X in [-10,10], Y in [0, 40]\\n   ```\\n\\n* Optionally, a file `example.path` containing a set of decision paths/sub-programs of interests.\\n   The user can specify these paths by listing the paths/sub-programs of interests as a list of True and False.\\n   For instance, in the example above, the path [True, True] corresponds to the sub-program Dadd(X,Y), the path [False] to the subprogram Dsub(X,Y), and the path [True] to the subprogram IF (Y > RtoD(0)) THEN Dadd(X,Y) ELSE Dmul(X,Y) ENDIF.\\n   The analysis is done for all the execution paths in the program, or better for all combination of real/FP execution path in the program).\\n   For the selected sub-programs, a dedicated error expression representing the round-off error estimation is computed.\\n   For the others, the tool will generate an overall error which is the maximum of all the round-off error estimations corresponding to these sub-programs.\\n   If the user does not select any sub-program of interest (None), the tool will produce the overall round-off error for the stable cases (when real and floating-point execution flows coincide) and the one for the unstable cases (when real and floating-point execution flows diverge).\\n   If the user is interested in a precise analysis of the entire program (All), the analysis will generate a semantic element for each combination of real/FP execution path in the program.\\n   Examples of possible input for the decision pahts are the following:\\n   ```\\n   example(X,Y): None\\n   ```\\n   or\\n   ```\\n   example(X,Y): All\\n   ```\\n   or\\n   ```\\n   example(X,Y): [True, True]\\n   ```\\n\\nMore examples can be found in the [PRECiSA benchmarks folder](PRECiSA/benchmarks).\\n\\nThe analysis performed by PRECiSA results in one upper-bound of the floating-point round-off error for each decision path of interest, an overall upper-bound for the rest of decision paths, and an overall upper-bound for the unstable test cases (when real and floating-point flows diverge).\\nAdditionally, PRECiSA generates two PVS theories:\\n\\n* a theory containing a collection of lemmas stating symbolic accumulated round-off error estimations for the input program, and\\n* a theory containing a collection of lemmas stating numerical accumulated round-off error estimations.  The numerical estimations are computed using Kodiak.\\n\\nAll the generated lemmas are equipped with PVS proof scripts that automatically discharge them.\\n\\n\\n## How to run the PRECiSA round-off error estimator\\n\\nWe assume that `precisa` (the executable of PRECiSA) is in the current directory.\\n\\nTo launch the round-off error analysis of PRECiSA with the default parameters run:\\n```\\n$ ./precisa analyze \"example.pvs\" \"example.input\"\\n```\\n- the first parameter is the path to the PVS program to be analyzed;\\n- the second one is the path to the file that indicates the initial values for the input variables of the input program;\\n\\n### Command Line Options\\n\\n- `--paths example.path` specifies the path to the file that indicates the decision paths of interest for every function in the program. The default is the empty set, i.e., there is no path of interest and the output of PRECiSA consists of two errors: one for the stable cases (when real and floating-point flows agree) and one for the unstable cases (when real and floating-point flows diverge).\\n\\n- Options for the branch-and-bound search used to compute the numerical estimation:\\n  - `--max-depth 7` (or `-d 7`) is the maximum depth of the branch-and-bound exploration with a default value of `7`.\\n  - `--precision 14` (or `-p 14`) is the negative exponent of `10` representing the numerical precision used.\\n  It has a default value of `14` which stands for a precision of <span style=\"white-space: nowrap;\"><math>10<sup>-14<sup></math></span>.\\n\\n- `--max-lemmas 50` (or `-l 50`) sets the maximum number of lemmas allowed to be generated by PRECiSA. This avoids having certificates too big to be treated. If your program generates a huge number of lemmas, this means probably that you have several nested if-then-else. In this case, try to run PRECiSA with the default settings, or try to set some decision paths of interests. Alternatively, you can activate the Stable Test Assumption.\\n\\n- `--assume stability` (or `-s`) if this option is activated, real and floating-point execution flows are assumed to coincide (Stable Test Assumption). Therefore, the analysis can be unsound since the cases where the execution paths diverge (unstable cases) are not considered.\\n\\n- `--no-collapsed-unstables` (or `-u`) if this option is activated, the unstable tests are not collapsed in a unique case.\\n\\n- `--smt-optimization` (or `-u`) if this option is activated, PRECiSA checks the satisfiability of each path condition by calling an external SMT solver through the FPRoCK tool. In this way, it is possible to detect and remove the spurious execution paths, improving the accuracy of the round-off error estimation.\\n\\nAn example of how to execute PRECiSA by manually setting some options is the following:\\n```\\n$ ./precisa analyze \"example.pvs\" \"example.input\" --paths \"example.path\" --max-depth 7 --precision 14 --max-lemmas 40\\n```\\n\\n## How to verify the PVS certificates\\n\\n[PVS version 6.0](http://pvs.csl.sri.com) and the development version\\nof the [NASA PVS Library](https://github.com/nasa/pvslib) are required\\nto proof-check the symbolic and the numerical certificates generated by PRECiSA in\\nPVS. Furthermore, the directory\\n`PVS` has to be added to the Unix environment variable\\n`PVS_LIBRARY_PATH`.  Depending upon your shell, one of the following lines\\nhas to be added to your startup script.  In C shell (csh or tcsh), put this line in\\n`~/.cshrc`, where `<precisapvsdir>` is the absolute path to the\\ndirectory `PVS`:\\n\\n~~~\\nsetenv PVS_LIBRARY_PATH \"<precisapvsdir>:$PVS_LIBRARY_PATH\"\\n~~~\\n\\nIn Borne shell (bash or sh), put this line in either `~/.bashrc or ~/.profile`:\\n\\n~~~\\nexport PVS_LIBRARY_PATH=\"<precisapvsdir>:$PVS_LIBRARY_PATH\"\\n~~~\\n\\nYou can use the proveit shell script to automatically check the proofs in the symbolic and numerical certificates generated by PRECiSA. For example, if you analyzed the program `example.pvs`, in the same folder you will find two files: `cert_example.pvs` and `num_cert_example.pvs`.\\n\\nTo check the correctness of the PVS theories in `cert_example.pvs` and `num_cert_example.pvs` you can run:\\n```\\n$ proveit -sc cert_example.pvs\\n$ proveit -sc num_cert_example.pvs\\n```\\n\\n### PVS basic troubleshooting ###\\n\\nIf the PVS verification is not behaving as expected, try cleaning the PVS binaries in the NASA PVS library. Simply run cleanbin-all in the NASA PVS library folder of your installation and try again.\\n\\n\\n\\n# Automatic generation of stable C code from PVS\\n\\nThe input to the PRECiSA C code generator are the following files:\\n\\n* A program `example.pvs` composed of real-valued functions. In its current version, PRECiSA accepts a subset of the language of the Prototype Verification System (PVS), including LET expressions, IF-THEN-ELSE constructions, function calls, and floating point values and operations such as: addition, multiplication, division, subtraction, floor, square root, trigonometric functions, logarithm, and exponential. For example:\\n   ```\\n   example: THEORY\\n   BEGIN\\n\\n\\texample (X,Y: real) : real =\\n\\t\\tIF (X >= 0 AND Y < 4)\\n\\t\\tTHEN IF (Y > 0)\\n\\t\\t\\t THEN X+Y\\n\\t\\t\\t ELSE X*Y\\n\\t\\t\\t ENDIF\\n        ELSE X-Y ENDIF\\n\\n   END example\\n   ```\\n\\n* A file `example.input` containing initial ranges for the input variables of the program. For example:\\n   ```\\n   example(X,Y): X in [-10,10], Y in [0, 40]\\n   ```\\n\\nThe output is a C floating-point program `example.c` instrumented to detect unstable tests.\\nThis program is annotated with ACSL contracts stating the relation between the floating-point program and the real number specification. This annotated program can be analyzed with the [Frama-C](https://frama-c.com/) static analyzer.\\n\\nBesides, PVS certificates are provided for ensuring that all the unstable tests are detected.\\nThese certificates can be automatically checked as explained [here](#How-to-verify-the-PVS-certificates).\\n\\n\\n## How to run the PRECiSA stable C code generator\\n\\nWe assume that `precisa` (the executable of PRECiSA) is in the current directory.\\n\\nTo launch the round-off error analysis of PRECiSA with the default parameters run:\\n```\\n$ ./precisa gen-code \"example.pvs\" \"example.input\"\\n```\\n\\n- the first parameter is the path to the PVS program to be analyzed;\\n- the second one is the path to the file that indicates the initial values for the input variables of the input program;\\n\\n### Command Line Options\\n\\n- `--format FORMAT` where FORMAT can be double or single, indicating the target format of the floating-point C code. The default value is double precision.\\n\\n\\n\\n# Additional information\\n\\n## Version\\n\\n*PRECiSA v-3.0.0* (July 2020)\\n\\n## Contact information\\nIf you have any question or problem, please contact:\\n\\n* [Laura Titolo](mailto:laura.titolo@nianet.org) (for PRECiSA)\\n* [Mariano Moscato](mailto:mariano.moscato@nianet.org) (for PVS)\\n* [Marco A. Feliu](mailto:marco.feliu@nianet.org) (for Kodiak)\\n* [C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (PRECiSA @ NASA)\\n\\n## Related Publications\\n\\n- Mariano Moscato, Laura Titolo, Marco Antonio Feliu Gabaldon and César Muñoz: A Provably Correct Floating-Point Implementation of a Point-in-Polygon Algorithm. FM 2019\\n\\n-\\tRocco Salvia, Laura Titolo, Marco A. Feliú, Mariano M. Moscato, César A. Muñoz, Zvonimir Rakamaric: A Mixed Real and Floating-Point Solver. NFM 2019: 363-370\\n\\n- \\tLaura Titolo, César A. Muñoz, Marco A. Feliú, Mariano M. Moscato:\\nEliminating Unstable Tests in Floating-Point Programs. LOPSTR 2018: 169-183\\n\\n- Laura Titolo, Marco A. Feliú, Mariano M. Moscato, César A. Muñoz:\\nAn Abstract Interpretation Framework for the Round-Off Error Analysis of Floating-Point Programs. VMCAI 2018: 516-537\\n\\n- Mariano M. Moscato, Laura Titolo, Aaron Dutle, César A. Muñoz:\\nAutomatic Estimation of Verified Floating-Point Round-Off Errors via Static Analysis. SAFECOMP 2017: 213-229\\n\\n\\n## License and Copyright Notice\\n\\nThe code in this repository is released under NASA\\'s Open Source Agreement.  See the directory [`LICENSES`](LICENSES).\\n\\n<pre>\\n\\nNotices:\\n\\nCopyright 2020 United States Government as represented by the Administrator of the National Aeronautics\\nand Space Administration. All Rights Reserved.\\n\\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND,\\nEITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY\\nTHAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES\\nOF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT,\\nANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT\\nDOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT,\\nIN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF\\nANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS\\nALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL\\nSOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES\\nGOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. \\nIF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,\\nEXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED\\nON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY\\nAND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS,\\nAS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY\\nFOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n</pre>\\n'},\n",
       " {'repo': 'nasa/cpr',\n",
       "  'language': 'OCaml',\n",
       "  'readme_contents': '# CPR*: Formally Verified Compact Position Reporting Algorithm \\n\\nThe Compact Position Reporting (CPR) algorithm consists of a set of functions defined in the standard RTCA-DO- 260B/Eurocae ED-102A, Minimum Operational Performance Standards for 1090 MHz extended squitter Automatic Dependent Surveillance - Broadcast (ADS- B) and Traffic Information Services - Broadcast (TIS-B). These function encode and decode aircraft positions. CPR* is a formally verified C implementation of CPR\\'s functions that use computer arithmetic in fixed- and floating-point formats.\\n\\n## The Fixed-Point Implementation\\n\\nThe directory [`C/fixed-point`](C/fixed-point) contains a fixed-point C implementation of the CPR functions.\\nThe [ACSL](https://frama-c.com/acsl.html) specification language was used to state correctness properties for the core components of this implementation.\\nSuch correctness properties assure that each C function behaves accordingly with a logical description of it written also in ACSL.\\nThe [Frama-C](https://frama-c.com/index.html) static analyzer was then used to process these annotated C functions and output verification conditions (VCs) expressed in [PVS](https://pvs.csl.sri.com/) language.\\nThe directory `PVS` contains the resulting VCs and proofs for them.\\nAdditionally, proofs of the correctness of the logical descriptions w.r.t. the aforementioned standard (under the modified set of requirements presented in [1]) can also be found in the same directory.\\n\\n## The Floating-Point Implementation\\n\\nThe floating-point C implementation can be found in the directory [`C/floating-point`](C/floating-point).\\nAs in the previous case, ACSL annotations along the code are used to relate the result of each core function with the result of its real valued specification.\\nSince in this case the implementation was developed using floating-point operations, the annotations take into consideration the maximum possible roundoff error occurred in the calculations performed on each operation. \\nIn order to deal with this kind of VCs, the Frama-C analyzer was used to generate [Gappa](http://gappa.gforge.inria.fr/) representations of them.\\nThese Gappa specifications were manually modified to add hints allowing the Gappa solver to automatically discharge the VCs.\\nThe modified Gappa files can be found in the [`Gappa`](Gappa) directory.\\nFinally, PVS was used to prove that the real valued specifications of the core functions are correct w.r.t. the standard assuming the set of corrected requirements reported in [1]. \\nThe [`PVS`](PVS) directory contains these proofs.\\nFor details on the verification process for the floating-point implementation, see [2].\\n\\n### Version\\n\\nCurrent version is 1.0.0\\n\\n### License\\n\\nThe code in this repository is released under NASA\\'s Open Source Agreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. \\n\\n## Contact\\n\\n[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.\\n\\n## Copyright Notice\\n\\nCopyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\n# Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n# Bibliography\\n\\n[1] A. Dutle, M. Moscato, L. Titolo, and C. Munoz. A formal analysis\\nof the compact position reporting algorithm. 9th Working Conference on\\nVerified Software: Theories, Tools, and Experiments, VSTTE 2017,\\nRevised Selected Papers, 10712:19–34, 2017.\\n\\n[2] L. Titolo, M. Moscato, C. A. Munoz, A. Dutle, and F. Bobot. A\\nformally verified floating-point implementation of the compact\\nposition reporting algorithm. In Klaus Havelund, Jan Peleska, Bill\\nRoscoe, and Erik de Vink, editors, Formal Methods, pages 364-381,\\nCham, 2018. Springer International Publishing.\\n'},\n",
       " {'repo': 'nasa/PolyCARP',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'PolyCARP \\n========\\n\\nPolyCARP (Algorithms and Software for Computations with Polygons)\\nis a package of algorithms, implemented in Java, C++, and Python, \\nfor computing containment,\\ncollision, resolution, and recovery information for polygons. The\\nintended applications of PolyCARP are related, but not limited, to\\nsafety critical systems in air traffic management.\\n\\nThis repository also the formal specification\\nof PolyCARP algorithms in the Prototype Verification System ([PVS](http://pvs.csl.sri.com)).\\n\\n### Documentation\\n\\nThe API document for PolyCARP is still work in progress. In the meantime,\\nplease refer to the release notes and the examples files\\n[`PolycarpExample.java`](Java/src/PolycarpExample.java) and [`PolycarpExample.cpp`](C++/src/PolycarpExample.cpp).\\n\\nFor technical information about the definitions and algorithms in this\\nrepository, visit [http://shemesh.larc.nasa.gov/fm/PolyCARP](http://shemesh.larc.nasa.gov/fm/PolyCARP).\\n\\n### Current Release\\n\\nPolyCARP@FormalATMv2.6.2 (March-18-2017) \\n\\n### License\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. \\n\\n### Contact\\n\\n[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.\\n\\n### Copyright Notice\\n\\nCopyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/LC',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"core Flight System (cFS) Limit Checker (LC)\\n======================================================\\n\\nOpen Source Release Readme\\n==========================\\n\\nLC Release 2.1.2\\n\\nDate: 10/7/2020\\n\\nIntroduction\\n-------------\\n  The Limit Checker (LC) is a core Flight System (cFS) application\\n  that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\n  The LC application monitors telemetry data points in a cFS system and compares \\n  the values against predefined threshold limits. When a threshold condition is \\n  encountered, an event message is issued and a Relative Time Sequence (RTS) \\n  command script may be initiated to respond/react to the threshold violation.  \\n\\n  The LC application is written in C and depends on the cFS Operating System\\n  Abstraction Layer (OSAL) and cFE components.  There is additional LC application\\n  specific configuration information contained in the application user's guide\\n  available in https://github.com/nasa/LC/tree/master/docs/users_guide\\n\\n  This software is licensed under the NASA Open Source Agreement.\\n  http://ti.arc.nasa.gov/opensource/nosa\\n\\n\\nSoftware Included\\n------------------\\n\\n  Limit Checker application (LC) 2.1.2\\n\\n\\nSoftware Required\\n------------------\\n\\n Operating System Abstraction Layer 5.0 or higher can be\\n obtained at https://github.com/nasa/osal\\n\\n core Flight Executive 6.8.0 or higher can be obtained at\\n https://github.com/nasa/cfe\\n\\n Note: An integrated bundle including the cFE, OSAL, and PSP can\\n be obtained at https://github.com/nasa/cfs\\n\\nAbout cFS\\n----------\\n  The cFS is a platform and project independent reusable software framework and\\n  set of reusable applications developed by NASA Goddard Space Flight Center.\\n  This framework is used as the basis for the flight software for satellite data\\n  systems and instruments, but can be used on other embedded systems.  More\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\nEOF\\n\"},\n",
       " {'repo': 'nasa/CS',\n",
       "  'language': 'C',\n",
       "  'readme_contents': 'core Flight System (cFS) Checksum Application (CS) \\n==================================================\\n\\nOpen Source Release Readme\\n==========================\\n\\nCS Release 2.4.2\\n\\nDate: \\n10/07/2020\\n\\nIntroduction\\n-------------\\n  The Checksum application (CS) is a core Flight System (cFS) application that \\n  is a plug in to the Core Flight Executive (cFE) component of the cFS.  \\n  \\n  The CS application is used for for ensuring the integrity of onboard memory.  \\n  CS calculates Cyclic Redundancy Checks (CRCs) on the different memory regions \\n  and compares the CRC values with a baseline value calculated at system startup. \\n  CS has the ability to ensure the integrity of cFE applications, cFE tables, the \\n  cFE core, the onboard operating system (OS), onboard EEPROM, as well as, any \\n  memory regions (\"Memory\") specified by the users.\\n\\n  The CS application is written in C and depends on the cFS Operating System \\n  Abstraction Layer (OSAL) and cFE components. There is additional CS application \\n  specific configuration information contained in the application user\\'s guide\\n  available in https://github.com/nasa/CS/tree/master/docs/users_guide\\n\\n  This software is licensed under the NASA Open Source Agreement. \\n  http://ti.arc.nasa.gov/opensource/nosa\\n \\n \\nSoftware Included\\n------------------\\n  Checksum application (CS) 2.4.2\\n  \\n \\nSoftware Required\\n------------------\\n\\n Operating System Abstraction Layer 5.0 or higher can be \\n obtained at https://github.com/nasa/osal\\n \\n core Flight Executive 6.8.0 or higher can be obtained at\\n https://github.com/nasa/cfe\\n\\n Note: An integrated bundle including the cFE, OSAL, and PSP can \\n be obtained at https://github.com/nasa/cfs\\n \\nAbout cFS\\n----------\\n  The cFS is a platform and project independent reusable software framework and\\n  set of reusable applications developed by NASA Goddard Space Flight Center.\\n  This framework is used as the basis for the flight software for satellite data\\n  systems and instruments, but can be used on other embedded systems.  More\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\nEOF\\n'},\n",
       " {'repo': 'nasa/MD',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"core Flight System (cFS) Memory Dwell Application (MD)\\n======================================================\\n\\nOpen Source Release Readme\\n==========================\\n\\nMD Release 2.3.3\\n\\nDate:\\n10/7/2020\\n\\nIntroduction\\n-------------\\n  The Memory Dwell application (MD) is a core Flight System (cFS) application\\n  that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\n  The MD application monitors memory addresses accessed by the CPU. This task\\n  is used for both debugging and monitoring unanticipated telemetry that had\\n  not been previously defined in the system prior to deployment.\\n\\n  The MD application is written in C and depends on the cFS Operating System\\n  Abstraction Layer (OSAL) and cFE components. There is additional MD application\\n  specific configuration information contained in the application's Doxygen\\n  user's guide available in https://github.com/nasa/MD/tree/master/docs.\\n\\n  This software is licensed under the Apache 2.0 license.\\n\\n\\nSoftware Included\\n------------------\\n  Memory Dwell application (MD) 2.3.3\\n\\n\\nSoftware Required\\n------------------\\n\\n Operating System Abstraction Layer 5.0.0 or higher can be\\n obtained at https://github.com/nasa/osal\\n\\n core Flight Executive 6.8.0 or higher can be obtained at\\n https://github.com/nasa/cfe\\n\\n Note: An integrated bundle including the cFE, OSAL, and PSP can\\n be obtained at https://github.com/nasa/cfs\\n\\n\\nAbout cFS\\n-----------\\n  The cFS is a platform and project independent reusable software framework and\\n  set of reusable applications developed by NASA Goddard Space Flight Center.\\n  This framework is used as the basis for the flight software for satellite data\\n  systems and instruments, but can be used on other embedded systems.  More\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\"},\n",
       " {'repo': 'nasa/WellClear',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': '![](docs/DAIDALUS.jpeg \"\")\\n\\n**Current version of DAIDALUS v2 is available from [https://github.com/nasa/daidalus](https://github.com/nasa/daidalus)**\\n========\\n\\nDetect and AvoID Alerting Logic for Unmanned Systems\\n========\\n\\nThis repository includes a prototype implementation written in Java and\\nC++ of DAIDALUS (Detect and Avoid Alerting Logic for Unmanned Systems).\\nDAIDALUS is a reference implementation of a detect and avoid concept\\nintended to support the integration of Unmanned Aircraft Systems into\\ncivil airspace.\\nThe repository also includes definitions in Matlab of a family of\\nwell-clear violation volumes and the formal specification\\nof DAIDALUS core algorithms in the Prototype Verification System ([PVS](http://pvs.csl.sri.com)).\\n\\nDAIDALUS is a\\nreference implementation of the detect and avoid (DAA) functional\\nrequirements  described in\\nAppendix G of DO-365, the Minimum Operational Performance Standards (MOPS)\\nfor Unmanned Aircraft Systems (UAS)  developed by RTCA\\nSpecial Committee 228 (SC-228). The current software release implements\\n\\n*  detection logic,\\n*  alerting logic based on hazard and non-hazard volumes, and\\n*  multi-level instantaneous and kinematic maneuver guidance.\\n\\n### Documentation\\n\\nA draft of user guide is available at https://nasa.github.io/WellClear. \\n\\nExample programs [`DaidalusExample.java`](DAIDALUS/Java/src/DaidalusExample.java) and \\n[`DaidalusExample.cpp`](DAIDALUS/C++/src/DaidalusExample.cpp) illustrate the main \\nfunctional capabilities of DAIDALUS in Java and C++, respectively.\\n\\nFor technical information about the definitions and algorithms in this\\nrepository, visit https://shemesh.larc.nasa.gov/fm/DAIDALUS.\\n\\n### Current Release\\n\\nThe release in this repository is DAIDALUSv1.0.2.\\n**The current version of DAIDALUS v2 is available from [https://github.com/nasa/daidalus](https://github.com/nasa/daidalus).**\\n\\n### License\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. \\n\\n### Contact\\n\\n[Cesar A. Munoz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center.\\n\\n### Logo\\n\\nThe DAIDALUS logo was designed by \\n[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC), NASA Langley Research Center.\\n\\n### Copyright Notice\\n\\nCopyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/AGTF30',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# AGTF30\\n\\nThe Advanced Geared Turbofan 30,000 (AGTF30) is a geared turbofan \\nsimulation that utilizes the Toolbox for the Modeling and Analysis \\nof Thermodynamic Systems (T-MATS) to create a steady-state and \\ndynamic engine model within MATLAB/Simulink. The engine model is \\nbased upon a futuristic geared turbofan concept and allows steady-state \\noperation throughout the flight envelope. Dynamic operation is \\nutilizes a baseline control system.\\nDesign tools included within the package include system linearization and\\nautomated plotting scripts.\\n\\nAGTF30 installation:\\n1) Install Matlab and Simulink, Developed in MATLAB 2015aSP1\\n2) Install the Toolbox for the Modeling and Analyais of Thermodyanmic Systems,\\ndeveloped with T-MATS version 1.3 (https://github.com/nasa/T-MATS/releases)\\n\\nRunning AGTF30 simulation:\\n1) Navigate to the AGTF30 folder\\n3) Run script \"setup_everything.m\",  this will generate all required variables within MATLAB \\n(namely: bus variables and the engine parameter structure \\'MWS\\'). \\n4) Open AGTF30 system simualtion: dynamic(Dyn), Linearization (Lin), or steady-state (SS).\\n5) Run the specified model.\\n\\nChanging run conditions:\\n\\nOpen setup_everyting and adjust Input.  structure as appropriate\\n\\nor\\n\\nVerify setup_everything sets Input.UseExcel = 1, update excel document define_inputs.xlsx\\n\\nor\\n\\nVerify setup_everything sets Input.UseExcel = 0, update define_inputs.m\\n\\nSystem inputs are defined (within the MWS structure) as vectors with each value matching with a time vector.\\nThe linearization and steady-state systems will only make use of the initial values, \\nNote: Linearization and Dynamic systems will use the steady-state solver to generate initial conditions.\\n\\nOutputs:\\n1) Steady state simulation outputs structure: out_SS, containing a large amount of data. Additionally it generate specific structures out_*\\n2) Linearization simulation outputs structure: out_Lin, containing a large amount of data. (ABCD matricies for state-space model are located within the structure)\\n3) Dynamic simulation outputs structure: out_Dyn, containing a large amount of data. Additionally it generate specific structures out_*\\n\\nAuto plotting scripts:\\n\\nThe AGTF30 makes use of T-MATS autoplotting scrips. These are defined within PlotDyn.m and PlotSS.m.\\nAfter running the Dyn model or SS model, run the appropriate script to generate the appropriate plots.\\n\\nReferences:\\n1) Jeffryes W. Chapman and Jonathan S. Litt. \"Control Design for an Advanced Geared Turbofan Engine\", 53rd AIAA/SAE/ASEE Joint Propulsion Conference, AIAA Propulsion and Energy Forum, (AIAA 2017-4820)\\n2) Jones, S.M., Haller, W.J., Tong, M.T., �An N+3 Technology Level Reference Propulsion System�, NASA/TM-2017-219501, 2017. '},\n",
       " {'repo': 'nasa/PyMKAD',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# PyMKAD\\n\\nThe world-wide aviation system is one of the most complex dynamical systems ever developed and is generating data at an extremely rapid rate. Most modern commercial aircraft record several hundred flight parameters including information from the guidance, navigation, and control systems, the avionics and propulsion systems, and the pilot inputs into the aircraft. These parameters may be continuous measurements or binary/categorical measurements recorded in one second intervals for the duration of the flight. Currently, most approaches to aviation safety are reactive, meaning that they are designed to react to an aviation safety incident or accident. PyMKAD is a novel approach based on the theory of multiple kernel learning to detect potential safety anomalies in very large data bases of discrete and continuous data from world-wide operations of commercial fleets. This code address an anomaly detection problem which includes both discrete and continuous data streams, where we assume that the discrete streams influence on the continuous streams. We also assume that atypical sequence of events in the discrete streams can lead to off-nominal system performance.  \\n\\nThe objective of this project is to automate the analysis of flight safety incidents in a way that combines both analysis of discrete and continuous parameters. \\n\\nThis repository contains the following files in its top level directory:\\n\\n* [PythonCode](PythonCode)  \\nThe source code of the repository includes: preprocessing modules, the main mkad code, and a post processing visualization tool. The code is uses a command line interface and a json file for configuring. \\n\\n* [documentation](documentation)  \\nDocuments describing how to configure and run the program, as well as how to interpret the results. \\n\\n\\n* [MKAD NOSA 2019.pdf](MKAD%20NOSA%202019.pdf)  \\nLicensing for MKAD\\n\\n\\n\\n\\n## Contact Info\\n\\nNASA Point of contact: Nikunj Oza <nikunj.c.oza@nasa.gov>, Data Science Group Lead.\\n\\nFor questions regarding the research and development of the algorithm, please contact Bryan Matthews <bryan.l.matthews@nasa.gov>, Senior Research Engineer.\\n\\n\\n## Copyright and Notices\\n\\nNotices:\\n\\nCopyright © 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nDisclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/polyfit',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# polyfit\\n\\nThis is a stand alone C++ program to fit a curve with a polynomial.\\n\\nTo compile: g++ -o Polyfit Polyfit.cpp\\n\\nInputs:\\n\\nk: Degree of the polynomial\\nfixedinter: Fixed the intercept (coefficient A0)\\nwtype: Weight: 0 = none (default), 1 = sigma, 2 = 1/sigma^2\\nalphaval: Critical apha value\\nx[]: Array of x values to be fitted\\ny[]: Array of y values to be fitted\\nerry[]: Array of error of y (if applicable)\\n\\n\\n'},\n",
       " {'repo': 'nasa/EdsLib',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# CCSDS SOIS Electronic Data Sheet Tool and Library\\n\\nThis repository contains an implementation of a tool and runtime library\\nfor embedded software to create and interpret data structures defined \\nusing Electronic Data Sheets per CCSDS book 876.0.\\n\\nThe full specification for Electronic Data Sheets is available here:\\n\\n[CCSDS 876.0-B-1](https://public.ccsds.org/Pubs/876x0b1.pdf)\\n\\nThis repository contains the basic tool to process EDS files, a\\nruntime library (EdsLib) for embedded software, and libraries \\nto interoperate with JSON, Lua, and Python.\\n\\nThis software is also intended to work with the Core Flight System:\\n\\n[cFS](https://github.com/nasa/cFS)\\n\\nA set of patches to CFS to enable EDS features is also available.\\n\\n## General Overview\\n\\nThe tool reads all EDS files and generates an in-memory document object model (DOM) \\nstructure which can then be queried by scripts which can in turn generate derived outputs \\nbased on the EDS information.\\n\\nThis DOM is conceptually similar to the Javascript DOM of HTML documents in a web browser,\\nbut very different in terms of usage and implementation as it represents a very different\\ntype of document.\\n\\nScripts currently exist for generating C header files and runtime data structures to work\\nwith the accompanying EdsLib runtime library.  However, applications can supply additional\\nscripts and generate custom outputs from the same DOM.\\n\\n## Components\\n\\nThe following subdirectories are contained within this source tree:\\n\\n- `tools` contains the build tool to read EDS files, generate the DOM tree, and run scripts\\n- `edslib` contains the runtime C library for dealing with EDS-defined objects\\n- `doc` contains additional information about the DOM structure\\n- `cfecfs` contains additional bindings/libraries for use with Core Flight System\\n\\nA separate CMake script is included in each subdirectory for building each component.\\n\\n## Execution\\n\\nOnce built, the tool is executed by supplying a set of XML files and processing\\nscripts on the command line, such as:\\n\\n```\\n$ sedstool MyEDSFile1.xml MyEDSFile2.xml MyScript1.lua MyScript2.lua ...\\n```\\n\\nA few command line options are recognized:\\n\\n- `-v` : Increase verbosity level.  Use twice for full debug trace.\\n- `-D NAME=VALUE` : Sets the symbolic NAME to VALUE for preprocessor substitutions\\n\\n\\nHowever, this tool is generally _not_ intended to be executed manually in a standalone\\ninstallation, but built and executed as part of a larger application build system, such\\nas Core Flight System (cFS).\\n\\nThe tool will first read _all_ the supplied XML files and build a DOM tree, and then it\\nwill invoke each Lua script _in alphanumeric order_ of filename.  Ordering is\\nvery important, as each script can build upon the results of the prior scripts.  To preserve\\nthe intended order of operations, each script supplied with this tool contains a two digit\\nnumeric prefix, which indicates the correct position in the set.  This way, when scripts are\\nexecuted in a simple alphanumeric order, and this will always produce the correct result.\\nFurthermore, additional scripts can be added into the sequence simply by choosing an appropriate\\nprefix number, without needing to specify explicit dependencies or complicated rules.\\n'},\n",
       " {'repo': 'nasa/cFS-EDS-GroundStation',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# cFS-EDS-GroundStation\\n\\nThis is a Python based ground station that interfaces with an Electronic Data Sheets supported Core Flight Systems instance.  \\nAlso, there are several utility python scripts that provide basic Telemetry and Telecommand functionality in a non-GUI format.\\n\\nThe user's manual can be found at docs/cFS-EDS-GroundStation Users Manual.docx\\n\\n## Prerequisistes\\n\\n  -python3-dev\\n  -python3-pyqt5\\n\\n## Configuring and running\\n\\nIt is recommended to use this software in conjunction with the cfe-eds-framework repository which can be found at:\\nhttps://github.com/jphickey/cfe-eds-framework\\nThis software package requries the EdsLib and CFE_MissionLib python modules from that repository to run.  The build process\\nalso automatically configues these files with the defined mission name.\\n\\n##\\n\\nFirst, the following cmake variables need to be turned on for both the GroundStation or assorted utilities to work.\\n\\nEDSLIB_PYTHON_BUILD_STANDALONE_MODULE:BOOL=ON\\nCFE_MISSIONLIB_PYTHON_BUILD_STANDALONE_MODULE:BOOL=ON\\nCONFIGURE_CFS_EDS_GROUNDSTATION:BOOL=ON\\n\\nThe first two options will compile the python bindings for EdsLib and CFE_MissionLib respectively.\\nThe third configures the python scripts in this folder with the mission name defined in the cFS build process\\nand output the scripts to the ${CFS_HOME}/build/exe/host/cFS-EDS-GroundStation/ folder.\\n\\nThe folder where the python modules get installed is:\\n${CFS_HOME}/build/exe/lib/python\\n\\nThis folder needs to be added to the PYTHON_PATH environment variable so the modules can be imported into Python.\\nThe folder that contains EdsLib and CFE_MissionLib .so files also needs to be added to the LD_LIBRARY_PATH\\nenviroment variable so Python can load these libraries.  For example the following lines can be added to ~/.bashrc\\n\\nCFS_HOME = /path/to/cfs/directory\\nexport PYTHONPATH=$PYTHONPATH:$CFS_HOME/build/exe/lib/python\\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CFS_HOME/build/exe/lib\\n\\nWith this set the cFS-EDS-GroundStation can be run with the following command\\n\\npython3 cFS-EDS-GroundStation.py\\n\\n# Utility python scripts\\n\\npython3 cmdUtil.py\\n\\nThis script runs through several prompts to send a command to a core flight instance.  The user inputs the instance name,\\ntopic, subcommand (if applicable), payload values (if applicable), and destination IP address.  The script will create, \\nfill, pack, and send a command to the desired IP address.\\n\\npython3 tlm_decode.py -p <port=5021>\\n\\nThis script will listen in on the specified port for telemetry messages.  When messages come in they are decoded\\nand the contents are displayed on the screen.\\n\\npython3 convert_tlm_file.py -f <filename>       or\\npython3 convert_tlm_file.py --file=<filename>   (recommended as this allows tab completion of file names)\\n\\nThis script takes the binary telementry files from the Telemetry System of the cFS-EDS-GroundStation, reads through all\\nof the messages in the file, and writes the decoded packet information in a csv fie of the same base name.\\n\"},\n",
       " {'repo': 'nasa/Kodiak',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '![](logo/Kodiak.jpeg \"\")\\n\\n*Kodiak*, a C++ Library for Rigorous Branch and Bound Computation\\n=================================================================\\n\\n*Kodiak* is a C++ library that implements a generic branch and bound\\nalgorithm for rigorous numerical approximations. Particular instances\\nof the branch and bound algorithm allow the user to refine and\\nisolate solutions to systems of nonlinear equations and inequalities,\\nglobal optimization problems, and bifurcation sets for systems of\\nODEs. Kodiak utilizes interval arithmetic (via the *filib++* library)\\nand Bernstein enclosure (for polynomials and rational functions) as\\nself-validating enclosure methods. Symbolic operations support\\nprocedures such as automatic partial differentiation.\\n\\n### Authors\\n\\nCesar A. Munoz (cesar.a.munoz@nasa.gov), NASA Langley Research Center\\n\\nMarco A. Feliu (marco.feliu@nianet.org), National Institute of Aerospace\\n\\n### Other Contributors\\n\\nAndrew P. Smith, formerly at National Institute of Aerospace, US.\\n\\nAnthony Narkawicz, formerly at NASA Langley Research Center, US.\\n\\nMantas Markevicius, formerly at University of York, UK.\\n\\n### Documentation\\n\\nCurrently, the main documentation is contained in this file.\\nPublications concerning the library and associated algorithms are in\\npreparation. There are also numerous comments in the source code.\\n\\n### Support and Contribution\\n\\nSee the instructions in this file and the linked resources.\\nFor further assistance, or to report problems and bugs, please\\ncontact the authors. Contributions to the library are welcomed.\\n\\nObtaining *Kodiak*\\n------------------\\n\\nThe repository is located at:\\n[https://github.com/nasa/Kodiak/](https://github.com/nasa/Kodiak/)\\n\\nLicense\\n-------\\n\\nThe Kodiak C++ Library is released under NASA\\'s Open Source Agreement.\\nSee the files `LICENSES/Kodiak-NOSA.pdf` and `LICENSES/Kodiak-BooleanChecker-NOSA.pdf`;\\nsee also the copyright notice at the end of this file.\\n\\nInstallation and Usage Options\\n------------------------------\\n\\n*Kodiak* is installed from source code.\\n*Kodiak* is run by encoding a problem in a C++ program file, then compiling and running it.\\n\\n### 2. Prerequisites\\n\\nIt is recommended to run *Kodiak* on a Linux or Mac computer with the\\nGNU C++ compiler; so far it has been successfully tested on Ubuntu Linux\\nand Mac OS X. Use of Windows is not supported, although it ought to be\\nfeasible, and the authors would welcome any report of successfully\\nrunning the software on Windows or any other system.\\n\\nThe following software should firstly be installed, if not already\\npresent (please follow the links for instructions and support):\\n\\n* *CMake* build tool (required):\\n  [https://cmake.org](https://cmake.org)\\n\\n* *Boost* libraries (required):  \\n  [http://www.boost.org/users/download/](http://www.boost.org/users/download/)  \\n  In addition to the headers, you need at least the library\\n  `serialization`. This library can be installed using \\n  `\\n  ./bootstrap.sh --with-libraries=serialization\\n  `\\n  and then `sudo ./b2 install`. Finally, you need to define the environment\\n  variable `BOOST_ROOT` to point to the directory where Boost\\'s\\n  `include` and `lib` directories were installed, e.g., `/usr/local`.\\n\\n* *filib++* interval library (required):\\n  [http://www2.math.uni-wuppertal.de/wrswt/software/filib.html](http://www2.math.uni-wuppertal.de/wrswt/software/filib.html)\\n  This library should be configured with the following options before making (`make` command)\\n  and installing it (maybe `sudo` will be needed prepended to the `make install` command in order to\\n  install the files in `/usr/local`):\\n  `\\n  ./configure CFLAGS=-fPIC CPPFLAGS=-fPIC CXXFLAGS=-fPIC\\n  `\\n\\n### 2. Build Library and Examples\\n\\nIf necessary, unzip the zip file in order to extract the files.\\nThe following files and directories should be present:\\n\\n* In the working directory: this `README.md` file and a `CMakeListst.txt` file\\n* `LICENSES`: licenses and copyrights for *Kodiak*\\n* `logo`: *Kodiak*\\'s logo and credits\\n* `src`: source code for the library\\n* `examples`: example C++ files  (`.cpp`) containing several problems\\n\\nIf any of the prerequisite libraries were installed in non-standard\\ndirectories, then the file `CMakeLists.txt` should be modified accordingly.\\n\\nCreate a `build` directory to keep the compiled libraries and make it\\nthe current working directory:\\n```\\n$ mkdir build\\n$ cd build\\n```\\n\\nNow, run *CMake* for creating the build scripts:\\n```\\n$ cmake ..\\n```\\n\\nFinally, build all targets by invoking the *CMake* build command:\\n```\\n$ cmake --build .\\n```\\n\\nUsing the Library\\n-----------------\\n\\nThe *Kodiak* library is used in your own C/C++ programs.\\nA good way to start is to take one of the existing `.cpp`\\nfiles in the `examples` directory and adapt it to your purposes.\\nYou can either invoke the compiler directly with a link to the\\n*filib++* and *Kodiak* libraries, or else add a new entry to\\nthe `examples/CMakeLists.txt` file.\\nFor C programs, please use the `src/Adapters/Codiak.h` header file to invoke *Kodiak*\\'s routines.\\n\\nBe aware that care must be taken with the order in which commands\\nare invoked. All variables should be declared before any variable\\nresolutions are set.\\n\\n## Version\\n\\n*Kodiak* ver. 2.0.2,  June 2020\\n\\n## Logo\\nThe Kodiak logo was designed by \\n[Mahyar Malekpour](http://shemesh.larc.nasa.gov/people/mrm/publications.htm#ETC) (NASA).\\n\\n## License and Copyright Notice\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES).\\n\\n<pre>\\n\\nNotices:\\n\\nCopyright 2017 United States Government as represented by the\\n   Administrator of the National Aeronautics and Space Administration.\\n   All Rights Reserved.\\n\\nDisclaimers:\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY\\nWARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,\\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE\\nWILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO\\nTHE SUBJECT SOFTWARE.  THIS AGREEMENT DOES NOT, IN ANY MANNER,\\nCONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT\\nOF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY\\nOTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\nFURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\nREGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\nAND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS\\nAGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND\\nSUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF\\nTHE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,\\nEXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM\\nPRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT\\nSOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE\\nREMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL\\nTERMINATION OF THIS AGREEMENT.\\n\\n</pre>\\n'},\n",
       " {'repo': 'nasa/gFTL',\n",
       "  'language': 'Fortran',\n",
       "  'readme_contents': '# The problem\\n\\nFortran only provides one type of container: *array*.  While Fortran\\narrays are exemplary for their intended purpose such as numerical\\nalgorinthms, they are poorly suited in many other contexts.  Arrays\\ncan be thought of as a particular case of a \"container\" that holds\\nmultiple entities.  As a container, arrays are well suited for random\\naccess to a fixed number of objects.  (Yes, Fortran arrays are\\ntechnically dynamic, but \"growing\" an array involves multiple steps.)\\n\\n\\nMany other languages provide additional types of containers that\\ncommonly arise in many contexts.  E.g., a vector (C++ STL) or List\\n(Java) are _growable_ containers of objects that automatically resize\\nwhen required to add a new object.  Another example is that of Map\\nwhich allows stores objects as key-value pairs, thereby allowing\\nretrieval of an object by providing it\\'s key.\\n\\n\\n# The solution\\n\\nThis package, gFTL, provides a mechanism to easily create robust\\ncontainers and associated iterators which can be used within Fortran\\napplications.  The primary methods are intended to be as close to\\ntheir C++ STL analogs as possible.  We have found that these\\ncontainers are a powerful productivity multiplier for certain types of\\nsoftware development, and hope that others find them to be just as useful.\\n\\nCurrently, the following three types of containers are provided.\\n* Vector (list)\\n* Set\\n* Map  (associated array)\\n\\nContributions of additional containers are very much welcomed.\\n\\n## Initial developers\\n\\n* Tom Clune\\n* Doron Feldman\\n\\n# Related package\\n\\nIt is worth noting that there is a similar package\\n[FTL](https://github.com/robertrueger/ftl) which may be of interest.\\ngFTL was developed independently of FTL, but was not open-sourced in\\ntime to claim the cooler name.\\n\\n\\n## Quick overview of gFTL vs FTL\\n\\nI expect this section to grow a bit more after the authors of the two\\npackages have had time to discuss.  It is highly desired that this\\nsection be factually correct.\\n\\n### Similarities\\n\\n* Both packages use the preprocessor that is built-in to essentially\\n  all modern Fortran compilers.\\n\\n### Differences\\n\\n* Naming conventions for gFTL are much closer to C++ STL.\\n\\n\\n\\n# Request support\\n\\nIf you have any questions, please contact:\\n\\n* Tom Clune  (Tom.Clune@nasa.gov)\\n\\n\\n'},\n",
       " {'repo': 'nasa/HK',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"core Flight System (cFS) Housekeeping Application (HK)\\n======================================================\\n\\nOpen Source Release Readme\\n==========================\\n\\nHK Release 2.4.3\\n\\nDate: 10/8/2020\\n\\nIntroduction\\n-------------\\n  The Housekeeping application (HK) is a core Flight System (cFS) application\\n  that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\n  The HK application is used for building and sending combined telemetry messages\\n  (from individual system applications) to the software bus for routing. Combining\\n  messages is performed in order to minimize downlink telemetry bandwidth.\\n  Combining certain data from multiple messages into one message eliminates the\\n  message headers that would be required if each message was sent individually.\\n  Combined messages are also useful for organizing certain types of data. This\\n  application may be used for data types other than housekeeping telemetry. HK\\n  provides the capability to generate multiple combined packets (a.k.a. output\\n  packets) so that data can be sent at different rates (e.g. a fast, medium and\\n  slow packet).\\n\\n  The HK application is written in C and depends on the cFS Operating System\\n  Abstraction Layer (OSAL) and cFE components.  There is additional HK application\\n  specific configuration information contained in the application user's guide\\n  available in https://github.com/nasa/HK/tree/master/docs/users_guide\\n\\n  This software is licensed under the Apache 2.0 license. \\n\\n\\nSoftware Included\\n------------------\\n\\n  Housekeeping application (HK) 2.4.3\\n\\n\\nSoftware Required\\n------------------\\n\\n Operating System Abstraction Layer 5.0 or higher can be\\n obtained at https://github.com/nasa/osal\\n\\n core Flight Executive 6.8.0 or higher can be obtained at\\n https://github.com/nasa/cfe\\n\\n Note: An integrated bundle including the cFE, OSAL, and PSP can\\n be obtained at https://github.com/nasa/cfs\\n\\nAbout cFS\\n----------\\n  The cFS is a platform and project independent reusable software framework and\\n  set of reusable applications developed by NASA Goddard Space Flight Center.\\n  This framework is used as the basis for the flight software for satellite data\\n  systems and instruments, but can be used on other embedded systems.  More\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\nEOF\\n\"},\n",
       " {'repo': 'nasa/HS',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"core Flight System (cFS) Health and Safety Application (HS) \\n===========================================================\\n\\nOpen Source Release Readme\\n==========================\\n\\nHS Release 2.3.2\\n\\nDate: \\n10/5/2020\\n\\nIntroduction\\n-------------\\n  The Health and Safety application (HS) is a core Flight System (cFS) \\n  application that is a plug in to the Core Flight Executive (cFE) component \\n  of the cFS.  \\n  \\n  The HS application provides functionality for Application Monitoring, \\n  Event Monitoring, Hardware Watchdog Servicing, Execution Counter Reporting\\n  (optional), and CPU Aliveness Indication (via UART). \\n\\n  The HS application is written in C and depends on the cFS Operating System \\n  Abstraction Layer (OSAL) and cFE components. There is additional HS \\n  application specific configuration information contained in the application \\n  user's guide available in: \\n  https://github.com/nasa/HS/tree/master/docs/users_guide\\n  \\n  This software is licensed under the Apache 2.0 license.  \\n \\n \\nSoftware Included\\n------------------\\n  Health and Safety application (HS) 2.3.2\\n  \\n \\nSoftware Required\\n------------------\\n\\n Operating System Abstraction Layer 5.0 or higher can be \\n obtained at https://github.com/nasa/osal\\n\\n core Flight Executive 6.8.0 or higher can be obtained at\\n https://github.com/nasa/cfe\\n\\n Note: An integrated bundle including the cFE, OSAL, and PSP can \\n be obtained at https://github.com/nasa/cfs\\n  \\nAbout cFS\\n-----------\\n  The cFS is a platform and project independent reusable software framework and\\n  set of reusable applications developed by NASA Goddard Space Flight Center.\\n  This framework is used as the basis for the flight software for satellite data\\n  systems and instruments, but can be used on other embedded systems.  More\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n                      \\nEOF                       \\n\"},\n",
       " {'repo': 'nasa/DS',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"core Flight System (cFS) Data Storage Application (DS)\\n======================================================\\n\\nOpen Source Release Readme\\n==========================\\n\\nDS Release 2.5.2\\n\\nDate: 10/5/2020\\n\\nIntroduction\\n-------------\\n  The Data Storage application (DS) is a core Flight System (cFS) application \\n  that is a plug in to the Core Flight Executive (cFE) component of the cFS.  \\n  The DS application is used for storing software bus messages in files. These \\n  files are generally stored on a storage device such as a solid state recorder \\n  but they could be stored on any file system. Another cFS application such as \\n  CFDP (CF) must be used in order to transfer the files created by DS from \\n  their onboard storage location to where they will be viewed and processed.\\n\\n  The DS application is written in C and depends on the cFS Operating System\\n  Abstraction Layer (OSAL) and cFE components.  There is additional DS application\\n  specific configuration information contained in the application user's guide\\n  available in https://github.com/nasa/DS/tree/master/docs/users_guide\\n\\n  This software is licensed under the Apache 2.0 license.\\n\\n\\nSoftware Included\\n------------------\\n\\n  Data Storage application (DS) 2.5.2\\n\\n\\nSoftware Required\\n------------------\\n\\n Operating System Abstraction Layer 5.0 or higher can be\\n obtained at https://github.com/nasa/osal\\n\\n core Flight Executive 6.8.0 or higher can be obtained at\\n https://github.com/nasa/cfe\\n\\n Note: An integrated bundle including the cFE, OSAL, and PSP can\\n be obtained at https://github.com/nasa/cfs\\n\\nAbout cFS\\n----------\\n  The cFS is a platform and project independent reusable software framework and\\n  set of reusable applications developed by NASA Goddard Space Flight Center.\\n  This framework is used as the basis for the flight software for satellite data\\n  systems and instruments, but can be used on other embedded systems.  More\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\nEOF\\n\"},\n",
       " {'repo': 'nasa/MXMCPy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# MXMCPy\\nmain: \\n[![Build Status](https://travis-ci.com/nasa/MXMCPy.svg?branch=main)](https://travis-ci.com/nasa/MXMCPy) \\n[![Coverage Status](https://coveralls.io/repos/github/nasa/MXMCPy/badge.svg?branch=main)](https://coveralls.io/github/nasa/MXMCPy?branch=main) \\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/nasa/MXMCPy.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/MXMCPy/alerts/)\\n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/nasa/MXMCPy.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/nasa/MXMCPy/context:python)\\n\\ndevelop: \\n[![Build Status](https://travis-ci.com/nasa/MXMCPy.svg?branch=develop)](https://travis-ci.com/nasa/MXMCPy) \\n[![Coverage Status](https://coveralls.io/repos/github/nasa/MXMCPy/badge.svg?branch=develop)](https://coveralls.io/github/nasa/MXMCPy?branch=develop) \\n\\n## General\\nMXMCPy is an open source package that implements many existing multi-model \\nMonte Carlo methods (MLMC, MFMC, ACV) for estimating statistics from expensive,\\nhigh-fidelity models by leveraging faster, low-fidelity models for speedup.\\n\\n## Getting Started\\n\\n### Installation\\n\\nMXMCPy can be easily installed using pip:\\n```shell\\npip install mxmcpy\\n```\\n\\nAlternatively, the MXMCPy repository can be cloned:\\n```shell\\ngit clone https://github.com/nasa/mxmcpy.git\\n```\\nand the dependencies can be installed manually as follows. \\n\\n### Dependencies\\nMXMCPy is intended for use with Python 3.x.  MXMCPy requires installation of a \\nfew dependencies which are relatively common for optimization/numerical methods\\nwith Python:\\n  - numpy\\n  - scipy\\n  - pandas\\n  - matplotlib\\n  - h5py\\n  - pytorch\\n  - pytest, pytest-mock (if the testing suite is to be run)\\n  \\nA `requirements.txt` file is included for easy installation of dependencies with \\n`pip` or `conda`.\\n\\nInstallation with pip:\\n```shell\\npip install -r requirements.txt\\n```\\n\\nInstallation with conda:\\n```shell\\nconda install --yes --file requirements.txt\\n```\\n\\n### Documentation\\nSee the [MXMCPy Read the Docs page](https://mxmcpy.readthedocs.io/).\\n\\n## Running Tests\\nAn extensive unit test suite is included with MXMCPy to help ensure proper \\ninstallation. The tests can be run using pytest on the tests directory, e.g., \\nby running:\\n```shell\\npython -m pytest tests \\n```\\nfrom the root directory of the repository.\\n\\n## Example Usage\\n\\nThe following code snippet shows the determination of an optimal sample\\nallocation for three models with assumed costs and covariance matrix using\\nthe MFMC algorithm:\\n\\n```python\\nimport numpy as np\\nfrom mxmc import Optimizer\\n\\nmodel_costs = np.array([1.0, 0.05, 0.001])\\ncovariance_matrix = np.array([[11.531, 11.523, 12.304],\\n                              [11.523, 11.518, 12.350],\\n                              [12.304, 12.350, 14.333]])\\n                             \\noptimizer = Optimizer(model_costs, covariance_matrix)\\nopt_result = optimizer.optimize(algorithm=\"mfmc\", target_cost=1000)\\n\\nprint(\"Optimal variance: \", opt_result.variance)\\nprint(\"# samples per model: \", opt_result.allocation.get_number_of_samples_per_model())\\n```\\n\\nFor more detailed examples using MXMCPy including end-to-end construction of\\nestimators, see the scripts in the [examples directory](examples/) or the \\n[end-to-end example](https://mxmcpy.readthedocs.io/en/main/ishigami_example.html)\\nin the documentation. \\n\\n## Contributing\\n1. Fork it (<https://github.com/nasa/mxmcpy/fork>)\\n2. Create your feature branch (`git checkout -b feature/fooBar`)\\n3. Commit your changes (`git commit -am \\'Add some fooBar\\'`)\\n4. Push to the branch (`git push origin feature/fooBar`)\\n5. Create a new Pull Request\\n\\n## Citing \\n\\nIf you use MXMCPy for your work, please cite the following reference:\\n\\n`\\nBomarito, G. F., Warner, J. E., Leser, P. E., Leser, W. P., and Morrill, L.: Multi Model Monte Carlo with Python (MXMCPy). NASA/TM–2020–220585. 2020.\\n`\\n\\n## Authors\\n  * Geoffrey Bomarito\\n  * James Warner\\n  * Patrick Leser\\n  * William Leser\\n  * Luke Morrill\\n  \\n## License \\n\\nNotices:\\nCopyright 2020 United States Government as represented by the Administrator of \\nthe National Aeronautics and Space Administration. No copyright is claimed in \\nthe United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF \\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED \\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY \\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR \\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR \\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE \\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN \\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, \\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS \\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY \\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, \\nIF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE \\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY \\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY \\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, \\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S \\nUSE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE \\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY \\nPRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR \\nANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS \\nAGREEMENT.\\n\\n\\n'},\n",
       " {'repo': 'nasa/PrognosticsAlgorithmLibrary',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# Prognostics Algorithm Library\\n\\nThe Prognostics Algorithm Library is a suite of algorithms implemented in the MATLAB programming language for model-based prognostics (remaining life computation). It includes algorithms for state estimation and prediction, including uncertainty propagation. The algorithms take as inputs component models developed in Matlab, and perform estimation and prediction functions. The library allows the rapid development of prognostics solutions for given models of components and systems. Different algorithms can be easily swapped to do comparative studies and evaluations of different algorithms to select the best for the application at hand.\\n\\n## Citation\\n\\nPublications making use of software products obtained from this repository are requested to acknowledge the assistance received by using this repository. Please cite: \"M. Daigle; Prognostics Algorithm Library [Computer software]. (2016). Retrieved from https://github.com/nasa/PrognosticsAlgorithmLibrary\".\\n\\n## Installation\\n\\nInstallation can be done in one of two ways. Either (1) use the MATLAB toolbox installer provided in the install folder, which will install the toolbox in your local toolboxes folder and add the folder to your MATLAB path, or (2) copy the source from the MATLAB folder to any desired directory, and add that directory to your MATLAB path. Do not add the subdirectories (the package directories) to your MATLAB path. If the first option is used, then the MATLAB add-on manager can be used to uninstall the package; otherwise, the installation can be removed manually by removing the directory from your MATLAB path and deleting the source.\\n\\n## User Manual\\n\\nInstructions for using the software can be found in the following sources:\\n- [Wiki](https://github.com/nasa/PrognosticsAlgorithmLibrary/wiki)\\n- [User Manual](https://github.com/nasa/PrognosticsAlgorithmLibrary/blob/master/docs/PrognosticsAlgorithmLibrary-UserManual.pdf)\\n\\n## Dependencies\\n\\nSome modules included in this library are dependent on the [Prognostics Model Library](https://github.com/nasa/PrognosticsModelLibrary).\\n\\n## Compatibility\\n\\nThe PrognosticsModelLibrary has been tested with Matlab R2016a, but should work with older versions, down to at least R2012a.\\n\\n## Contributions\\n\\nAll contributions are welcome. Issues may be opened using GitHub. To contribute directly, open a pull request against the \"develop\" branch. Pull requests will be evaluated and integrated into the next official release.\\n\\n## License\\n\\nThis software is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/PrognosticsAlgorithmLibrary/blob/master/LICENSE.pdf).\\n\\n## Notices\\n\\nCopyright ©\\xa02016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.\\xa0 No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\n### Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED\\xa0\"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\xa0 FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT\\xa0\"AS IS.\"\\n\\nWaiver and Indemnity:\\xa0 RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.\\xa0 IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.\\xa0 RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/PrognosticsModelLibrary',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# Prognostics Model Library\\n\\nThe Prognostics Model Library is a modeling framework focused on defining and building models for prognostics (computation of remaining useful life) of engineering systems, and provides a set of prognostics models for select components developed within this framework, suitable for use in prognostics applications for these components. The library currently includes models for valves, pumps, and batteries. The Prognostics Model Library is implemented in MATLAB. The implementation consists of a set of utilities for defining a model (specifying variables, parameters, and equations), simulating the model, and embedding it within common model-based prognostics algorithms. A user can use existing models within the library or construct new models with the provided framework.\\n\\n## Citation\\n\\nPublications making use of software products obtained from this repository are requested to acknowledge the assistance received by using this repository. Please cite: \"M. Daigle; Prognostics Model Library [Computer software]. (2016). Retrieved from https://github.com/nasa/PrognosticsModelLibrary\".\\n\\n## Installation\\n\\nInstallation can be done in one of two ways. Either (1) use the MATLAB toolbox installer provided in the install folder, which will install the toolbox in your local toolboxes folder and add the folder to your MATLAB path, or (2) copy the source from the MATLAB folder to any desired directory, and add that directory to your MATLAB path. Do not add the subdirectories (the package directories) to your MATLAB path. If the first option is used, then the MATLAB add-on manager can be used to uninstall the package; otherwise, the installation can be removed manually by removing the directory from your MATLAB path and deleting the source.\\n\\n## User Manual\\n\\nInstructions for using the software can be found in the following sources:\\n- [Wiki](https://github.com/nasa/PrognosticsModelLibrary/wiki)\\n- [User Manual](https://github.com/nasa/PrognosticsModelLibrary/blob/master/docs/PrognosticsModelLibrary-UserManual.pdf)\\n\\n## Compatibility\\n\\nThe PrognosticsModelLibrary has been tested with Matlab R2016a, but should work with older versions, down to at least R2012a.\\n\\n## Contributions\\n\\nAll contributions are welcome. Issues may be opened using GitHub. To contribute directly, open a pull request against the \"develop\" branch. Pull requests will be evaluated and integrated into the next official release.\\n\\n## License\\n\\nThis software is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/PrognosticsModelLibrary/blob/master/LICENSE.pdf).\\n\\n## Notices\\n\\nCopyright ©\\xa02016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.\\xa0 No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\n### Disclaimers\\u2028\\u2028\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED\\xa0\"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\xa0 FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT\\xa0\"AS IS.\"\\n\\nWaiver and Indemnity:\\xa0 RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.\\xa0 IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.\\xa0 RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/spaceapps-phenomena_detection',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': '# Phenomena Detection Challenge Resources\\n\\nThis repository provides a list of resources for the phenomena detection challenge, including satellite imagery, labeled data, labeling tools, and example code for imagery downloading, processing, and machine learning. <br>\\n\\nSelected pre-labeled data is available in the [labeled folder](data/labeled/), or participants may choose to use the resources in the [raw folder](data/raw/) to access a broader range of data and label it themselves or with our included tool ImageLabler. <br>\\n\\nThe [examples](examples/) folder contains example scripts for machine learning, as well as downloading satellite imagery.<br>\\n\\nBest of luck with the challenge! If you need clarification on challenge details please see contact information in the [contacts](contacts/) folder.\\n'},\n",
       " {'repo': 'nasa/World-Wind-Java',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': \"$Id$\\n\\nThis file explains the organization of the World Wind Subversion repository's trunk directories, and briefly outlines their contents.\\n\\ntrunk/WorldWind\\nThe 'WorldWind' folder contains the World Wind Java SDK project. Many resources are available at http://oneworldwind.org to help you understand and use World Wind. Key files and folders in the World Wind Java SDK:\\n- build.xml: Apache ANT build file for the World Wind Java SDK.\\n- src: Contains all Java source files for the World Wind Java SDK, except the World Wind WMS Server.\\n- server: Contains the World Wind WMS Server Java source files, build file, and deployment files.\\n- lib-external/gdal: Contains the GDAL native binaries libraries that may optionally be distributed with World Wind.\\n\\ntrunk/WWAndroid\\nthe 'WWAndroid' folder contains the World Wind Android SDK project. Many resource are available at http://oneworldwind.org/android to help you understand and use World Wind on Android. Key files and folders in the World Wind Android SDK:\\n- build.xml: Apache ANT build file for the World Wind Android SDK.\\n- src: Contains all Java source files for the World Wind Android SDK.\\n- examples: Contains example applications that use the World Wind Android SDK.\\n\\ntrunk/GDAL\\nThe 'GDAL' folder contains the GDAL native library project. This project produces the GDAL native libraries used by the World Wind Java SDK (see WorldWind/lib-external/gdal). The GDAL native library project contains detailed instructions for building the GDAL native libraries on the three supported platforms: Linux, Mac OS X, and Windows.\\n\"},\n",
       " {'repo': 'nasa/icc',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# icc-dev\\n\\nContains MATLAB simulation and optimization code for spacecraft orbiting small Solar System Bodies.\\n\\n## Quickstart\\n\\nThe examples folder contains example _main_ scripts, which demonstrate usage of the provided modules. The scripts should be executed from within this folder.\\n\\n___\\n\\n## Requirements\\n\\n- [Mosek](https://www.mosek.com/) or [CPLEX](https://www.ibm.com/analytics/cplex-optimizer). Both Mosek and CPLEX provide free academic versions. If neither MOSEK nor CPLEX are available, it is possible to fall back to MATLAB\\'s built-in `linprog` and `intlinprog`, but performance will be compromised.\\n- JPL\\'s [Small Body Dynamics Toolkit (SBDT)](https://engineering.purdue.edu/people/kathleen.howell.1/Publications/Conferences/2015_AAS_SBDT.pdf). SBDT is not open-source. A license can be requested from [download.jpl.nasa.gov](download.jpl.nasa.gov) mentioning NTR-49005. \\n- NASA\\'s [SPICE](https://naif.jpl.nasa.gov/naif/aboutspice.html) MATLAB toolkit (MICE), available at [this link](https://naif.jpl.nasa.gov/naif/toolkit_MATLAB.html).\\n- In order to automatically download SPICE data, the [Expect/TCL](https://en.wikipedia.org/wiki/Expect) scripting language should be available. On Debian-based systems, you can `sudo apt-get install expect`. On MacOS, `brew install expect` with [Homebrew](https://brew.sh/). On Windows, either use the Windows Subsystem for Linux, or manually download SPICE data as discussed below..\\n\\n### Setup\\n\\nSet the following environment variables:\\n\\n- `SBDT_PATH` should point to the folder where SBDT is installed.\\n- `NAIF_PATH` should point to the folder where NAIF data will be stored. This is typically the subfolder `utilities/spice` inside this repository.\\n\\nRun the script `utilities/spice/download_SPICE_kernel.sh` to download required data from NASA\\'s SPICE orbit database. If Expect/TCL is unavailable, manually download the orbit data for 433 EROS from [Horizons](https://ssd.jpl.nasa.gov/horizons.cgi) and save it as `${NAIF_PATH}/naif/generic_kernels/spk/asteroids/a433.bsp`.\\n\\nTo set environment variables:\\n\\n- On Linux-based systems, add the line `export SBDT_PATH=\"path/to/your/copy/of/sbdt\"` and `export NAIF_PATH=\"path/to/your/copy/of/naif\"` to `~/.bashrc`. If you use a different shell (e.g. zsh), add the lines to the appropriate `.*rc` file.\\n- On MacOS, follow [these instructions](https://www.mathworks.com/matlabcentral/answers/170268-how-do-i-set-environment-variables-on-mac-os-x#answer_165094).\\n  > **WARNING** On MacOS, setting environment variables in `~/.bashrc` or `~/.zshrc` will _not_ affect MATLAB unless MATLAB is launched from the terminal.\\n- On Windows, follow [these instructions](https://www.architectryan.com/2018/08/31/how-to-change-environment-variables-on-windows-10/).\\n\\nApply the patches in `utilities/sbdt` to the files `harmonic_gravity.c` and `harmonic_gravity_complex_3.cpp` in SBDT and recompile the corresponding MEX files.\\n___\\n\\n## Modules\\n\\n### small_body_dynamics\\n\\nIntegrators to simulate the orbital dynamics of a spacecraft in orbit around a small body.\\n\\n### network_flow_communication_optimizer\\n\\nOptimize the communication flows between spacecraft, given a set of orbits, data production, and a communication model.\\n\\n### relay_orbit_optimizer\\n\\nOptimize the orbit of relay spacecraft, given the orbits of science spacecraft, their data production, and the orbit of the carrier.\\n\\n### observed_points_optimizer\\n\\nOptimize the observation of spacecraft, given the orbits of science spacecraft.\\n\\n### monte_carlo_coverage_optimizer\\n\\nRuns a monte carlo to select spacecraft orbits. Calls the observed points optimizer to calculate the coverage reward of each trial orbit set. \\n\\n### utilities\\n\\nCommon utilities and supporting functions for icc simulation and analysis.\\n\\n### media\\n\\nVarious media.\\n\\n### Structure of Optimization Modules\\n\\nThe optimizer modules are composed of a _main_ function (is called outside the module) and several supporting functions which should only be called from within the module. Functions supporting multiple modules are placed in _utilities_.\\n\\n___\\n\\n## Interfaces\\n\\nThe system is defined through two special classes, which serve as the primary interfaces (inputs and outputs) between the modules. \\n\\n1. _SpacecraftSwarm_: Defines the swarm of spacecraft.\\n2. _SphericalHarmonicsGravityIntegrator_SBDT_: Defines the small body that the spacecraft orbit around and integrates trajectories using JPL\\'s Small Body Dynamics Integrator (SBDT).\\n\\nSee the _examples_ directory for usage examples.\\n\\n___\\n\\n## Troubleshooting\\n\\n### Troubleshooting SBDT\\n\\nSBDT is typically distributed as a `.zip` file. The archive contains tests in the `Tests` folder and examples in the `Demos` folder.\\n\\nCore SBDT functions are implemented in C and must be compiled to MATLAB _mex_ files to enable MATLAB to call them. The SBDT distribution contains pre-compiled MEX files for Windows, MacOS, and Linux inside the folder `CompiledMEX`.\\nIf you encounter issues trying to run SBDT, it may be advisable to recompile the MEX files from scratch. To do this,\\n\\n- Navigate to the SBDT root folder.\\n- In MATLAB, run `compileMexSBDT()`. A number of files will be generated in the folder. The file extension is `mexw64` on Windows, `mexa64` on Linux, and `mexmaci64` on MacOS.\\n- Copy the newly generated mex files to the CompiledMEX folder, overwriting the previous ones.\\n\\n### Missing function `setstructfields`\\n\\nSBDT internally makes use of the MATLAB function `setstructfields`, which\\nis provided by the Signal Processing toolbox.\\nIf you do not have access to the Signal Processing toolbox, the repository\\ncontains [a clean-room reimplementation](utilities/misc/setstructfields_ICC.m)\\nof the function (based on [this description](https://stackoverflow.com/questions/38645/what-are-some-efficient-ways-to-combine-two-structures-in-matlab/23767610#23767610)).\\nTo use it, rename the file from `setstructfields_ICC.m` to `setstructfields.m`\\nand ensure that it is in a folder on your MATLAB PATH.\\n\\n### Warning: P-file is older than M-file\\n\\nDepending on the SBDT distribution in use, you may receive the warning\\n\\n```matlab\\nWarning: P-file $PATH/TO/FILE.p is older than M-file $PATH/TO/FILE.m.\\n$PATH/TO/FILE.p may be obsolete and may need to be regenerated.\\nType \"help pcode\" for information about generating P-files from M-files.\\n\\n```\\n\\nYou can get rid of this warning by navigating to the SBDT folder in the terminal and running\\n\\n```bash\\nfind . -type f -name \"*.p\" -exec touch {} +\\n```\\n\\nThis will reset the last-modified time of all .p files to the current date and time.\\n\\n### Detailed instructions on how to set environment variables\\n\\n#### MacOS\\n\\n- Create a file named `environment_variables.plist` in `~/Library/LaunchAgents/` (the file name is immaterial, but the extension should be `.plist`).\\n\\n- Edit the file to contain the following text:\\n```xml\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n <!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\\n   <plist version=\"1.0\">\\n   <dict>\\n   <key>Label</key>\\n   <string>Set environment variables that will be picked up by Spotlight</string>\\n   <key>ProgramArguments</key>\\n   <array>\\n     <string>/bin/launchctl</string>\\n     <string>setenv</string>\\n     <string>SBDT_PATH</string>\\n     <string>/Users/frossi/Documents/JPL/ICC/SBDT</string>\\n     <string>NAIF_PATH</string>\\n     <string>/Users/frossi/Documents/JPL/ICC/icc-dev/utilities/spice/</string>\\n   </array>\\n   <key>RunAtLoad</key>\\n   <true/>\\n </dict>\\n </plist>\\n```\\n  Ensure that the SBDT and NAIF paths are set to the proper path on your machine.\\n- Run \\n```\\nlaunchctl load ~/Library/LaunchAgents/environment_variables.plist\\nlaunchctl start ~/Library/LaunchAgents/environment_variables.plist\\n```'},\n",
       " {'repo': 'nasa/bingocpp',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# BingoCpp #\\n\\n![Bingo Logo](media/logo.png)\\n\\nmaster: [![Build Status](https://travis-ci.com/nasa/bingocpp.svg?branch=master)](https://travis-ci.com/nasa/bingocpp) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingocpp/badge.svg?branch=master)](https://coveralls.io/github/nasa/bingocpp?branch=master)\\n\\ndevelop: [![Build Status](https://travis-ci.com/nasa/bingocpp.svg?branch=develop)](https://travis-ci.com/nasa/bingocpp) [![Coverage Status](https://coveralls.io/repos/github/nasa/bingocpp/badge.svg?branch=develop)](https://coveralls.io/github/nasa/bingocpp?branch=develop) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/ccd11c4092544eaca355722cea87272e)](https://www.codacy.com/app/bingo_developers/bingocpp?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=nasa/bingocpp&amp;utm_campaign=Badge_Grade)\\n\\n## General ##\\n\\nBingoCpp is part of the open source package Bingo for performing symbolic\\nregression.  BingoCpp contains the c++ implementation of a portion of the code\\nwithin bingo. \\n\\n## Getting Started ##\\n\\n### Cloning ###\\n\\nBingoCpp has 3 submodules: eigen, google test, and pybind.  To clone this\\nrepository and include the submodules, run the following command:\\n\\n```bash\\ngit clone --recurse-submodules https://github.com/nasa/bingocpp\\n```\\n\\n### Installation/Compiling with CMake ###\\n\\nInstalling from source requires git and a recent version of\\n[cmake](https://cmake.org/).\\n\\nInstallation can be performed using the typical out-of-source build flow:\\n\\n```bash\\nmkdir <path_to_source_dir>/build\\ncd <path_to_source_dir>/build\\ncmake .. -DCMAKE_BUILD_TYPE=Release\\nmake\\n```\\n\\n### Python Bindings ###\\n\\nThe python bindings that are needed for integration with bingo can be made by\\nrunning the following commend from the build directory:\\n\\n```bash\\nmake bingocpp\\n```\\n\\nA common error in the build of the python bindings is that the build must be\\nuse the same version of python that will run your bingo scripts.  Pybind\\nusually finds the default python on your machine during build, so the easiest\\nway to ensure consistent python versioning is to build bingocpp in a Python 3\\nvirtual environment.\\n\\n### Documentation ###\\n\\nSphynx is used for automatically generating API documentation for bingo. The\\nmost recent build of the documentation can be found in the repository at:\\ndoc/_build/html/index.htm\\n\\n## Running Tests ##\\n\\nSeveral unit and integration tests can be performed upon building, to ensure a\\nproper install.  The test suite can be started by running the following command\\nfrom the build directory:\\n\\n```bash\\nmake gtest\\n```\\n\\n## Usage Example ##\\n\\nTODO\\n\\n## Contributing ##\\n\\n1. Fork it (<https://github.com/nasa/bingo/fork>)\\n2. Create your feature branch (`git checkout -b feature/fooBar`)\\n3. Commit your changes (`git commit -am \\'Add some fooBar\\'`)\\n4. Push to the branch (`git push origin feature/fooBar`)\\n5. Create a new Pull Request\\n\\n## Versioning ##\\n\\nWe use [SemVer](http://semver.org/) for versioning. For the versions available,\\nsee the [tags on this repository](https://github.com/nasa/bingocpp/tags).\\n\\n## Authors ##\\n\\n* Geoffrey Bomarito\\n* Ethan Adams\\n* Tyler Townsend\\n  \\n## License ##\\n\\nCopyright 2018 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration. No copyright is claimed in\\nthe United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\nThe Bingo Mini-app framework is licensed under the Apache License, Version 2.0\\n(the \"License\"); you may not use this application except in compliance with the\\nLicense. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0 .\\n\\nUnless required by applicable law or agreed to in writing, software distributed \\nunder the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR \\nCONDITIONS OF ANY KIND, either express or implied. See the License for the \\nspecific language governing permissions and limitations under the License.\\n'},\n",
       " {'repo': 'nasa/stol-mode',\n",
       "  'language': 'Emacs Lisp',\n",
       "  'readme_contents': '**stol-mode** - Emacs major mode for Systems Test and Operations Language (STOL)\\n==========================================================\\n\\nWritten for use with STOL as used in [Integrated Test and Operations System\\n(ITOS)](https://itos.gsfc.nasa.gov/) 9.4.0.\\n\\n## Features ##\\n* Keyword, directive, and constant highlighting\\n\\n## Goals ##\\n* Indentation\\n* Insertion of double semi-colon (;;) when lines automatically break\\n\\n## Usage ##\\nSee the [Lisp Libraries page in the GNU Emacs\\nmanual](https://www.gnu.org/software/emacs/manual/html_node/emacs/Lisp-Libraries.html) for information on how\\nto get Emacs to load this library. Once loaded, the `M-x stol-mode` command activates stol-mode.\\n\\n## Notices\\n\\nCopyright © 2020 United States Government as represented by the Administrator of the National Aeronautics and\\nSpace Administration.  No copyright is claimed in the United States under Title 17, U.S. Code. All Other\\nRights Reserved.\\n\\n### Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED,\\nIMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO\\nSPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION,\\nIF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE\\nPRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\nAND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS\\nCONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE\\nRESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES\\nFROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY\\nAND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE\\nIMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/common-mapping-client',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': \"## Welcome to the Common Mapping Client!\\n\\n[![CircleCI](https://circleci.com/gh/nasa/common-mapping-client.svg?style=shield)](https://circleci.com/gh/nasa/common-mapping-client)\\n[![Dependencies Status](https://david-dm.org/nasa/common-mapping-client/status.svg)](https://david-dm.org/nasa/common-mapping-client)\\n[![Coverage Status](https://coveralls.io/repos/github/nasa/common-mapping-client/badge.svg?branch=master)](https://coveralls.io/github/nasa/common-mapping-client?branch=master)\\n[![license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0)\\n\\n\\n[![Preview](https://raw.githubusercontent.com/nasa/common-mapping-client/master/docs/core-docs/resources/screenshot_core.jpg)](https://nasa.github.io/common-mapping-client/branches/master/)\\n\\n### Overview\\nThe Common Mapping Client (CMC) is a foundation for web-based mapping applications that\\nleverages, combines, and abstracts certain commonly used mapping functionalities,\\nenabling developers to spend less time reinventing the wheel and more time solving\\ntheir specific problems. Sitting somewhere between a starter-kit and a framework,\\nCMC aims fulfill the basic needs of a mapping application without getting in the\\nway of any given project's unique goals.\\n\\nOver the years, there have been many projects that try to solve the same issue:\\nput data on a map and explore it. Unfortunately, there is an increasingly steep\\nhill to climb when it comes to actually starting one of these projects. All of\\nthem have to decide: Which framework should I start from? Which library\\nwill give me the features I need? How to I glue all these together with \\na code layout that I won't want to burn next week? CMC solves this by bundling\\ntogether a solid and modular base framework with robust mapping libraries,\\na well thought out structure, and a thick shield against feature-creep\\nto let you start building the cool stuff faster.\\n\\nWe maintain a list of further example projects and projects that began from CMC\\nso that you can easily get examples of building intricate and detailed workflows\\non top of this tool.\\n\\nView our [live demo](https://nasa.github.io/common-mapping-client/branches/master/).\\n\\n### Features\\n* 2D/3D Map library abstraction\\n* Map layer controls\\n* Map layer ingestion and merging (from JSON and WMTS capabilities XML)\\n* Highly customizable user analytics\\n* Configurable map projection\\n* Basic vector layer support\\n* Map extent synchronization across all maps (2D and 3D)\\n* Geodesic map geometry synchronization across all maps (2D and 3D)\\n* Global time widget, and interactive timeslider\\n* Adjustable map layer cache\\n* Shareable/loadable application state via url parameters with share widget (facebook, twitter, email, google plus)\\n* Core set of UI elements necessary for most basic applications\\n* Basemap switching\\n* Basic shape drawing tools in 2D and 3D\\n* Basic geodesic measurement (distance and area) tools in 2D and 3D\\n* Display help documentation from markdown files\\n* A preconfigured testing framework\\n* A preconfigured build process\\n* Handy development tools\\n  * Hot reloading\\n  * Local dev and production node servers\\n  * BrowserSync\\n\\n### Quickstart\\n\\n##### Install\\n1. Install `NodeJS`\\n2. Get the code\\n   1. Option A: Grab a tag that suits you\\n   2. Option B: Fork the repo into your new project and get the cutting edge goodness\\n   3. Option C: Clone the repo, contribute back, and get the cutting edge goodness\\n3. `npm install`: install node dependencies\\n4. `npm start`: build and server development code bundle\\n5. Start building.\\n\\n##### Build\\n* `npm run build`: build production code bundle\\n* `npm run open:dist`: serve production code bundle\\n\\n##### Test\\n* `npm run test`: Single run of tests\\n* flags\\n  * `--includecoretests`: Include cmc-core tests as well\\n  * `--nowebgl`: Run tests in phantomJS and skip tests that require webgl\\n  * `--watch`: Run tests with code watch\\n\\n### Contributing to the Project\\n\\n*Contributing Directly to CMC-Core*\\n\\nYou are more than welcome to create forks to fix bugs, add features or\\nenhancements or otherwise improve the tool. Please submit these changes through\\npull-requests for review.\\n\\n*Contributing to the CMC Ecosystem*\\n\\nIf you use CMC for your own project, please let us know so that we may list it\\nunder our [Example Projects](docs/core-docs/EXAMPLE_PROJECTS.md) for others to find inspiration in.\\n\\nIf you create a particularly useful or robust widget in your own CMC descendant,\\nplease create an example project demonstrating just that widget so that others\\nwho have the same problem down the road may find a solution faster.\\n\\n### Documentation Shortcut\\n\\n* [Walkthrough](https://github.com/AaronPlave/common-mapping-client-walkthrough)\\n* [Developer Guide](docs/core-docs/DEVELOPER_MANUAL.md)\\n* [Example Projects](docs/core-docs/EXAMPLE_PROJECTS.md)\\n\"},\n",
       " {'repo': 'nasa/ccmc-swpc-cat-web',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# ccmc-swpc-cat-web\\n\\n\\nccmc-swpc-cat-web is a new CCMC web implementation of the NOAA\\'s SWPC_CAT IDL program.  \\n\\nThe project page is hosted by the Community Coordinated Modeling Center (CCMC) at NASA Goddard Space Flight Center.\\n\\nOfficial site page https://ccmc.gsfc.nasa.gov/swpc_cat_web/\\n\\nccmc-swpc-cat-web source code is hosted on github under a permissive NASA open source license:\\n\\nhttps://github.com/nasa/ccmc-swpc-cat-web/\\n\\n## INSTALLATION\\nccmc-swpc-cat-web is a plotly-dash based web application, the following steps will start a python simple server:\\n\\n### prerequisites:\\n```\\npython3.7 already installed\\nvirtualenv already installed\\n``` \\n\\n### 1. create venv using python 3.7\\n```\\nNOTE: \"/usr/bin/python3.7\" is an example path, you might need to change this path\\nvirtualenv -p /usr/bin/python3.7 venv\\n```\\n\\n### 2. install requirements.txt\\n```\\nsource venv/bin/activate\\npip3 install -r requirements.txt \\n```\\n\\n### 3. start server\\n```\\nsource venv/bin/activate\\npython __SWPC_CAT__.py\\nVisit: http://127.0.0.1:8050/ in a web browser (preferably chrome)\\n'},\n",
       " {'repo': 'nasa/cumulus-message-adapter-python',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# cumulus-message-adapter-python\\n\\n[![CircleCI]](https://circleci.com/gh/nasa/cumulus-message-adapter-python)\\n[![PyPI version]](https://badge.fury.io/py/cumulus-message-adapter-python)\\n\\n## What is Cumulus?\\n\\nCumulus is a cloud-based data ingest, archive, distribution and management\\nprototype for NASA\\'s future Earth science data streams.\\n\\nRead the [Cumulus Documentation]\\n\\n## What is the Cumulus Message Adapter?\\n\\nThe Cumulus Message Adapter is a library that adapts incoming messages in the\\nCumulus protocol to a format more easily consumable by Cumulus tasks, invokes\\nthe tasks, and then adapts their response back to the Cumulus message protocol\\nto be sent to the next task.\\n\\n## Installation\\n\\n```plain\\npip install cumulus-message-adapter-python\\n```\\n\\n## Task definition\\n\\nIn order to use the Cumulus Message Adapter, you will need to create two\\nmethods in your task module: a handler function and a business logic function.\\n\\nThe handler function is a standard Lambda handler function which takes two\\nparameters (as specified by AWS): `event` and `context`.\\n\\nThe business logic function is where the actual work of your task occurs. It\\nshould take two parameters: `event` and `context`.\\n\\nThe `event` object contains two keys:\\n\\n* `input` - the task\\'s input, typically the `payload` of the message, produced\\n  at runtime\\n* `config` - the task\\'s configuration, with any templated variables resolved\\n\\nThe `context` parameter is the standard Lambda context as passed by AWS.\\n\\nThe return value of the business logic function will be placed in the\\n`payload` of the resulting Cumulus message.\\n\\nExpectations for input, config, and return values are all defined by the task,\\nand should be well documented. Tasks should thoughtfully consider their inputs\\nand return values, as breaking changes may have cascading effects on tasks\\nthroughout a workflow. Configuration changes are slightly less impactful, but\\nmust be communicated to those using the task.\\n\\n## Cumulus Message Adapter interface\\n\\nThe Cumulus Message adapter for python provides one method:\\n`run_cumulus_task`. It takes four parameters:\\n\\n* `task_function` - the function containing your business logic (as described\\n  above)\\n* `cumulus_message` - the event passed by Lambda, and should be a Cumulus\\n  Message, *or* a CMA parameter encapsulated message (see [Cumulus Workflow\\n  Documentation](https://nasa.github.io/cumulus/docs/workflows/input_output)):\\n\\n  ```json\\n  {\\n     \"cma\": {\\n        \"event\": \"<cumulus message object>\",\\n        \"SomeCMAConfigKey\": \"CMA configuration object>\"\\n     }\\n  }\\n  ```\\n\\n* `context` - the Lambda context\\n* `schemas` - optional: a dict with `input`, `config`, and `output` properties.\\n  Each should be a string set to the filepath of the corresponding JSON schema\\n  file. All three properties of this dict are optional. If ommitted, the message\\n  adapter will look in `/<task_root>/schemas/<schema_type>.json`, and if not\\n  found there, will be ignored.\\n* `taskargs` - Optional. Additional keyword arguments for the `task_function`\\n\\n## Example\\n\\nSimple example of using this package\\'s `run_cumulus_task` function as a wrapper\\naround another function:\\n\\n```python\\nfrom run_cumulus_task import run_cumulus_task\\n\\n# simple task that returns the event\\ndef task(event, context):\\n    return event\\n\\n# handler that is provided to aws lambda\\ndef handler(event, context):\\n    return run_cumulus_task(task, event, context)\\n```\\n\\nFor a full example see the [example folder](./example).\\n\\n## Creating a deployment package\\n\\nTasks that use this library are just standard AWS Lambda tasks. See\\n[creating release packages].\\n\\n## Usage in a Cumulus Deployment\\n\\nFor documenation on how to utilize this package in a Cumulus Deployment, view\\nthe [Cumulus Workflow Documenation].\\n\\n## Development\\n\\n### Dependency Installation\\n\\n```plain\\n$ pip install -r requirements-dev.txt\\n$ pip install -r requirements.txt\\n```\\n\\n### Logging with `CumulusLogger`\\n\\nIncluded in this package is the `cumulus_logger` which contains a logging class\\n`CumulusLogger` that standardizes the log format for Cumulus. Methods are\\nprovided to log error, fatal, warning, debug, info, and trace.\\n\\n**Import the `CumulusLogger` class:**\\n\\n```python\\nfrom cumulus_logger import CumulusLogger\\n```\\n\\n**Instantiate the logger inside the task definition (name and level are\\noptional):**\\n\\n```python\\nlogger = CumulusLogger(event, context)\\n```\\n\\n**Use the logging methods for different levels:**\\n\\n```python\\nlogger.trace(\\'<your message>\\')\\nlogger.debug(\\'<your message>\\')\\nlogger.info(\\'<your message>\\')\\nlogger.warn(\\'<your message>\\')\\nlogger.error(\\'<your message>\\')\\nlogger.fatal(\\'<your message>\\')\\n```\\n\\n**It can also take additional non-keyword and keyword arguments as in Python\\nLogger.**\\n\\nThe `msg` is the message format string, the `args` and `kwargs` are the\\narguments for string formatting.\\n\\nIf `exc_info` in `kwargs` is not `False`, the exception information in the\\n`exc_info` or `sys.exc_info()` is added to the message.\\n\\n```python\\nlogger.debug(msg, *args, **kwargs)\\n```\\n\\n**Example usage:**\\n\\n```python\\nimport os\\nimport sys\\n\\nfrom run_cumulus_task import run_cumulus_task\\nfrom cumulus_logger import CumulusLogger\\n\\n# instantiate CumulusLogger\\nlogger = CumulusLogger()\\n\\ndef task(event, context):\\n    logger.info(\\'task executed\\')\\n\\n    # log error when an exception is caught\\n    logger.error(\"task formatted message {} exc_info \", \"bar\", exc_info=True)\\n\\n    # return the output of the task\\n    return { \"example\": \"output\" }\\n\\ndef handler(event, context):\\n    # make sure event & context metadata is set in the logger\\n    logger.setMetadata(event, context)\\n    return run_cumulus_task(task, event, context)\\n```\\n\\n### Running Tests\\n\\nRunning tests requires [localstack](https://github.com/localstack/localstack).\\n\\nTests only require localstack running S3, which can be initiated with the\\nfollowing command:\\n\\n```plain\\n$ SERVICES=s3 localstack start\\n```\\n\\nAnd then you can check tests pass with the following nosetests command:\\n\\n```plain\\n$ CUMULUS_ENV=testing nosetests -v -s --with-doctest\\n```\\n\\n### Linting\\n\\n```plain\\n$ pylint run_cumulus_task.py\\n```\\n\\n## Why?\\n\\nThis approach has a few major advantages:\\n\\n1. It explicitly prevents tasks from making assumptions about data structures\\n   like `meta` and `cumulus_meta` that are owned internally and may therefore\\n   be broken in future updates. To gain access to fields in these structures,\\n   tasks must be passed the data explicitly in the workflow configuration.\\n1. It provides clearer ownership of the various data structures. Operators own\\n   `meta`. Cumulus owns `cumulus_meta`. Tasks define their own `config`,\\n   `input`, and `output` formats.\\n1. The Cumulus Message Adapter greatly simplifies running Lambda functions not\\n   explicitly created for Cumulus.\\n1. The approach greatly simplifies testing for tasks, as tasks don\\'t need to\\n   set up cumbersome structures to emulate the message protocol and can just\\n   test their business function.\\n\\n## License\\n\\n[Apache 2.0](LICENSE)\\n\\n[circleci]:\\n  https://circleci.com/gh/nasa/cumulus-message-adapter-python.svg?style=svg\\n[pypi version]:\\n  https://badge.fury.io/py/cumulus-message-adapter-python.svg\\n[Cumulus Documentation]:\\n  https://nasa.github.io/cumulus/\\n[creating release packages]:\\n  https://docs.aws.amazon.com/lambda/latest/dg/deployment-package-v2.html\\n[cumulus workflow documenation]:\\n  https://nasa.github.io/cumulus/docs/workflows/input_output\\n'},\n",
       " {'repo': 'nasa/ECI',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"# Overview\\n\\nThe cFS (Core Flight System) ECI (External Code Interface) is a software abstraction layer which allows the interfacing of externally-generated task/mission-specific code to the cFS via a generic set of wrapper code. The ECI enables direct integration of existing or autogenerated code without the need for hand-written interface code and allows access to cFS API's including table services, time services, the software bus, event services, and fault reporting. \\n\\nThe ECI accomplishes this by compiling with a interface definition (defined as a header file) which contains the data structures needed to manage the CFS interfaces for the external code. The result of compiling the ECI, the interface header file, and the external code is a full CFS application. This process eliminates the need for hand edits to integrate generated code, allowing quicker integration of code and reducing the probability of human error in the integration process.\\n\\n# Getting Started\\n\\nThe ECI has been used with CFE 6.5 on various missions and is tested against the CFE located [here](https://github.com/nasa/cFE). There are no known incompatibilities with older (or newer) versions of the CFE, but they have not been tested.\\n\\n# Status\\n\\n[![build passing](https://travis-ci.com/nasa/ECI.svg?branch=master)](https://travis-ci.com/nasa/ECI/)\\n\\n# Feedback\\n\\nPlease submit bug reports and feature requests via Github issues. Feature requests will be considered as they align with the requirements of NASA missions which are using the ECI.\"},\n",
       " {'repo': 'nasa/PrognosticsMetricsLibrary',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '# PrognosticsMetricsLibrary\\n\\nThis library provides MATLAB software that calculates prognostics metrics. The package is titled “+PrognosticsMetrics”, and a standalone MATLAB file “Tester.m” illustrates how to use the package. This tester creates dummy-output of a prognostic algorithm and explains how the software can be used to compute the performance metrics.\\n\\n## Prognostics\\nAfter diagnosing the faults of a system, performing Prognostics is useful for determining the evolution of these faults. Analysis of fault development is critical when predicting the Remaining Useful Life (RUL) of a system and setting criteria for landmarks or thresholds in the future. Whether data-driven or physics-based, models are utilized as representations of a system for Prognostic algorithms to be performed.\\n\\n## Prognostic Metrics\\nPrognostic algorithms are implemented in order to determine the Remaining Useful Life (RUL) of a system.  Prognostic Metrics are incorporated into the health management decision making process by analyzing the performance of these algorithms.  Establishing standard methods for performance assessment is essential in uniformly comparing certain aptitudes or measures across several algorithms.  Statistical evaluation methods are often advantageous when dealing with large datasets.\\n\\nSome sources of error that could be associated with a system include model inaccuracy, data noise, observer faults, etc. Additional considerations when determining which Prognostic Metrics to adopt are logistics, saftey, reliability, mission criticality, and economic viability (estimating cost savings expected from prognostic algorithm deployment).\\n\\n## Prognostic Framework Terminology\\n* PDF = Probability Density Function\\n* UUT = Unit Under Test\\n* PA = Prognostic Algorithm\\n* PHM = Prognostic Health Management\\n* RUL = Remaining Useful Life\\n* EoL = End of Life\\n* EoP = End of Prediction, time index of last prediction before EoL\\n* EoUP = End of Useful Predictions, time index where RUL is no longer useful to update\\n* HI = Health Index\\n* PoF = Probability of Failure\\n* FT = Failure Threshold, UUT is no longer usable\\n* RtF = Run to Failure, allow system to fail\\n* ROI = Return On Investment\\n* F = time index when fault becomes detectable\\n* D = time index when fault is detected\\n* P = time index of first prognostic prediction\\n* PH = Prognostic Horizon\\n\\n\\n## Inputs to the Package\\n\\nThe present version of metrics considers only the case when the algorithm predictions are described in terms of samples. Different predictions are available at different time instants. Let “t” denote the number of such time instants for which the algorithm’s predictions are available. At each such time instant, “N” samples of predictions (both EOL – End of Life, and RUL – Remaining Useful Life) are available. The samples also have weights associated with them; it is important to specify weights (the sum of weights should be equal to 1). If the samples are unweighted, then it is implied that the weight of each sample is 1/N.\\n\\nTo calculate all the possible metrics provided within this software package, a MATLAB structure “prognosisData” needs to be provided. The elements of this structure include:\\n\\n1.\\tprognosisData.time = (1 x t) time vector of prediction time points\\n2.\\tprognosisData.EOL.true = (1 x t) true EOL values at each prediction time\\n3.\\tprognosisData.EOL.values = (N x t) EOL prediction values at each prediction time.\\n4.\\tprognosisData.EOL.weights = (N x t) EOL prediction weights at each prediction time.\\n5.\\tprognosisData.RUL.true = (1 x t) true RUL values\\n6.\\tprognosisData.RUL.values = (N x t) RUL prediction values at each prediction time.\\n7.\\tprognosisData.RUL.weights = (N x t) RUL prediction weights at each prediction time.\\n8.\\tprognosisData.cputime = (1 x t) EOL/RUL computation times (optional)\\n\\n\\nIf unscented transform sampling [1] is used for prediction, then the samples have so-called sigma-points associated with them. These sigma-points need to be specified as a two-dimensional vector; note that only two sigma points are used within this MATLAB software package. For example, \\n> sigma=[0.5, 0.5] \\n\\nis a valid assignment. This assignment can be ignored and omitted if unscented transform sampling is not used.\\n\\nAlso, the alpha and beta performance requirement levels [2] need to be defined by the user, as a two-dimensional vector. For example, \\n> alphaBeta=[0.1, 0.5] \\n\\nis a valid assignment. Both the above numbers need to be between 0 and 1. The first argument is an allowed accuracy window; smaller the value, more stringent the requirement. The second argument is related to precision; larger the value, more stringent the requirement.\\n \\n## How to load and run the package contents?\\n\\nIn MATLAB, a package is represented by using “+” before the name of the folder containing the package. To load the package use the command: \\n> import PrognosticsMetrics.*\\n\\nThis package contains one simple function that calculates all the prognostics metrics. This function can be called using the code: \\n> computePrognosisMetrics(prognosisData,alphaBeta,sigma)\\n\\nWhile the first argument is the aforementioned structure, the second argument is the 2-dimensional vector containing alpha and beta requirements. These two arguments are mandatory for the function. The third optional argument contains the 2-dimensional vector of sigma-points, and needs to be provided only if unscented transform sampling is used for prediction.\\n\\n## Description of Output Metrics\\n\\n**Alpha-Lambda Performance** <br />\\nAlpha-Lambda Performance is a binary metric that outputs either 1 or 0. At a specific time index, lambda, it questions whether the prediction remains within a cone of accuracy (marked by alpha bounds) as the system approaches EoL.  If a desired condition is met, indicated by the probability mass being greater than Beta, minimal acceptable probability, at lambda time then the output is 1. Otherwise, the return is 0.\\n\\n**Relative Accuracy** <br />\\nRelative Accuracy is the measure of error in RUL prediction relative to the actual RUL. Because it indicates how accurately the algorithm is performing at a certain time, Prognostic algorithms with larger Relative Accuracies are more desirable.\\n\\n**Convergence** <br />\\nThe Convergence Metric determines the rate at which a Metric (ex: accuracy or precision) improves over time. As more information accumulates along with the system progression, it is assumed that algorithm performance also improves.  Convergence is calculated by finding the distance between the origin and centroid of area under the curve for a metric.\\n\\nSmaller the distance, faster the convergence. Faster the convergence indicates higher confidence in keeping the Prediction Horizon as large as possible.\\n\\n## Alpha-Lambda Performance Plot\\n\\nIn addition to computing the metrics, this software package can also plot the alpha-lambda performance [2] plot, using the command\\n> plotAlphaLambda(prognosisData,alphaBeta(1),alphaBeta(2))\\n\\nNote that there are three arguments, and all three are mandatory. The first is the input structure, the second and the third are the alpha and beta values respectively [2].\\n\\n\\n## References\\n\\n1. M. Daigle, A. Saxena, and K. Goebel, “An efficient deterministic approach to model-based prediction uncertainty estimation,” in Annual Conference of the Prognosticsand Health Management Society, 2012, Minneapolis, MN, USA.\\n2. Saxena, A., Celaya, J., Saha, B., Saha, S., & Goebel, K. Metrics for Offline Evaluation of Prognostic Performance. International Journal of Prognostics and Health Management, Vol. 1, No. 1,  21 pages, 2010.\\n'},\n",
       " {'repo': 'nasa/ISS_Camera_Geolocate',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'ISS Camera Geolocate README\\n----------------------------\\nThis is a Python software library that facilitates the geolocation of photographs and video frames from the International Space Station (ISS). The library provides functions that take camera and pointing information, along with publicly available ISS position information, and then will geolocate every pixel of the photograph in latitude and longitude. This enables geospatial analysis of astronaut photography from Earth, including pictures of clouds, lightning, coastlines, city lights, etc. Many images available from https://earth.jsc.nasa.gov/ can be fully geolocated using this software.\\n\\nThe code now also enables geolocation of the ISS Lightning Imaging Sensor (LIS) background imagery datasets. These data are available from http://dx.doi.org/10.5067/LIS/ISSLIS/DATA206 and http://dx.doi.org/10.5067/LIS/ISSLIS/DATA207.\\n\\nISS Camera Geolocate Installation\\n---------------------------------\\nISS Camera Geolocate works under Python 3.6+ on most Mac/Linux setups. Windows installation and other Python versions are currently untested.\\n\\nIn the main source directory:  \\n`python setup.py install`\\n\\nThe following dependencies need to be installed first:\\n\\n- A robust version of Python w/ most standard scientific packages (e.g., `numpy`, `datetime`, `astropy`, etc.) - Get one for free [here.](https://store.continuum.io/cshop/anaconda/)\\n- [SGP4](https://pypi.python.org/pypi/sgp4/)\\n-Cartopy\\n-Cython\\n-Xarray\\n\\nUsing ISS Camera Geolocate\\n--------------------------\\nTo access everything:\\n```\\nimport iss_camera_geolocate as icg\\n```\\n\\nDemonstration notebooks are in the notebooks directory.\\n\\nLatest release info:\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2585824.svg)](https://doi.org/10.5281/zenodo.2585824)\\n'},\n",
       " {'repo': 'nasa/data-nasa-gov-frontpage',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': '# data-nasa-gov-frontpage\\n\\nData.nasa.gov holds metadata harvested from more than 88 different NASA sites that share public data.\\n\\nThis front-end only page serves as an easy to change front-page for the system that hosts the catalog of datasets in data.nasa.gov (which is not on github).\\n\\n## Contributing\\nIf you find any factual errors or places where additions could be made, please add it as an issue or submit a pull request. We accept pull requests from the public. \\n\\n## Potentially Reusable Code Assets Leveraged by this Repository\\n\\n1. This page uses nasawds-2.0.7, which you can find here: https://github.com/nasa/nasawds which is based on GSA\\'s <a href=\"https://github.com/uswds/uswds\"> US web design service</a>. Both are open-source projects on Github.\\n\\n2. The <a href=\"https://github.com/nasa/data-nasa-gov-frontpage/tree/master/non-nasawds-assets/footer\">footer</a> is reused across several open-innovation pages.\\n\\n3. The page data_visualization.html contains a treemap that displays an aggregate data visualization of contents of data.nasa.gov data catalog. The data is represented by rectangles scaled by the number of datasets. Each rectangle reflects a unique combination of source, category, and keyword. This data is extracted from the metadata in data.json.\\n\\n## The Treemap Data Visualization Page\\n### Data Source:\\nThe aggregate data visualization code depends on a JSON file that follows a valid data.json schema as defined by the open data project here: https://project-open-data.cio.gov/ and here https://project-open-data.cio.gov/v1.1/schema/.\\n\\nTheoretically, you should be able to drop in any data.json from any federal agency. Your milage may vary, however. The visualization may not look as nice if the structure is different. For example, if source is shown as the first breakdown and 95% of your datasets are from a single source, than that rectange will take up the majority of the space. This won\\'t look very nice.\\n\\n### Data Processing:\\n`data_processing.py` is responsible for processing the data and can be found inside the `data_processing_scripts` folder. To run it, make sure your data source is in the same folder as the script and run `python3 data_processing.py`. This will produce `processed_data.json`, which is formatted as follows: \\n```\\n{\\n   \"name\": \"dataset\",\\n   \"children\": [\\n       {\\n           \"name\": <source>,\\n           \"children\": [\\n               {\\n                   \"name\": <category>,\\n                   \"children\": [\\n                       {\\n                           \"name\": <keyword>,\\n                           \"value\": <count>\\n                       }, ...\\n                   ]\\n               }, ...\\n           ]\\n       }, ...\\n   ]\\n}\\n```\\n**NOTE**: Category refers to \"theme\" in the original schema.\\n\\n#### Duplicates:\\nIf your source data has a duplicate source, you may add it to `duplicates.json` found inside the `data_processing_scripts` folder.\\n\\nExample:\\n```\\n{\\n  \"National Aeronautics Space Administration\": \"NASA\",\\n  ...\\n}\\n```\\nThis example will group both \"National Aeronautics Space Administration\" and \"NASA\" under the source name \"NASA\" in `data_processing.py`.\\n\\n#### Ignoring Data:\\nIf your source data has any source, category or keyword you want ignored, you may add it to `ignoreData.json` found inside the `data_processing_scripts` folder.\\n\\nExample: \\n```\\n{\\n  \"source\": [\"NASA\"],\\n  \"category\": [\"Earth Science\", \"Geospatial\"],\\n  \"keyword\": []\\n}\\n```\\nThis example will ignore all entries in which \"NASA\" is the source, and ignore all entires with \"Earth Science\" and \"Geospatial\" are categories. \\n**NOTE**: Category refers to \"theme\" in the original schema\\n\\n#### Keyword Count Minimum:\\n`keyword_count_threshold` sets the minimum number a keyword count must be to be added to `processed_data.json` and can be changed inside `data_processing.py` [here](https://github.com/nasa/data-nasa-gov-frontpage/blob/master/data_processing_scripts/data_processing.py#L3).\\n\\n### Data Visualization:\\nThe code used to visualize the data is `visualizations.js` and can be found inside `nasawds-2.07/js`. \\n\\n#### Expanding Acronyms to be Displayed in Treemap Legend:\\nIn `acronyms.json`, found inside `nasawds-2.0.7/json`, you may select acronyms you wish to be expanded for the purpose of displaying them in the treemap legend. Each acronym must have a `type` (either source or category), and a `name` (the acronym\\'s expansion).\\n\\nExample: \\n```\\n{\\n  \"NASA\": { \"type\": \"source\", \"name\": \"National Aeronautics Space Administration\"},\\n  \"GPM\": {\"type\": \"category\", \"name\": \"Global Precipitation Measurement\"}\\n}\\n```\\n\\n#### Treemap Rectangle and Legend Key Links:\\nWhen a user clicks on either a treemap rectangle or a legend key, they are redirected to data.nasa.gov\\'s data catalog page. Changing where a user is redirected can be done inside of `nasawds-2.07/js/visualizations.js`. Setting the treemap rectangle link is done [here](https://github.com/nasa/data-nasa-gov-frontpage/blob/master/nasawds-2.0.7/js/visualizations.js#L55), and setting the legend key link is done [here](https://github.com/nasa/data-nasa-gov-frontpage/blob/master/nasawds-2.0.7/js/visualizations.js#L120).\\n\\n#### Functions:\\n`create_treemap(<data>, <format>)` function takes the `processed_data.json` as the first argument, and the treemap format (d3.treemapSquarify, d3.treemapBinary, d3.treemapSlice, d3.treemapDice, d3.treemapSliceDice) as the second. This function is responsible for rendering and appending the treemap to the site. \\n\\n`changeNesting(<data>)` function takes the `processed_data.json` as an argument, and returns back the same data except it changes the grouping/nesting order from `source -> category -> keyword` to `category -> source -> keyword`.\\n\\n`clean_data_treemap(<data>)` function takes the `processed_data.json` as an argument, and is responsible for removing sources and categories that have no children, as well as swap out source and category acronyms listed in `acronyms.json` (found inside `nasawds-2.0.7/json`) using `swap_acronyms()`. This new cleaned dataset it returned. \\n\\n`swap_acronyms(<acronym>)` function takes an acronym string and returns its expantion if listed in `acronyms.json`. If no expantion is found, the original acronym string is returned.\\n\\n\\n#### Other D3.js visualizations that will work with same processed data format:\\n[Collapsible Tree](https://observablehq.com/@d3/collapsible-tree)\\n\\n[Circle Packing](https://observablehq.com/@d3/circle-packing)\\n\\n[Sunburst](https://observablehq.com/@d3/sunburst)\\n\\n[Icicle](https://observablehq.com/@d3/icicle)\\n\\n[Cluster Dendrogram](https://observablehq.com/@d3/cluster-dendrogram)\\n\\n\\n'},\n",
       " {'repo': 'nasa/K2CE',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# K2CE\\n\\n## The Kepler-K2 Cadence Events Application\\n\\nSince early 2018, the Kepler/K2 project has been performing a uniform global reprocessing of data from K2 Campaigns 0 through 14. Subsequent K2 campaigns (C15-C19) are being processed using the same processing pipeline. One of the major benefits of the reprocessing effort is that, for the first time, short-cadence (1-min) light curves are produced in addition to the standard long-cadence (30-min) light curves. Users have been cautioned that the Kepler pipeline detrending module (PDC), developed for use on original Kepler data, has not been tailored for use on short-cadence K2 observations. Systematics due to events on fast timescales,  such as thruster firings, are sometimes poorly corrected for many short-cadence targets. A Python data visualization and manipulation tool, called Kepler-K2 Cadence Events, has been developed that identifies and removes cadences associated with problematic thruster events, thus producing better light curves. Kepler-K2 Cadence Events can be used to visualize and manipulate light curve files and target pixel files from the Kepler, K2, and TESS missions.  We anticipate this software will be available from <http://code.nasa.gov> \\n\\n## Prequisites\\n\\nThe Kepler-K2 Cadence Events application should run on any computer with Python 3 .  The application was developed using the open-source Anaconda Distribution of Python 2.7 which is available at <https://www.anaconda.com/distribution>.  K2CE has now been ported to Python 3.7.3.\\n\\nThe application uses Lightkurve (<http://docs.lightkurve.org/>) which is a community-developed, open-source Python package which offers a \"beautiful and user-freindly way to analyze astronomical flux time seris data\".  It was specifically designed to analyze the pixels and lightcurves obtained by NASA\\'s Kepler, K2, and TESS exoplanet missions. Installation instructions are given at the following webpage: <http://docs.lightkurve.org/about/install.html>.\\n\\n## Running the Code\\n\\n### As a Stand-Alone Application from the Command Line\\n\\n(1) Copy the Python application (k2\\\\_cadence\\\\_events.py) into the local directory.\\n\\n(2) Make sure that the code is executable by typing (using MacOS X or Linux)\\n\\n\\tchmod u+x k2_cadence_events.py\\n\\n(3) The application can be run without any arguments.  \\n\\nIf you type the following command from the command line, you will see a demonstration of the application:\\n\\n\\t./k2_cadence_events.py\\n\\t\\nYou should see something like this:\\n\\n<pre>\\n./k2_cadence_events.py \\n**********************************************\\nKepler K2 Cadence Events (k2ce): Version 0.79\\n**********************************************\\n\\n*******************************************************\\n***** Use --help to see the command line options. *****\\n*******************************************************\\n\\n\\nUsing default target (exoplanet K2-99b):\\n\\n  from_archive=True\\n  target=212803289\\n  campaign=17\\n  cadence=short\\n\\nfilename=/Users/kmighell/.lightkurve-cache/mastDownload/K2/ktwo212803289-c17_sc/ktwo212803289-c17_slc.fits\\n\\nKepler/K2 short cadence Light Curve File\\n\\nK2 Campaign 17\\n\\ntarget: EPIC 212803289\\n\\n98490 cadences\\n\\nUsing default bitmask value of 1605636.\\n\\nThe bitmask value of 1605636 = 110001000000000000100\\n\\ntranslates as\\n\\n[\\'Coarse point\\', \\'Possible thruster firing\\', \\'Thruster firing\\', \\'No fine point\\']\\n\\n</pre>\\n\\nA plot should appear on your screen like this:\\n\\n![](README_fig1.png)\\n\\nOnce you are ready to leave the application, close the plot window (MacOS X: click on the red button on the upper-right corner of the plot window).\\n\\nThe application ends by writing a little more information:\\n\\n<pre>\\n1635 events\\n\\n1635 cadences flagged\\n</pre>\\n\\nThe demo plot shows the short cadence observations of EPIC 212803289\\n(exoplanet: K2-99b) that were obtained during K2 Campaign 17.\\n\\nThe short-cadence K2 light curve file was downloaded automatically from\\nthe Barbara A. Mikulski Archive for Space Telescopes (MAST) at the Space\\nTelescope Science Institute (STScI).\\n\\nThe **red points** are the observations that had a SAP_QUALITY value with either Bit21\\n(1048576 = (2\\\\*\\\\*20)) and/or Bit20 (524288 = (2\\\\*\\\\*19)) and/or Bit16 (32768 =\\n(2\\\\*\\\\*15)) and/or (4 = (2\\\\*\\\\*2)) set -- which indicates (1) an actual or\\n(2) probable thruster firing event, or the ***Kepler*** spacecraft was in (3) coarse point or (4) not in fine point **during an individual cadence observation**.\\n\\nThe **blue points** in the scatter plot are the \"**good observations**\" that ***did not*** have at least one of these bits set.\\n\\nOf the total of 98490 short-cadence observations, 1635 (1.66%) of the cadences were flagged as being a \"cadence event\".\\n\\nThe grey vertical bars near the top of the plot show events.\\nThe darker the grey, the greater the number of events in time.\\n\\nThe name of the Kepler/K2 short cadence light curve file analysed is shown\\non the right side of the plot. Note that this is where the file is stored\\nlocally.\\n\\n#### Command-line arguments \\n\\nTo see the command-line arguments, type\\n\\n\\tk2_cadence_events.py --help\\n\\t\\nor \\n\\t\\n\\tk2_cadence_events.py -h\\n\\nYou should see something like this:\\n\\t\\n<pre>\\n./k2_cadence_events.py \\n  [-h] [--help]  \\n  [--filename FILENAME]    \\n  [--bitmask BITMASK]  \\n  [--from_archive FROM_ARCHIVE]                            \\n  [--target TARGET] \\n  [--cadence CADENCE]\\n  [--campaign CAMPAIGN] \\n  [--tag TAG]\\n  [--plotfile PLOTFILE] \\n  [--scatter SCATTER]\\n  [--bars_yy BARS_YY] \\n  [--xlim XLIM]\\n  [--ylim YLIM] \\n  [--SAP_FLUX SAP_FLUX]\\n  [--report REPORT]\\n  [--report_filename REPORT_FILENAME]\\n  [--n_before N_BEFORE] \\n  [--n_after N_AFTER]\\n  [--bitmask_decode BITMASK_DECODE]\\n  [--bitmask_flags BITMASK_FLAGS]\\n  [--show_plot SHOW_PLOT]\\n  [--new_filename NEW_FILENAME]\\n  [--overwrite OVERWRITE] [--useTPF USETPF]\\n  [--xcut XCUT] \\n  [--ycut YCUT]\\n\\noptional arguments:\\n\\n  -h, --help            show this help message and exit\\n\\n  --filename FILENAME   Filename of the K2 light curve file (ktwo*llc.fits) to\\n                        be analyzed [default: None]\\n\\n  --bitmask BITMASK     Bitmask value (integer) specifying quality flag\\n                        bitmask of cadences to *show* events. See Table 2-3 of\\n                        the Kepler Archive Manual (KDMC-10009-006) for more\\n                        information [default: None]\\n\\n  --from_archive FROM_ARCHIVE\\n                        If True, get the data from the Mikulski Archive for\\n                        Space Telescopes (MAST) at the Space Telescope Science\\n                        Institute (STScI) [default=True]\\n\\n  --target TARGET       Target name or EPIC number [default: None]\\n\\n  --cadence CADENCE     Type of K2 cadence: \\'short\\' or \\'long\\' [default:\\n                        \\'long\\']\\n\\n  --campaign CAMPAIGN   K2 campaign number [default: None]\\n\\n  --tag TAG             String written at the start of the title of the plot\\n                        [default: None]\\n\\n  --plotfile PLOTFILE   Filename of the output plotfile (if any) [default:\\n                        None]\\n\\n  --scatter SCATTER     If True: the data is plotted as a scatter plot. If\\n                        False: the data is plotted as a line plot\\n                        [default=True]\\n\\n  --bars_yy BARS_YY     Used to set the Y axis location of the gray vertical\\n                        lines showing events [default: None]\\n\\n  --xlim XLIM           User-defined right and left limits for the X axis.\\n                        Example: xlim=\\'(3360,3390)\\' [default: None]\\n\\n  --ylim YLIM           User-defined bottom and top limits for the Y axis.\\n                        Example: ylim=\\'(0.0,1.1)\\' [default: None]\\n  --SAP_FLUX SAP_FLUX   If True, flux is SAP_FLUX. If False, flux is\\n                        PDCSAP_FLUX [default: True]\\n\\n  --report REPORT       If True, print out the time, flux, cadence number and\\n                        the QUALITY value fore each event [default: False]\\n\\n  --report_filename REPORT_FILENAME\\n                        Filename of the report (if any) [default: None]\\n\\n  --n_before N_BEFORE   Number of observations (cadences) before an event to\\n                        mark as bad [default: 0]\\n\\n  --n_after N_AFTER     Number of observations (cadences) after an event to\\n                        mark as bad [default: 0]\\n\\n  --bitmask_decode BITMASK_DECODE\\n                        If True, Decodes (translate) the bitmask value to K2\\n                        Quality Flag Events [default: False]\\n\\n  --bitmask_flags BITMASK_FLAGS\\n                        If True, show the QUALITY bit flags. See Table 2-3 of\\n                        the Kepler Archive Manual (KDMC-10008-006) for more\\n                        information. [default=False]\\n\\n  --show_plot SHOW_PLOT\\n                        If True, show the plot [default=True]\\n\\n  --new_filename NEW_FILENAME\\n                        Filename of the new long (llc) or short (slc) file\\n                        with the event cadences and the bad cadences removed\\n                        [default: None].\\n\\n  --overwrite OVERWRITE\\n                        If True, overwrite (\"clobber\") an existing output file\\n                        [default: False].\\n\\n  --useTPF USETPF       If False, return a KeplerLightCurveFile object. If\\n                        True, return a KeplerTargetPixelFile object [default:\\n                        False].\\n\\n  --xcut XCUT           Cadences with time (X axis) values within the xcut\\n                        limits will be flagged for removal. Example:\\n                        xcut=\\'(3510,3590)\\' [default: None]\\n\\n  --ycut YCUT           Cadences with normalized flux (Y axis) values within\\n                        the ycut limits will be flagged for removal. Example:\\n                        ycut=\\'(0.0,0.9)\\' [default: None]\\n</pre>\\n\\nThe above was reformatted for easier reading.\\n\\n### As a part of a Python script\\n\\n(1) Using your favorite editor, cut-and-paste the following text to create a\\nPython script called **spud.py**:\\n\\n<pre>\\n#!/usr/bin/env python\\nimport sys\\nimport os\\nimport k2_cadence_events as k2ce\\nnargs = len(sys.argv) - 1\\nif (nargs == 0):\\n    # interactive mode: no arguments\\n    k2ce.k2_cadence_events()\\nelse: \\n    # non-interactive mode: first argument is the name of the plotfile (e.g., foo.png)\\n    plotfile=sys.argv[1]  \\n    if (os.path.isfile(plotfile)):  # abort if file already exists\\n        print \\'***** ERROR ***** Requested file already exists: \\',plotfile\\n    else:\\n        k2ce.k2_cadence_events(plotfile=plotfile)\\n        print\\n        print \\'Show the docstring for k2_cadence_events() :\\'\\n        print k2ce.k2_cadence_events.__doc__\\n#EOF\\n</pre>\\n\\n(2) Make sure that the script is executable by typing \\n\\n\\tchmod u+x spud.py\\n\\t\\n(3) You can execute the script in its interactive mode (using no arguments):\\n\\n\\t./spud.py\\n\\t\\n(4) You can execute the script in its non-interactive mode using the first argument as the name of the output plotfile (e.g., spud_plot.png):\\n\\n\\t./spud.py spud_plot.png\\n\\t\\nAfter the plot is written the Python docstring for k2\\\\_cadence\\\\_envents() is shown.\\n\\n## Explanation of QUALITY and SAP_QUALITY bit values\\n\\n<pre>\\nName     Value : Explanation (Kepler/K2)\\nBit01        1 : Attitude tweak\\nBit02        2 : Safe mode\\nBit03        4 : Coarse point\\nBit04        8 : Earth point\\nBit05       16 : Zero crossing\\nBit06       32 : Desaturation event\\nBit07       64 : Argabrightening\\nBit08      128 : Cosmic ray in optimal aperture\\nBit09      256 : Manual exlude\\nBit10      512 : This bit not used by ***Kepler***\\nBit11     1024 : Sudden sensitivity dropout\\nBit12     2048 : Impulsive outlier\\nBit13     4096 : Argabrightening on CCD\\nBit14     8192 : Cosmic ray in collateral data\\nBit15    16384 : Detectpr amp,a;u\\nBit16    32768 : No fine point\\nBit17    65536 : No data\\nBit18   131072 : Rolling band in optimal aperture\\nBit19   262144 : Rolling band in full mask\\nBit20   524288 : Possible thruster Firing\\nBit21  1048576 : Thruster firing\\n</pre>\\n\\n## Contact\\n\\nKepler-K2 Cadence Events was created by Kenneth J. Mighell and supported by the Kepler/K2 Science Office.  You can contact the author at kenneth dot j dot mighell at nasa dot gov or kmighell at seti dot org.\\n\\n\\n## More information about using the application\\n\\nYou can learn more about the many options of the k2\\\\_cadence\\\\_events application by using running its demo Jupyter notebook called **k2\\\\_cadence\\\\_events.ipynb**.\\n\\n\\tjupyter notebook k2_cadence_events.ipynb\\n\\t\\n## Notices\\n\\nCopyright © 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNASA acknowledges the SETI Institute’s primary role in authoring and producing the Kepler-K2 Cadence Events application under Cooperative Agreement Number NNX13AD01A\\n\\n## Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n##### 2019SEP10\\n'},\n",
       " {'repo': 'nasa/FM',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"core Flight System (cFS) File Manager Application (FM) \\r\\n======================================================\\r\\n\\r\\nOpen Source Release Readme\\r\\n==========================\\r\\n\\r\\nFM Release 2.5.3\\r\\n\\r\\nDate: \\r\\n3/19/2020\\r\\n\\r\\nIntroduction\\r\\n---------------\\r\\n\\r\\n  The File Manager application (FM) is a core Flight System (cFS) application \\r\\n  that is a plug-in to the Core Flight Executive (cFE) component of the cFS.  \\r\\n  \\r\\n  The FM application provides onboard file system management services by \\r\\n  processing ground commands for copying, moving, and renaming files, \\r\\n  decompressing files, creating directories, deleting files and directories, \\r\\n  providing file and directory informational telemetry messages, and providing \\r\\n  open file and directory listings.\\r\\n\\r\\n  The FM application is written in C and depends on the cFS Operating System \\r\\n  Abstraction Layer (OSAL) and cFE components. There is additional FM application \\r\\n  specific configuration information contained in the application user's guide\\r\\n  available in https://github.com/nasa/FM/tree/master/docs/users_guide\\r\\n\\r\\n  This software is licensed under the NASA Open Source Agreement. \\r\\n  http://ti.arc.nasa.gov/opensource/nosa\\r\\n \\r\\n \\r\\nSoftware Included\\r\\n--------------------\\r\\n\\r\\n  File Manager application (FM) 2.5.3\\r\\n  \\r\\n \\r\\nSoftware Required\\r\\n--------------------\\r\\n\\r\\n  Operating System Abstraction Layer 5.0 or higher can be \\r\\n  obtained at https://github.com/nasa/osal\\r\\n \\r\\n  core Flight Executive 6.7.0 or higher can be obtained at\\r\\n  https://github.com/nasa/cfe\\r\\n\\r\\n  Note: An integrated bundle including the cFE, OSAL, and PSP can \\r\\n  be obtained at https://github.com/nasa/cfs\\r\\n\\r\\nAbout cFS\\r\\n------------\\r\\n\\r\\n  The cFS is a platform and project independent reusable software framework and\\r\\n  set of reusable applications developed by NASA Goddard Space Flight Center.  \\r\\n  This framework is used as the basis for the flight software for satellite data \\r\\n  systems and instruments, but can be used on other embedded systems.  More \\r\\n  information on the cFS can be found at http://cfs.gsfc.nasa.gov\\r\\n\\r\\n                      \\r\\nEOF                       \\r\\n\"},\n",
       " {'repo': 'nasa/georef_deploy',\n",
       "  'language': 'Ruby',\n",
       "  'readme_contents': 'Installation\\n============\\n\\nRequirements\\n~~~~~~~~~~~~\\n\\nOur reference platform for GeoRef is Ubuntu Linux 14.04 LTS,\\nrunning Python 2.7.6 and Django 1.9.2.  For development we use Django\\'s\\nbuilt-in development web server MySQL database.  \\n\\nWe develop using a VagrantBox VM running a Ubuntu Linux inside a Mac OS X host machine.\\nVagrant VM is strictly optional and only necessary if you are not running directly from a Ubuntu Linux Machine.\\n\\nOur image view is rendered using the OpenSeadragon open source image viewer. (openseadragon.github.io/)\\n\\n(Optional) Set up a Vagrant VM\\n~~~~~~~~~~~~~~~~~~~~\\nIf you are running on a mac, we highly encourage you to use Vagrant to set up \\na Ubuntu Development Instance. Our set up script works best within the Vagrant \\nenvironment running on Mac OSX.\\n\\nInstall VirtualBox. We have found that VirtualBox Version 4.3.10 works best with Vagrant.\\nWe highly recommend you download VirtualBox 4.3.10.\\nInstall the latest version of vagrant: \\u200bhttp://www.vagrantup.com/downloads\\n\\n\\nSet Up an Install Location\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nLet\\'s create a directory to hold the whole GeoRef installation\\nand capture the path in an environment variable we can use\\nin the instructions below::\\n\\n  export GEOCAM_DIR=$HOME/projects/geocam # or choose your own\\n  mkdir -p $GEOCAM_DIR\\n  \\n\\nGet the Source\\n~~~~~~~~~~~~~~\\n\\nCheck out our latest source revision with::\\n\\n  cd $GEOCAM_DIR\\n  git clone https://github.com/nasa/georef_deploy.git\\n\\n\\nFor more information on the Git version control system, visit `the Git home page`_.\\nYou can install Git on Ubuntu with::\\n\\n  sudo apt-get install git-all\\n\\n.. _the Git home page: http://git-scm.com/\\n\\n\\nRun the Setup Script\\n~~~~~~~~~~~~~~~~~~~~~\\nThe \"setup_site_vagrant.sh\" script initializes the vagrant box and it clones \\nall the submodules that are needed::\\n\\n    # go into the georef_deploy directory\\n    cd georef_deploy\\n    \\n    # if you are running inside a Vagrant VM do\\n    setup_site_vagrant.sh\\n\\n\\nIf you are running directly on a Ubuntu Linux Machine, you can skip the above shell\\nscript and run the following::\\n    sudo python $GEOCAM_DIR/georef_deploy/setup_site.py\\n    \\n    # You need to manually create couple symlinks if not running on vagrant\\n    sudo ln -s /home/geocam/georef_deploy georef_deploy\\n    sudo ln -s gds/georef/ georef\\n\\n\\nOverride settings.py\\n~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn the ``settings.py`` file, modify the ``DATABASES`` field to point to\\nyour Django MySQL database::\\n\\n    DATABASES = {\\n        \\'default\\': {\\n            \\'ENGINE\\': \\'django.db.backends.mysql\\',\\n            \\'NAME\\': \\'georef\\',\\n            \\'USER\\': \\'root\\',\\n            \\'PASSWORD\\': \\'vagrant\\',\\n            \\'HOST\\': \\'127.0.0.1\\',\\n            \\'PORT\\': \\'3306\\',\\n        }\\n    }\\n\\n\\nSetup the Data Directory\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\nYou must manually create the data directory and its sub folders. GeoRef will \\nwrite the image tiles to this directory.\\n\\n1. Create a data directory\\n    ``mkdir $GEOCAM_DIR/georef/data``\\n2. Create the overlays directory\\n    ``mkdir -p $GEOCAM_DIR/georef/data/geocamTiePoint/overlay_images``\\n3. Set the permissions\\n    ``chmod -R 777 $GEOCAM_DIR/georef/data``\\n\\n\\nSetup GeoRef\\n~~~~~~~~~~~~\\n\\nIf your development environment is set up inside Vagrant, cd into the georef_deploy \\ndirectory and do::\\n    vagrant ssh\\nAnd then run the following commands.\\n\\n\\nYou must create the following directory and files::\\n\\n # If you are not using Vagrant, do\\n     mkdir -p $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/ & touch $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/deepzoom.exception.log\\n\\n # If you are using Vagrant, do\\n     # deepzoom directory needs to be owned by www-data. Put it in /home/vagrant so that it can be owned by www-data (and not by user)\\n     mkdir -p /home/vagrant/deepzoom \\n     # create a symlink to deepzoom in the data dir\\n     ln -s /home/vagrant/deepzoom /home/vagrant/georef/data/deepzoom\\n\\n\\nInstall Earth Engine by following the instructions below: \\n    https://developers.google.com/earth-engine/python_install_manual\\n\\n\\nTo install Python dependencies, render icons and collect media for the\\nserver, run::\\n\\n  cd $GEOCAM_DIR/georef_deploy/georef\\n  ./manage.py bootstrap --yes\\n  source $GEOCAM_DIR/georef_deploy/georef/sourceme.sh genSourceme genSettings\\n  ./manage.py collectstatic  \\n  ./manage.py prep\\n\\nYou\\'ll need to source the ``sourceme.sh`` file every time you open a new\\nshell if you want to run GeoCam-related Python scripts such as starting\\nthe Django development web server.  The ``sourceme.sh`` file will also\\ntake care of activating your virtualenv environment in new shells (if\\nyou were in a virtualenv when you ran ``setup.py``).\\n\\n\\nTo initialize the database\\n    ``$GEOCAM_DIR/georef/manage.py makemigrations deepzoom``\\n\\n    ``$GEOCAM_DIR/georef/manage.py makemigrations geocamTiePoint``\\n\\n    ``$GEOCAM_DIR/georef/manage.py migrate``\\n\\nNote that the path to manage.py may be different if you are running inside Vagrant.\\n\\n\\nCreate a User Account  \\n~~~~~~~~~~~~~~~~~~~~~\\nUser name and password are required to use GeoRef. To create one, do::\\n    \\n    ./manage.py createsuperuser\\n\\nAnd follow the prompts.\\n\\n\\n\\nTry It Out\\n~~~~~~~~~~\\nNow you\\'re ready to try it out!  \\n\\nRestart the Apache server ``sudo apachectl restart``\\n\\nPoint your browser to \\u200bhttp://10.0.3.18/\\n\\n\\n.. o  __BEGIN_LICENSE__\\n.. o  Copyright (C) 2008-2010 United States Government as represented by\\n.. o  the Administrator of the National Aeronautics and Space Administration.\\n.. o  All Rights Reserved.\\n.. o  __END_LICENSE__\\n'},\n",
       " {'repo': 'nasa/Three-Dimensional-Nozzle-Design-Code',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Three-Dimensional-Nozzle-Design-Code\\n\\nThis package contains 3 programs written for the Microsoft Visual C++ compiler:\\n\\n1) 2D Method of Characteristics, MOC_GRID_BDE\\n\\t- Computes a planar or axisymmetric two-dimensional nozzle contour based on user inputs using method of characteristics\\n\\t\\n2) Streamline Tracing Tool, STT2001\\n\\t- Performs streamline tracing on a supplied nozzle solution grid (usually generated by MOC_GRID_BDE)\\n\\n3) 3D Method of Characteristics Design Tool, 3D_MOC\\n\\t- Computes three-dimensional flowfield properties for a given 3D nozzle contour. This could be a modified version of a nozzle contour generated by MOC_GRID_BDE.\\n\\n\\t\\n\\nEach program is contained in a separate folder containing source code and sample outputs. These programs are designed to use the Microsoft Foundation Class (MFC) libraries to generate the GUI windows for user input.\\n\\nDetailed descriptions of the mathematical basis and operation of each program are provided in the attached report: \\n\\n**[Rice, T., \"2D and 3D Method of Characteristic Tools for Complex Nozzle Development,\" JHU/APL Report RTDC-TPS-481. 2003.](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20030067852.pdf)**\\n'},\n",
       " {'repo': 'nasa/simplegrid',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '\\n==========\\nSimplegrid\\n==========\\n\\n-------------------------------------------------------------------------\\nSimple regional grid creation and refinement for ocean circulation models\\n-------------------------------------------------------------------------\\n\\n.. inclusion-marker-start-of-intro\\n\\nsimplegrid is a Python package for creating, refining, and joining horizontal\\nquadrilateral grids that are used in connection with the `MIT General\\nCirculation Model (MITgcm) <http://mitgcm.org/>`_.\\n\\nsimplegrid is based on equal great circle arc subdivision (hence the name) with\\ngeodesic computations provided by `pyproj/PROJ\\n<https://pypi.org/project/pyproj/>`_, and implements both Python-callable and\\ncommand-line functionality for embedded and scripted solutions. It also contains\\nseveral useful utilities for grid manipulation, file i/o, and coincident edge\\ndetection.\\n\\nGrid point location data are specified in decimal longitude and latitude, with\\nresulting MITgcm grid data in units of meters.\\n\\n.. inclusion-marker-end-of-intro\\n\\n.. inclusion-marker-start-of-installation\\n\\n------------\\nInstallation\\n------------\\n\\nRequirements\\n^^^^^^^^^^^^\\n\\nsimplegrid is compatible with Python 3.5+, with numpy, pyproj, and xESMF package\\ndependencies.\\n\\nIf xESMF is not already installed, the simplest approach is via an `anaconda\\n<https://anaconda.org/>`_ distribution using any one of several procedures\\noutlined in the `xESMF installation documentation\\n<https://xesmf.readthedocs.io/en/latest/installation.html>`_.  simplegrid may\\nthen be installed within the base distribution or in a virtual environment that\\nhas been granted access to the global site packages.\\n\\n\\nInstalling from github\\n^^^^^^^^^^^^^^^^^^^^^^\\n\\nsimplegrid is under active development. To obtain the latest development version\\nyou may clone the repository and install it::\\n\\n    git clone https://github.jpl.nasa.gov/gmoore/simplegrid.git\\n    cd simplegrid\\n    pip install .\\n\\n\\nBuilding the documentation\\n^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\nSource and build files are included to generate html-based documentation using\\n`Sphinx <https://www.sphinx-doc.org/>`_.\\nFrom the simplegrid git clone root directory, just::\\n\\n    make --directory=./simplegrid/docs html\\n\\nAnd point your browser to the resulting collection at::\\n\\n    <simplegrid_git_clone_root>/simplegrid/docs/build/html/index.html\\n\\n.. inclusion-marker-end-of-installation\\n\\n.. inclusion-marker-start-of-examples\\n\\n--------------------\\nQuick Start Examples\\n--------------------\\n\\nCreating a grid\\n^^^^^^^^^^^^^^^\\n\\nThe following creates a simple 10x10 grid over a one-by-one degree region with\\nnorthwest/southeast lon/lat corners at (1.,2.)/(2.,1.):\\n\\nfrom Python::\\n\\n    import simplegrid as sg\\n    (newgrid,newgrid_ni,newgrid_nj) = sg.mkgrid.mkgrid(\\n        lon1=1., lat1=2.,\\n        lon2=2., lat2=1.,\\n        lon_subscale=10, lat_subscale=10)\\n\\nfrom the command line::\\n\\n    sgmkgrid                            \\\\\\n        --lon1 1.                       \\\\\\n        --lat1 2.                       \\\\\\n        --lon2 2.                       \\\\\\n        --lat2 1.                       \\\\\\n        --lon_subscale 10               \\\\\\n        --lat_subscale 10               \\\\\\n        --outfile newgrid.mitgrid\\n\\nAll sixteen horizontal grid quantities (XC, YC, DXF, DYF, RAC, XG, YG, DXV, DYU,\\nRAZ, DXC, DYC, RAW, RAS, DXG, and DYG) are either written to a dictionary with\\ncorresponding key/value pairs in the case of Python or, in the command-line\\ncase, to a contiguous binary file in mitgridfile format.\\n\\nRefining a grid\\n^^^^^^^^^^^^^^^\\n\\nThe following performs a 10x10 refinement of the llc 90 model, tile005,\\n\"southwest\" corner cell. In the following, northwest/southeast input corner\\nlat/lon values for the region of interest have been determined by inspection and\\nneed not be as precisely defined as shown; simplegrid will perform\\nnearest-neighbor checks to determine the closest existing corner grids:\\n\\nfrom Python::\\n\\n    import simplegrid as sg\\n    (newgrid,newgrid_ni,newgrid_nj) = sg.regrid.regrid(\\n        strict=True, verbose=False,\\n        mitgridfile=\\'./data/tile005.mitgrid\\',\\n        ni=270, nj=90,\\n        lon1=-127.73445435, lat1=67.56064719,\\n        lon2=-128., lat2=67.40168504,\\n        lon_subscale=10, lat_subscale=10)\\n\\nfrom the command line::\\n\\n    sgregrid                            \\\\\\n        --mitgridfile tile005.mitgrid   \\\\\\n        --ni 270                        \\\\\\n        --nj  90                        \\\\\\n        --lon1 -127.73445435            \\\\\\n        --lat1   67.56064719            \\\\\\n        --lon2 -128.0                   \\\\\\n        --lat2   67.40168504            \\\\\\n        --lon_subscale 10               \\\\\\n        --lat_subscale 10               \\\\\\n        --outfile regrid005.mitgrid     \\\\\\n        --strict\\n\\nAs in the preceding mkgrid case, all horizontal grid quantities are either\\nwritten to a dictionary of name/value pairs in Python or, in the command-line\\ncase, to a contiguous binary file in mitgrid file format.\\n\\nIn addition to the mitgrid file input, the Python and command line interfaces to\\nregrid also support binary and comma-separated value (csv) input options; such\\nfiles would have been produced had an mitgrid file been read into matlab, for\\nexample, with XG and YG corner grid matrix output (the only mitgrid file\\nquantities, in fact, used by regrid) to intermediate files.\\n\\nfrom Python::\\n\\n    import simplegrid as sg\\n    # both *.bin and *.csv supported:\\n    (newgrid,newgrid_ni,newgrid_nj) = sg.regrid.regrid(\\n        xg_file=\\'./data/tile005_XG.bin\\',\\n        yg_file=\\'./data/tile005_YG.bin\\',\\n        ni=270, nj=90,\\n        lon1=-127.73445435, lat1=67.56064719,\\n        lon2=-128., lat2=67.40168504,\\n        lon_subscale=10, lat_subscale=10)\\n\\nand, from the command line::\\n\\n    # both *.bin and *.csv supported:\\n    sgregrid                            \\\\\\n        --xg_file tile005_XG.csv        \\\\\\n        --yg_file tile005_YG.csv        \\\\\\n        --ni 270                        \\\\\\n        --nj  90                        \\\\\\n        --lon1 -127.73445435            \\\\\\n        --lat1   67.56064719            \\\\\\n        --lon2 -128.0                   \\\\\\n        --lat2   67.40168504            \\\\\\n        --lon_subscale 10               \\\\\\n        --lat_subscale 10               \\\\\\n        --outfile regrid005.mitgrid\\n\\nDetermining boundary grid terms\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\nIn most cases, mitgrid data that spans tracer cells is undefined along\\nboundaries (for example, \"U\" cell quantities RAW and DXV along a tile\\'s western\\nand eastern edges).  \"addfringe\" functionality can compute this boundary, or\\nfringe, grid data if an adjacent tile is provided.\\n\\nThe following augments a 2x2 tile with boundary data computed using an eastern\\nedge-adjacent 2x2 tile:\\n\\nfrom Python::\\n\\n    import simplegrid as sg\\n    (tilea_edge,tileb_edge,new_tilea_grid) = sg.addfringe.addfringe(\\n        strict=True,\\n        tilea=\\'./data/tile_A_2x2.mitgrid\\',nia=2,nja=2,\\n        tileb=\\'./data/tile_B_E_2x2.mitgrid\\',nib=2,njb=2)\\n\\nnew_tilea_grid is a dictionary of mitgrid name/value pairs containing tile_A\\ninput data, augmented with eastern edge data computed using tile_B.  tilea_edge\\nand tileb_edge are integer indicators confirming the A and B edge matches: 0==N,\\n1==S, 2==E, and 3==W (in this example, tilea_edge will be eqal to 2, and\\ntileb_edge, 3).\\n\\nfrom the command line::\\n\\n    sgaddfringe                         \\\\\\n        --tilea tile_A_2x2.mitgrid      \\\\\\n        --nia 2                         \\\\\\n        --nja 2                         \\\\\\n        --tileb tile_B_E_2x2.mitgrid    \\\\\\n        --nib 2                         \\\\\\n        --njb 2                         \\\\\\n        --outfile addfringe_A_EW_2x2.mitgrid \\\\\\n        --strict\\n\\nAs in the Python example, the output file contains tile_A grid quantities,\\naugmented with eastern edge data computed using tile_B.  Output is to a combined\\nbinary file in mitgrid file format.\\n\\nJoining grids\\n^^^^^^^^^^^^^\\n\\nJoining, or \"stitching\", two tiles together produces a single entity, assigning\\ncommon-edge boundary quantities as appropriate.  The following joins two 2x2\\ntiles that match on their northern and southern edges, respectively, resulting\\nin a 2x4 mitgrid:\\n\\nfrom Python::\\n\\n    (newgrid,newgrid_ni,newgrid_nj) = sg.stitch.stitch(\\n        strict=True, verbose=False,\\n        tilea=\\'./data/tile_A_2x2.mitgrid\\',nia=2,nja=2,\\n        tileb=\\'./data/tile_B_N_2x2.mitgrid\\',nib=2,njb=2)\\n\\nAs in the previous examples, newgrid is a dictionary of mitgrid name/value\\npairs, and newgrid_ni and newgrid_nj provide the tracer cell row and column\\ncounts for the combined grid.\\n\\nfrom the command line::\\n\\n    sgstitch                            \\\\\\n        --tilea tile_A_2x2.mitgrid      \\\\\\n        --nia 2                         \\\\\\n        --nja 2                         \\\\\\n        --tileb tile_B_N_2x2.mitgrid    \\\\\\n        --nib 2                         \\\\\\n        --njb 2                         \\\\\\n        --outfile stitch_AB_NS_2x4.mitgrid \\\\\\n        --strict\\n\\nComputing open boundary conditions\\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\nGlobal, or parent, grid simulation results can be interpolated to the boundaries\\nof regional grids for use in connection with MITgcm\\'s OBCS package for regional\\nmodeling. The \"getobcs\" capability in simplegrid supports the generalized\\nmethods for describing open boundaries described in MITgcm\\'s `OBC physical\\nparameterization package\\n<https://mitgcm.readthedocs.io/en/latest/phys_pkgs/obcs.html>`_, and\\nautomatically generates boundary matrices for all results, depths, and times\\nfound in the global simulation results directory.\\n\\nThe following computes open boundary conditions for the (default) outer tracer\\ncell edges of a 20x16 regional grid using MITgcm\\'s lab_sea verification model\\nresults, storing the N, S, E, and W sets of boundary matrices in the directory\\n./run_obcs:\\n\\nfrom Python::\\n\\n    # regional grid (at 2x global grid resolution):\\n    (mg_region, ni_region, nj_region) = sg.regrid.regrid(\\n        mitgrid_matrices=parent_mitgrid,\\n        lon1=290., lat1=70., lon2=310., lat2=54.,\\n        lon_subscale=2, lat_subscale=2)\\n\\n    # interpolate global simulation results to regional boundaries:\\n    sg.getobcs.getobcs(\\n        parent_mitgrid_matrices=parent_mitgrid,\\n        parent_resultsdir = \\'./run\\',\\n        regional_mitgrid_matrices = mg_region,\\n        verbose=True)\\n\\n\\n.. inclusion-marker-end-of-examples\\n\\n'},\n",
       " {'repo': 'nasa/S4P',\n",
       "  'language': 'Perl',\n",
       "  'readme_contents': 'S4P \\n===\\n\\nS4P is the Simple, Scalable Script-based Science Processor.\\n\\nThe aim is to develop a simplified processing system to accommodate the increased load expected with the advent of reprocessing and launch of a new satellites.\\n\\nINSTALLATION\\n\\nTo install this module type the following:\\n\\n   perl Makefile.PL\\n   make\\n   make test\\n   make install\\n\\nDisclaimer: We will update the software but not maintain the pull requests.\\n\\nCOPYRIGHTS\\n==========\\nCopyright © 2002-2011 United States Government as represented by Administrator for The National Aeronautics and Space Administration. All Rights Reserved.\\n'},\n",
       " {'repo': 'nasa/S4PA',\n",
       "  'language': 'Perl',\n",
       "  'readme_contents': 'S4PA \\n====\\n\\nS4PA is the Simple, Scalable Script-based Science Processing Archive.\\nThe aim is to develop a simple data management system for disk-based\\narchives of science data.\\n\\nINSTALLATION\\n\\nTo install this module type the following:\\n\\n   perl Makefile.PL\\n   make\\n   make test\\n   make install\\n\\nCOPYRIGHTS\\n==========\\nCopyright © 2002-2011 United States Government as represented by Administrator for The National Aeronautics and Space Administration. All Rights Reserved.\\n'},\n",
       " {'repo': 'nasa/aos-dr',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '\\r\\n## AOS-DR: Autonomy Operating System (AOS) Diagnostic Reasoner (DR)\\r\\n-----------------------\\r\\n### About\\r\\nAOS-DR is software that performs runtime diagnosis of a system of interest. That is, it can determine when part of the system fails (fault/failure detection) and determine what part failed (fault/failure isolation). This software was first developed as part of the Autonomy Operating System project at NASA Ames Research Center. However, AOS-DR is general software that may be adapted to diagnose any system. The code and documentation will typically only use Diagnostic Reasoner, or DR, as the name of the software. \\\\\\r\\n\\\\\\r\\nThe software is implemented as an application that runs within Core Flight Executive (cFE). cFE is open-source flight software created by NASA, and maintained separately [here](https://github.com/nasa/cFE). AOS-DR cannot run without being part of the cFE framework. It also currently uses another cFE app called [Limit Checker (LC)](https://github.com/nasa/LC) to classify data from the system into pass/fail results, which are the input to AOS-DR. cFE runs on Linux and VxWorks; however, AOS-DR has only been used on Linux.\\\\\\r\\n\\\\\\r\\nAOS-DR uses a dependency matrix (D-matrix) approach for performing diagnosis. The D-matrix is system-specific, and maps pass/fail results into system failure modes. A user of AOS-DR will need to create a D-matrix as well as several other tables in order to adapt it for their system. For more details on how AOS-DR works, please see the user manual in the doc directory.\\r\\n\\r\\n-----------------------\\r\\n### Contact\\r\\nIf you have questions about AOS-DR, please contact Adam Sweet <<adam.sweet@nasa.gov>> or Chris Teubert <<christopher.a.teubert@nasa.gov>>.\\r\\n\\r\\n-----------------------\\r\\n### Contributing\\r\\n\\r\\nContributions to AOS-DR are welcome! Contributors will need to sign and submit a \"Contributor License Agreement\" (CLA), included in this source. \\r\\n\\r\\n-----------------------\\r\\n### Copyright and Notices\\r\\nThe AOS-DR code is released under the NASA Open Source Agreement Version 1.3 license. A copy of the license is distributed with the source code.\\\\\\r\\n\\\\\\r\\nCopyright © 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\r\\n\\r\\nDisclaimers:\\r\\n\\r\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\r\\n\\r\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\r\\n\\r\\n\\r\\n\\r\\n'},\n",
       " {'repo': 'nasa/MISR-Toolkit',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '\\nWelcome to the MISR Toolkit\\n---------------------------\\n\\nThe MISR Toolkit is a simplified programming interface to access MISR L1B2, L2, and MISR-HR,\\nconventional and ancillary data products. It is an interface built upon HDF-EOS\\nthat knows about MISR data products. It has the ability to:\\n\\n   - Specify regions to read based on geographic location and extent or the\\n     more traditional path and block range\\n   - Map between path, orbit, block, time range and geographic location\\n   - Automatically stitch, unpack and unscale MISR data while reading\\n   - Perform coordinate conversions between lat/lon, SOM x/y, block/line/sample\\n     and line/sample of a data plane, which means geolocation can be computed\\n     instantly without referring to an ancillary data set lookups\\n   - Retrieve pixel acquistion time from L1B2 product files\\n   - Read a slice of a multi-dimensional field into an 2-D data plane (eg. RetrAppMask[0][5])\\n   - Convert MISR product files to IDL ENVI files\\n\\nThe MISR Toolkit has been tested on Linux CentOS 7, Mac OS X 10.14.6 and Windows 10. \\nIt\\'s core interface is C. There are also bindings for Python (2.7 and 3.x supported) and IDL.\\nNote that Python 2.7 is end of life and deprecated in MISR Toolkit.\\n\\n\\nComplete documentation and function reference\\n---------------------------------------------\\n\\nUse a browser to view online documentation: https://nasa.github.io/MISR-Toolkit/html/index.html\\n\\nOffline documentation packages are also available for download on the github releases page.\\n\\n\\nThird Party Library Dependencies\\n--------------------------------\\n\\nThe MISR Toolkit depends on the following libraries. Download links are provide\\nfor your reference, but it is preferred to use the script \"download_libraries\"\\nin the source bundle \"scripts\" directory. Python users will still need to\\ndownload NumPy and install it according to NumPy instructions.\\n\\n   - HDF-EOS2.18v1.00 (https://observer.gsfc.nasa.gov/ftp/edhs/hdfeos/previous_releases)\\n   - HDF4.2.10        (https://support.hdfgroup.org/ftp/HDF/releases/HDF4.2.10/src)\\n   - hdf5-1.8.16      (https://support.hdfgroup.org/ftp/HDF5/releases/hdf5-1.8/hdf5-1.8.16/src)\\n   - netcdf-4.4.0     (ftp://ftp.unidata.ucar.edu/pub/netcdf/)\\n   - jpegsrc.v6b      (https://observer.gsfc.nasa.gov/ftp/edhs/hdfeos/previous_releases)\\n   - zlib-1.2.5       (https://observer.gsfc.nasa.gov/ftp/edhs/hdfeos/previous_releases)\\n   - NumPy 1.15 or later Python module (http://www.numpy.org/)\\n\\n\\nBinary Installation Instructions\\n--------------------------------\\n\\nBinary installations that do not require compilation are now available for Python 3.6, IDL, and C library users\\non Linux, Mac OS X, and Windows.\\nPython 2.7 binaries are also available but deprecated on Linux and Mac OS X.\\n\\nFor Python users, the most convenient option is downloading MISR Toolkit directly from PyPI using pip.\\n- python -m pip install -U pip\\n- python -m pip install -U wheel numpy\\n- python -m pip install -U MisrToolkit\\n\\nA downloadable python binary wheel distribution is also available on the releases page.\\n\\nFor IDL users, a IDL Dynamically Loadable Module (DLM) distribution is available on the releases page.\\nTo use it, download the package, extract it to a convenient location, and then follow the instructions in the README\\nto set the IDL_DLM_PATH enviornment variable or use IDL\\'s PREF_SET to tell IDL where you extracted the package.\\n\\n\\nSource Installation Instructions\\n--------------------------------\\n\\nLinux and MacOS X\\n\\nThe recommended location for the HDF-EOS/HDF libraries and the MISR Toolkit is /usr/local/hdfeoslibs\\nand /usr/local/Mtk-1.5.X respectively, because this location provides convenient access for multiple user\\naccounts.  It is not necessary, however, to install in /usr/local. The MISR Toolkit and HDF-EOS can be\\ninstalled anywhere.  Of course installing in /usr/local requires root or sudo privileges. Use the sudo\\ncommands below if you are a sudoer or simply become root before installing the HDF-EOS/HDF libraries\\nand before the \"make install\" step for the MISR Toolkit.\\n\\n1) Create a working directory\\n\\n   mkdir Mtk_tmp\\n   cd Mtk_tmp\\n\\n2) Extract Mtk-src-1.5.X.tar.gz\\n\\n   tar xzvf Mtk-src-1.5.X.tar.gz      (if not done already)\\n\\n3) Extract Mtk testdata (substitute Mtk-testdata-1.5.X with your version)\\n\\n   tar xzvf Mtk-testdata-1.5.X.tar.gz\\n\\n4) Download HDF-EOS/HDF, JPEG and ZLIB libraries\\n\\n   cd Mtk-src-1.5.X\\n   scripts/download_libraries\\n\\n   <Hit \"return\" for defaults>\\n\\n5) Install HDF-EOS/HDF, JPEG and ZLIB libraries (using the following script\\n   is necessary because it applies patches which are required for some systems,\\n   like Mac Universal Binaries, Linux and Linux64)\\n\\n   ** The next step requires root privileges to install into \\n   /usr/local/hdfeoslibs, otherwise choose another location and disregard sudo\\n\\n   sudo scripts/install_hdf+hdfeos\\n\\n   <Hit \"return\" for defaults (/usr/local/hdfeoslibs)>\\n\\n6) Build, test and install Mtk (substitute Mtk-src-1.5.X with your version)\\n\\n    Setup your HDF/HDFEOS environment\\n    for csh:\\n\\n       source <your-step5-path>/bin/hdfeos_env.csh\\n\\n    for bash:\\n\\n      source <your-step5-path>/bin/hdfeos_env.sh\\n\\n    Set MTK_INSTALLDIR environment variable (ex. /usr/local/Mtk-1.5.X)\\n    for csh:\\n\\n       setenv MTK_INSTALLDIR <your-path>/Mtk-1.5.X\\n\\n    for bash:\\n\\n       export MTK_INSTALLDIR=<your-path>/Mtk-1.5.X\\n\\n    Set IDL_DIR environment variable (if applicable)\\n    for csh:\\n\\n       setenv IDL_DIR <path-to-idl>/harris/idl\\n\\n    for bash:\\n\\n      export IDL_DIR=<path-to-idl>/harris/idl\\n\\n    To build, test and install (choose which libraries to build)\\n\\n       Everything - C, command-line utilities, IDL and Python\\n\\n          make\\n          make testall (If you have Mtk testdata installed and would like\\n                        to test C, IDL and Python)\\n\\n          ** The next step requires root privileges to install into\\n             /usr/local/Mtk-1.5.X, otherwise choose another location\\n             and disregard sudo\\n\\n          sudo MTK_INSTALLDIR=$MTK_INSTALLDIR make install\\n\\n       Or - C library and command-line utilites\\n\\n          make lib\\n          make cmdutil\\n          make test (If you have Mtk testdata installed and would like to\\n                     test only C routines)\\n\\n          ** The next step requires root privileges to install into\\n             /usr/local/Mtk-1.5.X, otherwise choose another location\\n             and disregard sudo\\n\\n          sudo make install\\n\\n       Or - IDL library\\n\\n          make idl\\n          make testidl (If you have Mtk testdata installed and would like to\\n                        test only IDL routines - requires user interaction)\\n\\n          ** The next step requires root privileges to install into\\n             /usr/local/Mtk-1.5.X, otherwise choose another location\\n             and disregard sudo\\n\\n          sudo make install\\n\\n       Or - Python library\\n\\n          make python\\n          make testpython (If you have Mtk testdata installed and would like to\\n                           test only Python routines)\\n\\n          ** The next step requires root privileges to install into\\n             /usr/local/Mtk-1.5.X, otherwise choose another location\\n             and disregard sudo\\n\\n          sudo make install\\n\\n       Other build targets and options\\n\\n          make help (for other build targets)\\n          make clean (to clean everything)\\n\\n       If the build complains about .d files ( This occurs when the HDF-EOS\\n       environment is not set)\\n\\n          scripts/rmdepend.sh (to remove dependency files)\\n          make\\n\\n7) To use Mtk\\n\\n   The HDF/HDFEOS environment is already set above\\n\\n   Setup your Mtk environment (pick any):\\n\\n      For csh:\\n         source $MTK_INSTALLDIR/bin/Mtk_c_env.csh       for C\\n         source $MTK_INSTALLDIR/bin/Mtk_idl_env.csh     for IDL\\n         source $MTK_INSTALLDIR/bin/Mtk_python_env.csh  for Python (Don\\'t need\\n            to do if installed in site-packages (see step 8))\\n\\n      For bash:\\n         . $MTK_INSTALLDIR/bin/Mtk_c_env.sh       for C\\n         . $MTK_INSTALLDIR/bin/Mtk_idl_env.sh     for IDL\\n         . $MTK_INSTALLDIR/bin/Mtk_python_env.sh  for Python (Don\\'t need to do\\n            if installed in site-packages (see step 8))\\n\\n   For C examples: $MTK_INSTALLDIR/examples/C\\n   For IDL examples: $MTK_INSTALLDIR/examples/idl\\n   For Python examples: $MTK_INSTALLDIR/examples/python\\n   For C command-line utilities source code: Mtk-src-1.5.X/src\\n   For IDL tests source code: Mtk-src-1.5.X/wrappers/idl\\n\\n8) Optional - You may want to install the MisrToolkit into you Python\\n   site-packages directory\\n\\n   cd Mtk-src-1.5.X/wrappers/python\\n   sudo python setup.py install\\n   setenv LD_LIBRARY_PATH $MTK_INSTALLDIR/lib\\n\\n9) Optional - After installing the Mtk_tmp directory, it\\'s contents is\\n   not needed, unless for reference and may be removed\\n\\n   cd ../..\\n   rm -rf Mtk_tmp\\n\\nWindows\\n\\nSee win64/README.txt in the source or binary bundle.\\n\\nKnown Problems\\n--------------\\n\\nThe IDL routine MTK_FIND_FILELIST() does not work properly.\\nBy default OS X swallows click-events on change of focus.\\n  If IDL test windows on MacOS X 10.14 don\\'t respond to clicks, do this command \\n    \"defaults write org.x.x11 wmclickthrough -bool true\"\\n  Then, once at a shell prompt, restart X11/XQuartz and retry\\n  \"make testidl\".\\n\\nIDL is not compatible with XQuartz versions greater than 2.7.9\\n  If IDL tests return an error of \"Error: attempt to add non-widget child \"dsm\" \\n    to parent \"idl\" which supports only widgets\"\" then you can either downgrade\\n\\tto XQuartz 2.7.9 or follow the procedure described at goo.gl/RvXOXy to add\\n\\t/opt/X11/lib/flat_namespace to the DYLD_LIBRARY_PATH set by your idl\\n\\tlauncher script (e.g. /Applications/harris/idl87/bin/idl)\\n'},\n",
       " {'repo': 'nasa/Rapid-Model-Import-Tool',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '<!DOCTYPE html>\\n    <html>\\n    <head>\\n        <meta http-equiv=\"Content-type\" content=\"text/html;charset=UTF-8\">\\n        <title>RMIT Repo</title>\\n        \\n        <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css\">\\n        <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css\">\\n        \\n        <style>\\n.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }\\n</style>\\n        <style>\\n            body {\\n                font-family: -apple-system, BlinkMacSystemFont, \\'Segoe WPC\\', \\'Segoe UI\\', \\'Ubuntu\\', \\'Droid Sans\\', sans-serif;\\n                font-size: 14px;\\n                line-height: 1.6;\\n            }\\n        </style>\\n        \\n        \\n    </head>\\n    <body>\\n        <h1 id=\"RMIT-Repo\">RMIT Repo</h1>\\n<p>RMIT(Rapid Model Import Tool) is a python executable that processes high fidelity CAD models for use in game engines such as Unity. This repository serves as the code base for this project.</p>\\n\\n<li><strong>William Little</strong> (Civil Service)</li>\\n<li><strong>Joseluis Chavez</strong>(Civil Service)</li>\\n<li><strong>Arjun Ayyangar</strong>(NIFS intern)</li>\\n<li><strong>Tanya Gupta</strong>(NIFS intern)</li>\\n<li><strong>Ben Fairlamb</strong>(NIFS intern)</li>\\n<li><strong>Kyle Mott</strong>(NIFS intern)</li>\\n<li><strong>Talyor Waddell</strong>(Civil Service)</li>\\n\\n</ul>\\n\\n    </body>\\n    </html>'},\n",
       " {'repo': 'nasa/MultiDop',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# MultiDop README\\n\\n<b>Note: MultiDop has been superseded by PyDDA (https://github.com/openradar/PyDDA). It is recommended that you install and use PyDDA rather than MultiDop.</b>\\n\\nMultiDop is a Python-based interface between the C-based DDA code developed at\\nUniversity of Oklahoma and the Python Atmospheric Radiation Measurement\\nRadar Toolkit (Py-ART) software developed at Argonne National Lab. Use this\\nsoftware to perform 3D wind analyses using 2-3 independent Doppler radars.\\n\\nTo install:\\n1. Edit the Makefile in the src/ directory to match the compiler and path\\n   information relevant to your system.\\n2. “python setup.py install” in the master directory.\\n\\nThe installation will compile the DDA binary, which is the analysis engine, as\\nwell as install the overall Python package. To see how to run the software,\\nview the example Jupyter notebook in the examples/ directory.\\n\\nMultiDop has been tested using Python 2.7 and Python 3.6.\\n\\nA conference presentation describing MultiDop and how to use it can be found at https://ams.confex.com/ams/97Annual/webprogram/Paper306647.html\\n\\nGet Py-ART here: http://arm-doe.github.io/pyart/\\n\\nMultiDop also requires numpy and xarray.\\nGet xarray here: http://xarray.pydata.org/en/stable/\\n\\nOriginal C code developed by Corey Potvin (OU/CIMMS) and Alan Shapiro (OU). Subsequent major C code improvements and real-data functionality added by Daniel Betten (OU) and Gordon Carrie (OU).\\n\\nPython code and C code modifications to ingest Py-ART grids were done by Timothy Lang (timothy.j.lang@nasa.gov).  \\n\\n> If you use this software to produce an analysis for a presentation or\\npublication, you *must* cite the following papers:\\nhttp://journals.ametsoc.org/doi/abs/10.1175/2009JTECHA1256.1\\n(Shapiro et al. 2009, JTECH)\\nhttp://journals.ametsoc.org/doi/abs/10.1175/JTECH-D-11-00019.1\\n(Potvin et al. 2012, JTECH)\\n\\nLatest release info:\\n[![DOI](https://zenodo.org/badge/84335317.svg)](https://zenodo.org/badge/latestdoi/84335317)\\n'},\n",
       " {'repo': 'nasa/openmct-demo',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': \"# Open MCT Live Demo\\n\\n__NOTE__: This demo is available online at https://openmct-demo.herokuapp.com. This guide is intended for those wishing to setup and run the Open MCT demo locally. \\n\\n## Dependencies\\nThe Open MCT demo depends on [node.js](https://nodejs.org/en/). Life is also easier with [git](https://git-scm.com/downloads), but it's not a requirement for installing and running the demo.\\n\\n## Setup and Installation\\nEither download or clone the `openmct-demo` repository. Git is the preferred way of working with the Open MCT Demo repository, but it's not a requirement to get the demo running. If you're using git, the steps necessary to download and install the Open MCT demo are included below\\n\\n```\\ngit clone https://github.com/nasa/openmct-demo.git\\ncd openmct-demo\\nnpm install\\nnpm start\\n```\\n\\nIf you're not using git, you can download a [zip of the repository](https://github.com/nasa/openmct-demo/archive/master.zip) and after unzipping it and switching to the directory it was unzipped to, run `npm install` and `npm start`.\\n\\n## Overview\\nThis is a functional demo of the [Open MCT](https://github.com/nasa/openmct) mission operations framework using a combination of real and mock data. The real data is historical weather data taken from the REMS instrument on the Curiosity rover, which is kindly made available via a web service provided by the Centro de Astrobiología of the Spanish National Research Council, to whom we are eternally grateful http://cab.inta-csic.es/rems/wp-content/plugins/marsweather-widget/api.php\\n\\n## Dependency on Open MCT Tutorials\\nThis demo depends on the [Open MCT Tutorial](https://github.com/nasa/openmct-tutorial) project. The demo reuses the realtime and historical servers and their associated adapters, but uses a customized dictionary and spacecraft object. This dependency should be considered when making significant code changes to the tutorials.\\n\"},\n",
       " {'repo': 'nasa/SCH',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Scheduler\\n\\nNASA core Flight System Scheduler Application\\n\\n## Description\\n\\nThe Scheduler application (SCH) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\nThe SCH application provides a method of generating software bus messages at pre-determined timing intervals. This allows the system to operate in a Time Division Multiplexed (TDM) fashion with deterministic behavior. The TDM major frame is defined by the Major Time Synchronization Signal used by the\\ncFE TIME Services (typically 1 Hz). The Minor Frame timing (number of slots executed within each Major Frame) is also configurable.\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/MiniWall',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n    <meta http-equiv=\"refresh\" content=\"0; URL=index.html\" />\\n    <title>MiniWall Software</title>\\n  </head>\\n  <body>\\n    <p>\\n      Read the <a href=\"index.html\">index.html</a> file for information on how the\\n      MiniWall software works and how to use it.\\n    </p>\\n  </body>\\n</html>\\n'},\n",
       " {'repo': 'nasa/kepler-pipeline',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Kepler Science Data Processing Pipeline\\n\\nThe Kepler telescope launched into orbit in March 2009, initiating\\nNASA’s first mission to discover Earth-size planets orbiting Sun-like\\nstars. Kepler simultaneously collected data for ∼160,000 target stars\\nover its four-year mission, identifying over 4700 planet candidates,\\n2300 confirmed or validated planets, and 2100 eclipsing binaries.\\nWhile Kepler was designed to discover exoplanets, the long term,\\nultra-high photometric precision measurements it achieved also make it\\na premier observational facility for stellar astrophysics, especially\\nin the field of asteroseismology, and for variable stars, such as RR\\nLyrae stars. The Kepler Science Operations Center (SOC) was developed\\nat NASA Ames Research Center to process the data acquired by Kepler\\nstarting with pixel-level calibrations all the way to identifying\\ntransiting planet signatures and subjecting them to a suite of\\ndiagnostic tests to establish or break confidence in their planetary\\nnature. Detecting small, rocky planets transiting Sun-like stars\\npresents a variety of daunting challenges, including achieving an\\nunprecedented photometric precision of ∼20 ppm on 6.5-hour timescales,\\nsupporting the science operations, management, and repeated\\nreprocessing of the accumulating data stream.\\n\\nThe scientific objective of the Kepler Mission is to explore the\\nstructure and diversity of planetary systems. This is achieved by\\nsurveying a large sample of stars to:\\n\\n* Determine the abundance of terrestrial and larger planets in or near\\nthe habitable zone of a wide variety of stars;\\n* Determine the distribution of sizes and shapes of the orbits of these planets;\\n* Estimate how many planets are in multiple-star systems;\\n* Determine the variety of orbit sizes and planet reflectivities,\\nradii, masses and densities of short-period giant planets;\\n* Identify additional members of each discovered planetary system\\n  using other techniques; and\\n* Determine the properties of those stars that harbor planetary systems.\\n\\nThis repository contains the source code of the Science Data\\nProcessing Pipeline. Please note that it is not expected that the\\nreader will be able to build or run this this software due to\\nthird-party licensing restrictions and dependencies and other\\ncomplications.\\n\\nThe top-level directory contains the following files:\\n\\n* [kscrm.pdf](kscrm.pdf)  \\nThe Kepler Source Code Road Map. This document contains most of the\\ninformation normally found in GitHub README files. Please read this\\nfirst.\\n* [source-code](source-code)  \\nThe source code itself.\\n* [parameters](parameters)  \\nThe configuration details for the last run of the Kepler Science Data\\nProcessing Pipeline.\\n* [MATHWORKS-LIMITED-LICENSE.docx](MATHWORKS-LIMITED-LICENSE.docx)  \\nThe license for files from MathWorks.\\n* [NASA-OPEN-SOURCE-AGREEMENT.doc](NASA-OPEN-SOURCE-AGREEMENT.doc)  \\nThe license for every other file.\\n\\n## Contact Info\\n\\nFor questions on the science, algorithms, and MATLAB code, please\\ncontact Jon Jenkins \\\\<<Jon.Jenkins@nasa.gov>\\\\>, Co-Investigator for\\nData Processing.\\n\\nFor questions on the \"plumbing\" and Java code, please contact Bill\\nWohler \\\\<<Bill.Wohler@nasa.gov>\\\\>, Senior Software Engineer.\\n\\n## Copyright and Notices\\n\\nThe Kepler Science Data Processing Pipeline code is released under the\\n[NASA Open Source Agreement Version 1.3\\nlicense](NASA-OPEN-SOURCE-AGREEMENT.doc).\\n\\nCode provided by MathWorks is released under the [MathWorks Limited\\nLicense](MATHWORKS-LIMITED-LICENSE.docx).\\n\\nCopyright © 2017 United States Government as represented by the\\nAdministrator of the National Aeronautics and Space Administration.\\nAll Rights Reserved.\\n\\nNASA acknowledges the SETI Institute’s primary role in authoring and\\nproducing the Kepler Data Processing Pipeline under Cooperative\\nAgreement Nos. NNA04CC63A, NNX07AD96A, NNX07AD98A, NNX11AI13A,\\nNNX11AI14A, NNX13AD01A & NNX13AD16A.\\n\\nThis file is available under the terms of the NASA Open Source Agreement\\n(NOSA). You should have received a copy of this agreement with the\\nKepler source code; see the file NASA-OPEN-SOURCE-AGREEMENT.doc.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY\\nWARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,\\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE\\nWILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM\\nTO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER,\\nCONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT\\nOF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY\\nOTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\nFURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\nREGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\nAND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS\\nAGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND\\nSUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT\\'S USE OF\\nTHE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,\\nEXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM\\nPRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT\\nSOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT\\'S SOLE\\nREMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL\\nTERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/cumulus-message-adapter',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Cumulus Message Adapter\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-message-adapter)\\n\\n`cumulus-message-adapter` is a command-line interface for preparing and outputting Cumulus Messages for Cumulus Tasks. `cumulus-message-adapter` helps Cumulus developers integrate a task into a Cumulus Workflow.\\n\\nRead more about how the `cumulus-message-adapter` works in the [CONTRACT.md](./CONTRACT.md).\\n\\n## Releases\\n\\n### Release Versions\\n\\nPlease note the following convention for release versions:\\n\\nX.Y.Z: where:\\n\\n* X is an organizational release that signifies the completion of a core set of functionality\\n* Y is a major version release that may include incompatible API changes and/or other breaking changes\\n* Z is a minor version that includes bugfixes and backwards compatible improvements\\n\\n### Continuous Integration\\n\\n[CircleCI](https://circleci.com/gh/nasa/cumulus-message-adapter) manages releases and release assets.\\n\\nWhenever CircleCI passes on the master branch of cumulus-message-adapter and `message_adapter/version.py` has been updated with a version that doesn\\'t match an existing tag, CircleCI will:\\n\\n* Create a new tag with `tag_name` of the string in `message_adapter/version.py`\\n* Create a new release using the new tag, with a name equal to `tag_name` (equal to version).\\n* Build a `cumulus-message-adapter.zip` file and attach it as a release asset to the newly created release. The zip file is created using the [`Makefile`](./Makefile) in the root of this repository.\\n\\nThese steps are fully detailed in the [`.circleci/config.yml`](./.circleci/config.yml) file.\\n\\n## Development\\n\\n### Dependency Installation\\n\\n```shell\\npip install -r requirements-dev.txt\\npip install -r requirements.txt\\n```\\n\\n### Running Tests\\n\\nRunning tests requires [localstack](https://github.com/localstack/localstack).\\n\\nTests only require localstack running S3, which can be initiated with the following command:\\n\\n```shell\\nSERVICES=s3 localstack start\\n```\\n\\nAnd then you can check tests pass with the following nosetests command:\\n\\n```shell\\nCUMULUS_ENV=testing nosetests -v -s\\n```\\n\\n### Linting\\n\\n```shell\\npylint message_adapter\\n```\\n\\n### Contributing\\n\\nIf changes are made to the codebase, you can create the cumulus-message-adapter zip archive for testing libraries that require it:\\n\\n```shell\\nmake clean\\nmake cumulus-message-adapter.zip\\n```\\n\\nThen you can run some integration tests:\\n\\n```shell\\n./examples/example-node-message-adapter-lib.js\\n```\\n\\n### Troubleshooting\\n\\n* Error: \"DistutilsOptionError: must supply either home or prefix/exec-prefix — not both\" when running `make cumulus-message-adapter.zip`\\n  * [Solution](https://stackoverflow.com/a/24357384)\\n'},\n",
       " {'repo': 'nasa/resample_GLISTIN_DEMs',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# resample_GLISTIN_DEMs\\nThis set of codes is a Python toolbox to download and resample GLISTIN-A radar interferometer digital elevation models from NASA\\'s Oceans Melting Greenland campaign. \\n\\nThe official OMG GLISTIN-A data product is hosted on NASA\\'s <a href=\"https://podaac.jpl.nasa.gov/\">PO.DAAC</a>: \\n<a href=\"https://podaac.jpl.nasa.gov/dataset/OMG_L3_ICE_ELEV_GLISTINA\">OMG_L3_ICE_ELEV_GLISTINA</a>\\n\\n<br>\\n<b>Swath Locations and Numbers</b>\\n<img src=\"GLISTIN-A_DEM_Index_Domains.jpg\" width=300 title=\"GLISTIN Swath Locations\">\\n\\n<br>\\n<b>Example gridded swath after resampling</b>\\n<img src=\"https://podaac.jpl.nasa.gov/Podaac/thumbnails/OMG_L3_ICE_ELEV_GLISTINA.jpg\" width=300  title=\"GLISTIN Swath Locations\">\\n\\n<br><br>\\n\\n## Getting Started\\nRequired Python packages: ```numpy```,```scipy```, ```argparse```, ```requests```, ```pyresample```, ```utm```, ```netCDF4```, ```xarray```, ```osr```\\n\\nFrom a fresh anaconda environment, it is recommended to use this installation sequence:\\n```\\npip install requests\\nconda install -c conda-forge pyresample\\npip install netCDF4\\npip install xarray\\nconda install -c conda-forge gdal\\npip install pyproj\\npip install utm\\npip install bs4\\nconda install -c conda-forge scipy\\n```\\n\\nAfter the installing the required packages,\\n1. Determine a directory on your local machine where the Level 1 data and resampled output products will be stored. This directory is referred to as \"dataFolder\" in the scripts provided.\\n2. Determine the indicies of the GLISTIN-A DEMs to downloaded. Refer to the <a href=\"GLISTIN-A_DEM_Index_Domains.pdf\">Swath Location Map</a> to see the indices of all swaths.\\n\\n## Steps to Generate Resampled GLISTIN-A DEMs in NetCDF format\\n\\n1. Download the Level-2 GLISTIN-A DEMs and associated metadata using **download_level_2_data.py**\\n2. (Optional) Download a geoid layer to reference the elevation measurements to mean sea level\\n3. Resample the Level-2 data using **resample_GLISTIN_DEMs.py** \\n\\n### Step 1: Download the Level-2 data GLISTIN-A DEMs and associated metadata using download_level_2_data.py\\n\\nUse **download_level_2_data.py** to download level 2 data from the [UAVSAR website](https://uavsar.jpl.nasa.gov/).\\n\\nKeywords:\\n- dataFolder (-d): (Required) Directory where resampled data will be stored on local machine.\\n\\n- swathIndices (-i): (Optional) Set of swath indices to resample, separated by spaces.  Default is -1: resample all swaths.\\n \\n- years (-y): (Optional) Set of years to download, separated by spaces. Default: -1 (download swaths in all years 2016-2019). \\n\\nExample command to download only years 2016 and 2017 for swath indices 1,2,3,4, and 52:\\n```\\npython download_level_2_data.py -d \\'/path/to/dataFolder\\' -y 2016 2017 -i 1 2 3 4 52\\n```\\n\\n\\nExample command to download all available years of data for swath index 52:\\n```\\npython download_level_2_data.py -d \\'/path/to/dataFolder\\' -i 52 \\n```\\n\\nAfter downloading the above example (swath 52, all years), you will have a directory structure with files that should look like the following:\\n```\\n> cd /path/to/dataFolder\\n> find .\\n.\\n./Raw\\n./Raw/2017\\n./Raw/2017/Data\\n./Raw/2017/Data/greenl_17914_17037_011_170321_ALTTBB_HH_04.hgt.grd\\n./Raw/2017/Metadata\\n./Raw/2017/Metadata/greenl_17914_17037_011_170321_ALTTBB_HH_04_metadata.txt\\n./Raw/2019\\n./Raw/2019/Data\\n./Raw/2019/Data/greenl_17914_19022_009_190413_ALTTBB_HH_01.hgt.grd\\n./Raw/2019/Metadata\\n./Raw/2019/Metadata/greenl_17914_19022_009_190413_ALTTBB_HH_01_metadata.txt\\n./Raw/2018\\n./Raw/2018/Data\\n./Raw/2018/Data/greenl_17914_18014_005_180315_ALTTBB_HH_01.hgt.grd\\n./Raw/2018/Metadata\\n./Raw/2018/Metadata/greenl_17914_18014_005_180315_ALTTBB_HH_01_metadata.txt\\n./Raw/2016\\n./Raw/2016/Data\\n./Raw/2016/Data/greenl_17914_16037_013_160330_ALTTBB_HH_03.hgt.grd\\n./Raw/2016/Metadata\\n./Raw/2016/Metadata/greenl_17914_16037_013_160330_ALTTBB_HH_03_metadata.txt\\n```\\n\\n### Step 2 (Optional): Download a geoid layer to reference the elevation measurements to mean sea level \\n\\nTo include a geoid file with your resampling, you must include a directory called \\'Geoid\\' within your specified dataFolder that contains your geoid file. Feel free to use any geoid that suits your purposes.  Here use the\\n<a href=https://link.springer.com/article/10.1007/s10712-016-9406-y>GOC05c geoid</a> of Fetcher et al. [2017].\\n\\nTo obtain the GOCO05c geoid and prepare it for use in resample_GLISTIN_DEMs, follow the following steps:\\n1. Go to http://icgem.gfz-potsdam.de/calcgrid\\n2. Under Model Selection, choose Longtime Model -> GOCO05c.\\n3. Under Functional Selection, choose geoid.\\n4. Under Geographic Selection, set longitude bounds to -75.9 to -9.8, latitude bounds to 55.2 to 86.8, and Grid Step to 0.1.\\n5. Leave all other parameters as their default valies, and \\'start computation\\'.\\n6. When file is complete, click Download Grid and save to **dataFolder/Geoid**\\n7. Finally, convert this file to a netCDF file using the **geoid_grid_to_nc.py** function.\\n\\nExample command to convert the geoid grid to a NetCDF file:\\n```\\npython geoid_grid_to_nc.py -d \\'/path/to/dataFolder\\' -g \\'GOCO05c_383e72b1d9fbea44d4c550a7446ff8fcb6a57aba0bfdd6293a3e4b72f86030aa.gdf\\'\\n```\\n\\nTo use a different geoid, you will likely have to modify **geoid_grid_to_nc.py**.\\n\\n\\n### Step 3: Resample the Level-2 data using **resample_GLISTIN_DEMs.py**\\n\\nTo resample data, use **resample_GLISTIN_DEMs.py**\\n\\nKeywords:\\n- dataFolder (-d): (Required) Directory where resampled data will be stored on local machine.\\n\\n- resolution (-r): (Optional) Horizontal resolution in meters of the new grid used for the resampling. Default: 50 (meters).\\n\\n- swathIndices (-i): (Optional) Set of swath indices to resample, separated by spaces.  Default is -1: resample all swaths.\\n\\n- years (-y): (Optional) Set of years to resample, separated by spaces. Default: -1 (resample swaths in all years 2016-2019). \\n\\n- projection (-p): (Optional) The projection of the output DEM. Input with an EPSG reference code as EPSG:XXXX or \\'UTM\\'.  Default: \\'UTM\\', the UTM zone that corresponds to the center lat/long of the swath. This data spans UTM zones 19N to 27N.\\n\\n- addGeoid (-g): (Optional) Choose 1 if you would like to add a geoid correction to the file, 0 otherwise. Default: 0, do not include a geoid file.  Geoid field must be downloaded manually prior to running this script using the instructions provided on github.com/NASA/glistin). \\n\\n\\nExample command to resample all DEMs for all years at 50m resolution, exluding the optional geoid layer:\\n```\\npython resample_GLISTIN_DEMs.py -d \\'/path/to/dataFolder\\' \\n```\\n\\nExample command to resample the DEMs for swath indices 1 2 3 4 and 52 in years 2018 and 2019 at 500m resolution, including the geoid layer:\\n```\\npython resample_GLISTIN_DEMs.py -d \\'/path/to/dataFolder\\' -i 1 2 3 4 52 -y 2018 2019 -g 1 -r 500\\n```\\n\\n#### Benchmarks\\nThe time required to resample the GLISTIN-A DEM data is a function of both resolution and the size of the initial swath. The resample time will increase with the requested grid spacing and number of points in the initital swath.\\n\\nFor example, swath 16 is a relatively small swath and the following wall-clock times were required for the resample:\\n- 50 m: 79 seconds\\n- 100 m: 173 seconds\\n- 500 m: 3332 seconds\\n\\nAs another example, swath 1 is a relatively large swath and the following wall-clock times were required for the resample:\\n- 50 m: 162 seconds\\n- 100 m: 276 seconds\\n- 500 m: 3842 seconds\\n'},\n",
       " {'repo': 'nasa/CrisisMappingToolkit',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# NASA Ames Crisis Mapping Toolkit\\n\\nThe Crisis Mapping Toolkit is a collection of algorithms and utilities for creating maps in response to crisis.\\nThe CMT relies on [Google Earth Engine (EE)](https://earthengine.google.org/) for much of its data processing.\\nThe CMT is released under the Apache 2 license.\\n\\nThe CMT is developed by the NASA Ames Intelligent Robotics Group, with generous support from the Google Crisis\\nResponse Team and the Google Earth Engine Team.\\n\\nThe CMT currently provides:\\n\\n- Algorithms to determine **flood extent from MODIS data**, such as\\n  multiple thresholding techniques, learned approaches, Dynamic Nearest\\n  Neighbor Search, and more.\\n- Algorithms to determine **flood extent from SAR data**, such as\\n  histogram thresholding and active contour.\\n- Algorithms to detect water and clouds in LANDSAT images.\\n- Various helpful utilities, such as:\\n    - An **improved visualization UI**, with a drop down menu of layers\\n      similar to the EE javascript playground.\\n    - **Local EE image download** and processing, for the occasional operation\\n      that cannot be done efficiently in EE.\\n    - A **configurable domain specification** to define problem domains and\\n      data sources in XML.\\n    - Functions for searching image catalogs.\\n\\nThe CMT is still under development, so expect changes and additional features.\\nPlease contact Brian Coltin (brian.j.coltin at nasa dot gov) with any questions.\\n\\n## Installation\\n\\n- Follow the instructions for installing [Google Earth Engine for Python](https://developers.google.com/earth-engine/python_install).\\n- Download the CMT source code from [Github](https://github.com/bcoltin/CrisisMappingToolkit).\\n- Install PyQt4.\\n- Install the CMT with \\n  ```\\n  python setup.py install\\n  ```\\n\\n## Documentation\\n\\nBefore calling any CMT function, you must initialize EE, either by calling\\nee.Initialize, or by using the cmt ee\\\\_authenticate package:\\n\\n```python\\nfrom cmt import ee_authenticate\\nee_authenticate.initialize()\\n```\\n\\n### Using the CMT UI\\n\\nTo use the CMT UI, replace your import of the EE map client:\\n\\n```python\\nfrom ee.mapclient import centerMap, addToMap\\n```\\n\\nwith \\n\\n```python\\nfrom cmt.mapclient_qt import centerMap, addToMap\\n```\\n\\nthen use the centerMap and addToMap functions exactly as before.\\nThat's it!\\n\\n### Using the CMT LocalEEImage\\n\\nSee the documentation in local_ee_image.py. When you construct a LocalEEImage,\\nthe image is downloaded from EE with the specified scale and bounding box\\nusing getDownloadURL. You can then access individual pixels or bands as PIL Images.\\nImages are cached locally so if you are testing on the same image you do not need to\\nwait to download every time. We recommend using LocalEEImage sparingly, only for\\noperations which cannot be performed through EE, as downloading the entire image\\nis expensive in both time and bandwidth.\\n\\n## Data Access\\n\\nData used in our examples has been uploaded as Assets in Earth Engine and should be\\naccessible without any special effort.  Unfortunately some datasets, such as \\nTerraSAR-X, cannot be uploaded.  If you find any missing data sets, contact the\\nproject maintainers to see if we can get it uploaded.\\n\\n## Licensing\\n\\nThe Crisis Mapping Toolkit is released under the Apache 2 license.\\n\\nCopyright (c) 2014, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.\\n\"},\n",
       " {'repo': 'nasa/Coordinate-systems-class-library',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Coordinate Systems Class Library\\nLibrary of classes representing various coordinate systems and providing the transformations between them. Coordinate systems represented are: East-North-Up (ENU), Downrange-Crossrange-Above (DCA), Latitude-Longitude-Altitude (LLA), Earth-Centered-Fixed (ECF), and Azimuth-Elevation-Range (AER).\\n\\n## Notices\\nCopyright 2020 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n \\n## Disclaimers\\n**No Warranty:** THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\n**Waiver and Indemnity:**  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/rbf',\n",
       "  'language': 'Fortran',\n",
       "  'readme_contents': '# rbf\\nMode shape interpolation via radial basis functions\\n\\nSee Examples/hirenasd for scripts showing all steps required to prepare FEM modes for use in FUN3D. The scripts will have to be adjusted for your PBS envirnoment. Also make sure that fun3d and tecplot are in your path. \\n\\nThe following help is shown by typing ./rbf without any arguments.\\n<pre>\\nrbf Version: 2.3.0-2019.10.02\\n\\n Purpose:  Interpolate mode shapes from FEM modes to CFD surface.\\n   Usage:  ./rbf -s fem_mode_shape -d cfd_mesh -i cfd_mode_shape [options] \\n Options:  \\n           -s  source_fem_mode_shape_file\\n           -d  destination_mesh_file\\n           -i  interpolated_mode_shape_file\\n           -iz ignore points whose values are zero\\n           -nk number of source (fem) nodes to keep\\n           -pk percent of source (fem) nodes to keep\\n           -wp write fem_source_points for debugging\\n           -p  primary_surface_file\\n           -b  radial_blend_distance_in_grid_units for use with -p\\n           -x  xsym_blend_distance_in_grid_units\\n           -y  ysym_blend_distance_in_grid_units\\n           -z  zsym_blend_distance_in_grid_units\\n           -cs compute spring connectivity from rbf.nml\\nRequires:  \\n          1) ascci formatted fem mode shapes with \\n             variables x,y,z,f1,f2,f3,f4,f5,f6\\nExamples:  \\n           ./rbf -s fem/mode001.txt -d project_ddfdrive_body1.dat \\\\\\n             -i project_body1_mode1.dat -nk 250 -y 4\\n           ./rbf -s fem/mode001.txt -d ddfdrive_allsurf.dat \\\\\\n             -i project_body1_mode1.dat -p ddfdrive_wing_tail.dat -b 3 -pk 25\\n   Built:  Fri Oct  4 11:20:17 PDT 2019 on Linux 4.12.14-95.19.1.20190617-nasa\\n Version:  2.3.0 (2019.10.02)\\n   About:  Steven.J.Massey@nasa.gov \\n</pre>\\nNotices:\\nCopyright 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nThe following license and copyright notices govern the noted 3rd party software package KDTREE2  included in the NASA SOFTWARE:\\n \\nThe KDTREE2 software is licensed under the terms of the Academic Free Software License, listed herein.  In addition, users of this software must give appropriate citation in relevant technical documentation or journal paper to the author, Matthew B. Kennel, Institute For Nonlinear Science, preferably via a reference to the www.arxiv.org repository of this document, {\\\\tt www.arxiv.org e-print: physics/0408067}.  This requirement will be deemed to be advisory and not mandatory as is necessary to permit the free inclusion of the present software with any software licensed under the terms of any version of the GNU General Public License, or GNU Library General Public License.\\n \\nAcademic Free License\\nVersion 1.1\\n \\nThis Academic Free License applies to any original work of authorship (the \"Original Work\") whose owner (the \"Licensor\") has placed the following notice immediately following the copyright notice for the Original Work: \"Licensed under the Academic Free License version 1.1.\"\\n \\nGrant of License. Licensor hereby grants to any person obtaining a copy of the Original Work (\"You\") a world-wide, royalty-free, non-exclusive, perpetual, non-sublicenseable license (1) to use, copy, modify, merge, publish, perform, distribute and/or sell copies of the Original Work and derivative works thereof, and (2) under patent claims owned or controlled by the Licensor that are embodied in the Original Work as furnished by the Licensor, to make, use, sell and offer for sale the Original Work and derivative works thereof, subject to the following conditions.\\n \\nRight of Attribution. Redistributions of the Original Work must reproduce all copyright notices in the Original Work as furnished by the Licensor, both in the Original Work itself and in any documentation and/or other materials provided with the distribution of the Original Work in executable form.\\n \\nExclusions from License Grant. Neither the names of Licensor, nor the names of any contributors to the Original Work, nor any of their trademarks or service marks, may be used to endorse or promote products derived from this Original Work without express prior written permission of the Licensor.\\n \\nWARRANTY AND DISCLAIMERS. LICENSOR WARRANTS THAT THE COPYRIGHT IN AND TO THE ORIGINAL WORK IS OWNED BY THE LICENSOR OR THAT THE ORIGINAL WORK IS DISTRIBUTED BY LICENSOR UNDER A VALID CURRENT LICENSE FROM THE COPYRIGHT OWNER. EXCEPT AS EXPRESSLY STATED IN THE IMMEDIATELY\\nPRECEEDING SENTENCE, THE ORIGINAL WORK IS PROVIDED UNDER THIS LICENSE ON AN \"AS IS\" BASIS, WITHOUT WARRANTY, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, THE WARRANTY OF NON-INFRINGEMENT AND WARRANTIES THAT THE ORIGINAL WORK IS MERCHANTABLE OR FIT FOR A\\nPARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY OF THE ORIGINAL WORK IS WITH YOU. THIS DISCLAIMER OF WARRANTY CONSTITUTES AN ESSENTIAL PART OF THIS LICENSE. NO LICENSE TO ORIGINAL WORK IS GRANTED HEREUNDER EXCEPT UNDER THIS DISCLAIMER.\\n \\nLIMITATION OF LIABILITY. UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, WHETHER TORT (INCLUDING NEGLIGENCE), CONTRACT, OR OTHERWISE, SHALL THE LICENSOR BE LIABLE TO ANY PERSON FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY CHARACTER ARISING\\nAS A RESULT OF THIS LICENSE OR THE USE OF THE ORIGINAL WORK INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF GOODWILL, WORK STOPPAGE, COMPUTER FAILURE OR MALFUNCTION, OR ANY AND ALL OTHER COMMERCIAL DAMAGES OR LOSSES, EVEN IF SUCH PERSON SHALL HAVE BEEN INFORMED OF THE\\nPOSSIBILITY OF SUCH DAMAGES. THIS LIMITATION OF LIABILITY SHALL NOT APPLY TO LIABILITY FOR DEATH OR PERSONAL INJURY RESULTING FROM SUCH PARTY\\'S NEGLIGENCE TO THE EXTENT APPLICABLE LAW PROHIBITS SUCH LIMITATION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THIS EXCLUSION AND LIMITATION MAY NOT APPLY TO YOU.\\n \\nLicense to Source Code. The term \"Source Code\" means the preferred form of the Original Work for making modifications to it and all available documentation describing how to access and modify the Original Work. Licensor hereby agrees to provide a machine-readable copy of the Source Code of the Original Work along with each copy of the Original Work that Licensor distributes. Licensor reserves the right to satisfy this obligation by placing a machine-readable copy of the Source Code in an information repository reasonably calculated to permit inexpensive and convenient access by You for as long as Licensor continues to distribute the Original Work, and by publishing the address of that information repository in a notice immediately following the copyright notice that applies to the Original Work.\\n \\nMutual Termination for Patent Action. This License shall terminate automatically and You may no longer exercise any of the rights granted to You by this License if You file a lawsuit in any court alleging that any OSI Certified open source software that is licensed under any license containing this \"Mutual Termination for Patent Action\" clause infringes any patent claims that are essential to use that software.\\n \\nThis license is Copyright (C) 2002 Lawrence E. Rosen. All rights reserved. Permission is hereby granted to copy and distribute this license without modification. This license may not be modified without at the express written permission of its copyright owner.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/Kepler-FLTI',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# Kepler-FLTI\\nKepler-FLTI.py - Illustrate using the Flux-Level Transit Injection (FLTI) Tests of TPS for Data Release 25.  The FLTI test output is in the FITs file format.  This code generates the figures in the documentation of FLTI\\n    \\nBurke, C.J. & Catanzarite, J. 2017, \"Planet Detection Metrics: Per-Target Flux-Level Transit Injection Tests of TPS for Data Release 25\", KSCI-19109-001\\n       \\n**Assumptions**: python packages astropy, numpy, and matplotlib are available and file kplr007702838_dr25_5008_flti.fits is available in the same directory as Kepler-FLTI.py\\n      \\n**Running**: python Kepler-FLTI.py\\n    \\n**Output**: Displays a series of figures and generates hardcopy\\n\\nNotices:\\n\\nCopyright © 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nNASA acknowledges the SETI Institute’s primary role in authoring and producing the Plotting Program for Kepler Planet Detection Efficiency Products under Cooperative Agreement Number NNX13AD01A.\\n\\n\\nDisclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/podaac_tools_and_services',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'podaac_tools_and_services\\n=========================\\nThis is a meta-repository which lists locations of code related to all tools and services software for `NASA JPL\\'s Physical Oceanography Distributed Active Archive Center (PO.DAAC) <https://podaac.jpl.nasa.gov>`__.\\n\\n|image7|\\n\\nWhat is PO.DAAC?\\n----------------\\nThe `PO.DAAC <https://podaac.jpl.nasa.gov>`__ is an element of the Earth Observing System Data and Information System (`EOSDIS <https://earthdata.nasa.gov/>`__). The EOSDIS provides science data to a wide community of users for NASA\\'s Science Mission Directorate. `PO.DAAC <https://podaac.jpl.nasa.gov>`__ has become the premier data center for measurements focused on ocean surface topography (OST), sea surface temperature (SST), ocean winds, sea surface salinity (SSS), gravity, ocean circulation and sea ice.\\n\\nWhat\\'s in this repository?\\n--------------------------\\nThis repository reflects an active catalog of all tools and services software pertaining to `PO.DAAC data access <https://podaac.jpl.nasa.gov/dataaccess>`__. If you have a suggestion for a new tool or would like to update the content here, please `open an issue <https://github.com/nasa/podaac_tools_and_services/issues>`__ or `send a pull request <https://github.com/nasa/podaac_tools_and_services/pulls>`__.\\n\\nWhere do I find detailed information on tools and services included in this repository?\\n---------------------------------------------------------------------------------------\\nEach repository has it\\'s own README file e.g. `data_animation/README.rst <https://github.com/nasa/podaac_tools_and_services/blob/master/data_animation/README.rst>`__\\n\\nKeeping Git submodules up-to-date\\n---------------------------------\\nIn order to keep the submodules as defined in [.gitmodules](https://github.com/nasa/podaac_tools_and_services/blob/master/.gitmodules) up-to-date it is necessary to periodically push updates. You can safely execute this command to do so::\\n\\n\\n    $ git submodule foreach git pull origin master\\n    $ git status //you will then see the changes which have been mode\\n    $ git add -A\\n    $ git commit -m \"Update submodules\"\\n    $ git push origin master\\n\\n\\nLicense\\n-------\\n| Unless noted explicitly, all code in this repository is licensed permissively under the `Apache License\\n  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.\\n| A copy of that license is distributed with each software project.\\n\\nCopyright and Export Classification\\n-----------------------------------\\n\\n::\\n\\n    Copyright 2019, by the California Institute of Technology. ALL RIGHTS RESERVED. \\n    United States Government Sponsorship acknowledged. Any commercial use must be \\n    negotiated with the Office of Technology Transfer at the California Institute \\n    of Technology.\\n    This software may be subject to U.S. export control laws. By accepting this software, \\n    the user agrees to comply with all applicable U.S. export laws and regulations. \\n    User has the responsibility to obtain export licenses, or other export authority \\n    as may be required before exporting such information to foreign countries or \\n    providing access to foreign persons.\\n\\n.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png\\n'},\n",
       " {'repo': 'nasa/podaacpy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"podaacpy\\n========\\n\\n|DOI| |license| |PyPI| |documentation| |Travis| |Coveralls| |Requirements Status| |Anaconda-Server Version| |Anaconda-Server Downloads| \\n\\n|DeepSource|\\n\\n|image7|\\n\\nA python utility library for interacting with NASA JPL's\\n`PO.DAAC <https://podaac.jpl.nasa.gov>`__\\n\\n\\nSoftware DOI\\n------------\\n\\nIf you are using Podaacpy in your research, please consider citing the software |DOI|. This DOI represents all versions, and will always resolve to the latest one. If you wish to reference actual versions, then please find the appropriate DOI's over at Zenodo.\\n\\n\\nWhat is PO.DAAC?\\n----------------\\n\\n| The Physical Oceanography Distributed Active Archive Center (PO.DAAC)\\n  is an element of the\\n| Earth Observing System Data and Information System\\n  (`EOSDIS <https://earthdata.nasa.gov/>`__).\\n| The EOSDIS provides science data to a wide community of users for\\n  NASA's Science Mission Directorate.\\n\\nWhat does podaacpy offer?\\n-------------------------\\n\\nThe library provides a Python toolkit for interacting with all\\n`PO.DAAC Web Services v3.2.2 APIs <https://podaac.jpl.nasa.gov/ws>`__, namely\\n\\n-  `PO.DAAC Web Services <https://podaac.jpl.nasa.gov/ws/>`__: services\\n   include\\n-  `Dataset\\n   Metadata <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__\\n   - retrieves the metadata of a dataset\\n-  `Granule\\n   Metadata <https://podaac.jpl.nasa.gov/ws/metadata/granule/index.html>`__\\n   - retrieves the metadata of a granule\\n-  `Search\\n   Dataset <https://podaac.jpl.nasa.gov/ws/search/dataset/index.html>`__\\n   - searches PO.DAAC's dataset catalog, over Level 2, Level 3, and\\n   Level 4 datasets\\n-  `Search\\n   Granule <https://podaac.jpl.nasa.gov/ws/search/granule/index.html>`__\\n   - does granule searching on PO.DAAC level 2 swath datasets\\n   (individual orbits of a satellite), and level 3 & 4 gridded datasets\\n   (time averaged to span the globe)\\n-  `Image\\n   Granule <https://podaac.jpl.nasa.gov/ws/image/granule/index.html>`__ -\\n   renders granules in the PO.DAAC's catalog to images such as jpeg\\n   and/or png\\n-  `Extract\\n   Granule <https://podaac.jpl.nasa.gov/ws/extract/granule/index.html>`__\\n   - subsets a granule in PO.DAAC catalog and produces either netcdf3 or\\n   hdf4 files\\n\\n-  | `Metadata Compliance\\n     Checker <https://podaac-uat.jpl.nasa.gov/mcc>`__: an online tool and\\n     web\\n   | service designed to check and validate the contents of netCDF and\\n     HDF granules for the\\n   | Climate and Forecast (CF) and Attribute Convention for Dataset\\n     Discovery (ACDD) metadata conventions.\\n\\n-  | `Level 2 Subsetting \\n      <https://podaac-tools.jpl.nasa.gov/hitide/>`__: allows users to subset \\n      and download popular PO.DAAC level 2 (swath) datasets.\\n\\n-  | `PO.DAAC Drive <https://podaac-tools.jpl.nasa.gov/drive/>`__: an HTTP based \\n      data access service. PO.DAAC Drive replicates much of the functionality \\n      of FTP while addressing many of its issues.\\n\\nAdditionally, Podaacpy provides the following ocean-related data services \\n\\n- `NASA OceanColor Web <https://oceancolor.gsfc.nasa.gov>`_:\\n\\n- `File Search <https://oceandata.sci.gsfc.nasa.gov/api/file_search>`_ -  locate publically available files within the NASA Ocean Data Processing System (ODPS)\\n- `Bulk data downloads via HTTP <https://oceancolor.gsfc.nasa.gov/forum/oceancolor/topic_show.pl?pid=12520>`_ - mimic FTP bulk data downloads using the `HTTP-based data distribution server <https://oceandata.sci.gsfc.nasa.gov>`_.\\n\\nInstallation\\n------------\\n\\nFrom the cheeseshop\\n\\n::\\n\\n    pip3 install podaacpy\\n    \\nor from conda\\n\\n::\\n\\n    conda install -c conda-forge podaacpy    \\n\\nor from source\\n\\n::\\n\\n    git clone https://github.com/nasa/podaacpy.git && cd podaacpy\\n    python3 setup.py install\\n\\nQuickstart\\n----------\\nCheck out the **examples** directory for our Jupyter notebook examples.\\n\\nTests\\n-----\\n\\n| podaacpy uses the popular\\n  `nose <http://nose.readthedocs.org/en/latest/>`__ testing suite for\\n  unit tests.\\n| You can run the podaacpy tests simply by running\\n\\n::\\n\\n    nosetests\\n\\nAdditonally, click on the build sticker at the top of this readme to be\\ndirected to the most recent build on\\n`travis-ci <https://travis-ci.org/nasa/podaacpy>`__.\\n\\nDocumentation\\n-------------\\n\\nYou can view the documentation online at\\n\\nhttp://podaacpy.readthedocs.org/en/latest/\\n\\nAlternatively, you can build the documentation manually as follows\\n\\n::\\n\\n    cd docs && make html\\n\\nDocumentation is then available in docs/build/html/\\n\\nCommunity, Support and Development\\n----------------------------------\\n\\n| Please open a ticket in the `issue\\n  tracker <https://github.com/nasa/podaacpy/issues>`__.\\n| Please use\\n  `labels <https://help.github.com/articles/applying-labels-to-issues-and-pull-requests/>`__\\n  to\\n| classify your issue.\\n\\nLicense\\n-------\\n\\n| podaacpy is licensed permissively under the `Apache License\\n  v2.0 <http://www.apache.org/licenses/LICENSE-2.0>`__.\\n| A copy of that license is distributed with this software.\\n\\nCopyright and Export Classification\\n-----------------------------------\\n\\n::\\n\\n    Copyright 2016-2019, by the California Institute of Technology. ALL RIGHTS RESERVED. \\n    United States Government Sponsorship acknowledged. Any commercial use must be \\n    negotiated with the Office of Technology Transfer at the California Institute \\n    of Technology.\\n    This software may be subject to U.S. export control laws. By accepting this software, \\n    the user agrees to comply with all applicable U.S. export laws and regulations. \\n    User has the responsibility to obtain export licenses, or other export authority \\n    as may be required before exporting such information to foreign countries or \\n    providing access to foreign persons.\\n\\n.. |DOI| image:: https://zenodo.org/badge/DOI/10.5281/zenodo.1751972.svg\\n   :target: https://doi.org/10.5281/zenodo.1751972\\n.. |license| image:: https://img.shields.io/github/license/nasa/podaacpy.svg?maxAge=2592000\\n   :target: http://www.apache.org/licenses/LICENSE-2.0\\n.. |PyPI| image:: https://img.shields.io/pypi/v/podaacpy.svg?maxAge=2592000?style=plastic\\n   :target: https://pypi.python.org/pypi/podaacpy\\n.. |documentation| image:: https://readthedocs.org/projects/podaacpy/badge/?version=latest\\n   :target: http://podaacpy.readthedocs.org/en/latest/\\n.. |Travis| image:: https://img.shields.io/travis/nasa/podaacpy.svg?maxAge=2592000?style=plastic\\n   :target: https://travis-ci.org/nasa/podaacpy\\n.. |Coveralls| image:: https://coveralls.io/repos/github/nasa/podaacpy/badge.svg?branch=master\\n   :target: https://coveralls.io/github/nasa/podaacpy?branch=master\\n.. |Requirements Status| image:: https://requires.io/github/nasa/podaacpy/requirements.svg?branch=master\\n   :target: https://requires.io/github/nasa/podaacpy/requirements/?branch=master\\n.. |Anaconda-Server Version| image:: https://anaconda.org/conda-forge/podaacpy/badges/version.svg\\n   :target: https://anaconda.org/conda-forge/podaacpy\\n.. |Anaconda-Server Downloads| image:: https://anaconda.org/conda-forge/podaacpy/badges/downloads.svg\\n   :target: https://anaconda.org/conda-forge/podaacpy\\n.. |image7| image:: https://podaac.jpl.nasa.gov/sites/default/files/image/custom_thumbs/podaac_logo.png\\n.. |DeepSource| image:: https://static.deepsource.io/deepsource-badge-light.svg\\n    :target: https://deepsource.io/gh/nasa/podaacpy/?ref=repository-badge\\n\\n\"},\n",
       " {'repo': 'nasa/terraform-aws-cumulus-thin-egress-app',\n",
       "  'language': 'HCL',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'nasa/VIIRS-demo',\n",
       "  'language': None,\n",
       "  'readme_contents': 'The documentation for the VIIRS Demonstration Software is contained in\\ndocumentation/NTR_1571760412_2019-11-14.pdf, documentation/SectionI.pdf\\nand documentation/SectionII.pdf.\\n\\n'},\n",
       " {'repo': 'nasa/SIL',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': \"# Overview\\n\\nThe Simulink Interface Layer (SIL) is an extention of the Simulink Coder generation tool which allows it to generate code which is compatible with the cFS ECI (External Code Interface). The ECI is a software abstraction layer which allows the interfacing of task/mission-specific code generated from Simulink (or other sources) to the Core Flight System (cFS) via a generic set of wrapper code. The SIL and ECI combined enables direct integration of code from the Simulink autocoding pipeline into the cFS without the need for interface code and allows access to cFS API's including table services, time services, the software bus, event services, and fault reporting. \\n\\nThe SIL accomplishes this by extending Simulink's code generation pipeline to produce a description of the model's interfaces which is utilized by generic wrapper code to initialize the model and the appropriate cFS interfaces. The SIL simplifies the integration of code as a CFS application and eliminates the need for hand edits to generated code, allowing quicker integration of code and reducing the probability of human error in the integration process.\\n\\n# Getting Started\\n\\nThe SIL is intended to be compatible with Matlab/Simulink 2017b and above on all Matlab-supported platforms. \\n\\nUsage of the SIL requires the following Matlab Toolboxes:\\n\\n- Simulink\\n- Embedded Coder\\n- Simulink Coder\\n- Matlab Coder\\n\\nPlease see the Simulink integration guide (located in the [doc](doc/) directory) for specific instructions on generating and integrating SIL-compatible code.\\n\\n# Testing [![build passing](https://travis-ci.com/nasa/SIL.svg?branch=master)](https://travis-ci.com/nasa/SIL/)\\n\\nThe SIL has been used with 2017b and 2018b and is currently tested with 2018b on Windows and Ubuntu. There are no known incompatibilities with supported versions or platforms, but they are not actively tested. \\n\\n# Feedback\\n\\nPlease submit bug reports and feature requests via Github issues. Feature requests will be considered as they align with the requirements of NASA missions which are using the SIL and to the extent that they minimize deviation from standard Matlab/Simulink analysis/development workflows and features.\"},\n",
       " {'repo': 'nasa/QuIP',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '\\nQuIP - Quick Image Processing\\n\\n2/9/2012\\tconverting to git repositories\\n3/25/2016\\tinstalled on guthub\\n\\nHow to compile without optimization?\\n\\n'},\n",
       " {'repo': 'nasa/MCT-Plugins',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'MCT-Plugins\\n===========\\n\\nEach plug-in may be built in one of two ways. If building MCT from source, these may be included alongside other projects and packaged by making appropriate changes to pom.xml within an assembly. As a convenience, ant scripts are included to build as stand-alone plugins. An existing platform build (such as the evaluation version) is required and may be specified as a property \"mct.dir\", i.e.:\\n\\nant -Dmct.dir=/Applications/MCT\\n\\n\\nAncestor View:\\n\\nA plug-in for viewing a graph of referencing components within MCT. Select \"Ancestor View\" to see a graph indicating which components (such as collections) refer to this component, with more information further up the tree.\\n\\n\\n**Chronology:**\\n\\nA set of interfaces used for communicating time-stamped information between plug-ins. \\n\\n\\n**Notebook:**\\n\\nA plug-in for making and maintaining notes within MCT. Notes may be annotated with other objects, such as telemetry elements, by dragging and dropping them into the note\\'s text field. Notes are also time-stamped, so they can be viewed in time-enabled views (such as timelines). Depends upon Chronology.\\n\\n\\n**Timeline:**\\n\\nA plug-in for viewing time-stamped information (notes, events) in a graphical timeline. The \"Timeline\" view shows event sequences, such as notebook entries, horizontally in relation to their occurrence in time. These events may be reorganized using drag and drop if the event sequence permits changes. Depends upon Chronology.\\n\\n\\n**Earth View:**\\n\\nA plug-in for viewing state vectors relative to the Earth. To view, create an \"Orbit\" object from the Create menu. You may set initial vectors (units are km and km/s respectively, and position is relative to Earth\\'s center; orbits are approximated at an accelerated rate and are not physically accurate). The resulting collection of state vectors can be viewed as spatial coordinates using the \"Orbit\" view.\\n\\nContains a true-color image of the Earth, owned by NASA, from the Visible Earth catalog. \\n\\nhttp://visibleearth.nasa.gov/view.php?id=73909\\n\\nR. Stockli, E. Vermote, N. Saleous, R. Simmon and D. Herring (2005). The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observatory. Corresponding author: rstockli@climate.gsfc.nasa.gov\\n\\n\\n**Quickstart Persistence:**\\n\\nProvides a simple in-memory persistence service populated with a small number of components and displays. To use, the compiled jar should be placed in the resources/platform of an MCT installation, in lieu of databasePersistence-1.1.0.jar. Note that the example plugin may need to be moved from resources/plugins to resources/platform as well, as this quickstart persistence service is pre-populated with example telemetry components.\\n\\n\\n**SatelliteTracker:**\\n\\nThis plug-in allows users to create satellites in MCT and track their orbits, in real-time, through various views:\\n <UL>\\n   <LI> Satellite Orbits in 3D via the Earth View plug-in.\\n   <LI> Real-time locations on a 2D Mercator Projection (A new to MCT; created within this plug-in).\\n   <LI> All of the views that come standard with the core-MCT distribution (i.e.: MultiColumn View, Plots over time, Alpha, etc.).\\n </UL> \\n\\nAlong with adding an interesting data-source to MCT, the true purpose of this plug-in is that the SatelliteTracker serves as a concrete example on how to write a plug-in for MCT.  As SatelliteTracker is a flagship example for plug-in development, comments have been added throughout the source-files to guide the developer on the design-style and requirements put-forth by MCT\\'s structure (and similarly, a section on the Wiki concerning developing-with MCT references the source contained within SatelliteTracker).\\n\\nSimilar to EarthView, this plug-in contains two true-color image of the Earth (one with and one without snow), owned by NASA, from the Visible Earth catalog.\\n\\nhttp://visibleearth.nasa.gov/view.php?id=73909\\n\\nR. Stockli, E. Vermote, N. Saleous, R. Simmon and D. Herring (2005). The Blue Marble Next Generation - A true color earth dataset including seasonal dynamics from MODIS. Published by the NASA Earth Observatory. Corresponding author: rstockli@climate.gsfc.nasa.gov\\n'},\n",
       " {'repo': 'nasa/ADOPT',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# ADOPT\\n\\nAlthough aviation accidents are rare, safety incidents occur more frequently and require a careful analysis to detect and mitigate risks in a timely manner. Analyzing safety incidents using operational data and producing event-based explanations is invaluable to airline companies as well as to governing organizations such as the Federal Aviation Administration (FAA) in the United States. However, this task is challenging because of the complexity involved in mining multi-dimensional heterogeneous time series data, the lack of time-step-wise annotation of events in a flight, and the lack of scalable tools to perform analysis over a large number of events. We propose a precursor mining algorithm: Automatic Discovery of Precursors in Time series data (ADOPT) that identifies events in the multidimensional time series that are correlated with the safety incident. Precursors are valuable to systems health and safety monitoring and in explaining and forecasting safety incidents. Current methods suffer from poor scalability to high dimensional time series data and are inefficient in capturing temporal behavior. We propose an approach by combining multiple-instance learning (MIL) and deep recurrent neural networks (DRNN) to take advantage of MIL\\'s ability to learn using weakly supervised data and DRNN\\'s ability to model temporal behavior. \\n\\n\\nThe objective of this project is to automate the analysis of flight safety incidents in a way that scales well and offers explanations. These explanations include:\\n\\n* When the degraded states start to appear?\\n* What are the degraded states?\\n* What is the likelihood of the event is to occur?\\n* What corrective actions can be taken?\\n\\nThis project aims to:\\n\\n* Create a novel deep temporal multiple-instance learning (DT-MIL) framework that combines multiple-instance learning with deep recurrent neural networks suitable for weakly-supervised learning problems involving time series or sequential data. \\n* Provide a novel approach to explaining safety incidents using precursors mined from data.\\n* Deliver a detailed evaluation of the DT-MIL model using real-world aviation data and comparison with baseline models. \\n* Perform a precursor analysis and explanation of high speed exceedance safety incident using flight data from a commercial airline\\n\\n\\n\\n\\n\\n\\n\\nThis repository contains the following files in its top level directory:\\n\\n* [source](source)  \\nThe source code of the repository, this includes the ADOPT model, GUI configuration tools, and a command line program that utilizes the model.\\n\\n* [documentation](documentation)  \\nDocuments describing how to configure and run the program, as well as how to interpret the results. \\n\\n* [datasets](datasets)  \\nA directory containing a sample dataset. Other datasets may also be added here by the user.\\n\\n* [requirements.txt](requirements.txt)   \\nGeneral module requirements for the program. A more specific requiremnts.txt can be found in [source](source).\\n\\n\\n* [ADOPT NASA Open Source Agreement.pdf](ADOPT%20NASA%20Open%20Source%20Agreement.pdf)  \\nLicensing for ADOPT\\n* [ADOPT Individual CLA.pdf](ADOPT%20Individual%20CLA.pdf)  \\nNASA Individual Contributor License Agreement\\n* [ADOPT Corporate CLA.pdf](ADOPT%20Corporate%20CLA.pdf)   \\nNASA Corporate Contributor License Agreement\\n\\n\\n\\n\\n## Contact Info\\n\\nNASA Point of contact: Nikunj Oza <nikunj.c.oza@nasa.gov>, Data Science Group Lead.\\n\\nFor questions regarding the research and development of the algorithm, please contact Bryan Matthews <bryan.l.matthews@nasa.gov>, Senior Research Engineer.\\n\\nFor questions regarding the source code, please contact Daniel Weckler <daniel.i.weckler@nasa.gov>, Software Engineer.\\n\\n\\n## Copyright and Notices\\n\\nNotices:\\n\\nCopyright © 2019 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nDisclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/utm-docs',\n",
       "  'language': 'Shell',\n",
       "  'readme_contents': \"# The UTM Documentation Repository\\n\\nThis repository is for maintaining publicly available NASA UTM documentation.  Read on for further details on why this repo exists.\\n\\n## Repository Status\\nAs of the time of this edit (21 Mar 2018), this repo is in a formation stage.  It should NOT be used as 'truth' for the state of UTM documentation by any entity at this time.\\n\\nThe repository will not ever be a comprehensive site for all UTM documentation.  There are many NASA UTM documents that are not yet ready for public release.  As those mature to a state where public release is reasonable, we will endeavor to include them here if they add to understanding the core UTM System concepts and implementation.\\n\\n## Audience\\nThe primary audience for this repository are current NASA UTM partners or those partners that are currently in the on-boarding process.  There may be value to others as well.\\n\\n## Externally Available Documentation Collections\\nIn this section we link to important documentation already available elsewhere.\\n\\n| External location        | Description |\\n| ------------------------ | ------------- |\\n| [NASA UTM Publication Repository](https://utm.arc.nasa.gov/documents.shtml) | This the full repository for all conference publications, NASA technical memos, and similar documents. We'll try to pull some of the key docs directly into this repository. |\\n| [NASA UTM APIs](https://github.com/nasa/utm-apis) | A github repository representing the truth in terms of the current state of NASA APIs. |\\n| [SwaggerHub API Documentation](https://app.swaggerhub.com/search?owner=utm)  | A convenience view of the NASA UTM APIs. *NOTE THAT THE GITHUB REPO IS TRUTH FOR THE APIs*. It is possible for the SwaggerHub documentation and our github API repo to drift. Developers must always reference the github repo for building compliant systems. Swagger hub is nice for navigating and discussing the API with others. | \\n\\n## Primary Concept Documents\\n| Document        | Pub Date | Description |\\n| ------------------------ | --- | ------------- |\\n| [UTM: Enabling Low-Altitude Airspace and UAS Operations](UTM-Original-TM-20140013436.pdf) | 2014 April | The original NASA Technical Memo describing UTM. |\\n| [Unmanned Aircraft System Traffic Management (UTM) Concept of Operations](Aviation2016_UTMConOps_AsPublished_v2.pdf) | 2016 June | The first version of the UTM ConOps. |\\n\\n## Documentation Supporting Software Development\\n\\n## Working Group Documents\\nThis section will contain publicly-releaseable documents that originated from within each working group within NASA UTM.  Again, we emphasize that these document lists do not represent the depth and breadth of work since not all documents are able to be publicly released.  We will endeavor to keep this list up to date, but also encourage folks to submit a github issue if you think something is missing or have a suggestion for a helpful doc to include.\\n\\nThe structure of the working groups is somewhat driven by the [NASA-FAA Research Transition Team for UTM](https://www.faa.gov/uas/research/utm/) organization structure.  Please see the [UTM RTT Plan](FAA_NASA_UAS_Traffic_Management_Research_Plan.pdf) for more details on the Research Transition Team.\\n\\n### Concepts Working Group\\nThe CWG will define the concept of UTM in terms of overall conceptual principles and\\nassumptions, including those associated with operations, supporting architecture,\\ninformation flows and exchanges, and FAA and UAS operator roles and responsibilities.\\nThis scoping and definition will: (1) ensure consistent messaging of a coordinated\\nFAA/NASA view of UTM; (2) guide the efforts of other UTM RTT working groups; and\\n(3) support the development of the UTM pilot program.\\n\\n### Data and Information Architecture Working Group\\nThe objective of the DWG is to identify and collaboratively research and develop\\ntechnical capabilities for the data/information exchange needed across stakeholders to\\nsupport UAS operations that meet NAS service expectations. The data exchange and\\ninformation architecture subgroup will work in conjunction with the Concepts &\\nScenarios, SAA, and Communications & Navigation subgroups to identify the data\\nexchange and information gaps and/or deltas associated with the UTM concepts,\\nstrategies, and system capabilities to support safe expansion of the UAS operating\\nenvelope across the NAS structure.\\n\\n| Document        | Pub Date | Description |\\n| ------------------------ | --- | ------------- |\\n| [UAS Reports (UREPs): Enabling Exchange of Observation Data Between UAS Operations](Rios_NASA-TechMemo_219462_UTM_UREP_20170214.pdf) | 2017 Feb | Description of the UREP concept and data schema. |\\n| [UTM Data Working Group Demonstration 1: Final Report](Rios_NASA-Tech-Memo-2017-219494v2.pdf) | 2017 April | Description and results from a collaborative simulation in support of TCL2. |\\n\\n\\n### Sense and Avoid Working Group\\nThe objective for this subgroup is to explore operator solutions to ensure that unmanned\\naircraft do not collide with other aircraft (unmanned or manned).\\n\\n### Communications and Navigation Working Group\\nThe objective for this subgroup is to explore operator solutions to ensure that UA are\\nunder operational control of the pilot (to the degree appropriate to the scenario) and\\nremain within a defined area (around a planned trajectory or as a defined area).\\n\"},\n",
       " {'repo': 'nasa/PointCloudsVR',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# PointCloudsVR\\nPointCloudsVR is a C++ Windows 10 desktop application that displays Point Clouds in Virtual Reality (VR) using [OpenVR](https://github.com/ValveSoftware/openvr) and [SteamVR](https://store.steampowered.com/steamvr).  It is built upon a number of open source libraries: [OpenSceneGraph](http://www.openscenegraph.org/), [OpenFrames](https://github.com/ravidavi/OpenFrames), [liblas](https://liblas.org/), and several others, as described in the [PointCloudsVR User Manual](doc/PointCloudsVR_User_Manual.pdf).  It can display several Point Cloud file formats out of the box: .las, .ply, and others supported by OpenSceneGraph, as well as custom formats, as defined in the C++ code for this project.  It has various 3D analysis tools, such as the ability to draw 3D lines in space.  It can be used for science data analysis that uses Point Clouds for its data format.  PointCloudsVR has been used to display and analyze: Lidar of trees, Lidar of snow, Gaia (an ESA spacecraft) star data, and the solar wind flowing around Mars, all in Virtual Reality.  The user may navigate 3D space using one\\'s VR headset and controllers. PointCloudsVR has been tested with both the HTC Vive / Vive Pro, and Oculus Rift.\\n\\n### Build and Run Instructions\\nSee [PointCloudsVR User Manual](doc/PointCloudsVR_User_Manual.pdf).  The manual also shows 2 screenshots of different applications (Gaia star data, and an .las file showing a tree) toward the end of the document.\\n\\n### Copyright Notice\\nCopyright © 2019 United States Government as represented by the Administrator of\\nthe National Aeronautics and Space Administration. No copyright is claimed in the\\nUnited States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\n### Contact Information\\n    Thomas Grubb\\n    NASA Goddard Space Flight Center\\n    Code 587\\n    Greenbelt, MD 20771\\n    \\n    Thomas.G.Grubb@nasa.gov\\n\\n### License Information\\n\\nThis software is licensed under the [NASA Open Source Agreement](LICENSE.txt).\\n\\n### Example of PointCloudsVR showing an .las Lidar file in VR:\\n\\n![Sample VR Screenshot](data/images/Screenshot%202017-12-12%2016.21.59.png \"Sample VR Screenshot\")\\n'},\n",
       " {'repo': 'nasa/Analysis_SDK', 'language': None, 'readme_contents': ''},\n",
       " {'repo': 'nasa/BrightComets',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'BrightComets Module\\n===================\\n\\nThe BrightComets Module detects bright comets in fits images taken by WISE\\nand in the future NEOCAM. These images can be downloaded from irsa.ipac.caltech.edu.\\n\\nThe repository may be downloaded and easily modified to detect objects in your own fits or jpg image dataset. E.g., detecting stars, galaxies, planets, or even non-astronomical data like street lights or people.\\n\\nThe code handles fits and jpg images. All region files must be compatible with SAO Image DS9 and have the .reg extension. \\n\\nWARNING: The code has been tested with an anaconda3 installation of python 3.6.5. It has not been tested with other installations of python\\n\\nAuthors: Nathan Blair, ncblair@me.com, Joe Masiero, Emily Kramer, Jana Chelsey, Adeline Paiment\\n\\nQuick Installation for Comet Detection\\n--------------------------------------\\n\\nUse this installation if you only want to use the package for detecting comets in wise images\\n\\nPreferably, do this in a python 3.6.5 virtual environment. If you have anaconda3, run:\\n::\\n    conda create -n brightcomets python=3.6.5 anaconda\\n    source activate brightcomets\\n\\nAnd when you are finished using the program, you can deactivate your environment by closing the terminal window or running:\\n::\\n    source deactivate\\n\\nIf you ever want to delete your virtual environment:\\n::\\n    conda remove --name brightcomets --all\\n\\nIn a terminal window:\\n**WARNING: MAKE SURE YOUR PATH HAS NO SPACES OR SPECIAL CHARACTERS**\\n::\\n    git clone https://github.com/nasa/BrightComets\\n    pip install -e BrightComets\\n\\nDetection\\n---------\\n\\nIn a python environment (you may need to run python with the command \"pythonw\" if you get an error saying Python is not installed as a framework):\\n\\nthe detector can take in 12 micrometer wavelength \\nrange infrared images taken by WISE/NEOWISE by specifying \\nthat you want to look detect in band_3 band_3\\n\\n.. code:: python \\n\\n    from brightcomets import pipeline\\n    pipeline.detect(\"path/to/12um_fits_file\", im_type=\"band_3\")\\n\\nor, if you have access to all 4 bands\\n\\n.. code:: python \\n\\n    from brightcomets import pipeline\\n\\n    # takes a list of fits files, from low ir bands to high ir bands.\\n    # intended for bands 2-4 (i.e., the list should include bands 1-4 or 2-4)\\n    pipeline.detect([\"path/to/low_band_fits_file\", \"path/to/next_band_fits_file\", \\n                    \"path/to/next_band_fits_file\", \"path/to/high_band_fits_file\"],\\n                    im_type=\"composite\")\\n\\n\\nTry this out on the given example fits data:\\n\\n.. code:: python\\n\\n    # From BrightComets\\n    pipeline.detect(\"brightcomets/WISE_data/00808a064/00808a064-w3.fits\", im_type=\"band_3\", do_show=True)\\n    import os\\n    pipeline.detect([f for f in os.listdir(\"brightcomets/WISE_data/00808a064\") if f.endswith(\".fits\")], im_type=\"composite\", do_show=True)\\n\\n\\nLong Installation for Custom Training/Retraining\\n------------------------------------------------\\n\\n**WARNING: IMAGE AND REGION FILES POINTED AT BY THE RETRAINING SCRIPT MAY BE ALTERED OR DELETED BY THE PROGRAM. KEEP A COPY OF YOUR FILES ELSEWHERE ON YOUR COMPUTER**\\n\\n**WARNING: IF YOU ALREADY INSTALLED A COPY OF BRIGHTCOMETS IN THE SHORT INSTALLATION, IT IS A GOOD IDEA TO REMOVE YOUR INSTALLATION BEFORE THE LONG INSTALLATION WITH \"pip uninstall brightcomets\"**\\n\\n0. (Optional, but highly recommended) Activate a virtual environment to install dependencies for the project. \\n    If you have conda, you can run:\\n    ::\\n        conda create -n brightcomets python=3.6.5 anaconda\\n        source activate brightcomets\\n    If you do not have conda, consult an online tutorial, but make sure that python version is 3.6.5\\n\\n    When you want to stop working on this project, don\\'t forget to run\\n    ::\\n        source deactivate\\n\\n    If you ever want to delete your virtual environment:\\n    ::\\n        conda remove --name brightcomets --all\\n\\n\\n1. Get the code, install requirements. \\n    In a terminal window: \\n    ::\\n        git clone https://github.com/joemasiero/BrightComets\\n        cd BrightComets\\n        pip install -r requirements.txt\\n\\n\\n2. Install the tf object_detection library in the BrightComets directory\\n    First, go to https://github.com/google/protobuf/releases and download protobuf-all-3.6.1.tar.gz\\n    ::\\n        # From the location where you downloaded protobuf (possibly Downloads)\\n        tar -xvf protobuf-all-3.6.1.tar.gz\\n        cd protobuf-3.6.1\\n        ./configure\\n        # This may take a while\\n        make\\n        sudo make install\\n        protoc --version # check installation worked\\n    Then, run the following commands:\\n    ::\\n        # From path/to/BrightComets\\n        git clone https://github.com/tensorflow/models.git\\n        cd models\\n        git checkout 3a05570f8d5845a4d56a078db8c32fc82465197f\\n        cd ..\\n        git clone https://github.com/cocodataset/cocoapi.git\\n        cd cocoapi/PythonAPI\\n        make\\n        cp -r pycocotools ../../models/research\\n        cd ../../models/research\\n        protoc object_detection/protos/*.proto --python_out=.\\n\\n    More info about this step can be found here_:\\n\\n    .. _here: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md\\n\\n3. Run the file_organization script. \\n    This will make some compatibility changes to the object_detection library and also add a line to your ~/.bash_profile file so that the object_detection library can be properly imported. \\n    ::\\n        # From BrightComets/brightcomets\\n        # cd ../../brightcomets\\n        python file_organization.py\\n    If this command gives you an error, you may not have a ~/.bash_profile file. And you will have to manually type the following line whenever you open a new terminal window and want to run the retraining script.\\n    ::\\n        # If the previous command gave an error\\n        # From BrightComets/models/research/\\n        export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\\n    Now, the library should be installed and prepared for retraining. \\n\\n\\n4. Download `SAO Image DS9`_ if you do not already have it\\n\\n    .. _SAO Image DS9: http://ds9.si.edu/site/Download.html\\n\\n5. Compile training data\\n    **WARNING: IMAGE AND REGION FILES POINTED AT BY THE RETRAINING SCRIPT MAY BE ALTERED OR DELETED BY THE PROGRAM. KEEP A COPY OF YOUR FILES ELSEWHERE ON YOUR COMPUTER**\\n\\n    Change to your BrightComets/brightcomets directory\\n\\n    Compile a folder with .fits and .reg files in it. For example, take a look at the WISE_data folder. The files don\\'t need to be structured in any specific way, however matching fits and regions files should have the same name.\\n\\n    File naming conventions:\\n\\n    A. Image files may either be of type .jpg or .fits. All .jpg files will be converted to .fits. \\n\\n    B. Image files will be resized to the shape 512 by 512. If they are not 512x512, they will be stretched to 512x512. \\n\\n    C. All image files should have unique names. \\n\\n    D. Regions are specified as .reg files and use the syntax conventions from SAO Image DS9 region files. They must have the .reg file extension. \\n\\n    E. All region files should correspond to a fits file by having the same name as that image file. \\n\\n    F. The dash (\"-\") character is an important keyword\\n        i. They allow the user to specify that two images represent different channels of the same image. \\n\\n        ii. For example, the files image1-w1, image1-w2, and image1-w3 will all be placed in the image1 folder, and can be used to make a composite image with w1 representing the red channel, w2 representing the green channel, and w3 representing the blue channel. \\n\\n        iii. The program supports up to 4 channels of the same image. They should be named w1, w2, w3, and w4. \\n\\n        iv. When training, you will have the option to train on all channels separately, just channel 3 (w3), or a composite of the three highest channels. \\n\\n        v. If there are two dash (\"-\") characters in the name of a file, the second dash and everything after it will be truncated. i.e. image1-w1-int1-abc.fits will be truncated to image1-w1 and placed into the w1 folder\\n\\n        vi. A region file corresponding to the composite of images in a folder will have the extension -comp. For example, a file may be named image1-comp.reg\\n\\n        vii. Be very careful when having dashes in your file names. They should only be used when followed by either w1, w2, w3, w4 or comp. And, they will signify that the images are different channels of the same image. \\n\\n        viii. When region and fits files are matched into folders, they are matched only by the uniqueness of the name before the first dash. Once the files have been organized, regions and fits files of corresponding channel will be matched during runtime. For example, image1-w2.fits will be matched with image1-w2.reg\\n\\n    **What if I don\\'t have region files?**\\n\\n    You will be prompted upon running the retraining script to create your own regions\\n\\n    **What if I have region files without corresponding fits files?**\\n\\n    Those region files will be deleted. Make sure to keep a copy elsewhere on your computer! \\n\\n6. Use config.py to set hyperparameters for training. \\n    A. Open BrightComets/brightcomets/config.py\\n\\n    B. Open SAO Image DS9, and keep it open for the rest of the training process.\\n\\n    C. Go to File > XPA > Information\\n\\n    D. In the config.py file, copy the text after XPA_METHOD and set the variable FITS_XPA_METHOD = \"YOUR_XPA_METHOD\"\\n\\n    E. In the config.py file, set pyversion to the command that you type in the terminal to invoke python 3.6.5. This is probably just \"python\", but could also be \"pythonw\" or \"python3\", for example. This is necessary because the program makes system calls to run the training and evaluation scripts. \\n\\n    F. If you are using custom training data, set color_key to a dictionary describing how your annotations are labelled, or how you wish them to be labelled if they are not yet labelled. Use default_color_key as a reference. \\n\\n    G. Default image resizing is to 512x512. If you change the image_size parameter here, it will be changed everywhere for all networks. It is recommended that you do not change this parameter, as changing the image size has not been heavily tested. \\n\\n7. Run the retraining script\\n    In a terminal window: \\n\\n    This will display all the retraining options you have\\n    ::\\n        # From BrightComets/brightcomets\\n        python retrain.py -h\\n\\n    If you get an error saying python is not installed as a framework, try\\n    ::\\n        # replace all future calls to python with pythonw\\n        pythonw retrain.py -h\\n\\n        # Also, change your pyversion variable in the config.py file. \\n\\n    The command you will run will look something like\\n    ::\\n        # From BrightComets/brightcomets\\n        # Make sure to replace Your_Custom_Data folder with the path to your folder\\n        python retrain.py --im_type band_3 --update_records --data_folders Your_Custom_Data --retrain --train_iterations 5000 --classes comet\\n\\n    This script does training, evaluation, and metrics all at once. \\n\\n    You can see the training and evaluation progress by going to a web browser while the script is running and searching **localhost:6006**\\n\\n8. Once you are satisfied with your model, you can download it. \\n    From the BrightComets directory\\n    ::\\n        # Inside the BrightComets Directory\\n        pip install -e .\\n    This will allow it be available everywhere on your computer. If you intend to make more changes locally. I recommend uninstalling it till you are finished making changes. Otherwise, you will have to update your installation every time you make changes for testing. \\n    Uninstall with:\\n    ::\\n        pip uninstall brightcomets\\n    This will not get rid of your local copy of the repository. \\n\\nObjects with comets already annotated (and some stars, planets, and defects annotated):\\nThese annotations are stored in regions-static and will always be checked. However, annotations given by the user will always be prioritized. \\ncomets = [\"C/2006 W3\", \"C/2007 Q3\", \"65P\", \"29P\", \"30P\", \"81P\", \"116P\", \"10P\", \"118P\", \"P/2010 H2\", \"C/2007 N3\"]\\nnot_comets = [\"mars\", \"jupiter\", \"alpha boo\", \"R dor\"]\\n\\nFAQ\\n===\\n\\n**Can you train multiple different kinds of models?**\\n\\nYes! You can, for example a model that looks at band_3 stars, a model that looks at band_3 comets, a model that looks at composite comets. You cannot train two different band_3 comet models however. \\n\\nNote that the config.py file is global to all models, while each call to retrain.py will only retrain a single model. The retrain script may be slow as it preprocesses the images every time it is run.\\n\\n**Will this program run on all operating systems?**\\n\\nThis module is built for mac. I make no gurantees that it will work on other operating systems. \\n\\n**My program isn\\'t working. What do I do?**\\n\\nAre you using the right version of python (3.6.5)? Do you have all necessary libraries installed (if not, install with pip)? Do you have your XPA_METHOD and pyversion properly set in your config file? Are your data files properly named, with dashes in the correct places? Did you reinstall the library with pip after making changes (You have to reinstall it every time you make changes locally). Make sure none of your filepaths have spaces or special characters like backslashes. \\n\\n\\nBrief Folder and File Descriptions\\n==================================\\n\\n1. BrightComets/references.md\\n    Some websites I referenced during the creation of the project, some comments point to these references. \\n2. BrightComets/requirements.txt\\n    All dependencies. Install requirements with pip install -r requirements.txt\\n3. BrightComets/setup.py\\n    File that allows the project to be installed via pip. pip install -e BrightComets\\n4. BrightComets/brightcomets\\n    a. data\\n        Where tensorflow records and label files are stored for all neural networks. These files are binary files, not human readable, but store all fits and regions files after preprocessing. \\n    b. master_data\\n        Fits and Regions files organized into test and train datasets\\n    c. models\\n        Tensorflow object detection models\\n        Check here for more models: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md\\n        This is where all tensorflow computational graphs and weights are stored for each model you train. \\n    d. regions-static\\n        All the regions that are provided upfront\\n    e. unused\\n        Files that I did not end up using but still have interesting code in them. The files are mostly intended to skirt os.system calls in the retraining script\\n    f. WISE_data\\n        An example of how training data can be organized.\\n    g. __init__.py\\n        A short file for making BrightComets a python package\\n    h. config.py\\n        Configuration file where the user sets training hyperparameters\\n    i. eval.py\\n        (Created by Google Tensorflow) Tensorflow object_detection file that calls does evaluation\\n    j. export_inference_graph.py\\n        (Created by Google Tensorflow) Tensorflow object_detection file that exports trained neural nets\\n    k. file_organization.py\\n        File that does all organization of training data, file movement, TFRecord creation, etc.\\n    l. image_methods.py\\n        File that has the main object detection algorithm, preprocessing algorithms, and fits/regions handling algorithms. \\n    m. pipeline.py\\n        File that allows user to use the detection algorithm\\n    n. retrain.py\\n        File that allows the user to retrain and organize/annotate training data\\n    o. tests.py\\n        Mostly deprecated file with some tests, mostly for image_methods functions\\n    p. train.py\\n        (Created by Google Tensorflow) Tensorflow object_detection file that initiates neural network training, called by retrain.py\\n    q. utils.py\\n        static methods and utils\\n'},\n",
       " {'repo': 'nasa/PyGNSS',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'nasa/OpenSPIFe',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '## Scheduling and Planning Interface for Exploration\\nThe OpenSPIFe user interface is designed to be a highly adaptable and user-customizable framework for viewing and manipulating plan and schedule data. In order to achieve this, SPIFe employs a composable plug-in architecture based on the open source Eclipse Rich Client Platform (RCP).\\n\\n## Target Platform\\nOpen SPIFe requires the Eclipse Rich Client Platform in order for it to run and includes third party software codes that are subject to the licenses and notices set forth below.\\n- Java SE 8 JDK\\n- Eclipse 4.3.2 (Modeling Tools)\\n- Eclipse Nebula features\\n- Eclipse Orbit third party libraries\\n- Eclipse RCP delta pack for multi-platform exports (only required to build multi-platform product artifacts)\\n\\n## Links\\n- [Project Homepage][home]\\n- [Code Repository][repo]\\n- [Bug Tracker][bugs]\\n\\n## How to submit patches\\nOne or more of:\\n\\n- [Fork the code](http://help.github.com/forking/) on GitHub and send a [pull request](http://github.com/guides/pull-requests).\\n- File them on the [bug tracker][bugs]\\n- Send them to the [mailing list][list]\\n\\nIf your contribution is significant, you may be asked to sign a contributor agreement, either as an [individual][clai] or as a [corporation][clac].\\n\\n\\n## Mailing List\\n- Send your messages to [open-spife@lists.nasa.gov][list].\\n\\n## License\\n\\nSee the license files for the original and updated contributions.  The initial release of Open SPIFe to open source is given by the NASA Open Source Agreement and third-party licenses including Apache License 2.0, Eclipse Public License 1.0, Mozilla Public License 2.0, and GNU General Public License 3.0.\\n\\nCopyright (c) 2019 United States Government as represented by the Administrator for The National Aeronautics and Space Administration.  All Rights Reserved.\\n \\n\\n[home]: https://github.com/nasa/OpenSPIFe/wiki\\n[repo]: https://github.com/nasa/OpenSPIFe\\n[bugs]: https://github.com/nasa/OpenSPIFe/issues\\n[list]: mailto:open-spife@lists.nasa.gov\\n[clac]: http://ti.arc.nasa.gov/m/project/nasa-vision-workbench/VW-CLA-Corp.pdf\\n[clai]: http://ti.arc.nasa.gov/m/project/nasa-vision-workbench/VW-CLA-Individual.pdf\\n'},\n",
       " {'repo': 'nasa/openmct-tutorial',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Open MCT Integration Tutorials\\n\\nThese tutorials will walk you through the simple process of integrating your telemetry systems with Open MCT.  In case you don\\'t have any telemetry systems, we\\'ve included a reference implementation of a historical and realtime server.  We\\'ll take you through the process of integrating those services with Open MCT.\\n\\n## Tutorial Prerequisites\\n\\n* [node.js](https://nodejs.org/en/)\\n    * Mac OS: We recommend using [Homebrew](https://brew.sh/) to install node.\\n    ```\\n    $ brew install node\\n    ```\\n    * Windows: https://nodejs.org/en/download/\\n    * linux: https://nodejs.org/en/download/\\n* [git](https://git-scm.com/)\\n    * Mac OS: If XCode is installed, git is likely to already be available from your command line. If not, git can be installed using [Homebrew](https://brew.sh/).\\n    ```\\n    $ brew install git\\n    ```\\n    * Windows: https://git-scm.com/downloads\\n    * linux: https://git-scm.com/downloads\\n\\nNeither git nor node.js are requirements for using Open MCT, however this tutorial assumes that both are installed. Also, command line familiarity is a plus, however the tutorial is written in such a way that it should be possible to copy-paste the steps verbatim into a POSIX command line.\\n\\n## Installing the tutorials\\n\\n```\\ngit clone https://github.com/nasa/openmct-tutorial.git\\ncd openmct-tutorial\\nnpm install\\nnpm start\\n```\\n\\nThis will clone the tutorials and install Open MCT from NPM.  It will also install the dependencies needed to run the provided telemetry server. The last command will start the server. The telemetry server provided is for demonstration purposes only, and is not intended to be used in a production environment.\\n\\nAt this point, you will be able to browse the tutorials in their completed state.  We have also tagged the repository at each step of the tutorial, so it is possible to skip to a particular step using git checkout.\\n\\neg.\\n\\n```\\ngit checkout -f part-X-step-N\\n```\\n\\nSubstituting the appropriate part and step numbers as necessary.\\n\\nThe recommended way of following the tutorials is to checkout the first step (the command is shown below), and then follow the tutorial by manually adding the code, but if you do get stuck you can use the tags to skip ahead. If you do get stuck, please let us know by [filing in issue in this repository](https://github.com/nasa/openmct-tutorial/issues/new) so that we can improve the tutorials.\\n\\nAll source files that you create while following this tutorial should be created in the `openmct-tutorial` directory, unless otherwise specified.\\n\\n## Part A: Running Open MCT\\n**Shortcut**: `git checkout -f part-a`\\n\\nWe\\'re going to define a single `index.html` page.  We\\'ll include the Open MCT library, configure a number of plugins, and then start the application.\\n\\n[index.html](https://github.com/nasa/openmct-tutorial/tree/part-b-step-1/index.html)\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Open MCT Tutorials</title>\\n    <script src=\"node_modules/openmct/dist/openmct.js\"></script>\\n    <script src=\"lib/http.js\"></script>\\n</head>\\n<body>\\n    <script>\\n        openmct.setAssetPath(\\'node_modules/openmct/dist\\');\\n        openmct.install(openmct.plugins.LocalStorage());\\n        openmct.install(openmct.plugins.MyItems());\\n        openmct.install(openmct.plugins.UTCTimeSystem());\\n        openmct.time.clock(\\'local\\', {start: -15 * 60 * 1000, end: 0});\\n        openmct.time.timeSystem(\\'utc\\');\\n        openmct.install(openmct.plugins.Espresso());\\n\\n        openmct.start();\\n    </script>\\n</body>\\n</html>\\n```\\n\\nWe have provided a basic server for the purpose of this tutorial, which will act as a web server as well as a telemetry source. This server is for demonstration purposes only. The Open MCT web client can be hosted on any http server. \\n\\nIf the server is not already running, run it now -\\n\\n```\\nnpm start\\n```\\n\\nIf you open a web browser and navigate to http://localhost:8080/ you will see the Open MCT application running. Currently it is populated with one object named `My Items`. \\n\\n![Open MCT](images/openmct-empty.png)\\n\\nIn this tutorial we will populate this tree with a number of objects representing telemetry points on a fictional spacecraft, and integrate with a telemetry server in order to receive and display telemetry for the spacecraft.\\n\\n# Part B - Populating the Object Tree\\n## Introduction\\nIn Open MCT everything is represented as a Domain Object, this includes sources of telemetry, telemetry points, and views for visualizing telemetry. Domain Objects are accessible from the object tree \\n\\n![Domain Objects are accessible from the object tree](images/object-tree.png)\\n\\nThe object tree is a hierarchical representation of all of the objects available in Open MCT. At the root of an object hierarchy is a root object. For this tutorial, we are going to create a new root object representing our spacecraft, and then populate it with objects representing the telemetry producing subsystems on our fictional spacecraft.\\n\\n## Step 1 - Defining a new plugin\\n**Shortcut:** `git checkout -f part-b-step-1`\\n\\nLet\\'s start by creating a new plugin to populate the object tree. We will include all of the code for this plugin in a new javascript file named `dictionary-plugin.js`. Let\\'s first create a very basic plugin that simply logs a message indicating that it\\'s been installed.\\n\\n[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-2/dictionary-plugin.js)\\n```javascript\\nfunction DictionaryPlugin() {\\n    return function install() {\\n        console.log(\"I\\'ve been installed!\");\\n    }\\n};\\n```\\nNote that when the `install` function is invoked, the [Open MCT API will be provided as the first parameter](https://github.com/nasa/openmct/blob/master/API.md#defining-and-installing-a-new-plugin). In this simple case we don\\'t use it, so it has been left out.\\n\\nNext, we\\'ll update index.html to include the file:\\n\\n[index.html](https://github.com/nasa/openmct-tutorial/blob/part-b-step-2/index.html)\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Open MCT Tutorials</title>\\n    <script src=\"node_modules/openmct/dist/openmct.js\"></script>\\n    <script src=\"lib/http.js\"></script>\\n    <script src=\"dictionary-plugin.js\"></script>\\n</head>\\n<body>\\n    <script>\\n        openmct.setAssetPath(\\'node_modules/openmct/dist\\');\\n        openmct.install(openmct.plugins.LocalStorage());\\n        openmct.install(openmct.plugins.MyItems());\\n        openmct.install(openmct.plugins.UTCTimeSystem());\\n        openmct.time.clock(\\'local\\', {start: -15 * 60 * 1000, end: 0});\\n        openmct.time.timeSystem(\\'utc\\');\\n        openmct.install(openmct.plugins.Espresso());\\n\\n        openmct.install(DictionaryPlugin());\\n\\n        openmct.start();\\n    </script>\\n</body>\\n</html>\\n```\\n\\nIf we reload the browser now, and open a javascript console, we should see the following message \\n\\n```\\nI\\'ve been installed.\\n```\\n\\nThe process of opening a javascript console differs depending on the browser being used. Instructions for launching the browser console in most modern browsers are [available here](http://webmasters.stackexchange.com/a/77337).\\n\\nIn summary, an Open MCT plugin is very simple: it\\'s an initialization function which receives the Open MCT API as the single argument.  It then uses the provided API to extend Open MCT.  Generally, we like plugins to return an initialization function so they can receive configuration.\\n\\n[Learn more about plugins here](https://github.com/nasa/openmct/blob/master/API.md#plugins)\\n\\n## Step 2 - Creating a new root node\\n**Shortcut:** `git checkout -f part-b-step-2`\\n\\nTo be able to access our spacecraft objects from the tree, we first need to define a root. We will use the Open MCT API to define a new root object representing our spacecraft. \\n\\n[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-3/dictionary-plugin.js)\\n```javascript\\nfunction DictionaryPlugin() {\\n    return function install(openmct) {\\n        openmct.objects.addRoot({\\n            namespace: \\'example.taxonomy\\',\\n            key: \\'spacecraft\\'\\n        });\\n    }\\n};\\n```\\n\\nA new root is added to the object tree using the `addRoot` function exposed by the Open MCT API. `addRoot` accepts an object identifier - defined as a javascript object with a `namespace` and a `key` attribute. [More information on objects and identifiers](https://github.com/nasa/openmct/blob/master/API.md#domain-objects-and-identifiers) is available in our API.\\n\\nIf we reload the browser now, we should see a new object in the tree.\\n \\n ![Open MCT](images/openmct-missing-root.png)\\n \\nCurrently it will appear as a question mark with `Missing: example.taxonomy:spacecraft` next to it. This is because for now all we\\'ve done is provide an identifier for the root node. In the next step, we will define an __Object Provider__, which will provide Open MCT with an object for this identifier. A [basic overview of object providers](https://github.com/nasa/openmct/blob/master/API.md#object-providers) is available in our API documentation.\\n\\n## Step 3 - Providing objects\\n**Shortcut:** `git checkout -f part-b-step-3`\\n\\nNow we will start populating the tree with objects. To do so, we will define an object provider. An Object Provider receives an object identifier, and returns a promise that resolve with an object for the given identifier (if available).  In this step we will produce some objects to represent the parts of the spacecraft that produce telemetry data, such as subsystems and instruments. Let\\'s call these telemetry producing things __telemetry points__. Below some code defining and registering an object provider for the new \"spacecraft\" root object:\\n\\n[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-4/dictionary-plugin.js)\\n```javascript\\nfunction getDictionary() {\\n    return http.get(\\'/dictionary.json\\')\\n        .then(function (result) {\\n            return result.data;\\n        });\\n}\\n\\nvar objectProvider = {\\n    get: function (identifier) {\\n        return getDictionary().then(function (dictionary) {\\n            if (identifier.key === \\'spacecraft\\') {\\n                return {\\n                    identifier: identifier,\\n                    name: dictionary.name,\\n                    type: \\'folder\\',\\n                    location: \\'ROOT\\'\\n                };\\n            }\\n        });\\n    }\\n};\\n\\nfunction DictionaryPlugin() {\\n    return function install(openmct) {\\n        openmct.objects.addRoot({\\n            namespace: \\'example.taxonomy\\',\\n            key: \\'spacecraft\\'\\n        });\\n        \\n        openmct.objects.addProvider(\\'example.taxonomy\\', objectProvider);\\n    }\\n};\\n```\\n\\nIf we reload our browser now, the unknown object in our tree should be replaced with an object named \"Example Spacecraft\" with a folder icon. \\n\\n![Open MCT with new Spacecraft root](images/openmct-root-folder.png)\\n\\nThe root object uses the builtin type `folder`. For the objects representing the telemetry points for our spacecraft, we will now register a new object type.\\n\\nSnippet from [dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-c/dictionary-plugin.js#L65-L69)\\n```javascript\\nopenmct.types.addType(\\'example.telemetry\\', {\\n    name: \\'Example Telemetry Point\\',\\n    description: \\'Example telemetry point from our happy tutorial.\\',\\n    cssClass: \\'icon-telemetry\\'\\n});\\n```\\n\\nHere we define a new type with a key of `example.telemetry`. For details on the attributes used to specify a new Type, please [see our documentation on object Types](https://github.com/nasa/openmct/blob/master/API.md#domain-object-types)\\n \\nFinally, let\\'s modify our object provider to return objects of our newly registered type. Our dictionary plugin will now look like this:\\n\\n[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-b-step-4/dictionary-plugin.js)\\n```javascript\\nfunction getDictionary() {\\n    return http.get(\\'/dictionary.json\\')\\n        .then(function (result) {\\n            return result.data;\\n        });\\n}\\n\\nvar objectProvider = {\\n    get: function (identifier) {\\n        return getDictionary().then(function (dictionary) {\\n            if (identifier.key === \\'spacecraft\\') {\\n                return {\\n                    identifier: identifier,\\n                    name: dictionary.name,\\n                    type: \\'folder\\',\\n                    location: \\'ROOT\\'\\n                };\\n            } else {\\n                var measurement = dictionary.measurements.filter(function (m) {\\n                    return m.key === identifier.key;\\n                })[0];\\n                return {\\n                    identifier: identifier,\\n                    name: measurement.name,\\n                    type: \\'example.telemetry\\',\\n                    telemetry: {\\n                        values: measurement.values\\n                    },\\n                    location: \\'example.taxonomy:spacecraft\\'\\n                };\\n            }\\n        });\\n    }\\n};\\n\\nfunction DictionaryPlugin() {\\n    return function install(openmct) {\\n        openmct.objects.addRoot({\\n            namespace: \\'example.taxonomy\\',\\n            key: \\'spacecraft\\'\\n        });\\n        \\n        openmct.objects.addProvider(\\'example.taxonomy\\', objectProvider);\\n    }\\n};\\n```\\n\\nAlthough we have now defined an Object Provider for both the \"Example Spacecraft\" and its children (the telemetry measurements), if we refresh our browser at this point we won\\'t see any more objects in the tree. This is because we haven\\'t defined the structure of the tree yet.\\n\\n## Step 4 - Populating the tree\\n**Shortcut:** `git checkout -f part-b-step-4`\\n\\nWe have defined a root node in [Step 2](https://github.com/nasa/openmct-tutorial/blob/part-b-step-3/dictionary-plugin.js) and we have provided some objects that will appear in the tree. Now we will provide structure to the tree and define the relationships between objects in the tree. This is achieved with a __[Composition Provider](https://github.com/nasa/openmct/blob/master/API.md#composition-providers)__.\\n\\nSnippet from [dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-c/dictionary-plugin.js#L36-L52)\\n```javascript\\nvar compositionProvider = {\\n    appliesTo: function (domainObject) {\\n        return domainObject.identifier.namespace === \\'example.taxonomy\\' &&\\n               domainObject.type === \\'folder\\';\\n    },\\n    load: function (domainObject) {\\n        return getDictionary()\\n            .then(function (dictionary) {\\n                return dictionary.measurements.map(function (m) {\\n                    return {\\n                        namespace: \\'example.taxonomy\\',\\n                        key: m.key\\n                    };\\n                });\\n            });\\n    }\\n};\\n\\nopenmct.composition.addProvider(compositionProvider);\\n```\\nA Composition Provider accepts a Domain Object, and provides identifiers for the children of that object. For the purposes of this tutorial we will return identifiers for the telemetry points available from our spacecraft. We build these from our spacecraft telemetry dictionary file.\\n\\nOur plugin should now look like this:\\n\\n[dictionary-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-c/dictionary-plugin.js)\\n```javascript\\nfunction getDictionary() {\\n    return http.get(\\'/dictionary.json\\')\\n        .then(function (result) {\\n            return result.data;\\n        });\\n}\\n\\nvar objectProvider = {\\n    get: function (identifier) {\\n        return getDictionary().then(function (dictionary) {\\n            if (identifier.key === \\'spacecraft\\') {\\n                return {\\n                    identifier: identifier,\\n                    name: dictionary.name,\\n                    type: \\'folder\\',\\n                    location: \\'ROOT\\'\\n                };\\n            } else {\\n                var measurement = dictionary.measurements.filter(function (m) {\\n                    return m.key === identifier.key;\\n                })[0];\\n                return {\\n                    identifier: identifier,\\n                    name: measurement.name,\\n                    type: \\'example.telemetry\\',\\n                    telemetry: {\\n                        values: measurement.values\\n                    },\\n                    location: \\'example.taxonomy:spacecraft\\'\\n                };\\n            }\\n        });\\n    }\\n};\\n\\nvar compositionProvider = {\\n    appliesTo: function (domainObject) {\\n        return domainObject.identifier.namespace === \\'example.taxonomy\\' &&\\n               domainObject.type === \\'folder\\';\\n    },\\n    load: function (domainObject) {\\n        return getDictionary()\\n            .then(function (dictionary) {\\n                return dictionary.measurements.map(function (m) {\\n                    return {\\n                        namespace: \\'example.taxonomy\\',\\n                        key: m.key\\n                    };\\n                });\\n            });\\n    }\\n};\\n\\nfunction DictionaryPlugin() {\\n    return function install(openmct) {\\n        openmct.objects.addRoot({\\n            namespace: \\'example.taxonomy\\',\\n            key: \\'spacecraft\\'\\n        });\\n    \\n        openmct.objects.addProvider(\\'example.taxonomy\\', objectProvider);\\n    \\n        openmct.composition.addProvider(compositionProvider);\\n    \\n        openmct.types.addType(\\'example.telemetry\\', {\\n            name: \\'Example Telemetry Point\\',\\n            description: \\'Example telemetry point from our happy tutorial.\\',\\n            cssClass: \\'icon-telemetry\\'\\n        });\\n    };\\n};\\n```\\n\\nAt this point, if we reload the page we should see a fully populated object tree. \\n\\n![Open MCT with spacecraft telemetry objects](images/telemetry-objects.png)\\n\\nClicking on our telemetry points will display views of those objects, but for now we don\\'t have any telemetry for them. The tutorial telemetry server will provide telemetry for these points, and in the following steps we will define some telemetry adapters to retrieve telemetry data from the server, and provide it to Open MCT. \\n\\n# Part C - Integrate/Provide/Request Telemetry\\n**Shortcut:** `git checkout -f part-c`\\n\\nOpen MCT supports receiving telemetry by requesting data from a telemetry store, and by subscribing to real-time telemetry updates. In this part of the tutorial we will define and register a telemetry adapter for requesting historical telemetry from our tutorial telemetry server. Let\\'s define our plugin in a new file named `historical-telemetry-plugin.js`\\n\\n[historical-telemetry-plugin.js](https://github.com/nasa/openmct-tutorial/blob/part-d/historical-telemetry-plugin.js)\\n```javascript\\n/**\\n * Basic historical telemetry plugin.\\n */\\n\\nfunction HistoricalTelemetryPlugin() {\\n    return function install (openmct) {\\n        var provider = {\\n            supportsRequest: function (domainObject) {\\n                return domainObject.type === \\'example.telemetry\\';\\n            },\\n            request: function (domainObject, options) {\\n                var url = \\'/history/\\' +\\n                    domainObject.identifier.key +\\n                    \\'?start=\\' + options.start +\\n                    \\'&end=\\' + options.end;\\n    \\n                return http.get(url)\\n                    .then(function (resp) {\\n                        return resp.data;\\n                    });\\n            }\\n        };\\n    \\n        openmct.telemetry.addProvider(provider);\\n    }\\n}\\n```\\n\\nThe telemetry adapter above defines two functions. The first of these, `supportsRequest`, is necessary to indicate that this telemetry adapter supports requesting telemetry from a telemetry store. The `request` function will retrieve telemetry data and return it to the Open MCT application for display.\\n\\nOur request function also accepts some options. Here we support the specification of a start and end date.\\n\\nWith our adapter defined, we need to update `index.html` to include it.\\n\\n[index.html](https://github.com/nasa/openmct-tutorial/blob/part-d/index.html)\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Open MCT Tutorials</title>\\n    <script src=\"node_modules/openmct/dist/openmct.js\"></script>\\n    <script src=\"lib/http.js\"></script>\\n    <script src=\"dictionary-plugin.js\"></script>\\n    <script src=\"historical-telemetry-plugin.js\"></script>\\n</head>\\n<body>\\n    <script>\\n        openmct.setAssetPath(\\'node_modules/openmct/dist\\');\\n        openmct.install(openmct.plugins.LocalStorage());\\n        openmct.install(openmct.plugins.MyItems());\\n        openmct.install(openmct.plugins.UTCTimeSystem());\\n        openmct.time.clock(\\'local\\', {start: -15 * 60 * 1000, end: 0});\\n        openmct.time.timeSystem(\\'utc\\');\\n        openmct.install(openmct.plugins.Espresso());\\n\\n        openmct.install(DictionaryPlugin());\\n        openmct.install(HistoricalTelemetryPlugin());\\n\\n        openmct.start();\\n    </script>\\n</body>\\n</html>\\n```\\n\\nAt this point If we refresh the page we should now see some telemetry for our telemetry points. For example, navigating to the \"Generator Temperature\" telemetry point should show us a plot of the telemetry generated since the server started running.\\n\\n# Part D - Subscribing to New Telemetry\\n**Shortcut:** `git checkout -f part-d`\\n\\nWe are now going to define a telemetry adapter that allows Open MCT to subscribe to our tutorial server for new telemetry as it becomes available. The process of defining a telemetry adapter for subscribing to real-time telemetry is similar to our previously defined historical telemetry adapter, except that we define a `supportsSubscribe` function to indicate that this adapter provides telemetry subscriptions, and a `subscribe` function for subscribing to updates. This adapter uses a simple messaging system for subscribing to telemetry updates over a websocket. \\n\\nLet\\'s define our new plugin in a file named `realtime-telemetry-plugin.js`.\\n\\n[realtime-telemetry-plugin.js](https://github.com/nasa/openmct-tutorial/blob/master/realtime-telemetry-plugin.js)\\n```javascript\\n/**\\n * Basic Realtime telemetry plugin using websockets.\\n */\\nfunction RealtimeTelemetryPlugin() {\\n    return function (openmct) {\\n        var socket = new WebSocket(location.origin.replace(/^http/, \\'ws\\') + \\'/realtime/\\');\\n        var listener = {};\\n    \\n        socket.onmessage = function (event) {\\n            point = JSON.parse(event.data);\\n            if (listener[point.id]) {\\n                listener[point.id](point);\\n            }\\n        };\\n        \\n        var provider = {\\n            supportsSubscribe: function (domainObject) {\\n                return domainObject.type === \\'example.telemetry\\';\\n            },\\n            subscribe: function (domainObject, callback) {\\n                listener[domainObject.identifier.key] = callback;\\n                socket.send(\\'subscribe \\' + domainObject.identifier.key);\\n                return function unsubscribe() {\\n                    delete listener[domainObject.identifier.key];\\n                    socket.send(\\'unsubscribe \\' + domainObject.identifier.key);\\n                };\\n            }\\n        };\\n        \\n        openmct.telemetry.addProvider(provider);\\n    }\\n}\\n```\\n\\nThe subscribe function accepts as arguments the Domain Object for which we are interested in telemetry, and a callback function. The callback function will be invoked with telemetry data as they become available.\\n\\nWith our realtime telemetry plugin defined, let\\'s include it from `index.html`.\\n\\n[index.html](https://github.com/nasa/openmct-tutorial/blob/master/index.html)\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Open MCT Tutorials</title>\\n    <script src=\"node_modules/openmct/dist/openmct.js\"></script>\\n    <script src=\"lib/http.js\"></script>\\n    <script src=\"dictionary-plugin.js\"></script>\\n    <script src=\"historical-telemetry-plugin.js\"></script>\\n    <script src=\"realtime-telemetry-plugin.js\"></script>\\n</head>\\n<body>\\n    <script>\\n        openmct.setAssetPath(\\'node_modules/openmct/dist\\');\\n        openmct.install(openmct.plugins.LocalStorage());\\n        openmct.install(openmct.plugins.MyItems());\\n        openmct.install(openmct.plugins.UTCTimeSystem());\\n        openmct.time.clock(\\'local\\', {start: -15 * 60 * 1000, end: 0});\\n        openmct.time.timeSystem(\\'utc\\');\\n        openmct.install(openmct.plugins.Espresso());\\n\\n        openmct.install(DictionaryPlugin());\\n        openmct.install(HistoricalTelemetryPlugin());\\n        openmct.install(RealtimeTelemetryPlugin());\\n\\n        openmct.start();\\n    </script>\\n</body>\\n</html>\\n```\\n\\nIf we refresh the page and navigate to one of our telemetry points we should now see telemetry flowing. For example, navigating to the \"Generator Temperature\" telemetry point should show us a plot of telemetry data that is now updated regularly.\\n'},\n",
       " {'repo': 'nasa/georef',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Installation\\n============\\n\\nRequirements\\n~~~~~~~~~~~~\\n\\nOur reference platform for GeoRef is Ubuntu Linux 14.04 LTS,\\nrunning Python 2.7.6 and Django 1.9.2.  For development we use Django\\'s\\nbuilt-in development web server MySQL database.  \\n\\nWe develop using a VagrantBox VM running a Ubuntu Linux inside a Mac OS X host machine.\\nVagrant VM is strictly optional and only necessary if you are not running directly from a Ubuntu Linux Machine.\\n\\nOur image view is rendered using the OpenSeadragon open source image viewer. (openseadragon.github.io/)\\n\\n(Optional) Set up a Vagrant VM\\n~~~~~~~~~~~~~~~~~~~~\\nIf you are running on a mac, we highly encourage you to use Vagrant to set up \\na Ubuntu Development Instance. Our set up script works best within the Vagrant \\nenvironment running on Mac OSX.\\n\\nInstall VirtualBox. We have found that VirtualBox Version 4.3.10 works best with Vagrant.\\nWe highly recommend you download VirtualBox 4.3.10.\\nInstall the latest version of vagrant: \\u200bhttp://www.vagrantup.com/downloads\\n\\n\\nSet Up an Install Location\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nLet\\'s create a directory to hold the whole GeoRef installation\\nand capture the path in an environment variable we can use\\nin the instructions below::\\n\\n  export GEOCAM_DIR=$HOME/projects/geocam # or choose your own\\n  mkdir -p $GEOCAM_DIR\\n  \\n\\nGet the Source\\n~~~~~~~~~~~~~~\\n\\nCheck out our latest source revision with::\\n\\n  cd $GEOCAM_DIR\\n  git clone https://github.com/nasa/georef_deploy.git\\n\\n\\nFor more information on the Git version control system, visit `the Git home page`_.\\nYou can install Git on Ubuntu with::\\n\\n  sudo apt-get install git-all\\n\\n.. _the Git home page: http://git-scm.com/\\n\\n\\nRun the Setup Script\\n~~~~~~~~~~~~~~~~~~~~~\\nThe \"setup_site_vagrant.sh\" script initializes the vagrant box and it clones \\nall the submodules that are needed::\\n\\n    # go into the georef_deploy directory\\n    cd georef_deploy\\n    \\n    # if you are running inside a Vagrant VM do\\n    setup_site_vagrant.sh\\n\\n\\nIf you are running directly on a Ubuntu Linux Machine, you can skip the above shell\\nscript and run the following::\\n    sudo python $GEOCAM_DIR/georef_deploy/setup_site.py\\n    \\n    # You need to manually create couple symlinks if not running on vagrant\\n    sudo ln -s /home/geocam/georef_deploy georef_deploy\\n    sudo ln -s gds/georef/ georef\\n\\n\\nOverride settings.py\\n~~~~~~~~~~~~~~~~~~~~~~~\\n\\nIn the ``settings.py`` file, modify the ``DATABASES`` field to point to\\nyour Django MySQL database::\\n\\n    DATABASES = {\\n        \\'default\\': {\\n            \\'ENGINE\\': \\'django.db.backends.mysql\\',\\n            \\'NAME\\': \\'georef\\',\\n            \\'USER\\': \\'root\\',\\n            \\'PASSWORD\\': \\'vagrant\\',\\n            \\'HOST\\': \\'127.0.0.1\\',\\n            \\'PORT\\': \\'3306\\',\\n        }\\n    }\\n\\n\\nSetup the Data Directory\\n~~~~~~~~~~~~~~~~~~~~~~~~~~\\nYou must manually create the data directory and its sub folders. GeoRef will \\nwrite the image tiles to this directory.\\n\\n1. Create a data directory\\n    ``mkdir $GEOCAM_DIR/georef/data``\\n2. Create the overlays directory\\n    ``mkdir -p $GEOCAM_DIR/georef/data/geocamTiePoint/overlay_images``\\n3. Set the permissions\\n    ``chmod -R 777 $GEOCAM_DIR/georef/data``\\n\\n\\nSetup GeoRef\\n~~~~~~~~~~~~\\n\\nIf your development environment is set up inside Vagrant, cd into the georef_deploy \\ndirectory and do::\\n    vagrant ssh\\nAnd then run the following commands.\\n\\n\\nYou must create the following directory and files::\\n\\n # If you are not using Vagrant, do\\n     mkdir -p $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/ & touch $GEOCAM_DIR/georef_deploy/georef/data/deepzoom/deepzoom.exception.log\\n\\n # If you are using Vagrant, do\\n     # deepzoom directory needs to be owned by www-data. Put it in /home/vagrant so that it can be owned by www-data (and not by user)\\n     mkdir -p /home/vagrant/deepzoom \\n     # create a symlink to deepzoom in the data dir\\n     ln -s /home/vagrant/deepzoom /home/vagrant/georef/data/deepzoom\\n\\n\\nInstall Earth Engine by following the instructions below: \\n    https://developers.google.com/earth-engine/python_install_manual\\n\\n\\nTo install Python dependencies, render icons and collect media for the\\nserver, run::\\n\\n  cd $GEOCAM_DIR/georef_deploy/georef\\n  ./manage.py bootstrap --yes\\n  source $GEOCAM_DIR/georef_deploy/georef/sourceme.sh genSourceme genSettings\\n  ./manage.py collectstatic  \\n  ./manage.py prep\\n\\nYou\\'ll need to source the ``sourceme.sh`` file every time you open a new\\nshell if you want to run GeoCam-related Python scripts such as starting\\nthe Django development web server.  The ``sourceme.sh`` file will also\\ntake care of activating your virtualenv environment in new shells (if\\nyou were in a virtualenv when you ran ``setup.py``).\\n\\n\\nTo initialize the database\\n    ``$GEOCAM_DIR/georef/manage.py makemigrations deepzoom``\\n\\n    ``$GEOCAM_DIR/georef/manage.py makemigrations geocamTiePoint``\\n\\n    ``$GEOCAM_DIR/georef/manage.py migrate``\\n\\nNote that the path to manage.py may be different if you are running inside Vagrant.\\n\\n\\nCreate a User Account  \\n~~~~~~~~~~~~~~~~~~~~~\\nUser name and password are required to use GeoRef. To create one, do::\\n    \\n    ./manage.py createsuperuser\\n\\nAnd follow the prompts.\\n\\n\\n\\nTry It Out\\n~~~~~~~~~~\\nNow you\\'re ready to try it out!  \\n\\nRestart the Apache server ``sudo apachectl restart``\\n\\nPoint your browser to \\u200bhttp://10.0.3.18/\\n\\n\\n.. o  __BEGIN_LICENSE__\\n.. o  Copyright (C) 2008-2010 United States Government as represented by\\n.. o  the Administrator of the National Aeronautics and Space Administration.\\n.. o  All Rights Reserved.\\n.. o  __END_LICENSE__\\n'},\n",
       " {'repo': 'nasa/PyTDA',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'PyTDA README\\n------------\\n\\nThis software provides Python functions that will estimate turbulence from\\nDoppler radar data. It is tested and working under Python 2.7-3.7.\\n\\nFor help see `HELP` file. For license see `LICENSE.md`.\\n\\n\\nInstallation\\n------------\\n\\nInstall [Py-ART](https://github.com/ARM-DOE/pyart).\\n\\nRun `python setup.py install` from the command line in the main PyTDA folder.\\n\\n\\nUsing PyTDA\\n-----------\\n```\\nimport pytda\\n```\\n\\nPyTDA (among other modules) is discussed in this [conference presentation]\\n(https://ams.confex.com/ams/95Annual/webprogram/Paper262779.html)\\n\\nSee the notebooks directory for a demonstration Jupyter notebook.\\n'},\n",
       " {'repo': 'nasa/UQ-Kernel-Mini-App',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# uq kernel miniapp\\n\\nA kernel for uncertainty quantification (UQ) codes at NASA.\\n\\n# Getting Started\\n\\n1) Clone or download this repository\\n* It is best to work with and modify the source code using Git ([install/update Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)). Using Git, clone the repository to your computer using `git clone https://github.com/nasa/UQ-Kernel-Mini-App.git`. Otherwise, the repository can be downloaded manually with the \"Clone or download\" button above. \\n2) Add the repository to your Python path\\n* On Mac/Linux, add `export PYTHONPATH=$PYTHONPATH:Path/To/Your/Repo/` to your ~/.bashrc file (click [here](https://stackoverflow.com/questions/3402168/permanently-add-a-directory-to-pythonpath) for more details). For Windows, see this [link](https://stackoverflow.com/questions/3701646/how-to-add-to-the-pythonpath-in-windows-so-it-finds-my-modules-packages). This makes it possible to run the code in this repository from anywhere on your computer.\\n3) Make sure you have the required Python modules\\n* The repository uses a few external Python modules (currently just `numpy` and `scipy`). Either install them manually, or using pip: `pip install -r requirements.txt` or Anaconda: `conda install --yes --file requirements.txt` from the top directory of the repository.\\n4) Test that everything is working correctly\\n* Navigate to the `tests/` directory in the repository and type `python engine_check.py`. If things are working as expected, you should see an output similar to:\\n```\\nEngine Check Results:\\n  Output is Correct!\\n  Theoretical execution time was 9.40658189601207\\n  Actual execution time was 9.961344003677368\\n  Efficiency was 0.9443\\n```\\n\\n# VT Capstone project\\n\\n## Challenge\\nDevise  an  interface  (front end)  and  computational  engine  (back end)  \\nthat  increases  the  usability, efficiency, and scalability of NASA \\nopen-source uncertainty quantification software.\\n\\n### Sub-challenge 1: interface\\n How can we best allow users from all disciplines to “plug” their own \\n computational model into our Python-based UQ software?\\n\\n#### Interface Requirements\\n\\nCurrently, NASA UQ software requires that a user develops a Python class\\nfor the model they\\'d like to analyze that has a standardized interface. \\nThe primary requirement of this interface is that their Python class has\\nan `evaluate` function that receives an array of input parameters for their\\nmodel and returns an array of outputs:\\n\\n ```python\\ndef evaluate(self, inputs):\\n\\t<Python code to implement an internal simulation or execute an external simulation>\\n\\treturn output\\n```\\n\\nSecondarily, all Python models must have a `cost` member variable that contains\\nthe approximate time to execute the model. The base interface for a `UQModel` \\nis defined in `uq_kernel/model.py`.  It illustrates these parts of a model that \\nmust be exposed to the UQ framework. \\n\\n#### Challenge problem focus\\n\\nThe challenge problem should focus on the most general type of `UQModel` \\nand how best to streamline the process of creating one to use with \\nNASA UQ software. This is the situation where a user has a simulation\\nexecutable (developed in any programming language) that can be run\\non the command line by providing an input file and, upon completion, writes \\nan output file with any simulation output data of interest. \\n \\nIn this case, the `UQModel` a user develops to use the NASA UQ\\nsoftware will always follow the general form:\\n```python\\ndef evaluate(self, inputs):\\n\\tself.write_input_file_to_disc(inputs)\\n\\tself.execute_model()\\n\\toutput = self.parse_output_file()\\n\\treturn output\\n```\\nwhere the `write_input_file_to_disc` function generates a problem-specific \\ninput file based on the `inputs` array and stores it, `execute_model` will\\nmake a system call in Python to execute the user\\'s simulation for the \\ninput file written previously and write an output file when it has completed,\\nand `parse_output_file` will load the output file and extract the relevant \\noutputs to return. \\n\\nIn general, a user will know the command to execute their simulation at the\\ncommand line and the format of their input file along with where the particular \\nparametersin the `inputs` array should go within it. From here, they need to \\nwrap this functionality inside of a `UQModel` Python class. We want to make \\nthis process as easy as possible by capitalizing on any code, procedures, patterns\\nthat are shared between most or all applications and simulations.\\n \\n#### UQModel Example \\n\\nThere is a simple example of a simulation and corresponding Python `UQModel`\\nin the `examples/spring_mass_uq_model/` to make this challenge more concrete.\\n\\n<img src=\"/imgs/spring_mass.png\" width=\"200\">\\n\\nThe `spring_mass_simulation.py` implements a simple spring-mass system and \\ncalculates the maximum displacement in the system for given parameters like\\nmass and acceleration due to gravity. For our purposes, we will assume this\\nis a \"blackbox\" executable, we don\\'t have access to the code inside,\\nwe just know we can run it on the command line using:\\n\\n`python spring_mass_simulation.py <name-of-input-file> <name-of-output-file>`\\n\\nwhere the input file contains values for mass, stiffness, etc. and the output\\nfile will contain a single number defining the maximum displacement. An example\\ninput file is provided (`spring_mass_inputs.txt`) so that the code can be \\ntested by typing `python spring_mass_simulation.py spring_mass_inputs.txt output.txt`.\\n\\nNow let\\'s say that the user wants to use NASA UQ software to study how \\nuncertainty in the spring stiffness effects their prediction of the \\nmaximum displacement. They need to define a `UQModel` class with and\\n`evaluate` function whose `inputs` array contains one value for stiffness \\nand the `output` array contains one value for the maximum displacement.\\nAn example of such a class can be seen in the `spring_mass_UQ_model.py`.\\nA sample script that initializes and evaluates the model for a few\\ndifferent stiffness values is provided in `run_spring_mass_uq_model.py`\\nfor illustration.\\n\\nHow can we help a user produce code/classes like inside of \\n`spring_mass_UQ_model.py` to \"plug\" their simulation into the NASA\\nUQ software? The user will have information about how to execute their\\nsimulation from the command line, the structure of their input file, and the\\nnames of pertinant inputs/outputs. Note that it is important to distinguish \\nbetween the parameters that will change from simulation to simulation (stiffness \\nin this example) versus those that will remain fixed (gravity, mass, etc.).\\nThe main procedure of 1) writing input file, 2) executing model, and 3) parsing \\noutput file / returning output(s) of interest will generally be followed, but\\nthese individual steps will require customization depending on the user\\'s \\nsimulation, input/output files, parameters of interest, etc. \\n\\n \\n### Implementation\\nA starting interface for a `UQModel` is defined in `uq_kernel/model.py`.  It \\nillustrates the parts of a model that must be exposed to the UQ framework. \\nDefining implementations of this interface to account for different possible \\nmodels is one option to pursue this challenge.  Alternative solutions are \\nencouraged as well.  Everything, including the basic interface in \\n`uq_kernel/model.py` can be modified to suit the chosen approach.\\n\\n### Sub-challenge 2: engine\\n**A load balancing issue for parallel computing:** How can we devise a strategy \\nto execute a set of computational models with *varying run times* an arbitrary \\nnumber of times each in an efficient and scalable manner.\\n\\n#### Problem Definition\\n**Given**\\n * A set of Python models (like the ones developed in sub-challenge 1)\\n * The estimated cost (run time) for each model\\n * The number of times to execute each model (with the inputs to the models \\n for each evaluation)\\n * Number of processors to run on\\n\\n**Determine**\\n\\nA strategy for spreading the executions of all models across the processors \\nsuch that all processors have a similar amount of work to do (minimize idle \\ntime for processors)\\n\\n#### Implementation\\n\\nDevelop an engine function with an interface that accepts a list of `UQmodel` \\nobjects and a list of `numpy` arrays defining the inputs for each model,\\n\\n<img src=\"/imgs/inputs.png\" width=\"600\">\\n\\nnote that in general the individual numpy arrays will have different lengths\\n(number of inputs). This function returns a list of numpy arrays defining\\nthe outputs\\n\\n<img src=\"/imgs/outputs.png\" width=\"600\">\\n\\nwhere each array has been generated by evaluating the appropriate `UQmodel`,\\nfor example: \\n\\n<img src=\"/imgs/run_model.png\" width=\"600\">\\n\\nassuming that `model-1` has `N1` inputs to evaluate. \\n\\nIf the approximate cost of each model is `C1`, ..., `CM`, then the total time \\nto run all of the models on one processor is `T-serial = N1 * C1 + N2 * C2 + ... NM * CM`. \\nThe specific goal is to write an engine function that has an execution time that \\nis as close to `T-parallel = T1 / P` as possible when run on `P` processors. An example of \\na serial engine function can be seen in the `tests/engine_check.py` in the \\n`simple_serial_engine` function. \\n\\nThe implementation of the engine is completely open; however, a solution that \\nworks in a distributed memory system is preferred. `mpi4py` is one example of a \\ndistributed processing tool that could be helpful, though others exist as well.\\n\\nA script for checking implementations of the engine is included in the miniapp: \\n`tests/engine_check.py`. The script runs the engine, checks its outputs, and \\ncomputes its efficiency.  The script is configurable to test many different \\npossibilities of model number, run times, etc.  Note that it is likely that the \\nscript will need to be modified to work in the context of the engine you \\ndevelop.  It should prove useful, nonetheless.\\n\\n#### Running a model in parallel with mpi4py\\n\\nAn example script demonstrating basic usage of `mpi4py` to evaluate a Python model\\nin parallel is given in `examples/parallel/run_model_with_mpi.py`. In order to run\\nthis script, `mpi4py` must be installed, which requires a working installation of MPI on your computer (look for [online resources](https://mpi4py.readthedocs.io/en/stable/install.html) if there are issues). Once installed properly, the script can be run with `mpirun -np <number of processors> python run_model_with_mpi.py` or `mpiexec -n <number of processors> python run_model_with_mpi.py` depending on your version of MPI. \\n'},\n",
       " {'repo': 'nasa/Reinforcement-Learning-Benchmarking',\n",
       "  'language': 'Shell',\n",
       "  'readme_contents': '# Reinforcement Learning Benchmarks\\n\\nThis project provides scripts for running several OpenAI Baselines algorithms on all MuJoCo or Roboschool gym environments in order to compare algorithm performance.\\n\\n## Prerequisites\\n\\nThe following Python packages are required:\\n\\n[gym](https://github.com/openai/gym)\\n\\n[roboschool](https://github.com/openai/roboschool) (Only required if you want to run Roboschool environments)\\n\\n[baselines](https://github.com/openai/baselines) (Note: baselines must be installed from source.  A pip install will give you an outdated version)\\n\\nmatplotlib\\n\\nargparse\\n\\nos\\n\\nThe MuJoCo software, [mjpro150](https://www.roboti.us/index.html) is required to run the MuJoCo environments.  The MuJoCo submodule for gym is also required, which can be installed via\\n\\n```bash\\npip install gym[mujoco]\\n```\\n\\n## How to run\\n\\nRun the bash script by typing\\n\\n```bash\\n./runExperiments.sh\\n```\\n\\nThe bash script accepts the following positional command line arguments:\\n\\n1: Subdirectory to save training data in.  Default is data.\\n\\n2: mujoco or roboschool - Choose which environment set to run.  Default is roboschool.\\n\\n3: Number of timesteps to run each algorithm.  Defualt is 1000000.\\n\\n4: Number of random seeds to run for each algorithm.  Default is 3.\\n\\nFor example, to save data to a subdirectory called \"test\", run the MuJoCo environments for 10000 timesteps with each algorithm, and run 5 different seeds for each algorithm, you would type\\n\\n```bash\\n./runExperiments.sh test mujoco 10000 5\\n```\\n\\n## Viewing results\\n\\nPlots of all learning curves are saved as pdf files in a subfolder inside the data folder called plots.  These plots show the episode reward vs. number of timesteps, and the smoothing function from the baselines plot utility is applied.  The shaded regions represent the variations across all seeds, and the solid line represents the mean across all seeds. \\n\\n---\\n\\n__**Notices**__:\\n\\n_Copyright **2019** United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved._\\n\\n__**Disclaimers**__\\n\\n_No Warranty_: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\n_Waiver and Indemnity_:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/vsm', 'language': 'Python', 'readme_contents': ''},\n",
       " {'repo': 'nasa/Giovanni',\n",
       "  'language': 'Perl',\n",
       "  'readme_contents': '# Giovanni:     The Bridge Between Data And Science \\nhttps://giovanni.gsfc.nasa.gov/giovanni/\\n\\nGiovanni is an online (Web) environment for the display and analysis of geophysical parameters in which the provenance (data lineage) can easily be accessed. \\n\\nGES DISC is making our Giovanni (currently at version 4.31)  code base available on github\\n\\n<h4> Getting Started with the code </h4>\\nAt the <a href=\"https://disc.gsfc.nasa.gov/\">GES DISC</a>, development of Giovanni is split into several repositories:\\n\\nEach subdirectory has either a Makefile or Perl Makefile.PL\\n<br/><b>agiovanni</b>\\n<br/><b>agiovanni_algorithms</b>\\n<br/><b>agiovanni_data_access</b>\\n<br/><b>agiovanni_www</b> \\n<br/><b>agiovanni_shapes</b>\\n<br/><b> agiovanni_giovanni</b><br/>\\n<br/><b>AESIR</b><br/>refers to Giovanni\\'s variable metadata database in SOLR. Giovanni\\'s earth science data file database is <a href=\"https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository\">CMR</a>\\n\\n<br/><b>agiovanni/Dev-Tools/other/rpmbuild</b><br/> Contains  a build script and RPM spec file that gives an indication as to Giovanni\\'s software dependencies.\\n\\n\\n<b>Disclaimer:We will update the software but not maintain the pull requests.</b>\\n\\n<br/>Direct comments and questions to the Giovanni Development Team at: <b>gsfc-help-disc@lists.nasa.gov</b>\\n\\n<br/>To give more indication of Giovanni\\'s dependencies:\\n<br/>Giovanni is powered by:\\n<br/><a href=\"http://nco.sourceforge.net/\">NCO netCDF Operator</a>\\n<br/><a href=\"https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository\">CMR Common Metadata Repository</a>\\n<br/><a href=\"http://developer.yahoo.com/yui/\">YUI</a>\\n<br/><a href=\"http://openlayers.org/\">OpenLayers</a>\\n<br/><a href=\"http://www.mapserver.org/ogc/\">MapServer</a>\\n<br/><a href=\"http://opendap.org/\">OPeNDAP</a>\\n\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'nasa/api-docs-stage', 'language': None, 'readme_contents': ''},\n",
       " {'repo': 'nasa/gen_msgids',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"# Core Flight System : Framework : Tool : Generate Message Ids\\n\\nThis repository contains NASA's Generate Message Ids Tool (gen_msgids), which is a framework component of the Core Flight System.\\n\\nThis lab application is a ground utility to Generate Message Ids used by cFE for cFS. It is intended to be located in the `tools/gen_msgids` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.\\n\\n## Release Notes\\n\\ngen_msgids version 1.0a is released as part of cFE 6.6.0a under the Apache 2.0 license, see [LICENSE](LICENSE-18128-Apache-2_0.pdf).\\n\\n## Known issues\\n\\nThis ground utility has not been updated to use the cmake framework.  The include path defined in Makefile will likely need to be updated for distributions.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n\\n\"},\n",
       " {'repo': 'nasa/meshNetwork',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# meshNetwork\\n\\nThe Mesh Network Communication System is a peer-to-peer communication network architecture that enables communication between network nodes of various types.  The initial primary goal of the system was to enable communication between small formations of cubesats or other small satellites, but the basic mesh architecture is applicable to data exchange between network assets of any type.  The system has been flight tested on formations of small unmanned aerial systems (sUAS) and shown to provide low latency data throughput for dynamic flight environments.\\n\\n### Documentation\\n\\nThe full documentation is in the doc/build/html directory.\\n\\n### License\\n\\nThe Mesh Network software is released under the NASA Open Source Agreement Version 1.3 [license](LICENSE).\\n\\n\\n### Python dependencies\\npyserial\\ncrcmod\\n'},\n",
       " {'repo': 'nasa/GTM_DesignSim',\n",
       "  'language': 'MATLAB',\n",
       "  'readme_contents': '#GTM_DesignSim: The Generic Transport Model\\n\\nThe GTM_DesignSim is a batch simulation intended for design and analysis of flight control laws.  It models the\\ndynamics of a 5.5% scale model of a generic transport aircraft.  It was developed to allow fault accomodating control\\nalgorithms to be developed and refined in simulation before being tested on an experimental subscale model.  \\n\\nSee:\\n\\nCunningham, K., Cox, D. E., Murri, D. G., and Riddick, S. E., *“A Piloted Evaluation of Damage Accommodating Flight Control Using a Remotely Piloted Vehicle,”* **AIAA-2011-6451**, AIAA Guidance, Navigation, and Control Conference and Exhibit, Portland Oregon, August 2011.\"   \\n\\nand the references therein for a summary of the research.  \\n\\nThe simulation, however, was released as open-source and has been found useful in many other applications where a\\nnon-linear large-envelope flight dynamics simulation is required.\\n\\n'},\n",
       " {'repo': 'nasa/MLMCPy',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'MLMCPy - **M**ulti-**L**evel **M**onte **C**arlo with **Py**thon\\n===================================================================\\n\\n<a href=\\'https://travis-ci.com/nasa/MLMCPy\\'><img src=\\'https://travis-ci.com/nasa/MLMCPy.svg?branch=master\\' alt=\\'Build Status\\' /></a> <a href=\\'https://coveralls.io/github/lukemorrill/MLMCPy?branch=master\\'><img src=\\'https://coveralls.io/repos/github/lukemorrill/MLMCPy/badge.svg?branch=master\\' alt=\\'Coverage Status\\' /></a>\\n\\nGeneral\\n--------\\n\\nMLMCPy is an open source Python implementation of the Multi-Level Monte Carlo (MLMC) method for uncertainty propagation. Once a user defines their computational model and specifies the uncertainty in the model input parameters, MLMCPy can be used to estimate the expected value of a quantity of interest to within a specified precision. Support is available to perform the required model evaluations in parallel (if mpi4py is installed) and extensions of the MLMC method are provided to calculate more advanced statistics (e.g., covariance, CDFs). \\n\\nDependencies\\n--------------\\n\\nMLMCPy is intended for use with Python 2.7 and relies on the following packages:\\n\\n* numpy\\n* scipy\\n* mpi4py (optional for running in parallel)\\n* pytest (optional for running unit tests)\\n\\nA requirements.txt file is included for easy installation of dependecies with pip:\\n\\n```\\npip install -r requirements.txt\\n```\\n\\nExample Usage\\n---------------\\n\\n```python\\n\\'\\'\\'\\nSimple example of propagating uncertainty through a spring-mass model using MLMC. \\nEstimates the expected value of the maximum displacement of the system when the spring \\nstiffness is a random variable. See the /examples/spring_mass/from_model/ for more details.\\n\\'\\'\\'\\n\\nimport numpy as np\\nimport sys\\n\\nfrom MLMCPy.input import RandomInput\\nfrom MLMCPy.mlmc import MLMCSimulator\\n\\n# Add path for example SpringMassModel to sys path.\\nsys.path.append(\\'./examples/spring_mass/from_model/spring_mass\\')\\nimport SpringMassModel\\n\\n# Step 1 - Define random variable for spring stiffness:\\n# Need to provide a sampleable function to create RandomInput instance in MLMCPy\\ndef beta_distribution(shift, scale, alpha, beta, size):\\n    return shift + scale*np.random.beta(alpha, beta, size)\\n\\nstiffness_distribution = RandomInput(distribution_function=beta_distribution,\\n                                     shift=1.0, scale=2.5, alpha=3., beta=2.)\\n\\n# Step 2 - Initialize spring-mass models. Here using three levels with MLMC.\\n# defined by different time steps\\nmodel_level1 = SpringMassModel(mass=1.5, time_step=1.0)\\nmodel_level2 = SpringMassModel(mass=1.5, time_step=0.1)\\nmodel_level3 = SpringMassModel(mass=1.5, time_step=0.01)\\nmodels = [model_level1, model_level2, model_level3]\\n\\n# Step 3 - Initialize MLMC & predict max displacement to specified error (0.1).\\nmlmc_simulator = MLMCSimulator(stiffness_distribution, models)\\n[estimates, sample_sizes, variances] = mlmc_simulator.simulate(epsilon=1e-1)\\n\\n```\\n\\nGetting Started\\n----------------\\nMLMCPy can be installed via pip from [PyPI](https://pypi.org/project/MLMCPy/):\\n\\n```\\npip install mlmcpy\\n```\\n\\nMLMCPy can also be installed using the `git clone` command:\\n\\n```\\ngit clone https://github.com/nasa/MLMCPy.git\\n```\\n\\nThe best way to get started with MLMCPy is to take a look at the scripts in the examples/ directory. A simple example of propagating uncertainty through a spring mass system can be found in the ``examples/spring_mass/from_model`` directory. There is a second example that demonstrates the case where a user has access to input-output data from multiple levels of models (rather than a model they can directly evaluate) in the ``examples/spring_mass/from_data/`` directory. For more information, see the source code documentation in ``docs/MLMCPy_documentation.pdf`` (a work in progress).\\n\\nTests\\n------\\nThe tests can be performed by running \"py.test\" from the tests/ directory to ensure a proper installation.\\n\\nDevelopers\\n-----------\\n\\nUQ Center of Excellence <br />\\nNASA Langley Research Center <br /> \\nHampton, Virginia <br /> \\n\\nThis software was funded by and developed under the High Performance Computing Incubator (HPCI) at NASA Langley Research Center. <br /> \\n\\nContributors: James Warner (james.e.warner@nasa.gov), Luke Morrill, Juan Barrientos\\n\\n\\nLicense\\n--------\\n\\nCopyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n \\n'},\n",
       " {'repo': 'nasa/astrobee_gds',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# Astrobee Ground Data System - Control Station software\\nAstrobee is a free-flying robot designed to operate as a payload inside the International \\nSpace Station. (Source code for Astrobee Flight software is available\\n [here](https://github.com/nasa/astrobee))\\n\\nThe **Astrobee Control Station** is an Eclipse RCP application that can command and monitor up to three \\nAstrobee robots or simulators. Astrobees are commanded via prewritten \\n **Plans**, or manually via single commands in **Teleoperate** mode.  Plans consist \\n of locations called Stations, tasks done at each Station, and Segments that connect the Stations.\\n  The Control Station also commands the Astrobee Docking Station to wake up Astrobees that are hibernating.\\n\\nThe Astrobee Control Station is an extension of the Visual Environment for Remote Virtual Exploration (VERVE) that has been customized to operate the Astrobee robot on the International Space Station (ISS). \\n\\nAstrobee Control Station Copyright © 2019, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.\\n\\nVerve Copyright © 2011, United States Government, as represented by the Administrator of the National Aeronautics and Space Administration. All rights reserved.\\n\\nThe Astrobee Control Station platform is licensed under the Apache License, Version 2.0 (the \\n\"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0. \\n \\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n\\nAstrobee Control Station includes a number of third party open source software listed below.  Find the complete listing of third-party software notices and licenses, see the separate “Astrobee Control Station Listing of Notices/Licenses, Including Third Party Software” pdf document in the LICENSE folder.\\n1.\\tA) Eclipse Core Runtime, B) Eclipse Core Filesystem, C) Eclipse Modeling Framework (EMF) ECore, and D) Eclipse Modeling Framework XMI, http://www.eclipse.org  \\n2.\\tArdor3D , https://github.com/Renanse/Ardor3D  \\n3.\\tJOGL (Java OpenGL), http://jogamp.org/ \\n4.\\tApache Log4j, http://logging.apache.org/log4j/ \\n5.\\tCodehaus Jackson https://github.com/codehaus/jackson  \\n\\n\\nThe **Overview** tab summarizes the states of known Astrobees, and sends wake, grab control, and hibernate commands. The **Guest Science** tab sends Plans and some teleoperate commands to as many as three Astrobees simultaneously. The **Run Plan** and **Teleoperate** tabs allow detailed control and monitoring of a single Astrobee. These four tabs are also available to astronauts as the Crew Control Station.\\n\\nThe Engineering Control Station includes six additional tabs. The **Plan Editor** provides \\na graphical interface to create a Plan for one Astrobee. The **Advanced Guest Science** tab,\\n like the simplified Guest Science tab available to crewmembers, commands and monitors three \\n Astrobees simultaneously, but it also allows the user to customize commands and to see Guest\\n  Science telemetry in greater detail. The **Advanced** and **Advanced 2** tabs display \\n  detailed engineering telemetry from one Astrobee, including faults, operating limits, \\n  power and disk usage, etc. The **Modeling** tab provides a graphical interface to edit \\n  keepout/keepin files, which are sent to the robot and displayed in the Control Station 3d \\n  view, and handrail files, which position handrail models within the Control Station 3d \\n  view. The Modeling tab also includes a CSV to Fplan Converter to facilitate the construction \\n  of repetitive plans with known coordinates. The **Debugging** tab displays engineering data \\n  helpful for debugging DDS communication with the robot.\\n\\n[Install instructions](docs/SETUP.md)\\n\\n[Usage instructions and documentation](docs/USAGE.md)\\n'},\n",
       " {'repo': 'nasa/conduit_cms',\n",
       "  'language': None,\n",
       "  'readme_contents': '# conduit_cms\\n\\nSource code will be checked in soon...\\n\\n# README\\n\\nDetailed design documentation is available on the wiki at https://wiki.earthdata.nasa.gov/display/EDDEV/Conduit.\\n\\nDatabase seeds for development are in db/seeds/development.rb.  Test pages for the following rake tasks(db:reset, edtopnav:ingest) \\nwill create pages in \"TEST\" project.\\n\\nRun `rake db:reset` to load the test data.\\n\\nRun `rake edtopnav:ingest` to create and load pages and redirects required for conduit web Top Navigation.  \\nThis is a required step after every database reset.  \\n\\nRun `rake edtopnav:delete` to delete pages and redirects required for conduit web Top Navigation.\\n\\nPage and redirect information required for ED top navigation is configured in \"spec/fixtures/ed_top_navigation.json\".  \\nAny changes to the json file should be preceded by  running `rake edtopnav:delete` \\nAfter updating the json file run  `rake edtopnav:ingest`\\n\\n\\nBy default, this will make the local workstation user an admin in the app; if\\nyour local username does not match your AUID, you can override this by\\nsetting the AUID_USER environment variable. For example:\\n\\n    AUID_USER=my_auid rake db:reset\\n\\nYou could also add `export AUID_USER=foo` to your .bash_profile to make this\\nchange permanently.\\n\\nConduit requires PostgreSQL 9.2 or above (and will soon require 9.3 or above).\\n\\n##Testing Launchpad Authentication in Dev Environment\\n\\nTo test authentication with Launchpad, run off any of these instead of the usual (localhost:3000):\\n\\thttps://conduit.dev.earthdata.nasa.gov\\n\\thttps://conduit.sit.earthdata.nasa.gov\\n\\thttps://conduit.uat.earthdata.nasa.gov\\n\\thttps://conduit.earthdata.nasa.gov\\n\\nAdd this line your /etc/hosts file depending on the environment you\\'re currently testing.\\n\\n```\\n127.0.0.1\\tconduit.dev.earthdata.nasa.gov\\n```\\n\\nYou\\'ll also need to alter your nginx.conf file. Add\\n\\n```\\n\\tssl_certificate /usr/local/etc/nginx/cert.crt;\\n    ssl_certificate_key /usr/local/etc/nginx/cert.key;\\n\\n    server {\\n        listen 443 ssl;\\n\\n        server_name conduit.dev.earthdata.nasa.gov; # Change this line for a different environment\\n\\n        ssl on;\\n        ssl_session_cache shared:SSL:1m;\\n        ssl_session_timeout 5m;\\n        ssl_ciphers HIGH:!aNULL:!MD5;\\n        ssl_prefer_server_ciphers on;\\n        location / {\\n            root html;\\n            index index.html index.htm;\\n\\n            proxy_pass http://conduit.dev.earthdata.nasa.gov:3000; # Change this line for a different environment\\n\\n            proxy_set_header Host $host;\\n            proxy_set_header X-Real-IP $remote_addr;\\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\\n            proxy_set_header X-Forwarded-Proto $scheme;\\n        }\\n    }\\n```\\nTo start the server run \"rails s -b 0.0.0.0\" then go to https://conduit.dev.earthdata.nasa.gov\\n\\nIf you change the environment you\\'re testing you\\'ll have to update the following values in the application.yml file.\\n\\tSAML_SP_ISSUER_BASE\\n\\tSAML_SP_ISSUER\\n\\tSAML_SP_ACS_URL\\n\\n\\nFor details about custom fields in pages that may need processing before rendering\\nsee [API Documentation](docs/api.md)\\n\\n\\n'},\n",
       " {'repo': 'nasa/PyAMPR',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': \"***************************\\nIF YOU ARE USING PYAMPR FOR IPHEX DATA, GO BACK TO THE GHRC SERVER AND GET THE\\nLATEST VERSION OF THE DATASET, AS WE HAVE FIXED THE 37 GHZ CHANNEL A AND B SWAP ISSUE.\\n***************************\\n\\nTitle/Version\\n-------------\\nPython AMPR Data Toolkit (PyAMPR) v1.7.1 \\nLast changed 08/07/2019  \\n\\n\\nLead Author\\n-----------\\nTimothy Lang  \\nNASA MSFC  \\ntimothy.j.lang@nasa.gov  \\n(256) 961-7861  \\n\\n\\nContributing Authors\\n--------------------\\nBrent Roberts  \\nNASA MSFC  \\njason.b.roberts@nasa.gov  \\n(256) 961-7477  \\n\\n\\nOverview\\n--------\\nThe Advanced Microwave Precipitation Radiometer (AMPR) is an airborne \\npassive microwave radiometer managed by NASA Marshall Space Flight Center.\\nDownload AMPR data from http://ghrc.nsstc.nasa.gov.\\nAMPR brightness temperature data from NASA field projects\\nare in ASCII or netCDF format. This python script defines a class that will \\nread in single file from an individual aircraft flight and pull out\\ntiming, brightness temperatures from each channel, geolocation, and\\nother information and store them as attributes using numpy \\narrays of the appropriate type. The file is read and the data are populated when\\nthe class is instantiated with the full path and name of an AMPR file.\\nNumerous visualization methods are provided, including track plots,\\nstrip charts, and Google Earth KMZs. In addition, polarization\\ndeconvolution is available.\\n\\n\\nInstallation and Use\\n--------------------\\nDependencies: Python 2.7 thru 3.7,  `numpy`,  `matplotlib`,  `cartopy`,\\n              `os`,  `time`,  `simplekml`,  `datetime`,  `calendar`, \\n              `codecs`,  `gzip`,  `netCDF4`\\nMost of these are provided with standard Python distributions.\\nYou may need to install `cartopy` via your Python distribution's\\npackage manager. The `simplekml` package can be found [here.](https://pypi.python.org/pypi/simplekml/ )\\n\\nIn the same directory as this `README` is `setup.py`, to install this\\npackage enter the following command at the prompt:\\n```\\npython setup.py install\\n```\\n\\nThen to import, in your python program include:\\n```\\nimport pyampr\\n```\\n\\nTo read an AMPR TB file type:\\n```\\nampr_data = pyampr.AmprTb('FILE_NAME_HERE', project='PROJECT_NAME_HERE')\\n```\\n\\nThen the ampr_data object will have access to all the plotting and analysis \\nmethods. Use `help(pyampr.AmprTb)` to find out more.\\n\\nIn particular, `help(pyampr.AmprTb.read_ampr_tb_level2b)` will give a full \\nrundown on the data structure.\\n\\nA demonstration IPython notebook can be found in the notebooks directory.\\n\\nA simple interactive testing notebook is available in the test directory.\\n\\nThis [conference presentation](https://ams.confex.com/ams/95Annual/webprogram/Paper262779.html) describes PyAMPR (among other modules).\\n\\n\\n\"},\n",
       " {'repo': 'nasa/xasgo', 'language': 'Julia', 'readme_contents': ''},\n",
       " {'repo': 'nasa/abaverify',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# abaverify\\nA python package built on [`unittest`](https://docs.python.org/2.7/library/unittest.html) for running verification tests on Abaqus user subroutines. Basic familiarity with unittest and Abaqus user subroutine development is assumed in this readme.\\n\\nThis software may be used, reproduced, and provided to others only as permitted under the terms of the agreement under which it was acquired from the U.S. Government. Neither title to, nor ownership of, the software is hereby transferred. This notice shall remain on all copies of the software.\\n\\nCopyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\nFor any questions, please contact the developers:\\n- Andrew Bergan | [andrew.c.bergan@nasa.gov](mailto:andrew.c.bergan@nasa.gov) | (W) 757-864-3744\\n- Frank Leone   | [frank.a.leone@nasa.gov](mailto:frank.a.leone@nasa.gov)     | (W) 757-864-3050\\n\\n## Getting-Started\\nThis package assumes that you have `python 2.x` and `git` installed. This packaged is designed for Abaqus 2016 and it has been used successfully with v6.14; it may or may not work with older versions. It also assumes that you have an Abaqus user subroutine in a git repository with a minimum directory structure as shown here:\\n```\\nrepo_dir/\\n    .git/\\n    for/\\n        usub.for\\n    tests/\\n        testOutput/\\n        abaqus_v6.env\\n        test_model1.inp\\n        test_model1_expected.py\\n        ...\\n        test_runner.py\\n    .gitignore\\n```\\n\\nThe user subroutine is stored in the `for/` directory and the verification tests are stored in the `<your_userSubroutine_repo_dir>/tests/` directory.\\n\\n### Install `abaverify`\\nAbaverify can be installed using the python utility `pip` (8.x+). The following sections provide a short summary of how to use `pip` to install `abaverify`. Clone `abaverify` into a convenient directory:\\n```\\n$  git clone https://github.com/nasa/abaverify.git\\n```\\nThen install using the `-e` option:\\n```\\n$  pip install -e path/to/abaverifyDir\\n```\\n\\nThat\\'s it.\\n\\nIf install fails with errors indicating an issue with `paramiko` or `cryptography`, see the [`paramiko` installation guide](http://www.paramiko.org/installing.html) for troubleshooting.\\n\\nThe remainder of this section describes how to build your own tests using `abaverify` (e.g., what goes inside the `test_model1.inp`, `test_model1_expected.py`, and `test_runner.py`) files. For a working example, checkout the sample verification test in the `abaverify/tests/tests/` directory. You can run the sample test with the command `python test_runner.py` from the `abaverify/tests/tests/` directory. Note, the default environment file (`abaverify/tests/tests`) is formatted for windows; linux users will need to modify the default environment file to the linux format.\\n\\n### Create `.inp` and `.py` files for each test model\\nA model file (`*.inp` or `*.py`) and corresponding results file (`*_expected.py`) with the same name must be created for each test case. These files are placed in the `tests/` directory. The model file is a typical Abaqus input deck or python script, so no detailed discussion is provided here (any Abaqus model should work). When tests are executed (with the command `python test_runner.py`), the models in the `tests/` directory are run in Abaqus.\\n\\nThe `*_expected.py` file defines the assertions that are run on the `odb` output from the analysis. After each analysis is completed, the script `abaverify/processresults.py` is called to collect the quantities defined in the `*_expected.py` file. The `*_expected.py` file must contain a list called `\"results\"` that contains an object for each result of interest. A typical result quantity would be the maximum stress for the stress component `S11`. For example:\\n```\\nparameters = {\\n    \"results\":\\n        [\\n            # Simple example to find max value of state variable d2\\n            {\\n                \"type\": \"max\",                                      # Specifies criteria to apply to the output quantity\\n                \"identifier\":                                       # Identifies which output quantity to interrogate\\n                    {\\n                        \"symbol\": \"SDV_CDM_d2\",\\n                        \"elset\": \"ALL\",\\n                        \"position\": \"Element 1 Int Point 1\"\\n                    },\\n                \"referenceValue\": 0.0                               # This the value that the result from the model is compared against\\n            },\\n            {\\n            ...\\n            <Additional result object here>\\n            ...\\n            }\\n        ]\\n}   \\n```\\nThe value found in the `odb` must match the reference value for the test to pass. In the case above, the test is simply to say that `SDV_CDM_d2` is always zero, since the range of `SDV_CDM_d2` happens to be between 0 and 1. Any history output quantity can be interrogated using one of the following criteria defined in the `type` field: `max`, `min`, `continuous`, `xy_infl_pt`, `disp_at_zero_y`, `log_stress_at_failure_init`, `slope`, or `finalValue`. Here\\'s a more complicated example:\\n```\\nparameters = {\\n    \"results\":\\n        [\\n            # More complicated case to find the slope of the stress strain curve within the interval 0.0001 < x < 0.005\\n            {\\n                \"type\": \"slope\",\\n                \"step\": \"Step-1\",                                   # By default all steps are used. You can specify any step with the step name\\n                \"identifier\": [                                     # The identifier here is an array since we are looking for the slope of a curve defined by x and y\\n                    { # x\\n                        \"symbol\": \"LE11\",\\n                        \"elset\": \"ALL\",\\n                        \"position\": \"Element 1 Int Point 1\"\\n                    },\\n                    { # y\\n                        \"symbol\": \"S11\",\\n                        \"elset\": \"ALL\",\\n                        \"position\": \"Element 1 Int Point 1\"\\n                    }\\n                ],\\n                \"window\": [0.0001, 0.005],                          # [min, max] in x        \\n                \"referenceValue\": 171420,                           # Reference value for E1\\n                \"tolerance\": 1714                                   # Require that the calculated value is within 1% of the reference value\\n            }\\n        ]\\n}\\n```\\nThe results array can contain as many result objects as needed to verify that the model has performed as designed. Assertions are run on each result object and if any one fails, the test is marked as failed.\\n\\n### Create a `test_runner.py` file\\nThe file `sample_usage.py` gives an example of how you call your newly created tests. By convention, this file is named as `test_runner.py`. This file must include:\\n\\n1. `import abaverify`.\\n2. classes that inherit `abaverify.TestCase` and define functions beginning with `test` following the usage of `unittest`. See the `sample_usage.py` for an example.\\n3. call to `runTests()` which takes one argument: the relative path to your user subroutine (omit the `.f` or `.for` ending, the code automatically appends it).\\n\\nFunctionality in `unittest` can be accessed via `abaverify.unittest`. One example of the use case for this is that `unittest` decorators can be applied to functions and classes in the `test_runner.py` file.\\n\\n### Running your tests\\nBefore running tests, make sure you place an Abaqus environment file in your project\\'s `tests/` directory. At a minimum, the environment file should include the options for compiling your subroutine. If you do not include your environment file, `abaverify` will give an error.\\n\\nYou can run your tests with the syntax defined by `unittest`. To run all tests, execute the following from the `tests` directory of your project:\\n```\\ntests $  python test_runner.py\\n```\\nAll of the tests that have been implemented will be run. The last few lines of output from these commands indicate the number of tests run and `OK` if they are all successful.\\n\\nTo run a single test, add the class and test name. For example for the input deck `test_CPS4R_tension.inp` type:\\n```\\ntests $  python test_runner.py SingleElementTests.test_CPS4R_tension\\n```\\n\\n## Command Line Options\\nVarious command line options can be used as described below.\\n- `-A` or `--abaqusCmd` can be used to override the abaqus command to specify a particular version of abaqus. By default, the abaqus command is `abaqus`. Specify a string after the option to use a different version of abaqus. For example:\\n```\\ntests $  python test_runner.py SingleElementTests.test_C3D8R_simpleShear12 -A abq6123\\n```\\n- `-c` or `--preCompileCode` can be specified to use `abaqus make` to compile the code into a binary before running one or more tests. A function that compiles the code must be provided to `abaverify` as an argument to the `runTests` function call in `test_runner.py`. The `usub_lib` option must be defined in the environment file.\\n- `-C` or `--cpus` can be used to run abaqus jobs on more than one cpu. By default, abaqus jobs are run on one cpu. To specify more than one cpu, use a command like:\\n```\\ntests $  python test_runner.py SingleElementTests.test_C3D8R_simpleShear12 -C 4\\n```\\n- `-d` or `--double` can be specified to run explicit jobs with double precision.\\n- `-e` or `--useExistingBinaries` can be specified to reuse the most recent compiled version of the code.\\n- `-i` or `--interactive` can be specified to print the Abaqus log data to the terminal.\\n- `-n` or `--doNotSaveODB` can be used to disable saving of x-y data to the odb. This is sometimes helpful when debugging post processing scripts in conjunction with `-r`\\n- `-r` or `--useExistingResults` can be specified to reuse the most recent test results. The net effect is that only the post-processing portion of the code is run, so you don\\'t have to wait for the model to run just to debug a `_expected.py` file or `processresults.py`.\\n- `-R` or `--remoteHost` can be specified to run the tests on a remote host, where the host information is passed as `user@server.com[:port][/path/to/run/dir]`. The default run directory is `<login_dir>/abaverify_temp/`. Looks for a file in the `tests/` directory called `abaverify_remote_options.py`, which can be used to set options for working with the remote server. An example of this file is available `abaverify/tests/tests/abaverify_remote_options.py`. Usage example:\\n```\\ntests $  python test_runner.py -R username@server.sample.com\\n```\\n- `-s` or equivalently `--specifyPathToSub` can be used to override the relative path to the user subroutine specified in the the call `abaverify.runTests()` in your `test_runner.py` file.\\n- `-t` or `--time` can be specified to print the run times for the compiler, packager, and solver to the terminal. For example:\\n```\\ntests $  python test_runner.py SingleElementTests.test_C3D8R_simpleShear12 --time\\n```\\n- `-V` or `--verbose` can be specified to print the Abaqus log data to the terminal.\\n- `-x` or `--expiration` can be specified to set a duration (in seconds) after which the job is killed to prevent long running jobs that may occur as a result of an error in the subroutine. A job-specific expiration can be set by adding `\"expiration\": <time in seconds>` to the parameters dictionary in a `*_expected.py` file.\\n\\n\\n## Results `type`\\nA variety of different types of results can be extracted from the odbs and compared with reference values. A list of each support type and brief explanation are provided below:\\n- `max`: finds the maximum value of an xy data set\\n- `min`: finds the minimum value of an xy data set\\n- `continuous`: finds the maximum delta between two sequential increments an xy data set\\n- `xy_infl_pt`: finds an inflection point in xy data set\\n- `disp_at_zero_y`: finds the displacement (implied as x value) where the y value is zero in an xy data set\\n- `log_stress_at_failure_init`: finds stress at failure (intended for checking failure criteria)\\n- `slope`: finds the slope of an xy data set\\n- `finalValue`: finds the y value at the last increment in the xy data set\\n- `x_at_peak_in_xy`: finds the x-value corresponding to the absolute peak in the y-value\\n- `tabular`: compares the values for a list of tuples specifying x, y points [(x1, y1), (x2, y2)...]. See example for further details\\n\\n### Eval Statement\\n\\nEvery abaverify results type (max, min, tabular, etc.) has a default usage where either\\none history object is specified (using the identifier dictionary object) or two history\\nobjects (x and y) are specified. Essentially, a value like displacement (u) and reaction force (rf)\\ncan be extracted from the abaqus results file and asserted against directly. \\n\\nThere are times when this is limiting and some combination of Abaqus quantities is more convenient to\\nbe asserted against. If the results type specifies only a single a single history object then an *evalStatement*\\nkey may be specified within the results specifier dictionary. If there results type specifies an x and y history\\nidentifier then two separate evalStatements ( *xEvalStatement* and *yEvalStatement* ). \\n\\nAn eval statement is generally an arithmetic combination of results history objects. An eval\\nstatement is a string where previously defined history objects maybe referenced (via there assigned *label* key).\\nAn example of an eval statement is given below:\\n\\n```\\n    \"evalStatement\": \"(d[\\'n1_U1\\'] + d[\\'n2_U1\\']) / 10.0\"\\n```\\n\\nIn the example above, *n1_U1* and *n2_U2* would be identifying dictionaries with those very labels (labels should\\ntherefore be unique) which are summed and then divided by 10.0.  \\n\\nNOTE: To access a result defined in an identifier object the general syntax is d[<label>]. This is because\\ninternally a dictionary of results (d) is created where label is made to be key. Therefore, to access the data\\nin the dictionary d one must use the python syntax to do so (d[<label>]).\\n\\n#### Tabular Example Using Eval Statement\\n\\nThe tabular example by default uses the two identifier dict objects to define x and y data respectively (which is thusly compared to a\\nlist of tuples specified as referenceValue). Additionally, a more advanced usage is allowed within the tabular option to specify a pythonic statement for combinging\\nmultiple identifier results into a set of x values (and y values). This is best seen by way of example:\\n\\n```\\n    length = 10\\n    area = 100\\n    ...\\n    \"results\": [\\n        {\\n            \"type\": \"tabular\",\\n            \"identifier\": [\\n                {   \"label\": \"x1\",\\n                    \"symbol\": \"U2\",\\n                    \"nset\": \"LOADAPP\"\\n                },\\n                {   \"label\": \"x2\",\\n                    \"symbol\": \"U2\",\\n                    \"position\": \"Node 4\",\\n                    \"nset\": \"LOADFOLLOWERS\"\\n                },\\n                {   \"label\": \"y\",\\n                    \"symbol\": \"RF2\",\\n                    \"nset\": \"LOADAPP\"\\n                }\\n            ],\\n            # Use eval statements to calculate a reference strain and stress val from abaqus output of force and disp\\n            \"xEvalStatement\": \"(d[\\'x1\\'] + d[\\'x2\\']) / (2 * {length})\".format(length=length),\\n            \"yEvalStatement\": \"d[\\'y\\']/ {area}\".format(area=area),\\n            \"referenceValue\": [\\n                            (0.0, 0.0), \\n                            (0.000582908, 1.49516), \\n                            (0.000944326, 2.4222), \\n                            (0.00138836, 3.56113)\\n                            ],\\n            \"tolerance\": (0.0001, 0.350)\\n        }\\n    ]\\n```\\n\\nIn the example above *label*s are given to identifier dictionaries (for subsequent use in evaluation statements). \\nThen a *xEvalStatement* and *yEvalStatement* is provided which can be any pythonic evaluatable expression (generally,\\nsome combination of the xy history results specified by the labeled identifier objects). In this example, two displacements\\nare extracted from the odb (labeled *x1* and *x2*). They are averaged together and then normalized by some length to determine \\na reference strain value. Because this combination is defined in the *xEvalStatement* these points will become the basis for\\nthe x\\'s. Similarly y points are defined by normalizing force by area for reference stress determination. After\\nthe definition of x and y points through eval statements the comparison for test is identical to the\\ndefault tabular implementation (comparison to referenceValue within specified tolerance). \\n\\n## Custom assertions in the test_runner.py file\\nUser-defined assertions can be added without modifying the abaverify code as follows. Optional arguments `func` and `arguments` are provided in the `self.runTest()` function call. `func` is a python function that receives three positional arguments: the abaverify object, the jobname, and the object passed to `arguments`. The user-defined code in `func` may implement any logic necessary and then use the abaverify object to make the necessary assertions.\\n\\n## Automatic testing\\nAbaverify has the capability to run a series of tests, generate a report, and plot run times against historical run times. See `automatic.py` and `automatic_testing_script.py` for details.\\n'},\n",
       " {'repo': 'nasa/gen_sch_tbl',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"# Core Flight System : Framework : Tool : Generate Schedule Table\\n\\nThis repository contains NASA's Generate Schedule Table Tool (gen_sch_tbl), which is a framework component of the Core Flight System.\\n\\nThis lab application is a ground utility to generate the binary schedule table for cFS. It is intended to be located in the `tools/gen_sch_tbl` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.\\n\\nSee readme.txt for more information.\\n\\n## Release Notes\\n\\ngen_sch_tbl version 1.0a is released as part of cFE 6.6.0a under the Apache 2.0 license, see [LICENSE](LICENSE-18128-Apache-2_0.pdf).\\n\\nNOTE - there may be other schedule table management schemes which may be more applicable for modern missions.  Contact the community as detailed below for more information.\\n\\n## Known issues\\n\\nThis ground utility was developed for a specific mission/configuration, and may not be applicable for general use.\\n\\n## Getting Help\\n\\nFor best results, submit issues:questions or issues:help wanted requests at https://github.com/nasa/cFS.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n\\n\"},\n",
       " {'repo': 'nasa/MINX',\n",
       "  'language': 'Prolog',\n",
       "  'readme_contents': '# MINX (MISR INteractive eXplorer)\\n---\\nMINX is an interactive visualization and analysis program written in IDL and designed to make MISR data more accessible to science users.\\nIts principal use is to retrieve heights and motion for aerosol plumes and clouds using stereoscopic methods.\\n\\nMINX is platform independent and has been tested on Mac OS X, MS Windows, and Linux.\\n\\n#### Binaries for Mac OS X, MS Windows, and Linux can be found under [MINX Releases](https://github.com/nasa/MINX/releases \"MINX Releases\")\\n##### Full Documentation can be found under the [webdoc](https://github.com/nasa/MINX/blob/master/webdoc \"MINX documentation\") directory\\n\\n  \\n\\n# How to run MINX from source\\n---\\n\\n### In the IDL Development Environment / IDL Workbench Import the Project\\n* File Menu -> Import\\n* Existing Projects into Workspace\\n* Next Button\\n* Browse to Select Root Directory\\n* Navigate to the root MINX directory containing MINX.prj and click the Open button\\n* Ensure the MINX project is displayed in the projects listing\\n* Finish Button\\n\\n### Build the Project\\n* Select the MINX project in Project Explorer\\n* Project Menu -> Build Project\\n\\n### Run the Project\\n* Select the MINX project in Project Explorer\\n* Run Menu -> Run Project MINX\\n\\n# How to build MINX binaries\\n---\\n\\n### In the IDL Development Environment / IDL Workbench Import the Project\\n* File Menu -> Import\\n* Existing Projects into Workspace\\n* Next Button\\n* Browse to Select Root Directory\\n* Navigate to the root MINX directory containing MINX.prj and click the Open button\\n* Ensure the MINX project is displayed in the projects listing\\n* Finish Button\\n\\n### Build the Project\\n* Select the MINX project in Project Explorer\\n* Project Menu -> Build Project\\n\\n### Use MAKE_RT at the IDL Console to Create a MINX Stand-alone IDL Runtime Distribution\\n\\n#### For Mac OS X\\n* `MAKE_RT, \\'MINX4\\', \\'/Users/<username>/<MINX_package_destination>\\', SAVEFILE=\\'/<MINX Source Path>/MINX/minx.sav\\', /VM, /MACINT64, /HIRES_MAPS`\\n\\n#### For Windows\\n* `MAKE_RT, \\'MINX4\\', \\'C:\\\\Users\\\\<username>\\\\<MINX_package_destination>\\', SAVEFILE=\\'C:\\\\<MINX Source Path>\\\\MINX\\\\minx.sav\\', /VM, /WIN32, /HIRES_MAPS`\\n\\n#### For Linux\\n* `MAKE_RT, \\'MINX4\\', \\'/home/<username>/<MINX_package_destination>\\', SAVEFILE=\\'/<MINX Source Path>/MINX/minx.sav\\', /VM, /LIN32, /HIRES_MAPS`\\n\\n*Note: Depending on your version of IDL and the current OS, you may also be able to specify the /WIN64 (for Windows 64-bit), /MACINT32 (for OS X 32-bit), or /LIN64 (for Linux 64-bit) keywords to build MINX packages*\\n\\n'},\n",
       " {'repo': 'nasa/GPU_SDR',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'GPU SDR\\n=======\\n<p>This project enable the readout of superconductive resonators using Ettus&reg USRP SDR platforms and Nvidia&reg GPUs. The goal of the project is to allow users to modify and customize every aspect of the data acquisition: from the realtime signal generation and demodulation to the analysis techniques.</p>\\n\\n![jpl-logo](server_docs/images/jpl-logo.png)\\n\\n<p>This project has been developed and tested in various laboratories at JPL and Caltech in the context of the JPL Flexible radio-frequency readout for multiplexed submillimeter-wave detectors RT&D</p>\\n\\nThe documentation for this software is available at [this page](http://www.its.caltech.edu/~minutolo/gpu_sdr_doc.html)\\n\\nQuickstart\\n----------\\nIn order to test and use the system you need to:\\n  1. Gather all the required hardware: see the shopping list section section [here](http://www.its.caltech.edu/~minutolo/server_docs/build/html/dd/d8d/md_server_docs_01_installation.html)\\n  2. Compile the server using the instruction provided [here](http://www.its.caltech.edu/~minutolo/server_docs/build/html/dd/d8d/md_server_docs_01_installation.html)\\n  3. Launch examples of the python API or follow the quickstart guide [here](http://www.its.caltech.edu/~minutolo/lib_docs/lib_docs_index.html)\\n'},\n",
       " {'repo': 'nasa/DdsJs',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Building and Packaging\\n\\n## Prerequisites\\n\\n1. ANTLR (http://www.antlr.org/) version 4.4 or higher. Note location of ANTLR JAR file.\\n2. StringTemplate (http://www.stringtemplate.org) version 4.0 or higher. Note location of StringTemplate JAR file.\\n3. CommonsCLI (http://commons.apache.org/proper/commons-cli/). Version 1.2 or higher. Note location of commons-cli \\n   JAR file.\\n4. CMake (http://www.cmake.org). Version 3.11.0 or higher. A version may be provided by Linux distributions.\\n5. Node.js (http://www.nodejs.org). Any NodeJS version belonging to major version 8. Note the actual version number.\\n6. `cmake-js` (installed via NPM).\\n7. A properly-licensed DDS provider distribution. Currently, only CoreDX (http://www.twinoaks.com). Version 4.2.0 or higher is supported.\\n8. Java Development Kit (http://java.oracle.com). Version 1.8 or higher. OpenJDK may also work if provided by your\\n   Linux distribution.\\n\\n## Build Preparation\\n\\n**DDS.js** requires that the files required to build Node.js native add-ons be already installed. This can be done\\nby first installing `cmake-js`, then letting it install the build files:\\n\\n    npm install -g cmake-js\\n    cmake-js install\\n\\n## Preferred Location of JAR Dependencies\\n\\nThe **DDS.js** build files prefer that the JAR files it depends on (ANTLR, StringTemplate, and CommonsCLI) be \\ninstalled in the `/usr/local/lib` directory. They may be installed in other locations, but doing so may make configuring the build system more difficult.\\n\\n## DDS Environment Settings\\n\\n### CoreDX\\n\\nThe **DDS.js** build files expect the following CoreDX-related environment variables be properly configured:\\n\\n*  `COREDX_TOP`: Should point to the top-level installation directory of CoreDX\\n*  `COREDX_TARGET`: Should indicate the platform specification of the target on which **DDS.js** will run. Usually\\n   this is the same as the platform building the software.\\n\\n### OpenSplice\\n\\nTBD\\n\\n### OpenDDS\\n\\nTBD\\n\\n## Build Configuration\\n\\nThe recommended way to build **DDS.js** is to create a build directory inside the top-level source directory (i.e., a \"build tree\") and perform the build from that directory.\\n\\n    cd <top-level>\\n    mkdir build\\n    cd build\\n    \\n**DDS.js** uses CMake as its build system. The command line parameters passed to CMake provide information regarding\\nwhere to find all the prerequisites. The available command line parameters are:\\n\\n*  `-DANTLR_VERSION=<ver>`: Version number of the ANTLR v4 JAR file. Should be evident from the JAR file name. This\\n   option may only be used if the ANTLR JAR file is installed in `/usr/local/lib`.\\n*  `-DCOMMONS_CLI_VERSION=<ver>`: Version number of the Commons-CLI JAR file. Should be evident from the JAR file \\n   name. This option may only be used if the Commons-CLI JAR file is installed in `/usr/local/lib`, or if the \\n   location of the JAR file is specified via `COMMONS_CLI_HOME`.\\n*  `-DCOMMONS_CLI_HOME=<dir>`: Location of the Commons-CLI JAR file. Not necessary if the JAR file is installed in\\n   `/usr/local/lib`.\\n*  `-DST_VERSION=<ver>`: Version number of the StringTemplate JAR file. Should be evident from the JAR file name.\\n   This option may only be used if the StringTemplate JAR file is installed in `/usr/local/lib`.\\n*  `-DNODEJS_VERSION=<ver>`: Version number of Node.js being used.\\n*  `-DWITH_DDS=<provider>`: Configure **DDS.js** to use a particular DDS provider. Currently, only `CoreDX` is supported.\\n*  `-DCMAKE_BUILD_TYPE=(Debug|Release)`: Type of build. Labels are self-explanatory.\\n*  `-DCMAKE_INSTALL_PREFIX=<directory>`: Directory where **DDS.js** will be installed. On user builds, it defaults to\\n   `${HOME}/Install`. Production builds usually are rooted in `/opt`.\\n\\nFor example, to configure the **DDS.js** build with ANTLR version 4.5, StringTemplate version 4.0.8, Commons-CLI\\nversion 1.2, and using CoreDX DDS, with all JARs installed in `/usr/local/lib`, and Node.js version 4.2.4, one would issue the\\nfollowing command (assuming the other prerequisites are met, and the command is being run from the build directory):\\n\\n    cmake -DCMAKE_BUILD_TYPE=Release \\\\\\n          -DANTLR_VERSION=4.5 \\\\\\n          -DCOMMONS_CLI_VERSION=1.2 \\\\\\n          -DST_VERSION=4.0.8 \\\\\\n          -DNODEJS_VERSION=4.2.4 \\\\\\n          -DWITH_DDS=CoreDX \\\\\\n          ..\\n\\nThe last argument (`..`; parent directory reference) is important, as it tells CMake where to find the \\n`CMakeLists.txt` file.\\n\\nAs of this writing, **DDS.js** works with the following package versions:\\n\\n| Package  | Version  |\\n| -------- | -------- |\\n| ANTLR  | 4.5.1  |\\n| Commons CLI  | 1.3.1  |\\n| StringTemplate  | 4.0.8  |\\n| NodeJS  | 8.11.4  |\\n\\n## Building\\n\\nAfter CMake successfully completes the build environment configuration, building **DDS.js** may be done by\\nsimply issuing a `make`.\\n\\n## Packaging\\n\\nOnce the build is complete, CMake may be used to create installable packages. On Intel/Linux systems, the follwing\\ncommand produces two (2) installable DEB archives. On other Linux systems, it produces two (2) TAR-GZ archives:\\n\\n    make package\\n\\nThis default behavior can be circumvented by calling the CPack tool (part of CMake) directly with a package generator specification. For example, to generate TAR-GZ archives even in Intel/Linux systems, the following command must be run from the build tree:\\n\\n    cpack -G TGZ\\n\\nThe resulting archives will bear in their name the **DDS.js** package name, version, target operating system, target\\nsystem processor, and whether the archive contains runtime files or files used for development. For example, a\\nset of TAR-GZ archives built for a Linux x86-64 host would look like this:\\n\\n*  `ddsjs-1.0.0-Linux-x86_64-Bin.tar.gz`: Archive containing the shared libraries that are needed at runtime.\\n*  `ddsjs-1.0.0-Linux-x86_64-Dev.tar.gz`: Archive containing header files and build scripts that are needed when\\n   building a NodeJS add-on with **DDS.js**.\\n\\nA set of DEB archives would look as follows (when using CoreDX as the DDS provider):\\n\\n*  `ddsjs-coredx-bin_1.0.0-custom_amd64.deb`: Archive containing the shared libraries that are needed at runtime.\\n*  `ddsjs-coredx-dev_1.0.0-custom_amd64.deb`: Archive containing header files and build scripts that are needed when\\n   building a NodeJS add-on with **DDS.js**.\\n\\n\\n## Installation\\n\\nWhen installing from TAR-GZ archives, simply unpacking then on a target directory will create a\\ndirectory for **DDS.js** and unpack all the files in their appropriate location. When `CMAKE_INSTALL_PREFIX` specified\\na global location, it is expected that the archive be unpacked at the root of the file system:\\n\\n    tar -C / -zxvf ddsjs-1.0.0-Linux-x86_64-Bin.tar.gz\\n\\nBy default, `CMAKE_INSTALL_PREFIX` is configured to be `/opt/ASTRA`.\\n\\n## Post-installation\\n\\n**DDS.js** must have the shared libraries in its \"Bin\" package be registered with the system\\'s dynamic linker. On Linux\\nsystems, this usually involves adding an entry to the `/etc/ld.so.conf` file pointing to the directory where the\\nshared libraries were installed.\\n\\n# Using DDS.js\\n\\n## Workspace Preparation\\n\\nThe DDS.js package works best when used in conjunction with the CMake.js\\n(https://www.npmjs.com/package/cmake-js) NPM package. A typical DDS.js enabled\\nadd-on would have the following files in its project root directory:\\n\\n*  `CMakeLists.txt`: CMake file containing the build instructions for the\\n   add-on.\\n*  `package.json`: JSON file that describes the NodeJS add-on.\\n*  `index.js`: Top-level, or \"main\", JavaScript file for the add-on.\\n*  **IDL File**: The DDS IDL file describing the data types to use in the\\n   add-on.\\n\\nIn order for users of the add-on to be able to re-compile the source files, the\\n`package.json` file must contain an `install` script fragment that calls\\n`cmake-js compile`, as well as contain dependencies on `cmake-js` and (highly\\nrecommended) `bindings`.\\n\\nRefer to the `examples` directory in the source distribution in order to glean\\nwhat the typical Node.js add-on package source tree should look like. In it,\\nthere is a very simple IDL file that will serve as the basis for the examples\\nthat follow:\\n\\n    module HostMonitor {\\n\\n    typedef string<256> HostNameType;\\n\\n    struct OverallInformation {\\n        HostNameType hostName;\\n        float cpuUtilization;\\n        float memoryUtilization;\\n    };\\n\\n    };\\n\\nAn excellent idea for exploring **DDS.js** is to copy the contents of the \\n`examples` directory onto a different location, thus any experimentation does\\nnot affect the **DDS.js** source area.\\n\\n## Building Node.js Add-on\\n\\nAfter the aforementioned files are created, the command `cmake-js build` should\\nbuild the add-on.\\n\\n## Distributing the Add-on\\n\\nOnce the custom add-on is built using the **DDS.js** tools and libraries, it may be\\npackaged for distribution using the `npm pack` command (run from the source\\ntree). The aforementioned command will produce a `*.tgz` archive that can then\\nbe installed onto the target application as a Node.js module. The `Bin`-variants\\nof **DDS.js** must also be distributed with the add-on.\\n\\n# Using the DDS.js Built Add-on\\n\\nWith the NPM package containing the **DDS.js** built add-on installed on the\\ntarget application, the API provided by said add-on can be used as any other\\nNode.js module. Following is a sample Node.js \"console\" application that\\nsubscribes to the `OverallInformation` samples defined in the example module\\'s\\n`HostMonitor.idl` file:\\n\\n    const DDS = require(\"dds-hostmonitor\").DDS;\\n    const HostMonitor = require(\"dds-hostmonitor\").HostMonitor;\\n\\n    var particip = DDS.createDomainParticipant(\\n        /* Domain ID */0\\n    );\\n\\n    var subscriber = particip.createSubscriber();\\n\\n    var overallInfoTopic = particip.createTopic(\\n        HostMonitor.OverallInformationTopic\\n    );\\n\\n    var overallInfoReader = subscriber.createDataReader(\\n        overallInfoTopic\\n    );\\n\\n    /* Callable that receives samples from \"take()\" */\\n    var printSamples = (error, samples, sampleInfos) => {\\n        if (error) {\\n            console.out(\"ERROR in take(): \" + error);\\n        } else {\\n            console.out(JSON.stringify(samples));\\n        }\\n    };\\n\\n    /* Do a \"take()\" every 1000 milliseconds */\\n    var timerObj = setInterval(\\n        () => {\\n            overallInfoReader.take(\\n                /* Max sample count */ 100,\\n                /* Unused */ 0,\\n                /* Callback */ printSamples\\n            );\\n        },\\n        1000\\n    );\\n\\n    process.on(\"SIGINT\", () => {\\n        /* Cleanup after detecting Ctrl+C */\\n        clearInterval(timerObj);\\n        timerObj = null;\\n        particip.deleteContainedEntities();\\n        overallInfoReader = null;\\n        overallInfoTopic = null;\\n        subscriber = null;\\n        particip = null;\\n    });\\n\\nInclusion of the **DDS.js** module is by the name given in `package.json`. The\\nlone JS file in the example directory, `index.js`, basically relays the\\ninclusion to the module via the `bindings` package. The aforementioned file can\\ncontain additional content at the developer\\'s discretion.\\n\\nThe module produced by **DDS.js** will contain *two* namespaces within it. The\\nfirst namespace, called `DDS`, will contain the standard DDS calls and\\ndefinitions (such as QoS structures). The only use of this namespace in the\\nprevious code appears when creating the DDS Domain Participant with\\n`DDS.createDomainParticipant()`. The call is required to specify the DDS domain\\nID as the first argument. The second argument is optional and, if specified,\\nmust be an instance of the `DDS.DomainParticipantQos`. The recommended way to\\nacquire an instance of this structure is to call\\n`DDS.getDefaultParticipantQos()`. The fields in the returned object reflect\\nthose of the QoS structure defined in standard DDS.\\n\\nFrom the participant spawn several of the standard objects. In this simple\\nexample, the participant is used to create a `Subscriber` instance via the\\n`createSubscriber()` method in the returned participant object. The participant\\nobject also creates the topic, but creation of IDL-specific topics requires a\\ndiscussion of the other produced namespace.\\n\\nThe second namespace in the module will take its name directly from the\\ntop-level IDL module name in the provided source IDL file. In this case, the\\ntop-level module is called `HostMonitor`, thus the produced namespace uses the\\nsame name. This module contains definitions for all of the data structures and\\ntopics defined in the IDL file. In this case, only one structure/topic called\\n`OverallInformation` is defined. In order to create a topic instance, a helper\\nobject that follows the pattern `<TopicName>Topic` is created for every topic\\nworthy data structure found in the IDL file. For `OverallInformation`, thus, the\\nname of the helper object is `OverallInformationTopic`. Passing this helper to\\nthe DDS participant\\'s `createTopic()` call takes care of both topic creation and\\ntype registration. The default type name is used when registering the type\\n(by specifying `nullptr` in the underlying `register_type()` C++ call).\\n\\nThe sample code should run, attempting to `take()` samples every second, and\\nprinting whatever it received. At the time of this writing, **DDS.js** does not\\nsupport event-driven sample ingest, such as those that could be defined via\\neither a `WaitSet` or via callbacks. The code should stop upon receiving a\\n`SIGINT` event, usually via `Ctrl+C`.\\n\\n# Bindings Reference\\n\\nThe following table illustrates the API bindings available as of this writing to\\nJavaScript developers, and their proper analogue in the standard DDS C++\\nbindings. Note that only a small fraction of standard binding API calls area\\ncurrently avaialable in this JavaScript implementation. Any calls not scoped to\\na class in the JavaScript column reside directly in the `DDS` namespace. All\\nsymbols in the C++ column reside within the `DDS::` namespace. As a general\\nrule, any C++ calls that use *snake case* to define their symbol names were\\ntransformed to *camel case* in order to better abide by JavaScript conventions.\\n\\n| JS API call  | Equivalent C++ call  |\\n| ------------ | -------------------- |\\n| `createDomainParticipant()`  | `DomainParticipantFactory::create_domain_participant()`  |\\n| `DomainParticipant.enable()`  | `DomainParticipant::enable()`  |\\n| `DomainParticipant.createPublisher()`  | `DomainParticipant::create_publisher()`  |\\n| `DomainParticipant.createSubscriber()`  | `DomainParticipant::create_subscriber()`  |\\n| `DomainParticipant.createTopic()`  | `DomainParticipant::create_topic()`  |\\n| `DomainParticipant.getDiscoveredParticipants()`  | `DomainParticipant::get_discovered_participants()`  |\\n| `DomainParticipant.getDiscoveredParticipantData()`  | `DomainParticipant::get_discovered_participant_data()`  |\\n| `DomainParticipant.deleteContainedEntities()`  | `DomainParticipant::delete_contained_entities()`  |\\n| `DomainParticipant.addTransport()`  | `DomainParticipant::add_transport()`<sup>*</sup>  |\\n| `Subscriber.createDataReader()`  | `Subscriber::create_datareader()`  |\\n| `Subscriber.getDefaultDataReaderQos()`  | `Subsriber::get_default_datareader_qos()`  |\\n| `Publisher.createDataWriter()`  | `Publisher::create_datawriter()`  |\\n| `Publisher.getDefaultDataWriterQos()`  | `Publisher::get_default_datawriter_qos()`  |\\n| `DataReader.take()`  | `DataReader::take()`  |\\n| `DataReader.getStatusChanges()`  | `DataReader::get_status_changes()`  |\\n| `DataReader.getLivelinessChangedStatus()`  | `DataReader::get_liveliness_changed_status()`  |\\n| `DataReader.getSubscriptionMatchedStatus()`  | `DataReader::get_subscription_matched_status()`  |\\n| `DataReader.getSampleLostStatus()`  | `DataReader::get_sample_lost_status()`  |\\n| `DataReader.getRequestedIncompatibleQosStatus()`  | `DataReader::get_requested_incompatible_qos_status()`  |\\n| `DataReader.getSampleRejectedStatus()`  | `DataReader::get_sample_rejected_status()`  |\\n| `DataReader.getMatchedPublications()`  | `DataReader::get_matched_publications()`  |\\n| `DataReader.getMatchedPublicationData()`  | `DataReader::get_matched_publication_data()`  |\\n| `DataWriter.write()`  | `DataWriter::write()`  |\\n| `DataWriter.getStatusChanges()`   | `DataWriter::get_status_changes()`  |\\n| `DataWriter.getMatchedSubscriptions()`  | `DataWriter::get_matched_subscriptions()`  |\\n| `DataWriter.getMatchedSubscriptionData()`  | `DataWriter::get_matched_subscription_data()`  |\\n| `DataWriter.registerInstance()`  | `DataWriter::register_instance()`  |\\n| `DataWriter.unregisterInstance()`  | `DataWriter::unregister_instance()`  |\\n| `DataWriter.dispose()`  | `DataWriter::dispose()`  |\\n\\n<sup>*</sup> Denotes a feature only available with **CoreDX**.\\n\\nAs far as the IDL productions fed through to **DDS.js**, no name alterations are\\ndone. The data types specified in the productions are mapped as follows:\\n\\n| IDL Type(s)  | Mapped in JS As  |\\n| ------------ | ---------------- |\\n| `struct`  | `Object`  |\\n| `long`, `short`, `octet`, `float`, `double`  | `Number`  |\\n| `string` (bounded and unbounded)  | `String`<sup>1</sup>  |\\n| `sequence` (bounded and unbounded)  | `Array`<sup>2</sup>  |\\n\\n<sup>1</sup> Any bounds specified in the IDL are not currently enforced in\\nJavaScript.\\n\\n<sup>2</sup> Only element homogeneity specified in the IDL is enforced in\\nJavaScript, and the enforcement only manifests upon calls to the **DDS.js** API.\\n\\nNamespaces found in the IDL file(s) processed are turned into Node.js modules,\\nobserving any hierarchy specified in the source IDL.\\n\\n# Applicable Licenses\\n\\n## ANTLR and StringTemplate\\n\\n_[The BSD License]_\\nCopyright (c) 2012 Terence Parr and Sam Harwell\\nAll rights reserved.\\n*  Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\n*  Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\\n*  Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\\n*  Neither the name of the author nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\\n'},\n",
       " {'repo': 'nasa/CF',\n",
       "  'language': 'C',\n",
       "  'readme_contents': \"# CCSDS File Delivery Protocol\\n\\nNASA core Flight System File Transfer Application\\n\\n## Description\\n\\nThe CCSDS File Delivery Protocol (CFDP) application (CF) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\nThe CF application is used for transmitting and receiving files. To transfer files using CFDP, the CF application must communicate with a CFDP compliant peer.\\n\\nCF sends and receives file information and file-data in Protocol Data Units (PDUs) that are compliant with the CFDP standard protocol defined in the CCSDS 727.0-B-4 Blue Book. The PDUs are transferred to and from the CF application via CCSDS packets on the cFE's software bus middleware.\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa\\n\"},\n",
       " {'repo': 'nasa/CFS-101',\n",
       "  'language': None,\n",
       "  'readme_contents': '# CFS-101\\r\\n\\r\\n1. Read the README.txt file for instructions on getting started.\\r\\n\\r\\nTip: \"Download\" option seems to be faster then \"Clone\" option.\\r\\n\\r\\n-------------------------------------------------------------------------------\\r\\n\\r\\nCHANGE LISTS:\\r\\n\\r\\n2019/06/18 - Correct & cleanup the Training Guide.\\r\\n             Convert VmWare VM image to VirtualBox VM image.\\r\\n\\t\\t\\t (We will be maintaining VirtualBox VM image from here on.)\\r\\n\\t\\t\\t Update instructions in README.txt file relating to running the new VM image. \\r\\n\\t\\t\\t \\r\\n2019/02/05 - Add note on extracting the VM from .zip files as reported in issue #8.\\r\\n             (Thank you for reporting it, desmfryan.)\\t\\t\\t \\r\\n2019/01/30 - Add fixes to the CFS-101 Guide as reported in issues #4 & #6.\\r\\n\\t\\t\\t (Many thanks to users, xpromache & cscase, for reporting them.)\\r\\n\\r\\n2018/10/17 - Update the training guide such that code snippets can be copy&paste.\\r\\n             (Many thanks to Kevin McCluney!)\\r\\n'},\n",
       " {'repo': 'nasa/MCMCPy',\n",
       "  'language': 'OpenEdge ABL',\n",
       "  'readme_contents': 'MCMCPy - **M**arkov **C**hain **M**onte **C**arlo **S**ampling with **Py**thon \\n==========================================================================\\nPython module for uncertainty quantification using a Markov chain Monte\\nCarlo sampler.\\n\\nMCMCPy is a wrapper around the popular PyMC package (https://github.com/pymc-devs/pymc)\\nfor Python 2.7. The purpose of the MCMCPy module is to (1) standardize the\\nformat of the input and output of the underlying PyMC code and (2) reduce the\\ninherent complexity of PyMC by pre-defining a statistical model of a commonly-used\\nform. The MCMCPy module was originally released as part of the SMCPy code\\n(https://github.com/nasa/SMCPy), but, in some cases, it is possible to isolate\\nMCMCPy and use it directly without calling SMCPy\\'s primary module.\\n \\nTo operate MCMCPy, the user supplies a computational model built in Python 2.7,\\ndefines prior distributions for each of the model parameters to be estimated, and\\nprovides data to be used for calibration. These are roughly the same steps required\\nto operate SMCPy. Markov chain Monte Carlo sampling can be conducted with ease\\nthrough instantiation of the MCMCSampler class and a call to the sample() method.\\nThe output of this process is an approximation of the parameter posterior probability\\ndistribution conditioned on the data provided.\\n\\nThis software was funded by and developed under the High Performance Computing Incubator\\n(HPCI) at NASA Langley Research Center.\\n\\n----------------------------------------------------------------------------------------------\\nNotices:\\nCopyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n \\n'},\n",
       " {'repo': 'nasa/perfutils-java',\n",
       "  'language': None,\n",
       "  'readme_contents': \"# Core Flight System : Framework : Tool : Performance Utility\\n\\nThis repository contains NASA's Performance Utility Tool (perfutils-java), which is a framework component of the Core Flight System.\\n\\nThis lab application is a ground utility to analyze performance information generated by cFS. It is intended to be located in the `tools/perfutils-java` subdirectory of a cFS Mission Tree.  The Core Flight System is bundled at https://github.com/nasa/cFS (which includes this tool as a submodule), which includes build and execution instructions.\\n\\nSee CPM\\\\ Users\\\\ Guide.docx for more information.\\n\\n## Release Notes\\n\\nperfutils-java version 1.0.0a release is pending.\\n\\n## Known issues\\n\\nThis version has known compatibility issues.  The expectation is this repo will be updated once through the release processes.  Contact the community below for more information.\\n\\n## Getting Help\\n\\nThe cFS community page http://coreflightsystem.org should be your first stop for getting help. Please post questions to http://coreflightsystem.org/questions/. There is also a forum at http://coreflightsystem.org/forums/ for more general discussions.\\n\\nOfficial cFS page: http://cfs.gsfc.nasa.gov\\n\\n\"},\n",
       " {'repo': 'nasa/multipath-tcp-tools',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# multipath-tcp-tools\\n\\n## A collection tools for analysis and configuration of Multipath Transmission Control Protocol (MPTCP)\\n\\n### network-traffic-analysis-tools\\n---\\nThe network-traffic-analysis-tools directory contains a collection of applications written in C to help both *analyze* and *visualize* MPTCP packet traces.\\n\\n### mptcp-over-ppp-links\\n---\\nThe mptcp-over-ppp-links directory contains the scripts and files needed to support the type of MPTCP over PPP tests used in this experiment.\\n\\nThis resulting system leverages MPTCP to provide long-lived, responsive TCP connections that would have previously stalled or timed out using MLPPP.  The system as a whole provides better fairness, stability, and responsiveness, allowing multiple data flows to share the available resources equally and automatically.  It provides a more efficient and reliable communication channel.  This can be done without impacting the scientific payloads directly, changing only the ground station and aircraft gateway.\\n\\nPackages of these files are available in the downloads/releases section.  However, if you need the source or are curious as to which files are modified, this is the place to look.\\n\\n### multipath-udp-proxy\\n--- \\nThe multipath-udp-proxy directory contains a python script used to proxy a tun interface to the available PPP links, effectively creating a \"Multipath-UDP\" that adapts to link outages.\\n\\n### mptcp-kernel-patches\\n---\\nThe mptcp-kernel-patches directory contains the patches made to the 3.18 and 4.4 MPTCP kernels used in these builds.\\n'},\n",
       " {'repo': 'nasa/ARC-SGE-interns-19',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': '\\n# Summary of CASA Spring Internship Project 2019\\nBy Andrew Li and Roshni Biswas\\\\\\nMentor: Dr. Christopher Potter\\n'},\n",
       " {'repo': 'nasa/RHEAS',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# RHEAS  [![Build Status](https://travis-ci.org/nasa/RHEAS.svg?branch=master)](https://travis-ci.org/nasa/RHEAS)\\n\\n[![Join the chat at https://gitter.im/nasa/RHEAS](https://badges.gitter.im/nasa/RHEAS.svg)](https://gitter.im/nasa/RHEAS?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\\n\\nThe Regional Hydrologic Extremes Assessment System (RHEAS) is a modular hydrologic modeling framework that has been developed at JPL. At the core of the system lies a hydrologic model that can be run both in nowcasting (i.e. estimation of the current time period) and forecasting (i.e. estimation for future time periods) modes. The nowcasting simulations are constrained by assimilating a suite of Earth Science satellite observations, resulting in optimal estimates of directly and indirectly observed water variables. The latter nowcast estimates can then be used to initialize the 30- to 180-day forecasts. Datasets are automatically fetched from various sources (OpenDAP, FTP etc.) and ingested in a spatially-enabled PostGIS database, allowing for easy dissemination of maps and data.\\n\\nDocumentation for RHEAS can be found at [Read the Docs](http://rheas.readthedocs.org/en/latest/).\\n\\nA peer-reviewed journal article on RHEAS is also [available](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0176506).\\n'},\n",
       " {'repo': 'nasa/georef_imageregistration',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'The Image Registration module is intended to help automate the process of registering\\nimages to the correct location on the ground.  It consists of two main components:\\nA - Fetching RGB satellite imagery of Earth to use for comparison.\\nB - Searching for the correct image registration parameters.\\n\\n\\nStep A is performed using Google\\'s Earth Engine platform.\\nTo install Earth Engine, follow these steps:\\n1 - Apply for a beta signup: https://docs.google.com/forms/d/17-LSoJQcBUGIwfplrBFLv0ULYhOahHJs2MwRF2XkrcM/viewform\\n  - Your application should be accepted quickly.\\n2 - Follow the steps for Python installation of Earth Engine: https://developers.google.com/earth-engine/python_install\\n3 - Make sure the path in imageregistration/ImageFetcher/ee_authenticate.py is pointed to the credentials file created in step 2.\\n\\nStep B is performed using a C++ program relying on OpenCV 3.0\\nTo install, follow these steps:\\n1 - Build OpenCV 3.0 with the contributor modules package.\\n\\t(follow tutorial here: http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/)\\n  - Use the below cmake (assumes opencv_contrib is in /usr/local/lib/opencv_contrib)\\n\\tsudo cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D OPENCV_EXTRA_MODULES_PATH=/usr/local/lib/opencv_contrib/modules -D BUILD_EXAMPLES=ON ..\\n\\tAnd make sure to do \"make install\" as well as \"make\"!!!\\n\\t\\t  \\n  - Building OpenCV may not go smoothly, so we will have to update this file with more specific instructions\\n    as we go.\\n  - Sample install instructions here may be useful: http://www.pyimagesearch.com/2015/06/22/install-opencv-3-0-and-python-2-7-on-ubuntu/\\n  \\n2 - Build the ImageRegistration C++ code.\\n  - I used the following CMake line to do this:\\n    cmake ..  -DOPENCV_INSTALL_DIR=/home/smcmich1/programs/opencv_install/\\n    (for grace it\\'s: cmake ..  -DOPENCV_INSTALL_DIR=/usr/local/)\\n  - Once this is built, everything should be ready to use.\\n\\n==============================\\nGrace\\'s notes:\\nTwo step process A&B\\nA: center point + zoom level are used to fetch new images\\nB: Tries to align the images together and returns a transformation matrix (image to lat lon)\\n\\nAlso gives a three part confidence estimate (I should take the \"high\" and ignore the others)\\n\\nTakes optional fields (referencedImagePath and referencedGeoTransform), which take the similar image and the transform. If you do this step, it will use the given image as a reference image and will skip the first step. \\nThis is good when there are sequence of images of the same area.\\n\\nregisterImage.py is the main function.\\n\\n==============================\\n\\nOffline processing TODO:\\n\\n---- How to connect to georef on the stage machine -----\\n\\nssh -L 1234:127.0.0.1:443  geocam-stage.jsc.nasa.gov\\nIn web browser: https://localhost:1234 (add exception if needed)\\n\\n\\nDB change requests:\\n    - Make issMRF a unique value in all tables where we use it.\\n    - Do something with the registration info in the overlays table.\\n        - Move simple numbers out of the extras field (size, rotation, focal length, center, nadir, bounds, etc).\\n        - Add a \"writtenToFile\" field.\\n        - By mirroring our existing registration table fields we can make searches easier using joins.\\n\\n- TODO: Images which are in our DB but not in the input DB file are not supported.\\n\\n- Add UNIQUE flag if not already done.\\n\\n- Disable GUI writing of gtiff files.  ---> Go ahead and start running the tools, ignoring this step!\\n- Verify offline tools output folder.\\n- Set up automatic offline tools.\\n    - How should they be running?\\n- Verify that offline tools are running.\\n- Test out the GUI, make sure everything works properly.\\n- More testing\\n\\n- Double check why our RMS fit error is so high.\\n- Switch from prints to logging\\n\\n\\nVerify local is working:\\niss027 - 005051, 50\\n\\nNo local:\\nMISSION\\t-->\\tTOTAL\\tNONE\\tLOW\\tHIGH\\tHIGH_FRACTION\\nISS027\\t-->\\t1710\\t638 \\t499\\t573 \\t0.34\\nLocal: \\nMISSION\\t-->\\tTOTAL\\tNONE\\tLOW\\tHIGH\\tHIGH_FRACTION\\nISS027\\t-->\\t1710\\t990\\t    15\\t705\\t    0.41\\n\\n\\n\\nExamples for demo:\\n43-122588 = -22.2, -67.8  --> Dist = 4000m\\n44-903    = -15.5, 123.2  --> Dist = 3600m\\n44-868    = -21.7, 115.1  --> Dist = 5600m\\n44-1998   =  34.7,  10.8  --> Dist = 6600m\\n\\n\\n\\n--> Final idealized design\\n    = Multiple asynchronous processes that all feed into the same SQL database.\\n        - Image detector = Add new images to the DB.\\n        - Metadata fetcher = Fetch ISS metadata.\\n        - Geosense fetcher = Add Geosense metadata.\\n        - Image matcher = Perform image alignment, generate GCP list.\\n        - Output generator = Use GCPs to generate final output image.\\n        --> The georef GUI will only edit SQL rows and flag the output generator.\\n\\n-> Use a common info fetching function for frame, similar to what we have now.\\n    - Each tool can access just the info it needs.\\n\\n\\n- Switch from SQLite to MYSQL\\n    - Waiting on new DB to stabilize\\n\\n- Port functionality from command line tool to seperate command line async processes.\\n\\n- Expand/improve DB wrapper classes so all tools and GUI can use the wrapper.\\n    - Started new MySQL wrapper\\n\\n\\n\\n\\n- File describing the input data system?\\n\\n- Handle overwrite options better, including re-fetch\\n\\n- Tune the uncertainty constants\\n\\n- IP registration improvements:    \\n    - Small amounts of clouds cause all the match image IP to fall on them!\\n      What can we do to alleviate this effect?\\n    - Snow covered images have a similar effect.\\n    - Large cities and some other detailed regions fail because the IP are scattered around the entire image\\n      and we don\\'t have enough density to find the small ISS image without a massive run time.\\n      - Could just use a huge amount of IP...\\n      - Try a first-pass registration at a lower resolution?\\n      - Try a second-pass registration based on an initial low-confidence registration?\\n\\n    - Check image saturation and re-process if too much white?  \\n  \\n    - Performance is MUCH better on images which have similar lighting/color\\n      conditions.  Could possibly get significant improvements by improving\\n      our image preprocessing steps.\\n  \\n\\n- Double check batch local matching\\n- Verify that we can process from each mission\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
       " {'repo': 'nasa/TrickFMI',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# TrickFMI: A Functional Mockup Interface (FMI) Standard Implementation for Trick Base Models and Simulations\\n\\n## Brief Abstract:\\n\\nThis software supports FMI based model exchange with Trick based simulations.\\n\\nSimulation is a key technology used in the conception, design, development and operation of human space systems.  As these space system become more and more complex, so do the models used in the simulations of these systems.  This software provides a practical method for exchanging models between NASA, its contractors and its international partners.\\n\\nThe Functional Mockup Interface (FMI) standard was developed in partnership with governmental, academic and commercial entities in the European Union.  This standard is used to support the exchange of component models for complex system simulations throughout Europe and the United States.  Trick simulations are used all across NASA for simulations that support human spaceflight activities.  However, until now, there were no means to use FMI based models in a Trick based simulation or a method for providing Trick based models that were FMI compliant.  This software provides implementation software to do both.\\n\\nThere are two principal components to the software:\\n- A C based software implementation for wrapping Trick based C models that provide an FMI compliant interface;\\n- A collection of C++ classes that can be used in a Trick based simulation to use an FMI compliant model (FMU).\\n\\n## Copyright:\\nCopyright 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n\\n---\\n\\nTrickFMI is released under the NASA Open Source Agreement Version 1.3 [license](https://github.com/nasa/trickfmi/blob/master/LICENSE).\\n'},\n",
       " {'repo': 'nasa/Kepler-PyKE',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# PyKE\\n***A suite of Python/PyRAF tools to analyze Kepler data.***\\n\\nFor more information and documentation,\\nvisit http://keplerscience.arc.nasa.gov/software.html#pyke\\n\\n## Installation with PyRAF\\n\\nThe easiest way to install PyKE is through the astroconda-iraf channel:\\nhttp://astroconda.readthedocs.io/en/latest/installation.html#legacy-software-stack-with-iraf\\n\\nAfter that, run the following commands on a terminal of your preference:\\n\\n1. ``mkiraf``\\n2. ``pyraf``\\n3. ``kepler``\\n\\n\\n## Acknowledgement\\nIf you find this code useful in your research, please consider [citing](http://adsabs.harvard.edu/abs/2012ascl.soft08004S):\\nTitle:\\tPyKE: Reduction and analysis of Kepler Simple Aperture Photometry data\\n```\\nAuthors:          Still, Martin; Barclay, Tom\\nPublication:      Astrophysics Source Code Library, record ascl:1208.004\\nPublication Date: 08/2012\\n```\\n\\n*This package was mostly developed by Tom Barclay ([@mrtommyb](http://www.github.com/mrtommyb)) and Martin Still.*\\n\\n## Support\\nUsers are welcome to open [issues](https://github.com/KeplerGO/PyKE/issues) involving any aspects of this software.\\n\\nFeel free to contact us also through: keplergo@mail.arc.nasa.gov\\n'},\n",
       " {'repo': 'nasa/cratous',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# CRATOUS\\nCRoss-Application Translator for Operational Unmanned Systems\\n(CRATOUS) is a software bridge that enables the communication between\\n[UxAS](https://github.com/afrl-rq/OpenUxAS) and\\n[ICAROUS](https://github.com/nasa/icarous).\\n\\n### License and Copyright Notice\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSE`](LICENSE).\\n\\n<pre>\\n\\nNotices:\\n\\nCopyright 2018 United States Government as represented by the Administrator\\nof the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nDisclaimers:\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY\\nOF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT\\nLIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO\\nSPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\\nPARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE\\nSUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF\\nPROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN\\nANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR\\nRECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR\\nANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER,\\nGOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING\\nTHIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES\\nIT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST\\nTHE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS\\nANY PRIOR RECIPIENT. IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN\\nANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S\\nUSE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT\\'S SOLE REMEDY FOR\\nANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS\\nAGREEMENT.\\n\\n</pre>\\n\\n---\\n\\n## Installation Guide\\n\\n### Requirement installation\\n\\n(This was tested on an Ubuntu 32 bit system)\\n\\n1. Install [UxAS](https://github.com/afrl-rq/OpenUxAS) and verify it is running correctly.\\n2. Switch to the ```ICAROUS_integration``` branch of UxAS and recompile UxAS.\\n3. Install [ICAROUS](https://github.com/nasa/icarous) and verify it is running correctly.\\n4. Switch ICAROUS branch to `cratous`:\\n```\\ngit checkout cratous\\n```\\n\\n\\n### CRATOUS Installation\\n1. Copy the cratous repository into the `cFS/apps` folder relative to your ICAROUS installation with:\\n\\n```\\ngit clone http://github.com/nasa/cratous cFS/apps/cratous\\n```\\n\\n2. Rebuild icarous from the top folder (icarous):\\n\\n```\\nmkdir build\\ncd build\\ncmake -DCRATOUS=ON ..\\nmake -j9 cpu1-install\\n```\\n\\n3. At this point it should be good to run. To test, run the examples found in `OpenUxAS/example/07_.../`. The file `runDemo.sh` in each of these should run the example without any other input.\\nNote: You may need to set the value in `/proc/sys/fs/mqueue/max_msg` to `1000` so that ICAROUS can run without sudo.\\n'},\n",
       " {'repo': 'nasa/FPRoCK',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'FPRoCK\\n------\\n\\n*FPRoCK v-0.1* (February 2019)\\n\\n*FPRoCK* (Floating-Point and Real ChecKer) is a prototype tool that decides the satisfiability of a mixed real and floating-point formula.\\nTo this aim, it transforms a mixed formula into an equi-satisfiable one over the reals.\\nThis transformed formula is then input to different off-the-shelf SMT solvers for resolution.\\nFPRoCK returns an assignment satisfying the formula if it exists; otherwise, it returns `unsat` indicating that the set is unsatisfiable.\\n\\nThe input to *FPRoCK* is a text file containing the following information:\\n\\n* the list of floating-point variables with their precision (single or double) and *optional* initial range, for example:\\n\\n```\\nFloat: X[double] = [-2000.0;2000.0],\\n       Y[double] = [-2000.0;2000.0]\\n```\\n\\n* the list of real variables with their *optional* initial range, for example:\\n\\n```\\nReal: Real_X[real] = [-2000.0;2000.0],\\n      Real_Y[real] = [-2000.0;2000.0],\\n      Err_X[real],\\n      Err_Y[real]\\n```\\n\\n* the set of constraints/formulas to check for satisfiability, for example:\\n\\n```\\nErr_X >= abs(Real_X - X)\\nErr_Y >= abs(Real_Y - Y)\\nErr_X = 1.1368683772161603e-13\\nErr_Y = 1.1368683772161603e-13\\n```\\n\\nIn its current version, *FPRoCK* supports conjuntions (and), negations (not), equalities and disequalities (=, !=, <, >, <=, >=)  basic arithmetic operators over the reals (+,-,\\\\*,/) and over the floats (+FP,-FP,\\\\*FP,/FP), and the absolute value operator (abs).\\nConstant integer values are required to be written with a .0 at the end, e.g. 2.0. Scientific format is also accepted, e.g. 1.1368683772161603e-13.\\n\\nPrerequisites\\n-------------\\n\\nTo install *FPRoCK* you will need to install:\\n\\n* Python 2.7 and pip\\n    * install the `bigfloat` library with the command `pip install bigfloat`\\n* MathSAT (http://mathsat.fbk.eu/)\\n* Z3 (https://github.com/Z3Prover/z3)\\n* CVC4 (http://cvc4.cs.stanford.edu/web/)\\n* Colibri (https://soprano-project.fr/download_colibri.html)\\n\\nThe SMT solvers and the python interpreter should be available in the `PATH` environment variable.\\n\\n\\nInstallation and use\\n--------------------\\n\\nTo install *FPRoCK* the repository has to be copied to a desired location. For example, with `git` we can clone the repository in the `/fprock` folder:\\n\\n```\\ngit clone https://github.com/nasa/FPRoCK.git /fprock\\n```\\n\\nTo execute *FPRoCK* the `executor.py` script has to be called from the root folder of the repository:\\n\\n```\\ncd /fprock\\npython executor.py\\n```\\n\\nThe last command will display some usage help.\\n\\nFor example, to check the benchmark `eps-line/ceb_5.fpr` (which is `unsat`) you have to specify a timeout, a rounding mode and an input file in sequence, as in:\\n\\n```\\npython executor.py 10 rnd-to-zero  benchmarks/eps_line/ceb_5.fpr\\n```\\n\\n*FPRoCK* will create a directory `results` with one file of results for each combination of SMT solver and search strategy used.\\nThe result of *FPRoCK* is the content of those files.\\nFor example, to check the result of the previous example you can execute a command like:\\n\\n```\\ncat results/*\\n```\\n\\nAnd you would get an output like the following telling you that the formula is `unsat`:\\n\\n```\\nunsat\\n(error \"Cannot get the current model unless immediately preceded by SAT/INVALID or UNKNOWN response.\")\\nunsat\\n(error \"model generation not enabled\")\\nunsat\\n(error \"model generation not enabled\")\\nunsat\\n(error \"model generation not enabled\")\\nunsat\\n(error \"line 13029 column 10: model is not available\")\\nunsat\\n(error \"line 4344 column 10: model is not available\")\\nunsat\\n(error \"line 6580 column 10: model is not available\")\\n```\\n\\nSome result files maybe empty because the SMT timed out. The different results should all agree. If two different results `sat`/`unsat` are found for a problem it should be reported since it would imply an error in *FPRoCK* or in the underlying SMT solvers used.\\n\\n\\nUsing Docker\\n------------\\n\\nA `Dockerfile` is provided in the `/fprock/Docker/` folder in order to build a *Docker* image that can be use to install and run automatically *FPRoCK*.\\nThe [Docker](https://www.docker.com/) tool has to be installed in order to use this approach.\\nThe following commands build and run a *Docker* image with *FPRoCK*:\\n\\n```\\ncd /fprock/Docker\\ndocker build -t fprock .\\ndocker run -it fprock /bin/bash\\n```\\n\\nWhen running an *FPRoCK* image the tool is executed identically:\\n\\n```\\ncd  /fprock\\npython executor.py 10 rnd-to-zero  benchmarks/eps_line/ceb_5.fpr\\n```\\n\\n\\nContact information\\n-------------------\\n\\n[C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam)\\n\\n### License and Copyright Notice\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES).\\n\\n<pre>\\n\\nNotices:\\n\\nCopyright 2019 United States Government as represented by the\\n   Administrator of the National Aeronautics and Space Administration.\\n   All Rights Reserved.\\n\\nDisclaimers:\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY\\nWARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY,\\nINCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE\\nWILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF\\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM\\nINFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO\\nTHE SUBJECT SOFTWARE.  THIS AGREEMENT DOES NOT, IN ANY MANNER,\\nCONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT\\nOF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY\\nOTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.\\nFURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES\\nREGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\nAND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS\\nAGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND\\nSUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF\\nTHE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES,\\nEXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM\\nPRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT\\nSOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE\\nREMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL\\nTERMINATION OF THIS AGREEMENT.\\n\\n</pre>\\n\\n'},\n",
       " {'repo': 'nasa/kepler-robovetter',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# The Kepler DR25 Robovetter\\n\\nThe DR25 Kepler Robovetter is a robotic decision-making code that dispositions each Threshold Crossing Event (TCE) from the Kepler pipeline into Planet Candidates (PCs) and False Positives (FPs). The Robovetter also provides four major flags to designate each FP TCE as Not Transit-Like (NTL), a Stellar Eclipse (SS), a Centroid Offset (CO), and/or an Ephemeris Match (EM). It also produces a score ranging from 0.0 to 1.0 that indicates the Robovetter\\'s disposition confidence, where 1.0 indicates strong confidence in PC, and 0.0 indicates strong confidence in FP. Finally, the Robovetter provides comments in a text string that indicate the specific tests each FP TCE fails and provides supplemental information on all TCEs as necessary.\\n\\nMore information can be found at: https://exoplanetarchive.ipac.caltech.edu/docs/PurposeOfKOITable.html#q1-q17dr25\\n\\n\\n## Compiling and Running the Code\\n\\nThe DR25 Robovetter code is provided, along with the necessary input files that contain every metric the Robovetter uses to make its decisions, and the resultant output files, so that users may validate their implementation. (Note that input files are compressed using tar/gzip and and should be uncompressed via \"tar xvzf FILENAME\" before being used.)\\n\\n\\n### Prerequisites\\n\\nThe code is written in C++11 and only requires the standard C++ library (specifically, the required libraries are iomanip, iostream, fstream, vector, sstream and random). It has been tested on Linux and Mac to work with:\\n  - The g++ complier (Minimum version 4.7.2 tested - earlier versions unlikely to work.)\\n  - The clang++ compiler (Versions 3.4 and 3.5 tested. Version 3.3 may work, but untested. Earlier than 3.3 will not work.)\\n  - The Intel icpc compiler (Version 17 tested. Earlier versions as far back as 11 very likely to work, but untested.)\\n\\nNote that Danley Hsu was able to compile and run the code on Windows, and he has graciously provided detailed instructions in the file https://github.com/nasa/kepler-robovetter/blob/master/windows-compile-instructions. These instructions have not been verified to work on another Windows machine, but we provide them in case anyone using a Windows machine finds them helpful.\\n\\n\\n### Compiling\\n\\nTo compile the code, use your available C++ compiler with the -std=c++11 option, and a recommended O2 level of optimization. For example, use one of the commands below based on your available compiler:\\n\\n```\\ng++     -std=c++11 -O2 -o robovet Kepler-RoboVetter-DR25.cpp\\n```\\n```\\nclang++ -std=c++11 -O2 -o robovet Kepler-RoboVetter-DR25.cpp\\n```\\n```\\nicpc    -std=c++11 -O2 -o robovet Kepler-RoboVetter-DR25.cpp\\n```\\n\\n### Running the Code\\n\\nRun as \"./robovet INPUTFILE  OUTFILE  NMC\"\\n\\nINPUTFILE is the name of the input file for a particular run. OBS is the Observed run. INV is the Inverted run. SCR1 is the Scrambled (ordering #1) run. SCR2 is the Scrambled (ordering #2) run. SCR3 is the Scrambled (ordering #3) run. INJ1 is the Injected group 1 run (on-target injected planets.) INJ2 is the Injected group 2 run (off-target planets.) INJ3 is the Injected group 3 run (Eclipsing Binaries). For more details on each run, see https://exoplanetarchive.ipac.caltech.edu/docs/KSCI-19114-001.pdf\\n\\nOUTFILE is the name of the output file, as desired by the user.\\n\\nNMC is the number of Monte Carlo iterations desired for computing the scores. A value of at least 100 is recommended for useable scores, and preferably at least 1,000. A value of 10,000 was used for calculating the scores archived at NExScI and the output files provided here.\\n\\nFor example:\\n\\n```\\n./robovet kplr_dr25_obs_robovetter_input.txt kplr_dr25_obs_robovetter_output.txt 10000\\n```\\n\\nwill run the Robovetter on the OBS data, and perform 10,000 Monte Carlo runs to compute the score. The resulting output file should exactly match that provided in this GitHub repository. To replicate the other data sets (INV, SCR1, SCR2, SCR3, INJ1, INJ2, INJ3) one would use the following commands:\\n\\n```\\n./robovet kplr_dr25_inv_robovetter_input.txt kplr_dr25_inv_robovetter_output.txt 10000\\n```\\n```\\n./robovet kplr_dr25_scr1_robovetter_input.txt kplr_dr25_scr1_robovetter_output.txt 10000\\n```\\n```\\n./robovet kplr_dr25_scr2_robovetter_input.txt kplr_dr25_scr2_robovetter_output.txt 10000\\n```\\n```\\n./robovet kplr_dr25_scr3_robovetter_input.txt kplr_dr25_scr3_robovetter_output.txt 10000\\n```\\n```\\n./robovet kplr_dr25_inj1_robovetter_input.txt kplr_dr25_inj1_robovetter_output.txt 10000\\n```\\n```\\n./robovet kplr_dr25_inj2_robovetter_input.txt kplr_dr25_inj2_robovetter_output.txt 10000\\n```\\n```\\n./robovet kplr_dr25_inj3_robovetter_input.txt kplr_dr25_inj3_robovetter_output.txt 10000\\n```\\n\\n\\n### Terminal output\\n\\nIn addition to the output file that is created, the code writes to the command line the currently executing task (e.g., reading in the data or vetting the TCEs) and the current Monte-Carlo iteration number so that users can monitor progress.\\n\\n\\n## Acknowledgments\\n\\nPlease reference Thompson et al. 2018, ApJS, 235, 38 (http://adsabs.harvard.edu/abs/2018ApJS..235...38T) if you make use of this code or the files provided.\\n\\n\\n## Notices\\n\\nCopyright © 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNASA acknowledges the SETI Institute’s primary role in authoring and producing the Kepler Robovetter under Cooperative Agreement Number NNX13AD01A\\n\\n\\n## Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT. IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW. RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/DualPol',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'DualPol README\\n--------------\\nThis is an object-oriented Python module that facilitates precipitation retrievals (e.g., hydrometeor type, precipitation rate, precipitation mass, particle size distribution information) from polarimetric radar data. It leverages existing open source radar software packages to perform all-in-one QC and retrievals that are then easily visualized or saved using existing software.\\n\\nDualPol Installation\\n--------------------\\nDualPol works under Python 2.7  and 3.4-3.6 on most Mac/Linux setups. Windows installation and other Python versions are currently untested.\\n\\nIn the main source directory:  \\n`python setup.py install`\\n\\nThe following dependencies need to be installed first:\\n\\n- A robust version of Python 2.7  or 3.4-3.6 w/ most standard scientific packages (e.g., `numpy`, `matplotlib`, `pandas`, etc.) - Get one for free [here.](https://store.continuum.io/cshop/anaconda/)\\n- [The Python Atmospheric Radiation Measurement (ARM) Radar Toolkit (Py-ART)](https://github.com/ARM-DOE/pyart)\\n- [CSU_RadarTools](https://github.com/CSU-Radarmet/CSU_RadarTools)\\n- [SkewT](https://pypi.python.org/pypi/SkewT) - a Python 3 version can be found [here.](https://github.com/tjlang/SkewT)\\n\\nSpecific import calls in the DualPol source code:\\n\\n```\\nfrom __future__ import print_function\\nimport numpy as np\\nimport warnings\\nimport time\\nimport pyart\\nimport matplotlib.colors as colors\\nfrom pyart.io.common import radar_coords_to_cart\\nfrom skewt import SkewT\\nfrom csu_radartools import (csu_fhc, csu_liquid_ice_mass, csu_blended_rain,\\n                            csu_dsd, csu_kdp)\\n```\\n\\nUsing DualPol\\n-------------\\nTo access everything:\\n```\\nimport dualpol\\n```\\nA demonstration notebook is in the notebooks directory.\\n\\nRelease info:\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2585820.svg)](https://doi.org/10.5281/zenodo.2585820)\\n'},\n",
       " {'repo': 'nasa/SCA',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Stored Command Absolute \\n\\nNASA Core Flight System Stored Command Absolute (SCA) Application\\n\\n## Description\\n\\nThe Stored Command Absolute (SCA) application is a core Flight System (cFS) application that is a plug in to \\nthe Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed \\nby NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems \\nand instruments, but can be used on other embedded systems. More information on the cFS can be found at \\n[http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\nThe SCA application allows a system to be commanded 24 hours a day using sequences of absolute time tagged\\nsequences.  Each command has a time tag associated with it, permitting the command to be released for distribution at\\npredetermined times.  SCA allows 5 absolute time tagged command sequences to be run concurrently.  Unlike the Stored Command\\n(SC) application, SCA relies on a text-based format to specify command sequences.  This application is a prototype.\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa'},\n",
       " {'repo': 'nasa/lager',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': 'Lager\\n-----------\\n\\nLight-weight Accumulator Gathering Efficiently in Real-time   \\n   \\nLager is a light-weight logging system.\\n   \\nMaster   \\n[![pipeline status](https://js-er-code.jsc.nasa.gov/lager/lager/badges/master/pipeline.svg)](https://js-er-code.jsc.nasa.gov/lager/lager/commits/master) [![coverage report](https://js-er-code.jsc.nasa.gov/lager/lager/badges/master/coverage.svg)](https://artifacts.jsc.nasa.gov/artifactory/doc/lager/lager/master/coverage/index.html)   \\n   \\nDevelop   \\n[![pipeline status](https://js-er-code.jsc.nasa.gov/lager/lager/badges/develop/pipeline.svg)](https://js-er-code.jsc.nasa.gov/lager/lager/commits/develop) [![coverage report](https://js-er-code.jsc.nasa.gov/lager/lager/badges/develop/coverage.svg)](https://artifacts.jsc.nasa.gov/artifactory/doc/lager/lager/develop/coverage/index.html)   \\n\\n### Documentation\\n\\n* [Design Document](doc/design.md)\\n\\n### Dependencies Linux\\n\\nPackage dependencies   \\n\\n`sudo apt install -y git cmake build-essential uuid-dev libxerces-c-dev cppcheck`   \\n\\n[CMake](https://cmake.org) >= 3.1.3\\n[XercesC](https://xerces.apache.org/xerces-c/) >= 3.1.1   \\n[ZeroMQ](https://github.com/zeromq/libzmq) >= 4.2.2 required.  To install from source:   \\n\\n```\\ncd\\ngit clone https://github.com/zeromq/libzmq\\ncd libzmq\\ngit checkout v4.2.2\\nmkdir build\\ncd build\\ncmake ..\\nmake\\nsudo make install\\ncd\\ngit clone https://github.com/zeromq/cppzmq\\ncd cppzmq\\ngit checkout v4.2.2\\nsudo cp zmq.hpp /usr/local/include/\\n```\\n\\n### Building Linux\\n\\n1. `mkdir build`   \\n2. `cd build`   \\n3. `cmake ..`   \\n4. `make`   \\n\\n### Dependencies Windows\\n\\n1. Clone [libzmq](https://github.com/zeromq/libzmq) and use build directions located in `./builds/msvc/`.   \\n2. Clone [cppzmq](https://github.com/zeromq/cppzmq) and copy the `*.hpp` files into your `libzmq/include` directory.   \\n3. Set an environment variable `ZeroMQ_ROOT_DIR` to the full path to the libzmq directory.\\n\\n### Building Windows\\n\\nReplace `Visual Studio 15 2017` with your appropriate version.   \\n   \\n1. `mkdir build`   \\n2. `cd build`   \\n3. `cmake -G \"Visual Studio 15 2017\" ..`   \\n4. `cmake --build .`   \\n\\n### Examples\\n\\nSee `src/tap_test_main.cpp`, `src/mug_test_main.cpp`, and `src/bartender_main.cpp`\\n\\n### Run examples\\n\\nOpen a terminal window and run the bartender   \\n   \\n`./test_bartender`   \\n   \\nOpen a second terminal window and run the tap   \\n   \\n`./test_tap`   \\n   \\nOpen a third terminal window and run the mug   \\n   \\n`./test_mug`  \\n'},\n",
       " {'repo': 'nasa/DERT',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '![ ](https://github.com/nasa/DERT/blob/master/dert/html/images/dert_small.png)\\n\\n# Desktop Exploration of Remote Terrain \\n\\nDesktop Exploration of Remote Terrain (DERT) is a software tool for exploring large Digital Terrain Models (DTM) in 3D. It aids in understanding topography and spatial relationships of terrain features, as well as performing simple analysis tasks relevant to the planetary science community.\\n\\nDERT was developed by the Autonomous Systems and Robotics Area of the Intelligent Systems Division at NASA Ames Research Center. It leverages techniques implemented for science planning support applications provided to a number of NASA missions including Phoenix Mars Lander (PML) and Mars Science Laboratory (MSL). \\n\\nDERT was funded by the Mars Reconnaissance Orbiter (MRO) mission and developed in collaboration with members of the MRO Context Camera (CTX) science team. DERT is licensed under the NASA Open Source Agreement (NOSA).\\n\\nSee demonstrations of DERT [here](https://github.com/nasa/DERT/wiki/Demonstrations).\\n\\n[Find out more . . .](https://github.com/nasa/DERT/wiki)\\n\\nPrebuilt releases are available on the [releases](https://github.com/nasa/DERT/releases) tab for this site. Additionally, versions that include the JPL SPICE kernels as well as some prebuilt landscapes are available [here](https://github.com/nasa/DERT/wiki/Download). These releases are built with the Java 1.8 JDK so the corresponding JRE is required to run them.\\n\\n## Contributions\\n\\nWe are not adding contributors at this time.  However, we welcome your feedback. Please let us know of any issues you find by submitting them to this site.\\n\\n## LICENSE\\n\\nDERT is a viewer for digital terrain models created from data collected during NASA missions.\\n\\nDERT is released under the NASA Open Source Agreement (NOSA) found in the “LICENSE” folder where you downloaded DERT.\\n\\nDERT includes 3rd Party software. The complete copyright notice listing for DERT is:\\n\\nCopyright © 2015 United States Government as represented by the Administrator of the National Aeronautics and Space\\nAdministration.  No copyright is claimed in the United States under Title 17, U.S.Code. All Other Rights Reserved.\\n\\nDesktop Exploration of Remote Terrain (DERT) was written with the aid of a number of free, open source\\nlibraries. These libraries and their notices are listed below. Find the complete third party license listings in the\\nseparate “DERT Third Party Licenses” pdf document found where you downloaded DERT in the LICENSE folder.\\n \\n \\n**JogAmp Ardor3D Continuation**, Copyright © 2008-2012 Ardor Labs, Inc.\\n\\n \\n**JOGL**, Copyright 2010 JogAmp Community. All rights reserved.\\n\\t \\n\\tJOGL Portions Sun Microsystems, Copyright © 2003-2009 Sun Microsystems, Inc. All Rights Reserved. \\n\\tJOGL Portions Silicon Graphics, Copyright © 1991-2000 Silicon Graphics, Inc. \\n\\tLight Weight Java Gaming Library Project (LWJGL), Copyright © 2002-2004 LWJGL Project All rights reserved. \\n\\tTile Rendering Library - Brain Paul, Copyright © 1997-2005 Brian Paul. All Rights Reserved. \\n\\tOpenKODE, EGL, OpenGL, OpenGL ES1 & ES2, Copyright © 2007-2010 The Khronos Group Inc. \\n\\tCg, Copyright © 2002, NVIDIA Corporation. \\n\\tTypecast - David Schweinsberg, Copyright © 1999-2003 The Apache Software Foundation. All rights reserved. \\n\\tPNGJ - Herman J. Gonzalez and Shawn Hartsock, Copyright © 2004 The Apache Software Foundation. All rights reserved. \\n\\tApache Harmony - Open Source Java SE, Copyright © 2006, 2010 The Apache Software Foundation.\\n\\n \\n **GlueGen**, Copyright © 2010 JogAmp Community. All rights reserved.\\n \\n\\tGlueGen Portions - Sun Microsystems, Copyright © 2003-2005 Sun Microsystems, Inc. All Rights Reserved.\\n\\n \\n**Guava**, Copyright © 2010 The Guava Authors.\\n\\n\\n**XStream**, Copyright © 2003-2006, Joe Walnes, Copyright © 2006-2009, 2011 XStream Committers. All rights reserved.\\n\\n\\n**SPICE**, Copyright © 2003, California Institute of Technology. U.S. Government sponsorship acknowledged.\\n\\n \\n**LibTIFF**, Copyright © 1988-1997 Sam Leffler, Copyright © 1991-1997 Silicon Graphics, Inc.\\n\\n \\n**PROJ.4**, Copyright © 2000, Frank Warmerdam.\\n \\n\\n### Disclaimers\\n\\n\\tNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND,\\n\\tEITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY\\n\\tTHAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF\\n\\tMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY\\n\\tWARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT\\n\\tDOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT\\n\\tDOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY\\n\\tPRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR\\n\\tANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER,\\n\\tGOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY\\n\\tSOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\n\\tWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED\\n\\tSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\n\\tRECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES,\\n\\tDEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES\\n\\tFROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE,\\n\\tRECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS\\n\\tCONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT\\n\\tPERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE,\\n\\tUNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/AprilNav',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '<img src=\"AprilNavLogo.jpg\" align=\"right\" />\\n\\n## AprilNav\\n\\n[![license](https://img.shields.io/badge/license-LGPL%202.1-blue.svg)](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html)\\n\\n## Overview\\n\\nAprilNav is a mobile indoor real-time landmark navigation system. Using printable 2D barcodes, a HD\\ncamera, and a computer, AprilNav is a low cost, scalable, and accurate system for vehicular autonomous\\nnavigation and localization. Matrices (or 2D barcodes) mounted on the ceiling of a room act as a landmark for a camera located anywhere in the same room with up to 5cm of accuracy. AprilNav has sundry of potential applications ranging from robotics education to manufacturing and warehouse vehicles and facilities.\\n\\nOur team at NASA - MSFC has adapted code (AprilTags) originally created by Edward Olson at University of Michigan and adapted by additional authors at Carnegie Mellon and MIT under the LGPL 2.1 license. AprilTags outputs the location of multiple 2D barcodes located anywhere in 3D space. AprilNav expands upon the AprilTags library estimating the pose of a camera given known coordinates of unique tags. \\n\\nAprilNav has been tested and run on MacOS 10.4, Ubuntu 12.04, and Rasbian.\\n\\n### Prerequisites\\n\\nInstall the following dependencies to run the program (The following was tested on Ubuntu 12.04 and Raspberry Pi running Raspian: \\n\\n```\\nsudo apt-get install subversion cmake libopencv-dev libeigen3-dev libv4l-dev xterm\\n```\\n\\nTo install on Mac OS using homebrew:\\n```\\nsudo port install pkgconfig opencv eigen3\\n```\\n\\n\\n### Installing\\n\\nOnce installed, navigate to the AprilNav directory and compile with:\\n\\n```\\nmake\\n```\\n\\nAfter compiling, run with:\\n\\n```\\n./build/bin/AprilNav\\n```\\n\\n***Note: Be sure to specify the propper tag size using the -S flag. Display a list of additional customizations using the -h flag\\n\\n## Authors\\n\\n* **Tristan Schuler** - *NASA MSFC* \\n* **Greta Studier** - *NASA MSFC* \\n\\n## Project History\\n\\nAprilTags was developed by Professor Edwin Olson of the University of\\nMichigan.  His Java implementation is available on this web site:\\n  http://april.eecs.umich.edu.\\n\\nOlson\\'s Java code was ported to C++ and integrated into the Tekkotsu\\nframework by Jeffrey Boyland and David Touretzky:\\n  http://wiki.tekkotsu.org/index.php/AprilTags\\n\\nMichael Kaess (kaess@mit.edu) and Hordur Johannson (hordurj@mit.edu) of MIT\\nfurther modified the code and made a standalone library for AprilTags:\\n  http://people.csail.mit.edu/kaess/apriltags/\\n'},\n",
       " {'repo': 'nasa/sitepod',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': '# Sitepod\\n\\n[![Latest Stable Version](https://poser.pugx.org/nasa/sitepod/v/stable)](https://packagist.org/packages/nasa/sitepod)\\n[![Total Downloads](https://poser.pugx.org/nasa/sitepod/downloads)](https://packagist.org/packages/nasa/sitepod)\\n[![Latest Unstable Version](https://poser.pugx.org/nasa/sitepod/v/unstable)](https://packagist.org/packages/nasa/sitepod)\\n[![License](https://poser.pugx.org/nasa/sitepod/license)](https://packagist.org/packages/nasa/sitepod)\\n[![composer.lock](https://poser.pugx.org/nasa/sitepod/composerlock)](https://packagist.org/packages/nasa/sitepod)\\n[![Travis](https://img.shields.io/travis/nasa/podaacpy.svg?maxAge=2592000?style=plastic)](https://www.travis-ci.org/nasa/sitepod)\\n[![Requirements Status](https://requires.io/github/nasa/sitepod/requirements.svg?branch=master)](https://requires.io/github/nasa/sitepod/requirements/?branch=master)\\n[![Quality Gate](https://sonarqube.com/api/badges/gate?key=nasa:sitepod)](https://sonarcloud.io/dashboard?id=nasa:sitepod)\\n\\nSitepod; A Sitemap Generator written in PHP. Sitepod is build using the [Fat-Free Framework](https://fatfreeframework.com), a powerful yet easy-to-use PHP micro-framework designed to build dynamic and robust web applications.\\n\\n# Installation from the Composer PHP Package Manager\\nSitemap can be installed using the [Composer](https://getcomposer.org/) \\ndependency manager for PHP as follows\\n```\\n$ composer require nasa/sitepod\\n```\\nThis will install Sitepod into a directory structure ```vendor/nasa/sitepod/``` relative to \\nwherever the command was executed.\\n\\nAlternatively, you can install the master branch from source\\n\\n# Installation from Source\\n\\nSee [Sitepod Installation](https://github.com/nasa/sitepod/wiki/Sitepod-Installation)\\n\\n# Configuration and Usage\\n\\nSee [Sitepod Setup and Usage](https://github.com/nasa/sitepod/wiki/Setup-and-Usage)\\n\\n# Community, Support and Development\\nPlease open a ticket in the [Sitepod issue tracker](https://github.com/nasa/sitepod/issues).\\nPlease use labels to classify your issue.\\n\\n# License\\nThis code has been forked from the now unmaintained [phpSitemapNG](http://enarion.net/tools/phpsitemapng/)\\nThe code is licensed permissively under the [GPLv3 License](https://www.gnu.org/licenses/gpl-3.0.en.html) which respects the origin authors work e.g. Tobias Kluge, enarion.net.\\nA copy of the GPLv3 License can be found below and is also included as part of this software.\\n```\\nThis file is part of Sitepod.\\n\\nSitepod is free software: you can redistribute it and/or modify\\nit under the terms of the GNU General Public License as published by\\nthe Free Software Foundation, either version 3 of the License, or\\n(at your option) any later version.\\n\\nSitepod is distributed in the hope that it will be useful,\\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\\nGNU General Public License for more details.\\n\\nYou should have received a copy of the GNU General Public License\\nalong with Sitepod.  If not, see <http://www.gnu.org/licenses/>.\\n```\\n'},\n",
       " {'repo': 'nasa/CFS_IO_LIB',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Input/Output Library\\n\\nNASA core Flight System Input/Output Library\\n\\n## Description\\n\\nThe I/O Library (IO_LIB) is a collection of protocol libraries to be called by the CI/TO application custom implementations.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. \\nhttp://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/georef_geocamutilweb',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '\\n``geocamUtilWeb`` is a set of utilities used by Django web apps in the\\nGeoCam Share app collection.  It includes the following utilities.\\n\\nMultiSettings\\n~~~~~~~~~~~~~\\n\\nA settings container object built out of an ordered list of child\\nsettings objects.  When you request the value of an attribute, it\\nreturns the value found in the first child that defines that attribute.\\n\\nWe typically use ``MultiSettings`` when apps extend Django settings by\\ndefining new app-specific variables.  For example, if you have an app\\n``geocamAwesome`` you can put the following in\\n``geocamAwesome/defaultSettings.py``::\\n\\n  GEOCAM_AWESOME_ENABLED = True\\n\\nand in ``geocamAwesome/__init__.py``::\\n\\n  import django.conf.settings\\n  from geocamUtil.MultiSettings import MultiSettings\\n  from geocamAwesome import defaultSettings\\n  \\n  settings = MultiSettings(django.conf.settings, defaultSettings)\\n\\nthen you can run::\\n\\n  $ ./manage.py shell\\n  >>> from geocamAwesome import settings\\n  >>> settings.GEOCAM_AWESOME_ENABLED\\n  True\\n\\nbut if a site administrator adds this line to their site-level\\n``settings.py``::\\n\\n  GEOCAM_AWESOME_ENABLED = False\\n\\nyou would see::\\n\\n  $ ./manage.py shell\\n  >>> from geocamAwesome import settings\\n  >>> settings.GEOCAM_AWESOME_ENABLED\\n  False\\n\\nThe advantage of this approach is that site administrators don\\'t need to\\nadd all of your app\\'s extended settings to their ``settings.py`` file if\\nthey like the defaults, but they can override any setting in a uniform\\nway.\\n\\nActually, ``MultiSettings`` does not depend on Django at all.  It will\\nwork with any kind of child container object as long as its fields can\\nbe accessed using dot notation.\\n\\nmodels.UuidField\\n~~~~~~~~~~~~~~~~\\n\\nA Django model field that stores a `universally unique identifier`_.\\nWhen you first save a model with a ``UuidField``, if the UUID value is\\nnot already set, it is automatically populated with a random (or \"type\\n4\") UUID encoded as a ``CharField`` in the standard UUID display format,\\nwhich is a series of hex digits separated by hyphens.\\n\\n.. _universally unique identifier: XXX\\n\\nYou might want to use a ``UuidField`` if you have multiple instances of\\nyour Django app on different hosts and you need to identify the same\\nobject across instances.  We typically do *not* use a ``UuidField`` as the\\nprimary key for a model to avoid a performance penalty.\\n\\nforms.UuidField\\n~~~~~~~~~~~~~~~\\n\\nA Django form field corresponding to the same-name model field.\\nValidates that the user entered a sequence of hex digits separated by\\nhyphens.\\n\\nmodels.ExtrasField\\n~~~~~~~~~~~~~~~~~~\\n\\nA Django model field for storing extra schema-free data.  You can get\\nand set arbitrary properties on the extra field, which can be comprised\\nof strings, numbers, dictionaries, arrays, booleans, and ``None``.\\nThese properties are stored in a database ``TextField`` as a\\nJSON-encoded set of key-value pairs.\\n\\n.. o  __BEGIN_LICENSE__\\n.. o  Copyright (C) 2008-2010 United States Government as represented by\\n.. o  the Administrator of the National Aeronautics and Space Administration.\\n.. o  All Rights Reserved.\\n.. o  __END_LICENSE__'},\n",
       " {'repo': 'nasa/georef_geocamtiepoint',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': 'Coordinate Systems\\n==================\\n\\nGeoRef uses two main coordinate systems:\\n\\n * The image coordinate system measures position in pixels (x, y) where\\n   (0, 0) is the upper-left corner of the image, x increases to the\\n   right, and y increases down.\\n * The Spherical Mercator coordinate system expresses position on the\\n   Earth\\'s surface. (x, y) coordinates. Roughly speaking, x increases to\\n   the east and y increases to the north. The origin matches the origin\\n   in lat/lon coordinates. The scale of the units approximates\\n   displacement in meters.  This system is also known as EPSG:3857 or\\n   EPSG:900913.\\n\\nTwo-way conversions between lat/lon and Spherical Mercator can be found\\nin the ``latLonToMeters`` and ``metersToLatLon`` functions:\\n\\n * `JavaScript coordinate conversions <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/static/geocamTiePoint/js/coords.js>`_\\n * `Python coordinate conversions <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/quadTree.py>`_\\n\\nSome other references:\\n\\n * `Google Maps Coordinates, Tile Bounds, and Projection <http://www.maptiler.org/google-maps-coordinates-tile-bounds-projection/>`_\\n * `PROJ.4 FAQ: Google Mercator <http://trac.osgeo.org/proj/wiki/FAQ#ChangingEllipsoidWhycantIconvertfromWGS84toGoogleEarthVirtualGlobeMercator>`_\\n\\nExport Format\\n=============\\n\\nExporting an overlay produces a gzip-compressed tar archive containing\\nfollowing files: \\n\\n[imageid]-no_warp.tif\\n\\n-this is a GeoTIFF version of the photo that is unmodified (unwarped) in an image sense, but contains a bunch of metadata header fields indicating the list of \\n\\tcontrol/tie/correspondence points found for alignment and some alignment fit uncertainty measures.  This version gives an end user all they\\'d need to create\\n\\ttheir own aligned image from the embedded control points.\\n\\n[imageid]-warp.tif\\n\\n-this is a GeoTIFF version of the photo that is actually modified to be warped/aligned to a map, with transparency around the warped photo to fit inside a \\n\\trectangular image as usual.  It does not contain a header with the list of tie points, but it contains fields with alignment fit measures.\\n\\n[imageid]-no_warp_metadata.txt\\n\\n-this is a text file containing a formatted dump of the header fields present in the -no_warp.tif version so that someone can get the important data without \\n\\tretrieving the image.\\n\\n[imageid]-warp_metadata.txt\\n\\n-same as no_warp_metadata.txt but instead corresponding to the -warp.tif version.\\n\\n[imageid]-uncertainty-no_warp.tif\\n\\n-This is a special synthetic image (single channel floating point) where the number at each pixel represents the uncertainty (standard deviation) in meters we \\n\\testimate for our fit at that pixel.  This provides data to do automated analysis of the relative accuracy of our alignment at each pixel -- \\n\\tit will be more accurate near tie points, and worse further away.\\n\\n[imageid]-uncertainty-no_warp.tif\\n\\n-analogous to the -uncertainty-no_warp.tif file, this is a warped/aligned version of the uncertainty image.  It contains two floating point channels, the first \\n\\tis the uncertainty as in the unwarped version, and the second is a \"mask\" that is 0 where there is no uncertainty data (the warped image doesn\\'t exist there) \\n\\tor 255 if there is.\\n\\n\\nMeta-Data Format: meta.json\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\nThe ``transform`` field represents a best-fit transform that maps image\\ncoordinates to Spherical Mercator coordinates. Depending on the number\\nof tie points specified, the transform can be expressed in two forms:\\n\\n * ``\"type\": \"projective\"``. This is a 2D projective transform. Used when\\n   fewer than 7 tie point pairs are specified. The ``matrix`` field is a\\n   3x3 transformation matrix ``M`` specified in row-major order. To apply\\n   the transform:\\n\\n   * Start with image coordinates ``(x, y)``.\\n\\n   * Convert to a length-3 column vector ``u`` in homogeneous coordinates: ``u = (x, y, 1)``\\n\\n   * Matrix multiply ``(x0, y0, w) = M * u``.\\n\\n   * Normalize homogeneous coordinates: ``x\\' = x0 / w``, ``y\\' = y0 / w``.\\n\\n   * The resulting Spherical Mercator coordinates are ``(x\\', y\\')``.\\n\\n * ``\"type\": \"quadratic2\"``. This transform is similar to the projective\\n   transform but adds higher-order terms to achieve a better fit when\\n   the overlay image uses a different map projection from the base\\n   layer. Used when 7 or more tie point pairs are specified. Please\\n   refer to the code for full details. Some points of interest:\\n\\n   * Note that despite the name, this transform is *not* exactly\\n     quadratic. In order to ensure the transform has a simple analytical\\n     inverse, corrections are applied serially, which incidentally\\n     introduces some 4th-order and 6th-order terms.\\n\\n   * The ``matrix`` field has essentially the same interpretation as for\\n     the \\'projective\\' transform.\\n\\n   * In order to help with numerical stability during optimization, the\\n     last step of the transform is to scale the result by 1e+7.  Because\\n     of this, the matrix entries will appear much smaller than those in\\n     the projective transform.\\n\\n   * The coefficients for higher-order terms are encoded in the\\n     ``quadraticTerms`` field. If all of those terms are 0, the\\n     ``quadratic2`` transform reduces to a ``projective`` transform.\\n\\nSee the alignment transform reference implementations in the\\n``ProjectiveTransform`` and ``QuadraticTransform2`` classes:\\n\\n * `JavaScript alignment transforms <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/static/geocamTiePoint/js/transform.js>`_\\n * `Python alignment transforms <https://github.com/geocam/geocamTiePoint/blob/master/geocamTiePoint/transform.py>`_\\n\\n.. o __BEGIN_LICENSE__\\n.. o Copyright (C) 2008-2010 United States Government as represented by\\n.. o the Administrator of the National Aeronautics and Space Administration.\\n.. o All Rights Reserved.\\n.. o __END_LICENSE__\\n'},\n",
       " {'repo': 'nasa/pyCMR',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# Python CMR Client Library\\n\\nPython client library that abstracts CMR API calls for search, ingest, update, and deletion of collections and granules.\\n\\n\\n\\n## Setup\\n\\nRun:\\n\\n    $ python setup.py install\\n\\nOr\\n\\n    $ pip install -e .\\n\\n### Usage\\n\\nPopulate your CMR credentials into a `cmr.cfg` file, using `cmr.cfg.example` as a template. Alternatively, on instantiation, if no CFG file is provided then the CMR object will load credentials from these environment variables: `CMR_PROVIDER`, `CMR_USERNAME`, `CMR_PASSWORD`, and `CMR_CLIENT_ID`.\\n\\nThen, run:\\n\\n```python\\n$ python\\n>>> from pyCMR.pyCMR import CMR\\n>>> cmr = CMR('/path/to/my/cmr.cfg')  # or `cmr = CMR()` to load from env vars\\n```\\n\\n## Test\\n\\n```bash\\npython setup.py test\\n```\\n\"},\n",
       " {'repo': 'nasa/Computational-Materials-Miniapp1',\n",
       "  'language': None,\n",
       "  'readme_contents': '\\nThis mini-app is a simple test code for calculating potential energy, \\nforces and stresses of an atomistic structure using EAM potential for Al.\\n(Mishin, Y., Farkas, D., Mehl, M.J., Papaconstantopoulos, D.A., 1999. \\nInteratomic potentials for monoatomic metals from experimental data and \\nab initio calculations. Phys. Rev. B., 59, 3393-3407.)\\n\\n-----------------------------------------------------------------------\\nRequirements:\\n* Linux\\n* FORTRAN compiler (2003 or newer)\\n-----------------------------------------------------------------------\\nEdit the provided makefile for specific compiler option of your choice\\n-----------------------------------------------------------------------\\nCompilation: use the provided makefile with the following options:\\n> make            ! compiles with -O3 optimization on !\\n> make DEBUG=TRUE ! compiles with check and warning flags on !\\n\\n-----------------------------------------------------------------------\\nExample (test) directory:\\n./CM_MINI1.test\\n\\nRunning a tets case:\\nrun CM_mini1.bat script from CM_MINI1.test directory:\\n> CM_mini1.bat\\n\\n-----------------------------------------------------------------------\\nRequired input files:\\n\\npot.dat - interatomic potential setup file\\n./NiCoAl_dat  - folder with potential files as described in pot.dat\\nstructure.plt - input atomic structure file\\n\\n--- Available test structures ---\\nin STR directory:\\nNi85Al15_N4k.plt - 4,000 atoms Ni3Al crystal\\nNi85Al15_N64k.plt - 64,000 atoms Ni3Al crystal\\nNi85Al15_N256k.plt - 256,000 atoms Ni3Al crystal\\n \\nUse any of the above structures by linking them to structure.plt, e.g.,\\nln -s ./STR/Al_N500_T100K.plt structure.plt\\n \\n-----------------------------------------------------------------------\\nExecution:   \\n\\ncd ../CM_MINI1.test\\n./CM_mini1\\n\\nExecution options: (see also ./CM_MINI1.test/CM_mini1.bat script)\\n\\n./CM_mini1           # run, using default options - see below\\n./CM_mini1 -n 10     # do 10 itterations (default: -n 1)\\n./CM_mini1 -n 10 -r  # do 10 itterations with random changes\\n                     # (default: no randomization)\\n./CM_mini1 -v 1      # run version 1 (default: -v 0) - gives a possibility\\n                     # to test different versions of a subroutine.\\n                     # currently: -v 0 executes the code using\\n                     # subroutine get_neighbors (CM_mini1.f)\\n                     # while -v 1 executes the code using\\n                     # subroutine get_neighbors_vect (CM_mini1.f)\\n                     # see subroutine force_global(iver):\\n-----------------------------------------------------------------------\\n       select case (iver)\\n        case(0) \\n         call get_neighbors\\n        case(1) \\n         call get_neighbors_vect\\n        case default \\n         call get_neighbors\\n       end select\\n=======================================================================\\n Notices:\\n Copyright 2018 United States Government as represented by the \\n Administrator of the National Aeronautics and Space Administration. \\n All Rights Reserved.\\n \\n Disclaimers:\\n No Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY\\n WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, \\n INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE \\n WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF \\n MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM \\n INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR \\n FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM \\n TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT,IN ANY MANNER, \\n CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT \\n OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS \\n OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE. \\n FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES \\n REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE,\\n AND DISTRIBUTES IT \"AS IS.\"\\u2028\\n \\n Waiver and Indemnity:\\n RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES\\n GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR \\n RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY \\n LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH\\n USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, \\n RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND\\n HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND \\n SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT\\n PERMITTED BY LAW. RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL \\n BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n\\n'},\n",
       " {'repo': 'nasa/nasapress-companion',\n",
       "  'language': 'PHP',\n",
       "  'readme_contents': \"# NASAPress Companion\\n\\nUses Memcache to cache API calls to nasa.gov and technology.nasa.gov.\\n\\nSearch the plugin for `todo-config`. These comments mark the locations where you'll likely need to make customizations for your site.\\n\\n## Shortcodes\\n\\ncategory-list\\n\\nchildren-list\\n\\nportal-posts\\n\\nspinoff-posts\\n\"},\n",
       " {'repo': 'nasa/content-guide',\n",
       "  'language': 'CSS',\n",
       "  'readme_contents': '## NASA Glenn Content Guide\\n\\nThis is the repository for the [NASA Glenn Content Guide](https://nasa.github.io/content-guide/). This guide was developed for NASA Glenn employees, but we hope it’s a useful reference for anyone. It is a fork of the [18F Content Guide](https://pages.18f.gov/content-guide/)\\n\\n### Editing the Guide\\nThe pages in the guide are broken into four sections corresponding to four folders in the [```_pages```](https://github.com/nasa/content-guide/tree/master/_pages) folder. The markdown files within each section can be edited using GitHub. If you are new to editing files in GitHub [this guide](https://help.github.com/articles/editing-files-in-your-repository/) should help. Just ignore the part about creating a new branch and commit your changes directly to the master branch. If you are new to using markdown [this guide](https://guides.github.com/features/mastering-markdown/) should help. When you commit your changes to the master branch CircleCI will automatically push your changes to the [site on GitHub Pages](https://nasa.github.io/content-guide/). It may take up to a few minutes for your changes to be viewable on the site.\\n\\n### Public domain\\n\\nThis project is in the worldwide [public domain](LICENSE.md).\\n\\n> As a work of the United States government, this project is in the public domain within the United States.\\n\\n> Additionally, we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication.\\n'},\n",
       " {'repo': 'nasa/scifen-solver',\n",
       "  'language': 'C',\n",
       "  'readme_contents': 'This mini-app is a simple interface to PETSc Krylov subspace iterative solvers \\nthat are suitable for linear-elastic finite element models. The executable \\nsimply loads a system matrix, load vector (RHS), and optional reference \\nsolution files that were saved in PETSc binary format, then solves it. It is \\nintended for single-core performance comparisons.\\n\\nRequirements:\\n* Linux\\n* MPI\\n* PETSc (versions 3.6.3 and 3.9.3 tested) \\n* A suitable C compiler such as cc, gcc, or icc.\\n\\n********************************** IMPORTANT **********************************\\nEdit make-local.inc to set PETSC and MPI directories correctly, and to choose \\na compiler.\\n\\nIn ksp-solve.h, the preprocessor macro PETSC_HACK_POGX is set to \"NULL\" or \\nto \"NULL,NULL\", for compatibility with different versions of PETSc.\\n*******************************************************************************\\n\\nUsage:\\nThe makefile is the easiest way to compile, test, and obtain timings with this \\nmini-app. Timing logs are saved in a directory named after the executing system \\n(hostname). The bash shell script provided, log-parse.sh, is used in the \\nmakefile to parse the output from several solutions into a convenient comma-\\ndelimited (CSV) data file with a similar name.\\n\\nmake exe        : recompile the executable, ksp-solve\\nmake test-solve : short (100 iteration) test\\nmake test       : runs several tests\\nmake time-all   : loops through several tolerances and solvers\\nmake solve-lu   : solves the system by preconditioning with an LU decomposition\\n\\nThe executable, ksp-solve, requires at least a matrix [k] and vector [p] to\\nsolve [k][u]=[p] for unknown [u]. The command-line options below are required.\\n\\nksp-solve\\n -rmat <input_file> : read matrix file [k]\\n -rrhs <input_file> : read load vector file [p]\\n\\nThe remaining are optional.\\n -v <verbosity>     : default is 1\\n -ruin <input_file> : read initial guess solution file [u]\\n -rref <input_file> : read reference solution file [u]\\n -wsol <output_file>: write solution file [u]\\n -wcsv <output_file>: write data to a CSV file\\n\\nWhen -rref is given, the true (not preconditioned) norm of the residual between \\nthe computed solution and the provided reference solution will be reported. \\nPETSc options may also be provided to ksp-solve, though only some are used.\\nThe PETSc options below are applied for the examples in the makefile. See the\\nPETSc documentation for more information about these options.\\n\\n -ksp_converged_reason            : prints convergence information\\n -ksp_norm_type preconditioned    : use preconditioned residual norm\\n -log_view  :<filename>           : full path to save PETSc log file\\n -ksp_rtol   <relative tolerance> : default is PETSC_DEFAULT\\n -ksp_max_it <max iterations>     : default is size of RHS vector [p]\\n -pc_type    <preconditioner>     : jacobi, lu\\n -ksp_type   <solver>             : (See table)\\n\\nSuitable FEM System Solvers:\\n---------------------------------------------------------------------------\\n <solver>                      Method                        Sym    Unsym  \\n---------------------------------------------------------------------------\\n    cg                   Conjugate Gradient                   ✓            \\n   fcg               Flexible Conjugate Gradient              ✓            \\n    cr          (Preconditioned) Conjugate Residuals          ✓            \\n                                                                           \\n  symmlq                       SYMMLQ                         ✓            \\n   cgs               Conjugate Gradient Squared               ✓       ✓    \\n   lcd                left conjugate direction                ✓       ✓    \\n                                                                           \\n   bicg            Biconjugate gradient (Unstable)                         \\n   bcgs      BiCGStab (Stabilized BiConjugate Gradient)               ✓    \\n  ibcgs             IBiCGStab (Improved BiCGStab)                     ✓    \\n  bcgsl            Variant of Enhanced BiCGStab(L)                    ✓    \\n                                                                           \\n  minres                  Minimum Residual                                 \\n  gmres             Generalized Minimal Residual                      ✓    \\n  lgmres          Augmented w/ error approximation                    ✓    \\n  dgmres                   Deflated GMRES                             ✓    \\n                                                                           \\n  tfqmr      transpose free QMR (quasi minimal residual)                   \\n\\nSolve by LU Decomposition\\n---------------------------------------------------------------------------\\n  preonly                preconditioner only\\n\\nThe following do not support left preconditioning or precoditioned residual \\nnorm calculation.\\n---------------------------------------------------------------------------\\n   cgr      Preconditioned Generalized Conjugate Residual     ✓       ✓    \\n  fbcgsr           Equivalent Variant of FBiCGSTab                    ✓    \\n  fbcgs                  Flexible BiCGStab                            ✓    \\n  fgmres        Flexible Generalized Minimal Residual                 ✓    \\n\\n\\n===============================================================================\\nNotices:\\nCopyright 2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. No copyright is claimed in the United States under Title 17, U.S. Code. All Other Rights Reserved.\\n \\nDisclaimers\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n \\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/cumulus-circleci-image',\n",
       "  'language': 'Shell',\n",
       "  'readme_contents': '# Base CircleCI image for testing Cumulus Core\\n\\n[![CircleCI](https://circleci.com/gh/nasa/cumulus-circleci-image.svg?style=svg)](https://circleci.com/gh/nasa/cumulus-circleci-image)\\n\\n### Local Build\\n\\n     $ docker build . -t cumuluss/circleci:node-8.11\\n\\n### Local Test\\n\\n     $ docker run --rm -it -p 20:20 -p 21:21 -p 47400-47470:47400-47470 -p 3030:3030 -p 2222:2222 cumuluss/circleci:8.11 /bin/bash\\n\\n### Provider Server\\n\\nTo run this image as a provider server with the test data:\\n\\n    $ docker run -it -e ON_AWS=true -p 20:20 -p 21:21 -p 47400-47470:47400-47470 -p 3030:3030 cumuluss/circleci:node-8.11 start\\n\\nIn the above example the test-data is loaded from the latest version of @cumulus/test-data package on npm.\\n\\nIf you need to load test-data other than the ones published to npm make sure to set `TEST_DATA_S3_PATH` and point to the location of test files. For example:\\n\\n    $ docker run -it -e ON_AWS=true -e TEST_DATA_S3_PATH=s3://cumulus-data-shared/@cumulus/test-data/ -p 20:20 -p 21:21 -p 47400-47470:47400-47470 -p 3030:3030 cumuluss/circleci:node-8.11 start\\n\\n## Credit\\n\\n- [Bogem FTP Docker Image](https://github.com/bogem/dockerfiles/tree/master/ftp)\\n- [atmoz SFTP Docker Image](https://github.com/atmoz/sftp)'},\n",
       " {'repo': 'nasa/TCML',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'readme_TCML\\n\\nTCML version 1.0.0\\nDate of most recent update: 01/04/2018\\nDeveloper: Jonathan Kratz (NASA Glenn Research Center)\\n\\nGENERAL INFORMATION\\n\\nThe Tip Clearance Modeling Library (TCML) was developed to model the dynamic variation of high pressure \\nturbine (HPT) tip clearance in aero-engine turbomachinery. The TCML blockset contains blocks to predict\\nthe dynamic and steady-state tip clearance using a method similar to what is described in the references\\nbelow:\\n\\n[1] Chapman, J., Kratz, J., Guo, T.H., and Litt, J., �Integrated Turbine Tip Clearance and Gas Turbine \\n    Engine Simulation,� Proceedings of the 52nd AIAA/ASME/SAE/ASEE Joint Propulsion Conference, Salt Lake \\n    City, UT, 2016.\\n[2] Kratz, J., Chapman, J., and Guo, T.H., �A Parametric Study of Actuator Requirements for Active Turbine\\n    Tip Clearance Control of a Modern High Bypass Turbofan Engine,� Proceedings of the 2017 ASME Turbo Expo,\\n    Charlotte, NC, 2017.\\n[3] Kratz, J., and Chapman, J., �Active Turbine Tip Clearance Control Trade Space Analysis of an Advanced\\n    Geared Turbofan Engine,� Proceedings of the 54nd AIAA/ASME/SAE/ASEE Joint Propulsion Conference, \\n    Cincinnati, OH, 2018. To be published.\\n\\nMost resently this blockset has been used in a HPT tip clearance study conducted for an advanced geared\\nturbofan with a compact gas turbine (CGT). See Ref. [3].\\n\\nThe library is conducive to use with dynamic engine models, particulary those created with the Toolbox \\nfor Modeling and Analysis of Thermodynamic Systems (T-MATS). TCML could be viewed as a compliment to \\nT-MATS. Infact, the TCML blockset uses a few block from T-MATS version 1.1.3.13. The links to the T-MATS\\nlibrary have been broken to enable use of TCML without a T-MATS installation.\\n\\nThe tip clearance modeling method is a physics-based system level approach. The help menu\\'s of the blocks\\nprovide a description of the method. In additon, the user is referred to the references listed above.\\n\\nNote that TCML was developed using MATLAB/Simulink R2015a on a Windows PC with the Windows 7 operating\\nsystem. TCML is not gauranteed to work on different operating systems or different versions of MATLAB.\\n\\n\\n\\nTCML ORGANIZATION\\n\\nWith the \"Trunk\" folder are files to install and uninstall TCML. Two folders \"TCML_Library\" and \\n\"TCML_Examples\". \"TCML_Library\" contains the library file \"lib_TipClearance.slx\" and the folders\\n\"Support\" and \"MATLAB_Scripts\". \"Support\" containts the help files for the Simulink blocks and \\n\"MATLAB_Scripts\" contains MATLAB functions that support the Simulink blocks. The folder \"TCML_Examples\"\\ncontains an example to illustrate the usage of the blocks inside the library.\\n\\n\\n\\nINSTALLING TCML\\n\\n1. Download TSAT from the GIT server PUT URL HERE, click the green download button for the latest version, \\n   and extract the files to a folder that can be accessed by MATLAB, ensuring there are no spaces in the \\n   path name.\\n2. Open up MATLAB and navigate to the directory that TSAT was saved.\\n3. Run install_TCML.m. This will setup the paths for TSAT. A temporary install should only save the paths \\n   for the current MATLAB session while the permanent install option will save the paths to MATLAB until \\n   the uninstall script is ran or the paths are manually removed. If the user does not have elevated \\n   privileges, the paths may not be saved properly. If the paths have not been saved, new paths must be \\n   manually added to the pathdef.m file. To do this click on the �Set Path� icon in the MATLAB toolbar and \\n   add the following paths by navigating to them and selecting them: \\n\\t� Trunk\\\\TCML_Library\\n\\t� Trunk\\\\TCML_Library\\\\Support\\n\\t� Trunk\\\\TCML_Library\\\\MATLAB_Scripts\\n   Save the paths before exiting the dialog box.\\n4. Open up Simulink and verify that the TCML library shows up in the library browser.\\n5. Open up one of the examples in the �Trunk\\\\TCML_Examples� folder and attempt to execute it to verify \\n   that the TCML library is on the path and the library blocks can be used.\\n\\n\\n\\nUNINSTALLING TCML\\n\\n1. Run uninstall_TCML.m. This will remove the paths that were added during the TCML install.\\n\\ta. If the paths were added manually during installation then they must be removed manually using \\n \\t   the �Set Path� tool in MATLAB.\\n'},\n",
       " {'repo': 'nasa/TSAT',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'readme_main.txt\\n\\nTSAT version 1.0.0\\nDate of most recent update: 02/22/2018\\nDeveloper: Jonathan Kratz (NASA Glenn Research Center)\\n\\nThe Thermal Systems Analysis Toolbox (TSAT) is a MATLAB/Simulink based tool for modeling and analysis\\nof dynamic and steady-state thermal systems involving heat transfer. TSAT consists of a Simulink library\\nthat will appear in your Simulink library browser when installed. Each block has a help file that can\\nbe reached by right clicking the block and clicking \"help\". The help files will provide general modeling \\ninformation of what the block does; will described inputs, parameters, and outputs; and will provide \\nreferences if appropriate. In addition to the help files, several examples are provided that illustrate\\nusage of several of the TSAT blocks. TSAT also provides several MATLAB functions that can be used in a\\nvariety of ways. Descriptions of function inputs, outputs, and usage is provided by comments within the \\nfunction files.\\n\\nThe \"Trunk\" folder contains all the tools including the Simulink libraries, the MATLAB tools, the help\\nfiles, and examples. The \"Resources\" folder contains the TSAT Quickstart Guide (TSATquicksart.pdf) and \\ncould include various other materials if future updates of the software package are made. The quickstart \\nguide is recommended reading before attempting to install, uninstall, or use TSAT.\\n\\nTSAT was created during the development of thermal models of aero-engines for the purpose of approximating\\nthe thermal environment relevant for control system components when considering the application of \\ndistributed engine control. Also of interest was thermal modeling relevent to turbine tip clearance control.\\nA significant portion of the TSAT library blocks and MATLAB tools are a direct result of these efforts. At \\nthe core of the library are its 1D and 2D conduction blocks used to model conduction through solid structures.\\nThe library also has various options of estabilishing boundary conditions and it contains various general-use \\ntools as well. \\n\\nNOTE: TSAT and the examples it comes with were developed using MATLAB/Simulink R2015a. Although compatability\\nwith newer versions is not thought to be an issue, the user should be aware of the potential for \\ncompatability issues.'},\n",
       " {'repo': 'nasa/cumulus-ecs-task-python',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# cumulus-ecs-task-python\\n\\nUse this Docker image to run a Python AWS Lambda function in AWS [ECS](https://aws.amazon.com/ecs/). It mimics [cumulus-ecs-tas](https://github.com/cumulus-nasa/cumulus-ecs-task) for Node.js AWS Lambda functions.\\n\\n## About\\n\\ncumulus-ecs-task-python is a Docker image that can run Lambda functions as ECS services.\\n\\nWhen included in a Cumulus workflow and deployed to AWS, it will download a specified Lambda function, and act as an activity in a Step Functions workflow.\\n\\n## Usage\\n\\n[See documentation in cumulus-ecs-task](https://github.com/cumulus-nasa/cumulus-ecs-task/blob/master/README.md#usage).\\n\\n## Building\\n\\n```\\nexport VERSION=0.0.2\\ndocker build -t cumuluss/cumulus-ecs-task-python:$VERSION .\\n```\\n\\n## Pushing to docker\\n\\n```\\n# docker login\\ndocker tag cumuluss/cumulus-ecs-task-python:$VERSION cumuluss/cumulus-ecs-task-python:$VERSION\\ndocker push cumuluss/cumulus-ecs-task-python:$VERSION\\n```\\n\\n## Testing\\n\\n```\\nexport AWS_ACCESS_KEY_ID=<AWS_ACCESS_KEY_ID>\\nexport AWS_SECRET_ACCESS_KEY=<AWS_SECRET_ACCESS_KEY>\\n\\ndocker run -it -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\\\\n -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\\\\n  cumuluss/cumulus-ecs-task-python:$VERSION \\\\\\n  cumulus-ecs-task \\\\\\n  --activityArn arn:aws:states:us-east-1:433612427488:activity:cce-DownloadTiles-Activity \\\\\\n  --lambdaArn arn:aws:lambda:us-east-1:433612427488:function:cce-ViirsProcessing\\n```\\n'},\n",
       " {'repo': 'nasa/cumulus-process-py-seed',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# cumulus-process-py-seed\\n\\n[Cumulus](https://github.com/cumulus-nasa/cumulus) is a cloud-based framework for distributing and processing data, used by NASA\\'s Distributed Active Archive Center\\'s (DAAC\\'s). A Cumulus workflow is made up of tasks, such as finding new data at an FTP site, downloading, publishing metadata, etc. \\n\\n[cumulus-process-py](https://github.com/cumulus-nasa/cumulus-process-py) is a Python library that makes it easy to create Cumulus tasks in Python that are primarily for processing data. The cumulus-process-py library includes convenience functions that tend to be common across processing tasks. It also provides the ability to run processing at the command line in addition to the within the Cumulus framework, which can be very valuable for development.\\n\\nThis repository, cumulus-process-py-seed, is a project template for creating a new Cumulus task in Python. Python is a good choice for processing data with existing code libraries because Python\\'s subprocess library for spawning commands. This allows legacy code to be integrated into Cumulus while taking advantage of the convenience of Python for wrapping the legacy code.\\n\\n### The Process class\\nBefore delving into setting up a new task it\\'s important to understand the basics of the cumulus_process library. The main part of the library is made up of the *Process* class which is meant to be subclassed in other projects. The processing unique to this new Process child class is defined in the *process* member function.\\n\\nThe Process class provides a host of functionality through the use of member functions and attributes. Entrypoints (AWS Lambda handler, Activity handler, and CLI) are also provided through the use of member functions, so that any child of the Process class will automatically inherit the functions and the entrypoints. The main advantage of using a class is that it minimizes the need for passing around a lot of variables between functions, while remaining very flexible as any of the existing functions can be overridden.\\n\\n### GDAL and Geolambda\\nThis template project utilizes [Cumulus Geolambda](https://github.com/cumulus-nasa/geolambda) to provide common geospatial libraries (e.g., proj.4, GDAL). If these libraries are not required then some of the files here can be simplified further. These are noted in the steps below.\\n\\n## Creating a new cumulus-process based task\\n\\nFollow the steps below to update the template files in this project in the creation of a new task. This will create a new pip-installable Python project, as well as a command line utility.\\n\\n#### 1. Update setup.py and rename project folder\\n\\nThe setup.py file needs to be updated with the name of the project. Replace ${PROJECT} with what the name of the Python package will be. It is suggested to include preface the project name with \\'cumulus_\\' to avoid any potential conflicts with other packages and for clarity (e.g., cumulus_modis, cumulus_aster, cumulus_mynewdatasource).\\n\\nThe *entry_points* field in setup allows the name of the command line interface (CLI) to be specified.\\n```\\n    entry_points={\\n        \\'console_scripts\\': [\\'${PROJECT}=${PROJECT}.main:Process.cli\\']\\n    },\\n```\\nThe first part of the string, *${PROJECT}=* specifies the name of the CLI program, while the second half *${PROJECT}.main:Process.cli* specifies the path to the CLI entrypoint in your main module. For example:\\n```\\n    entry_points={\\n        \\'console_scripts\\': [\\'cumulus_mynewtask=cumulus_mynewtask.main:Process.cli\\']\\n    },\\n```\\nThis will install a CLI program that can be called as **cumulus_mynewtask** and will point to the cli function in your Process class.\\n\\nAt this time also rename the *project* folder to match the same name as your project.\\n\\n#### 2.  Write Process subclass (project/main.py)\\n\\nThe Process subclass will be a class specific to the Processing of a new type of data. While any of the Process members can be overridden in this subclass, most users will only need provide two functions: the *default_keys* property function, and the *process* member function.\\n\\nThe *default_keys* property is a way to hard-code a default set of keys and file patterns to identify which files are what so they can be referenced in the *process* function. It is used as a fallback when the parent class *input_keys* property fails to retrieve keys from the cumulus message config because none exist. For example:\\n```\\n    @property\\n    def default_keys(self):\\n        return {\\n            \\'hdf\\': r\"^.*hdf$\",\\n            \\'thumbnail\\': r\"^BROWSE.*hdf$\",\\n            \\'meta\\': r\"^.*met$\"\\n        }\\n```\\nWe now can reference the keys *hdf, thumbnail, and meta* in the process function to get filenames rather than have to parse the input filenames manually.\\n\\nThe process function is where the actual processing code goes, and is a member function of your Process subclass. An instance of your Process subclass is created for a set of input files (if the CLI is used) or message (if used within the Cumulus framework). The Process subclass is instantiated with the the input file names and/or the config information from a Cumulus message, which is stored and accessible from within the Process function.\\n\\nA complete list of member functions that you can use within the process class is given in the section below, but the most commonly used will be the *fetch* and *upload* functions. There are just a few requirements when writing a new Process subclass.\\n\\n- **Clean up files**: This process may be deployed as a Lambda function or as a StepFunction activity running on ECS. In this type of deployment the containers can be reused between processes so it is important that any temporary files created are deleted. Files downloaded with the self.fetch() are automatically deleted when processing is over, the developer does not need to delete these.\\n- **Add output files to self.output**: Any output files that will be uploaded should be added to the self.output list. All files in self.output can be uploaded at once with the self.upload_output_files() function, and they will also be deleted when processing is over. Alternately, files can be uploaded with the self.upload(filename) function without adding them to self.output, in which case they need to be manuallly deleted.\\n- **Return S3 URLs from process function**: The process function must return the S3 URLs for all output files that were uploaded to S3.\\n\\n#### 3. Update requirements files and MANIFEST.in\\n\\nThe requirements.txt file already includes the reference to cumulus-process. The requirements-dev.txt file contains requirements needed for development and testing, for now it just includes the *coverage* package for testing test code coverage. Update the requirements files to include your own Python requirements needed.\\n\\nThe MANIFEST.in file specifies files, other than Python source files, in the project directory that should be included in the deploy. Python files are included automatically, but other scripts, executables, small data or config files, etc. should be specified in the MANIFEST file. For more info see the [Python packaging tutorial](https://packaging.python.org/tutorials/distributing-packages/#manifest-in).\\n\\n#### 4. Write test payload\\n\\nThe included test_main.py file shoud not need modification and is configured to read in a test payload and run the process function. The example payload here is based on MODIS.\\n```\\n{\\n  \"config\": {\\n    \"buckets\": {\\n      \"internal\": \"cumulus\",\\n      \"private\": \"cumulus\",\\n      \"public\": \"cumulus\",\\n      \"protected\": \"cumulus\"\\n    },\\n    \"granuleIdExtraction\": \"(.*)\",\\n    \"url_path\": \"testing/cumulus-modis\",\\n    \"files_config\": [\\n      {\\n        \"regex\": \"^M.*\\\\\\\\.hdf$\"\\n      },\\n      {\\n        \"regex\": \"^M.*\\\\\\\\.hdf.met$\"\\n      },\\n      {\\n        \"regex\": \"^M.*\\\\\\\\.jpg$\"\\n      },\\n      {\\n        \"regex\": \"^M.*\\\\\\\\.meta\\\\\\\\.xml$\"\\n      }\\n    ]\\n  },\\n  \"input\": [\\n    \"s3://cumulus/testdata/modis/MOD09GQ.A2017001.h11v04.006.2017014010124.hdf\",\\n    \"s3://cumulus/testdata/modis/MOD09GQ.A2017001.h11v04.006.2017014010124.hdf.met\"\\n  ]\\n}\\n```\\n\\nThere are several important pieces in the payload. The first is the list of input files. These could be local files, but it would make it difficult to test unless the data was included in the repository. For large data files this is not recommended. Instead, these can be put on an S3 bucket and the test process will fetch and process them.\\n\\nThe config section of the payload specifies the buckets to be used, as well as the files_config section. These fields are used in conjunction to determine where files should be published to. The Process.get_publish_info(filename) function will iterate through the regexes in files_config to find a matching file, then will use the *bucket* and *url_path* fields to generate the complete s3 URL. If this information is not provided (such as when calling from the command line with only a list of input files and no config info), then files will not be uploaded.\\n\\n#### 5. Write Dockerfile and docker-compose.yml\\n\\nThe basic included Dockerfile simply installs the requirements files and the Python package. In this template it is using a Geolambda image which allows one to easily deploy to AWS Lambda (while including the common geospatial libraries). If geolambda is not used, then the developer will need to take care of packaging for Lambda themselves.\\n\\nIf the new Process subclass requires separate compiled code, then the Dockerfile is where that should be compiled and installed. This can be done in a variety of ways, but the recommended way is to install any compiled binaries with the PROJECT directory (alongside the Python files). Then, include these in the MANIFEST.in file so they are included in deployment, then call the executable from the Python code by getting the path dynamically:\\n```\\nexepath = os.path.dirname(__file__)\\nmyexe = os.path.join(exepath, \\'my.exe\\')\\n```\\n\\nAt the end of the Dockerfile, set the entrypoint to your CLI program specified in setup.py.\\n```\\nENTRYPOINT ${CLI_NAME}\\n```\\n\\nThe included docker-compose provides several services for running and testing your process, as well as several services for deploying and testing to AWS Lambda. See the [docker-compose reference page](https://docs.docker.com/compose/compose-file/compose-file-v2/) for more info on docker-compose services. The image is built with\\n```\\n$ docker-compose build\\n```\\nand a specific service can be run with\\n```\\n$ docker-compose run servicename\\n```\\nThe services in the included docker-compose.yml are:\\n\\n- **base**: This simply builds the image, and if run will drop you into an interactive bash shell in the container\\n- **cumulus**: This will run the CLI program specified in your setup.py file (assuming ENTRYPOINT has been set in the Dockerfile)\\n- **test**: This runs the tests, with code coverage included\\n- **package**: This creates a zipfile of needed system libraries and Python dependencies suitable for deploying to AWS Lambda.\\n- **testpackage**: This runs the tests using the packaged files and a plain Amazon Linux image (see bin/ files below)\\n- **deploy**: This calls a deploy script to push the Lambda zip file to S3 (see bin/ files below)\\n\\n#### 6. Update package/deploy scripts if using Lambda (optional)\\n\\nIf deploying to Lambda there are two scripts in the bin/ directory that are used for packaging and deploying, although in most cases they will not need to be edited.\\n\\n- **deploy-to-s3.sh**: This script runs lambda-package.sh to create a zip file then uploads the file to s3, renamed with the version # of the package.\\n- **lambda-package.sh**: This file is a placeholder that just calls lambda-package-base.sh which is a script provided by Geolambda that packages the geospatial system libraries and Python packages on the system. This lambda-package.sh script can be used for performing additional steps beyond what is included in lambda-package-base.sh\\n\\n#### 7. Update lambda_handler if using Lambda (optional)\\nThe last step, if using AWS Lambda, is to update the lambda/lambda_handler.py file to import your new Process subclass from your Python package. Update the import line to import the correct package name that was set in setup.py.\\n```\\nfrom ${PROJECT} import MyProcess\\n```\\n\\n## Process Command Line Interface\\nIn addition to Lambda and Activity handlers, the Process class will automatically supply a CLI to your new Process subclass. Call it on the command line with \"-h\" to see online help. The CLI supports three different commands:\\n\\n- **process**: This allows you to pass in a list of remote or local input files and generate local output. Useful for development.\\n- **payload**: This takes in a simple payload message, not a Cumulus message. Useful for testing.\\n- **activity**: Given an AWS SF Activity ARN, this runs a Cumulus activity (which expects Cumulus messages). Useful for production.\\n\\n## Process Member functions\\n\\nBelow are descriptions of the Process class member functions (included properties and class methods). These can be caleld from within your *MyProcess.process()* function. They can also be overridden if needed.\\n\\n### Properties\\n\\n##### Process.input_keys\\nAs described above, input_keys provides keys and file patterns so that files can be identified in the process function. These will be retrieved from the Cumulus message, or default to default_keys when none exist there. This allows a generalised process to dynamically handle a wide range of file patterns, or simply use default_keys when this functionality is not needed.\\nNote: In Cumulus workflows regex patterns for the input files are usually included, which are not the same as input keys. These regex expressions included in the Cumulus message are specific and used for validation. The regex patterns used in the process() function are simply for indentifying types of files and as such can be more general than the detailed regexes defined in the Cumulus workflow. An example of the difference is input file patterns for the collection, versus input_keys for a whole product family.\\n\\n##### Process.default_keys\\nAs described above, default_keys is meant to supply hard-coded keys and file patterns so that files can be identified in the process function. It is used as a fallback when no keys are specified in the Cumulus message config.\\n\\n##### Process.gid\\nThe gid property returns the \"Granule ID\", based upon the input files. The default property tries to generate the GID using the following 3 methods, in order of preference:\\n\\n- 1. If *granuleIdExtraction* is provided in the config part the message it will be used against the 1st file passed in to generate the GID.\\n- 2. Otherwise, it will try to determine the GID by finding a common prefix among all input files.\\n- 3. If there is no common prefix the GID is the filename of the first input file.\\n\\nIf other behavior is desired for determining GID, this property should be overridden.\\n\\n### Member Functions\\n##### Process.__init__(input, path=None, config={})\\nThe __init__ function is what is called to initialize a Process instance. It takes in:\\n\\n- **input**: a list of input files, these can be local files or S3 URLs and are stored in self.input\\n- **path**: A local path to store output files and temp files. The valus is stored in self.path Defaults to a tmp directory\\n- **config**: The config part of a Cumulus message, which is stored in self.config\\n\\n##### Process.fetch(key, remote=False)\\nThe fetch function is used to download remote input files and store them in *self.path*. Any and all input files matching the provided key (see input_keys) are downloaded and returned as a list. If *remote=True* then the original input filenames (remote or not) will be returned, and not downloaded. This can be useful to retrieve the remote filename when local processing is not required.\\n\\n##### Process.fetch_all(remote=False)\\nThis convenience function will run Process.fetch for all keys provided in Process.input_keys and return the resulting filenames as a dictionary.\\n\\n##### Process.upload_file(filename)\\nFor the provided filename, the Process class will try and retrieve publication info about it from the Process.config dictionary. If publishing info is not available upload_file will jsut return the local filename. If publication info is available then the filename will be uploaded to the proper bucket and the S3 url will be returned. See *Process.get_publish_info(filename)* for more info.\\n\\n##### Process.upload_output_files()\\nThis function will upload all files (using Process.upload_file) for all files in the self.output list.\\n\\n##### Process.clean_downloads(), Process.clean_output(), Process.clean_all()\\nProcess.clean_downloads will delete all downloaded files (these are stored in self.downloads by the Process.fetch function).\\nProcess.clean_output will delete all local output files.\\nProcess.clean_all will call the above two functions.\\n\\n##### Process.get_publish_info(filename)\\nUsing the passed in filename and the Process.config information, this will generate the S3 URL the file should be uploaded to, as well as the http URL that can be used to access it. The URL may to a publicly available bucket, or may be a private bucket accessible only through the URL.\\n```\\n{\\n    \"s3\": \"s3://bucket/path/myfile.dat\",\\n    \"http\": \"http://mydomain.com/path/myfile.dat\"\\n}\\n```\\n\\n### Class Methods\\n\\n##### Process.dicttoxml(meta, pretty=False, root=\\'Granule\\')\\nThis function takes in a Python dictionary and converts it to XML. This was written to create XML files suitable for posting to the NAS Common Metadata Repository (CMR). If pretty=True then indents will be used when writing it.\\nNote: CMR requires data in a specific order, but Python dictionaries are unordered. This function will accept a Python ordered dictionary as well:\\n```\\nfrom collections import OrderedDict as od\\nmydict = od([(\\'key1\\', \\'val1\\'), (\\'key2\\', \\'val2\\')])\\nmyxml = self.dicttoxml(mydict)\\n```\\nThis will create an ordered dictionary and then create the XML, ensuring that key1 in the XML comes before key2.\\n\\nThe related convenience function *Process.write_metadata(meta, fout, pretty=False)* will convert a metadata dictionary to XML and write it to a file named *fout*\\n\\n##### Process.run_command(cmd)\\nThis takes in a string command and spawns it using subprocess. Output will be logged and if an error occurs it will throw a RuntimeError.\\n\\n##### Process.gunzip(filename, remove=False)\\nThis function takes in a gzipped file and unzips it, creating a new file. If remove=True the original file will be deleted after.\\n\\n##### Process.basename(filename)\\nThis returns the basename of the file, without the path and without the extension.\\n\\n### Handlers\\nThese are handlers (aka Entrypoints) that can be used to perform a complete run. They are all class methods.\\n\\n##### Process.run(*args, **kwargs, noclean=False)\\nThe run function combines the initialization of a Process class, running the *process()* function, and cleaning up the input and output files afterward (if noclean=False). All of the other handlers end up calling this function.\\n\\n##### Process.handler(event, context=None, path=None, noclean=False)\\n#### Process.cumulus_handler(event, context=None)\\nThe handler function takes in a simple payload (as seen in test/payload.json), not a Cumulus message and calls Process.run().\\nThe cumulus_handler function takes is the same as *Process.handler()* except it takes in a Cumulus message. It automatically sets noclean=False and uses a tmp directory for path. This is the entrypoint that would be used by a Lambda function.\\n\\n##### Process.cli()\\nThis is the entrypoint called by the Command Line Interface\\n\\n##### Process.activity(arn=os.getenv(\\'ACTIVITY_ARN\\'))\\n##### Process.cumulus_activity(arn=os.getenv(\\'ACTIVITY_ARN\\'))\\nThis is the handler that is called to run an AWS Step Function activity. Pass in the ARN for the Activity and tasks will be consumed, instantiating a new Myprocess instance for each message.\\n\\n'},\n",
       " {'repo': 'nasa/geolambda',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': \"# geolambda: geospatial Docker image and packaging for Amazon Linux\\n\\nThe geolambda project aims to make it easier to develop and deploy code to AWS Lambda functions, however the Docker images available also provide a ready to go Docker image based on Amazon Linux that contains common geospatial libaries and packages for other purposes.\\n\\n## Usage\\n\\nUnless interested in modifying the geolambda images themselves, most users will find most useful the product of this repository: a series of images available on Dockerhub. This repository contains a series of Dockerfiles that build upon one another to provide different versions of a geospatial Docker image for different appications.\\n\\n### Available image tags\\n\\nThe developmentseed/geolambda image in Docker Hub has several available tags:\\n\\n- **base**: The base image consists of an Amazon base image with python added, along with the boto3 and nose libraries. The purpose of the main image is to replicate the base system in Lambda with the nose testing framework added so that a deployable package can be tested on an image like what is used in the Lambda container.\\n- **core**: The core image contains system dependencies that are used by other other images. It also defines version numbers for packages that are installed in other images.\\n- **min**: GDAL2, with a minimized set of packages/drivers is installed with Python support, along with proj.4\\n- **hdf**: GDAL2, with HDF4 and 5 support added (including szip and proj.4)\\n- **cloud**: GDAL2, with libraries common for cloud access: GeoTiff and Jpeg2000 (using OpenJPEG)\\n- **full**: GDAL2 with all libraries compiled in other images\\n\\nPull whichever one is most appropriate with the docker command:\\n\\n\\t$ docker pull developmentseed/geolambda:<tag>\\n\\n### Creating a new geolambda based project\\n\\nThe geolambda image will most often be used an image used in the creation of a package suitable for deploying to an AWS Lambda function. There are two main use cases:\\n\\n- No additional libraries are required, and the client application is a simpler handler function that draws upon libraries and packages that are already included in the geolambda image. In this case the only files really needed are the docker-compose.yml file and the handler.py function.\\n- Additional libraries, either installed via PyPi or Git, and custom modules are needed to run the Lambda function. In this case a new Dockerfile is needed to create a new image that will be used (along with docker-compose.yml and the handler.py function)\\n\\nIn either case, the files in the geolambda-seed directory in this repository can be used as a template to create your new Lambda function.\\n\\n### Deploying to Lambda\\n\\nThe geolambda imgaes contain two scripts for collecting and packaging all the files needed to deploy to a Lambda function (the zip file can either be uploaded directly to a Lambda function or added to S3).\\n\\n\\n### geolambda Development\\n\\nContributions to the geolambda project are encouraged. The goal is to provide a turnkey method for developing and deploying geospatial Python based projects to Amazon Web Services. The 'master' branch in this repository contains the current state as deployed to the developmentseed/geolambda image on Dockerhub. The 'develop' branch is the development version and is not deployed to Dockerhub. To use the develop branch the images must be locally built first.\\n\"},\n",
       " {'repo': 'nasa/CCSDS-MAL-Http-Binding-Xml-Encoding',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# CCSDS Message Abstraction Layer (MAL) Prototype using Http Binding & XML Encoding\\n* Visit CCSDS at [Homepage](https://public.ccsds.org/default.aspx)\\n* Visit MAL at [Wiki](https://en.wikipedia.org/wiki/Message_Abstraction_Layer)\\n* Visit Http Binding & XML Encoding at [specification](https://public.ccsds.org/Lists/CCSDS%205243R1/524x3r1.pdfgi)\\n## About\\nIn order to verify the clarity and validity of the specification documents, two independent prototypes are implemented. \\nAfter both prototypes are completed, they are verified by interoperability tests using a [test bed](https://github.com/esa/CCSDS_MO_TESTBEDS).\\nFour different types of tests were conducted and passed. \\n1. Prototype-A Server to Prototype-B Client\\n2. Prototype-B Server to Prototype-A Client\\n3. Prototype-A Server to Prototype-A Client\\n4. Prototype-B Server to Prototype-B Client\\n## License\\nCopyright © 2017-2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n## Installations\\n### Prerequisites\\n* Java-1.8\\n* Maven\\n### Modules\\n1. **CCSDS_MAL_Encode_HTTP**: XML Encoder converting `MAL java objects` to `XML` or an `XML` document to `MAL java objects`\\n2. **CCSDS_MAL_HEADER_HTTP**: Header Encoder converting HTTP headers into MAL header and vice versa\\n3. **CCSDS_MAL_TRANSPORT_HTTP**: Http Server to send and receive CCSDS messages\\n4. **CCSDS_MAL_IP_TEST**: Integration tests to verify Http Server is working as intended. \\n### Instructions\\n1. Clone the repo\\n2. In `root` directory, run `mvn package`\\n   * Compiling java files\\n   * Running unit tests\\n   * Running integration tests\\n   * Creating a fat jar by combining all the libraries\\n   \\n\\n'},\n",
       " {'repo': 'nasa/FEI',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '# FEI5 Client Repository\\nVisit FEI5 at [FEI Overview](https://www-mipl.jpl.nasa.gov/mdms/Fei/feiOverview.html)\\n## About\\nThe File Exchange Interface (FEI5) service offers secure file transaction, store, transport, and management services. FEI5 is the science data product management and distribution service used by most major space missions. The service offers a transaction-oriented approach in file management. That is, all concurrent updates to the same data product are prohibited. All uncommitted file transactions are automatically rolled back. The latest distribution, FEI5 software code name Komodo, is a complete redesign from its predecessors, which adopts the latest computing technologies and standards.\\n## License\\nCopyright © 2002-2018 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n    http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\\n## Installations\\n### Prerequisites \\n* Java-1.7 or later\\n* [Apache Ant](https://ant.apache.org/)\\n## Instructions\\n1. Clone this repository to a new directory. \\n2. build FEI5 Client by running `ant clean package`\\n   * alternatively, `ant clean all` can be used to generate `javadoc`, and `checkstyle` tasks. \\n   * `java` class files are saved in `build/classes/`\\n  \\n3. 3 jar files and 2 distribution packages are created in `dist/`\\n   * mdms.jar\\n   * mdms-komodo-client.jar\\n   * mdms-komodo-lib.jar\\n   * mdms-fei5.zip\\n   * mdms-fei5.tar.gz\\n4. Use distribution packages to create an FEI5-Client instance. The instruction is included inside the distribution packages. \\n\\n\\n\\n\\n'},\n",
       " {'repo': 'nasa/PLEXIL5',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': 'PLEXIL5 \\n========\\nThe Plan Execution Interchange Language ([PLEXIL](http://plexil.sourceforge.net)) is an open source\\nsynchronous language developed by [NASA](https://www.nasa.gov) for commanding and monitoring\\nautonomous systems. PLEXIL Formal Interactive Verification Environment\\n(PLEXIL5) is a tool that implements a formal executable semantics of\\nPLEXIL. PLEXIL5 provides a graphical interface that enable access to\\nformal verification techniques such as model-checking, symbolic\\nexecution, theorem proving, and static analysis of plans. The\\ngraphical environment supports formula editing and visualization of\\ncounterexamples, interactive simulation of plans at different\\ngranularity levels, and random initialization of external environment\\nvariables.\\n\\n\\nPLEXIL’s operational semantics has been formally specified and key\\nmeta-theoretical properties of the language, such as determinism and\\ncompositionality, have been\\n[formally verified](https://shemesh.larc.nasa.gov/fm/PLEXIL)  in the [Prototype\\nVerification System](http://pvs.csl.sri.com/) (PVS). This formalization yields a formal\\nexecutable semantics of the language that serves as an efficient\\nformal interpreter and reference implementation of PLEXIL. This formal\\nsemantics is at the core of the verification capabilities of \\nPLEXIL5. The formal analysis capabilities offered by PLEXIL5 are based\\non PLEXIL’s rewriting logic semantics written in the [Maude](http://maude.cs.illinois.edu/) system and verified in\\nPVS.\\n\\nThe graphical user interface has been developed in Java using\\nthe model-view-controller pattern. The object oriented model\\nrepresents the hierarchical structure of plans, their execution\\nbehavior, and the external environment. The view consists of several\\nclasses that present the user with views of the tree-like-structure of\\nplans. The controller consists of a custom controller-facade class and\\nlistener classes using and extending the Java framework.  PLEXIL5\\nsupports a number of input formats defining plans. For this purpose,\\nthe tool links a series of parsers and translators that internally\\n\\n1.  generate the format supported by the rewriting logic semantics of the\\nlanguage implemented in Maude and\\n\\n2. construct an object oriented plan model from Maude’s output.\\n\\n Java and Maude communicate as processes at the operating system’s\\nlevel with help of the Java/Maude Integration API, developed as part\\nof the PLEXIL5 framework.\\n\\n### Current Release\\n\\nPLEXIL5 v0.0 (May-31-2018)\\n\\n### Requirements\\n\\n* [Apache Ant](https://ant.apache.org/)\\n\\n* [Maude Interpreter Binaries](http://maude.cs.uiuc.edu/download/). Copy maude.linux64, maude.linux, maude.intelDarwin to `plexilite` folder.\\n\\n## Compiling and running\\n\\nIn `build` directory:\\n\\n```bash\\n$ ant run\\n```\\n\\n### License\\n\\nThe code in this repository is released under NASA\\'s Open Source\\nAgreement.  See the directory [`LICENSES`](LICENSES); see also the copyright notice at the end of this file. \\n\\n### Authors\\n\\n* [C&eacute;sar A. Mu&ntilde;oz](http://shemesh.larc.nasa.gov/people/cam) (cesar.a.munoz@nasa.gov), NASA Langley Research Center, USA. \\n* Hector Cadavid, Escuela Colombiana de Ingenier&iacute;a, Colombia.\\n* Camilo Rocha, Pontificia Universidad Javeriana de Cali, Colombia.\\n* Marco Feli&uacute;, National Institute of Aerospace, USA. \\n\\n### Copyright Notice\\n\\nCopyright 2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/dictionaries',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': 'This repository contains a collection of NASA thesauri, dictionaries, taxonomies, and related documents, as well as a Python script to load concepts from a SKOS file.\\n\\n**What You\\'ll Find Here**\\n* Dictionaries\\n  * Acronyms\\n    * *Acronym Dictionary* from the NASA Center for AeroSpace Information\\n    * Human Space Flight acronym finder\\n    * Johnson Space Center (JSC) \"Acronym Central\"\\n    * Kennedy Space Center (KSC) list of acronyms\\n  * *Concepts of Mathematics for Students of Physics and Engineering: A Dictionary* by Joseph C. Kolecki\\n  * Planetary Data System Data Dictionary\\n* Taxonomies\\n  * NASA Taxonomy 2.0\\n* Thesauri\\n  * NASA Thesaurus\\n* Python script\\n  * bin/*load_terms_from_skos.py*\\n  * *requirements.txt*\\n\\n\\n## Installation of Python Environment\\n\\nThere are any number of ways to set up your python environment to\\nuse this code. My preferred one (described below) is using\\n[virtualenv](https://pypi.python.org/pypi/virtualenv).\\n\\n```bash\\n# clone this repository to your local machine\\ngit clone https://github.com/nasa/dictionaries.git\\n\\n# switch to the local repository dir\\ncd dictionaries\\n\\n# install virtualenv environment for python3\\nvirtualenv -p <python_3_exe> ./env\\n\\n# activate your environment\\nsource lib/bin/activate.sh\\n\\n# install requirements\\npip install -r requirements.txt\\n\\n#add aggregate dictionary code to python path\\n# there are many ways to do this, but for example\\n# for the bash shell on MacOS/Linux the command is:\\nexport PYTHONPATH=`pwd`\\n\\n```\\n'},\n",
       " {'repo': 'nasa/SingleDop',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': \"SingleDop README\\n----------------\\nSingleDop is a software module, written in the Python programming language, that will retrieve two-dimensional low-level winds from either real or simulated Doppler radar data. It mimics the functionality of the algorithm described in the following reference:\\n- Xu et al., 2006: Background error covariance functions for vector wind analyses using Doppler-radar radial-velocity observations. Q. J. R. Meteorol. Soc., 132, 2887-2904.  \\n\\nThe interface is simplified to a single line of code in the end user's Python scripts, making implementation of the algorithm in their research analyses very easy. The software package also interfaces well with other open source radar packages, such as the [Python ARM Radar Toolkit (Py-ART)](https://github.com/ARM-DOE/pyart). Simple visualization (including vector and contour plots) and save/load routines (to preserve analysis results) are also provided.\\n\\nSingleDop Installation\\n----------------------\\nSingleDop works under Python 2.7 and 3.4 on most Mac/Linux setups. Windows installation and other Python versions are currently untested.\\n\\nTo install:  \\n`python setup.py install`\\n\\nThe following dependencies need to be installed first:\\n\\n- A robust version of Python 2.7 or 3.4 w/ most standard scientific packages (e.g., numpy, matplotlib, scipy, etc.) - Get one for free here: https://store.continuum.io/cshop/anaconda/\\n- [The Python Atmospheric Radiation Measurement (ARM) Radar Toolkit (Py-ART)](https://github.com/ARM-DOE/pyart)\\n- [Python Turbulence Detection Algorithm (PyTDA)](https://github.com/nasa/PyTDA)\\n- [xarray - optional](http://xarray.pydata.org/en/stable/)\\n\\nSpecific import calls in the SingleDop source code:\\n```\\nfrom __future__ import division, print_function\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom matplotlib import cm\\nimport sys\\nimport scipy\\nimport math\\nimport time\\nimport warnings\\nimport pyart\\nfrom pytda import get_sweep_data, get_sweep_azimuths, get_sweep_elevations, \\\\\\n                  flatten_and_reduce_data_array\\nfrom .common import radar_coords_to_cart\\nfrom .cmap_map import lighten_cmap\\ntry:\\n    import xarray\\nexcept ImportError:\\n    warnings.warn(\\n        'xarray not installed, cannot load or save SingleDop datasets')  \\n```\\n\\nUsing SingleDop\\n---------------\\nTo access everything:\\n```\\nimport singledop\\n```\\n\\nA demonstration notebook is in the notebooks directory.\\n\"},\n",
       " {'repo': 'nasa/glm_ql',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# glm_ql\\nquick-look imagery (PNG and GeoTIFF) when provided Level2 Operational GLM (Geostationary Lightning Mapper) data files\\n'},\n",
       " {'repo': 'nasa/CFS_CI',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Command Ingest\\n\\nNASA core Flight System Command Ingest Application\\n\\n## Description\\n\\nCommand Ingest (CI) application is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\nThe Command Ingest (CI) Application is responsible for receiving commands from an external source (such as a ground station) over a transport channel, and to forward the command to the appropriate application over the cFE Software Bus (SB).\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. \\nhttp://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/CFS_TO',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Telemetry Output\\n\\nNASA core Flight System Telemetry Output Application\\n\\n## Description\\n\\nTelemetry Output (TO) application is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at http://cfs.gsfc.nasa.gov\\n\\nThe Telemetry Output (TO) Application is responsible for transmitting telemetry to external destination(s) (such as a ground station) over transport devices(s).\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. \\nhttp://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/TLNS3D',\n",
       "  'language': 'Fortran',\n",
       "  'readme_contents': 'The TLNS3D code was developed to solve Reynolds-averaged Navier-Stokes\\nEquations to simulate turbulent, viscous flows over three-dimensional\\nconfigurations. A general multiblock grid approach is used to model\\ncomplex configurations.  A multi-stage Runge-Kutta pseudo-time stepping\\nscheme is coupled with residual smoothing and multigrid acceleration\\ntechniques to form an efficient algorithm for solving transonic viscous\\nflows over aerodynamic configurations of practical interest.\\n\\nThe TLNS3D framework is licensed under the Apache License, Version 2.0\\n(the \"License\"); you may not use this application except in compliance\\nwith the License. You may obtain a copy of the License at\\nhttp://www.apache.org/licenses/LICENSE-2.0. \\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n'},\n",
       " {'repo': 'nasa/MISR-View',\n",
       "  'language': 'IDL',\n",
       "  'readme_contents': '0123456789012345678901234567890123456789012345678901234567890123456789012\\n==============\\nREADME\\n==============\\nMarch 8, 2018\\n\\nThis document is intended to be a brief guide to setting up and running\\nmisr_view.  For more detailed information, please consult the misr_view\\nuser\\'s guide, located in the \"ug5.3_html\" directory under the \"external\"\\ndirectory or online at https://nasa.github.io/MISR-View/ .\\nThe FAQ below offers additional information.\\n\\nOnce misr_view is unpacked, there should be two top-level directories,\\n\"external\" and \"src\".  The \"src\" directory contains all of the source\\ncode for all of the modules that comprise misr_view.  The \"external\"\\ndirectory contains all files that are needed to operate misr_view.\\nGenerally speaking, the user does not need to recompile the source\\ncode.\\n\\nFor information regarding the history of changes to misr_view, please\\nrefer to the file \"CHANGELOG\".\\n\\nPLEASE READ THE FAQ AT THE END OF THIS DOCUMENT FOR ANSWERS TO COMMON\\nQUESTIONS REGARDING THE INFORMATION CONTAINED HEREIN.\\n\\n=========================================================================\\n\\nOPTION #1\\nOPTION #1\\nOPTION #1\\nOPTION #1\\nOPTION #1\\nOPTION #1\\n\\n\\nSTARTING UP MISR_VIEW WITHOUT A LICENSED VERSION OF IDL\\n\\n=========================================================================\\nA \"licensed version of IDL\" means that you have purchased a copy of IDL\\nfrom Harris Geospatial Solutions, Inc. (http://www.harrisgeospatial.com).\\nIf you have never dealt with this company, you probably do not have a \\nvalid IDL license.\\n\\nIn order to run misr_view without an IDL license, the IDL Virtual Machine\\n(IDL-VM) must be downloaded from the HARRIS website. Due to export \\ncontrol policies, download and installation of IDL (and subsequently the\\nIDL Virtual Machine) requires that you complete the registration and \\nverification process via the HARRIS website.  Once done, click on \\n\\'My Account\\' in the top right-hand corner, navigate to \"Downloads\", and\\nthen \"All Downloads\". \\nSelect the appropriate platform to begin the download and installation \\nprocess.\\nInstallation of the Virtual Machine will not be covered in this document.\\n\\n---------------\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\n---------------\\n(1) Start the IDL-VM by double-clicking the IDL-VM icon or selecting the\\nIDL-VM via the Start button.\\n\\n(2) Dismiss the IDL-VM window that appears by clicking the \\n\"click to continue\" button.\\n\\n(3) When the selection dialog box appears, find and select the\\n\"misr_view.sav\" file and click the \"OK\" button.  \"misr_view.sav\" will\\nbe located in the \"external\" directory.\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n-------------\\nMacintosh OSX\\nMacintosh OSX\\nMacintosh OSX\\nMacintosh OSX\\nMacintosh OSX\\n-------------\\nThere are two ways to invoke the IDL-VM on a Mac, either via a terminal\\nwindow, or by clicking on the IDL-VM icon.  The latter will be covered \\nhere, the former below.\\n\\n(1) Start the IDL-VM by double-clicking the IDL-VM icon.\\n\\n(2) Dismiss the IDL-VM window that appears by clicking the \\n\"click to continue\" button.\\n\\n(3) When the selection dialog box appears, find and select the\\n\"misr_view.sav\" file and click the \"OK\" button.  \"misr_view.sav\" will\\nbe located in the \"external\" directory.\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n-----------------------------\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\n-----------------------------\\nPlease note that it is assumed that the IDL-VM has been configured\\non your system.  If the steps below do not work, most likely the\\nIDL-VM is not set up properly on your system.  Consult with a system\\nadministrator if you have one, or look at the online documentation\\nfor the IDL-VM (http://www.rsinc.com/idlvm/idlvm.pdf).\\n\\n(1) In a terminal window, type:\\n\\nidl -vm\\n\\n(2) Dismiss the IDL-VM window that appears by clicking the \\n\"click to continue\" button.\\n\\n(3) When the selection dialog box appears, find and select the\\n\"misr_view.sav\" file and click the \"OK\" button.  \"misr_view.sav\" will\\nbe located in the \"external\" directory.\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n\\n=========================================================================\\n\\nOPTION #2\\nOPTION #2\\nOPTION #2\\nOPTION #2\\nOPTION #2\\nOPTION #2\\n\\n\\nSTARTING UP MISR_VIEW WITH A LICENSED VERSION OF IDL (PRE-COMPILED CODE)\\n\\n=========================================================================\\nA \"licensed version of IDL\" means that you have purchased a copy of IDL\\nfrom Harris Geospatial Solutions, Inc. (http://www.harrisgeospatial.com).\\nIf you have never dealt with this company, you probably do not have a \\nvalid IDL license.\\nPlease see how to run misr_view without an IDL license (above)... Please\\nnote that misr_view users who have a valid IDL license can run misr_view\\nas unlicensed users do...\\n\\nIf you *do* have a valid IDL license, follow the instructions below for\\nyour specific operating system if you want to invoke misr_view from\\npre-compiled code:\\n\\n---------------\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\n---------------\\n(1) Start up IDL.\\n\\n(2) Make the \"external\" directory the current active directory.  For\\nexample, if the desired current directory is \\nC:\\\\misr_stuff\\\\misr_view\\\\external\\\\, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'C:\\\\misr_stuff\\\\misr_view\\\\external\\\\\\'\\n\\n(3) At the IDL command line, type the following:\\n\\nRESTORE,\\'misr_view.sav\\'\\n\\n(4) Then, simply type:\\n\\nmisr_view\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n------------------------------\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\n------------------------------\\n(1) Start up IDL in either command-line mode or the IDL Development \\nInterface, \"idlde\".\\n(2) Make the \"external\" directory the current active directory.  For\\nexample, if the desired current directory is \\n/home/misr/misr_view/external/, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'/home/misr/misr_view/external/\\'\\n\\n(3) At the IDL command line, type the following:\\n\\nRESTORE,\\'misr_view.sav\\'\\n\\n(4) Then, simply type:\\n\\nmisr_view\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n\\n=========================================================================\\n\\nOPTION #3\\nOPTION #3\\nOPTION #3\\nOPTION #3\\nOPTION #3\\nOPTION #3\\n\\n\\nSTARTING UP MISR_VIEW WITH A LICENSED VERSION OF IDL (SOURCE CODE)\\n\\n=========================================================================\\nA \"licensed version of IDL\" means that you have purchased a copy of IDL\\nfrom Harris Geospatial Solutions, Inc. (http://www.harrisgeospatial.com).\\nIf you have never dealt with this company, you probably do not have a \\nvalid IDL license.\\nPlease see how to run misr_view without an IDL license (above)... Please\\nnote that misr_view users who have a valid IDL license can run misr_view\\nas unlicensed users do...\\n\\nIf you *do* have a valid IDL license, follow the instructions below for\\nyour specific operating system if you want to compile and run misr_view\\nfrom source code:\\n\\n---------------\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\nWindows XP/2000/7/Vista\\n---------------\\n(1) Start up IDL.\\n\\n(2) Make the \"src\" directory the current active directory.  For\\nexample, if the desired current directory is \\nC:\\\\misr_stuff\\\\misr_view\\\\src\\\\, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'C:\\\\misr_stuff\\\\misr_view\\\\src\\\\\\'\\n\\n(3) At the IDL command line, type the following:\\n\\n.compile compile_all\\nRESOLVE_ALL\\n\\n(4) Make the \"external\" directory the current active directory.  For\\nexample, if the desired current directory is \\nC:\\\\misr_stuff\\\\misr_view\\\\external\\\\, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'C:\\\\misr_stuff\\\\misr_view\\\\external\\\\\\'\\n\\n(5) Then, simply type:\\n\\nmisr_view\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n-----------------------------\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\nUNIX/Linux/Macintosh OSX/IRIX\\n-----------------------------\\n(1) Start up IDL in either command-line mode or the IDL Development \\nInterface, \"idlde\".\\n(2) Make the \"src\" directory the current active directory.  For\\nexample, if the desired current directory is \\n/home/misr/misr_view/src/, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'/home/misr/misr_view/src/\\'\\n\\n(3) At the IDL command line, type the following:\\n\\n.compile compile_all\\nRESOLVE_ALL\\n\\n(4) Make the \"external\" directory the current active directory.  For\\nexample, if the desired current directory is \\n/home/misr/misr_view/external/, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'/home/misr/misr_view/external/\\'\\n\\n\\n(5) Then, simply type:\\n\\nmisr_view\\n\\nThe data selection interface and main console should appear on the\\nscreen.\\n\\n\\n-------------------------------------------------------------------------\\n   FFFFFFFFFF     AAAAAAAAAAA     QQQQQQQQQQ\\n   FFFFFFFFFF     AAAAAAAAAAA     QQQQQQQQQQ\\n   FF             AA       AA     QQ      QQ\\n   FF             AA       AA     QQ      QQ\\n   FFFFFF         AAAAAAAAAAA     QQ      QQ\\n   FFFFFF         AAAAAAAAAAA     QQ      QQ\\n   FF             AA       AA     QQ      QQ\\n   FF             AA       AA     QQ      QQ\\n   FF             AA       AA     QQQQQQQQQQQQ\\n   FF             AA       AA     QQQQQQQQQQQQQ\\n                                             QQ\\n-------------------------------------------------------------------------\\n\\n\\n=========================================================================\\n>>> QUESTION: How do I open and access the misr_view user\\'s guide?\\n\\n>>> ANSWER: First, you must have a common browser available, such as\\nNetscape, Firefox, Internet Explorer, Mozilla, or Safari.  The assumption\\nhere is that the user knows how to start up the browser application.\\nOnce the browser is active, use the \"Open File...\" or \"Open...\" menu item\\nunder the \"File\" menu to open the file \"misr_view_ug5.3.html\", which is\\nlocated in the \"ug5.3_html\" directory under the \"external\" directory.\\n=========================================================================\\n\\n=========================================================================\\n>>> QUESTION: Are there any differences in functionailty when running\\nmisr_view without a valid IDL license?\\n\\n>>> ANSWER: Yes.  When running misr_view with the IDL-VM, users will not\\nbe able to modify or create transform files.  Some transform files are\\nprovided with misr_view, and these should be sufficient for most users.\\nThe transform interface is described in detail in the misr_view user\\'s\\nguide.\\n=========================================================================\\n\\n=========================================================================\\n>>> QUESTION: How does one make a directory \"active\" on a PC so that a \\nlicensed IDL user can run misr_view?\\n\\n>>> ANSWER: There are several ways to do this; look in the IDL documents\\nto see how to add directories to a search list.  Another way to do this\\nis by using the IDL command \"CD\".  For example, if the desired current\\ndirectory is C:\\\\misr_stuff\\\\misr_view\\\\external\\\\, the following command\\nwould be entered at the IDL command line:\\n\\nCD,\\'C:\\\\misr_stuff\\\\misr_view\\\\external\\\\\\'\\n=========================================================================\\n0123456789012345678901234567890123456789012345678901234567890123456789012\\n\\n'},\n",
       " {'repo': 'nasa/mplStyle',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '.. _plot2d_styles:\\n\\nPlotting Styles\\n===============\\n\\nThe style system allow you to customize the look of your plots easily\\nand to maintain a consistent style on a large number of plots.  Styles\\nare used to control how plot elements look (color, marker style, line\\nstyle, font, etc.) and not how the plot is structured (line\\nvs. marker, which tick formatter to use, etc.).\\n\\nThey can be combined and chained together hierarchically.  They can be\\nedited programmaticaly via the python shell, or in a text editor.\\n\\nAll of these examples assume that you have imported the plot and style\\nmanager into your script:\\n\\n.. code-block:: python\\n\\n   import pylab\\n   from mplStyle import mgr as smgr\\n\\n\\nBackground and Problems\\n=======================\\n\\nThis style system was extracted from a much larger Python project and\\nso the current naming conventions, coding style, and package structure\\nis not ideal.  The conventions were those of the larger project and\\nhave not been updated yet to be PEP8/MPL compliant.  The various\\ninternal dependencies were extracted from the larger system as well so\\nthe current package structure could be made a lot simpler.  This\\noverview document was also extracted from a larger custom Sphinx\\ndocumentation project and needs to be updated to work properly as a\\nstandalone RST document.\\n\\nIf there is sufficient interest in this kind of style system, the hope\\nis that it can modified into a standard MPL package and delivered with\\nMPL.  At that point these style, documentation, and test conventions\\ncan be addressed.\\n\\n------------------------------------------------------------------------\\n\\n.. _plot2d_styles_overview:\\n\\nOverview\\n--------\\n\\nThe style system has two primary user interfaces: the style manager class,\\nand the style class.  The style manager is responsible for creating styles,\\napplying styles to plots, saving and loading styles, and remembering\\nwhich styles were applied to which plots so they can be automatically\\nupdated.\\n\\nThe style class is responsible for storing the set of properties to\\nchange in the plot.  Styles have a variety of display parameters,\\napplying to different parts of the plot.  Each parameter can be set or\\nunset.  When the style is applied to a plot, those parameters which\\nare set will be implemented; those parameters which are unset will be\\nignored.\\n\\nThe structure of a style reflects the structure of a figure: in a figure,\\nthere are usually  several contained objects (for example, an Axes object, \\na Line object, etc.).  Likewise, in a style the parameters are organized by\\nthe type of object they affect: there are portions that affect only Axes\\nobjects, only Line objects, etc.\\n\\nThe main style structure has some default attributes (bgColor,\\nfgColor, line, patch, text) which apply to any appropriate plot\\nelement.  If you set a property in the text attribute of a style, it\\nwill apply to any text in the plot (which is a nice way to control\\nfonts, colors, etc).  The figure and axes attributes are used to\\ncontrol specific parts of the plot (e.g. axes.xAxes applies just to\\nthe X axis and axes.yAxes applies just to the Y axes).\\n\\nThe basic MplStyle structure looks like this:\\n\\n      +--------------+-----------------------------+-------------------------------------+\\n      | **Property** | **Type**                    | **Description**                     |\\n      +==============+=============================+=====================================+\\n      | axes         | `Axes <doc/axes.rst>`__     | Controls how to style an Axes and   |\\n      |              |                             | all it\\'s components.                |\\n      +--------------+-----------------------------+-------------------------------------+\\n      | bgColor      | `Color <doc/color.rst>`__   | The default background color to use |\\n      |              |                             | for a sub-style if none is          |\\n      |              |                             | is specified (i.e. the default      |\\n      |              |                             | background color).                  |\\n      +--------------+-----------------------------+-------------------------------------+\\n      | fgColor      | `Color <doc/color.rst>`__   | The default foreground color to use |\\n      |              |                             | for a sub-style if none is          |\\n      |              |                             | is specified (i.e. the default      |\\n      |              |                             | foreground color).                  |\\n      +--------------+-----------------------------+-------------------------------------+\\n      | figure       | `Figure <doc/figure.rst>`__ | Controls how to style a Figure.     |\\n      +--------------+-----------------------------+-------------------------------------+\\n      | line         | `Line <doc/line.rst>`__     | This specifies the style properties |\\n      |              |                             | for line elements plotted on an     |\\n      |              |                             | Axes.                               |\\n      +--------------+-----------------------------+-------------------------------------+\\n      | patch        | `Patch <doc/patch.rst>`__   | This specifies the style properties |\\n      |              |                             | for patch elements plotted on an    |\\n      |              |                             | Axes.                               |\\n      +--------------+-----------------------------+-------------------------------------+\\n      | text         | `Text <doc/text.rst>`__     | The default text style to use.      |\\n      |              |                             | Sub-style elements may override for |\\n      |              |                             | specific pieces of text.            |\\n      +--------------+-----------------------------+-------------------------------------+\\n\\n.. note::\\n\\n   The full list of available parameters can be found here:\\n   `Full List <doc/style_all.rst>`__ \\n\\n------------------------------------------------------------------------\\n\\n.. _plot2d_styles_create:\\n\\nStyle Creation\\n--------------\\n\\nStyles are created by calling the manager create method with the name\\nof the style.  Once the style has been created, set the parameters on\\nthe style using the names of the style attributes (see the top level\\nstyle structure above for the possible names)\\n\\n.. code-block:: python\\n\\n   # Create the style\\n   style = smgr.create( \"Big Title\" )\\n\\n   # Change the axes title font.\\n   style.axes.title.font.family = \"sans-serif\"\\n   style.axes.title.font.size = 24\\n\\nYou can also pass a dictionary of style attributes to the create\\nmethod to create and initialize the style in one call.  The keys of\\nthe dictionary are the string form of the variable path: so the\\nvariable style.this.parameter.path becomes the dictionary key\\n\\'this.parameter.path\\'.  The value associated with each key needs to be\\nof the proper type for the parameter; some require floating point\\nvalues, integers, booleans, or other strings.\\n\\n.. code-block:: python\\n\\n   # Create the style\\n   style = smgr.create( \"Big Title\", {\\n                        \"axes.title.font.family\" : \"sans-serif\",\\n                        \"axes.title.font.size\" : 24,\\n                        } )\\n\\n\\nWhen defining a style, you can optionally name a parent style.  When the \\nstyle is applied, the parent style is automatically applied first.  This\\nmeans that a child style will overwrite the settings contained in the\\nparent style, if and when those styles conflict.\\n\\n.. code-block:: python\\n\\n   # Create the style to make fonts larger and easier to read.\\n   s1 = smgr.create( \"Big Text\" )\\n   s1.text.font.scale = 1.5\\n\\n   # Create a new style, with a parent style   \\n   s2 = smgr.create( \\'Presentation\\', parent=\\'Big Text\\' )\\n\\n   # Set something particular to the child style\\n   s2.figure.width = 800\\n   s2.figure.height = 600\\n\\n   # Big Text will be applied before the other parts of Presentation\\n   smgr.apply( fig, \\'Presentation\\' )\\n\\nIt should be noted that in the above example the \\'Big Text\\' style is\\n*not* overwritten when we create the \\'Presentation\\' style.  This is\\nbecause when we create styles in this manner, they are automatically\\nregistered and stored in a style manager class.  They can then be\\naccess later by name.\\n\\n\\nSetting Attributes\\n------------------\\n\\nEach Style object has a set of parameters affecting how plots are\\ndisplayed.  The parameters are unset by default; they will not affect\\nthe display of a plot unless they are set.  In an existing style\\nobject, you can use Python\\'s dot syntax to access and set parameters.\\n\\nTo access an already defined style, use the find() method on the manager\\n\\n.. code-block:: python\\n\\n   # Find a previous define dstyle\\n   style = smgr.find( \"Big Text\" )\\n\\n   # Change some of the style attributes\\n   style.text.font.size = 16\\n\\n\\nApplying Styles to a Plot\\n-------------------------\\n\\nStyles can be applied to any matplotlib plot element (figures, axes,\\naxis, lines, etc).  Applying the style to the figure is most common\\nuse case.  When you apply a style to a figure, it will search the\\nfigure for various plot elements and axes and recursively apply the\\nsame style to them (the same is true when calling apply only on the\\naxes).\\n\\nThe style attributes dictate which matplotlib elements are modified.\\nSo the attribute axes.bgColor will only change the color of the axes\\nwhile the attribute figure.bgColor will change the background for the\\nfigure.\\n\\nStyle are applied using the apply method on the manager.  You can pass\\nin the style object or the name of the style to apply.\\n\\n.. code-block:: python\\n\\n   fig, ax = pylab.subplots()\\n   lines = ax.plot( [1, 2, 3, 4, 5], [2, 1, 0, 1, 2] )\\n   ax.set_title( \"This is the Title\" )\\n   \\n   # Create the style and set some fields\\n   style = smgr.create( \"Big Title\" )\\n   style.axes.title.font.family = \"sans-serif\"\\n   style.axes.title.font.size = 24\\n\\n   # Apply the style to the figure.\\n   smgr.apply( fig, style )\\n\\n   # Apply a list of styles to just the lines.\\n   smgr.apply( lines, [ \"Dashed Lines\", \"Blue Lines\", \"Bold Lines\" ] )\\n\\nThe style manager will recursively walk down through the input plot\\nelement and apply the style.  So if a plot contains four subplots, the\\naxes style will be applied to each of the four subplots and the text\\nstyle will apply to all the text in the plot.  If you want to apply\\nthe style only the input object (say an input figure), pass\\nrecurse=False to the apply method.\\n\\n.. code-block:: python\\n\\n   # Apply the style only to the figure\\n   smgr.apply( fig, \\'Figure Style\\', recurse=False )\\n\\n\\nUpdating and Re-applying Styles\\n-------------------------------\\n\\nWhen the manager applyes a style to the figure (or to any other other\\nplotting element), the manager will remember what style was applied to\\nwhich element, so if you later modify any styles, the changes can be\\nautomatically applied to the plot elements by calling the reapply\\nmethod.\\n\\n.. code-block:: python\\n\\n   # Modify the style\\n   style.axes.title.font.size = 16\\n\\n   # Apply the update to everything that is currently using it.\\n   smgr.reapply()\\n\\nThis will change the fonts from size 24 (the original \"Big Title\"\\nsize) to the new size of 16 and update the plot.  The reapply() method\\nwill update any and all plots that have styes applied to them.\\n\\n\\nSaving & Loading\\n----------------\\n\\nThe style manager can be used to save and load styles to a persistent\\nform.  Each style is saved into a file with the form\\n\\'Style_Name.mplstyle\\'.  Style files are human readable, Python files\\nand may be edited by the user.  Styles are NOT automatically saved and\\nloaded by the manager (though that could change based on user\\nfeedback).\\n\\n.. note::\\n\\n   Style names including a space \\' \\' will be changed to use an\\n   underscore \\'_\\' when saved as a .mplstyle file.  For Example, \\n   a style named \"DSS 16\" will be saved as \"DSS_16.mplstyle\".\\n\\nTo save the current set of styles, use the manager save method.  To\\nload all the available styles, use the load method.\\n\\n.. code-block:: python\\n\\n   # Save the current styles to $HOME/.masar/styles\\n   smgr.save()\\n\\n   # Save the current styles to the local directory.\\n   smgr.save( \".\" )\\n\\n   # Load all available styles.\\n   smgr.load()\\n\\nWhen loading styles, the manager will use a search path that looks for\\nstyles in the following order (high priority to low priority):\\n\\n#. The current directory.\\n#. The user\\'s home directory: $HOME/.matplotlib/styles/\\n\\nStyles that are defined in more than one of these locations will use\\nthe first definition.  This way, each user can override and customize\\ncertain Monte styles to their liking; they can also use different\\ndirectories to try out different style options in parallel.  You can\\nchange the list of directories to look in by modifying your STYLEPATH\\nenvironment variable.\\n\\nYou can also manipulate the loading and saving of styles in your\\nPython script directly.  The \"path\" variable on the style manager is a\\nsimple Python list of directory names.  By changing the path, you can\\nchange what styles are loaded:\\n\\n.. code-block:: python\\n\\n   # Add a search path and load the styles.\\n   smgr.path.append( \"/proj/scripts/styles\" )\\n   smgr.load()\\n\\n\\nTagging Plot Elements\\n---------------------\\n\\n.. _plot2d_styles_tags:\\n\\nTagging or style tags are way to filter which plot elements (figure,\\naxes, lines, etc) a style is applied to by setting a tag (string name)\\nto a plot element.  The script that creates the plot tags each element\\nwith a name.  When a style is applied to an element, the tag input can\\nbe specified to limit which elements get changed.\\n\\nLet\\'s say you have a plot that shows two lines for each DSN complex\\n(Goldstone, Canberra, and Madrid).  The plotting script has access to\\nthose lines and knows which complex they are a part of but the lines\\nare hard to get to after the plotting script is finished.  If the\\nplotting script tags the lines with the complex name like this:\\n\\n.. code-block:: python\\n\\n   def createPlot():\\n      fig, ax = pylab.subplots()\\n      # create data to plot, layout plot, etc.\\n\\n      l = ax.plot( gldX, gldY )\\n      smgr.tag( l, \"Goldstone\" )\\n\\n      l = ax.plot( madX, madY )\\n      smgr.tag( l, \"Madrid\" )\\n\\n      l = ax.plot( canX, canY )\\n      smgr.tag( l, \"Canberra\" )\\n\\n      return fig\\n\\nThe calling script can use those tags to apply styles to the\\nindividual lines without having direct access to them.  Both the\\napply() and set() functions can use the tag keyword to filter which\\nelements are used.\\n\\n.. code-block:: python\\n\\n   fig = createPlot()\\n\\n   # Apply the \\'Goldstone Style\\' to elements tagged Goldstone\\n   smgr.apply( fig, \"Goldstone Style\", tag=\"Goldstone\" )\\n\\n   # Change every line tagged Canberra to be blue.\\n   smgr.set( fig, { \\'line.color\\' : \\'blue\\' }, tag=\"Canberra\" )\\n\\nTags are a powerful tool that allows you to write complicated plotting\\nscripts and then control individual elements in those plots using\\nstyles from outside the plotting script.\\n\\n\\n.. _plot2d_styles_unmanaged:\\n\\nSetting Attributes and Unmanaged Styles\\n---------------------------------------\\n\\nThe style system can also be used to quickly set plot attributes\\nwithout creating a style by calling the manager set() method.  This\\nmethod can accept either a single style attribute or a dictionary of\\nstyle attributes and can use the tag system to filter which plot\\nelements are set.\\n\\n.. code-block:: python\\n\\n   # Change the background color to black.\\n   smgr.set( fig, \"bgColor\", \"black\" )\\n\\n   # Change the multiple attributes.\\n   smgr.set( fig, { \"bgColor\" : \"black\",\\n                    \"fgColor\" : \"white\",\\n                    \"text.font.scale\" : 1.25 } )\\n\\n   # Change lines tagged \\'DSS 14\\' to gold.\\n   smgr.set( fig, \"line.color\", \"gold\", tag=\"DSS 14\" )\\n\\nAn \"unmanaged\" style can be created using the style constructor and\\napplied directly to a plot.  The style manager will have no knowlege\\nof this style and so reapply will not work, and the style will not be\\nsaved.\\n\\n.. code-block:: python\\n\\n   import mpy.plot.style as S\\n\\n   # Unmanaged style - won\\'t be saved.\\n   style = S.MplStyle( \\'dummy\\' )\\n\\n   # Must use style.apply(), smgr.apply() won\\'t work.\\n   style.apply( fig )\\n\\n\\n------------------------------------------------------------------------\\n\\n.. _plot2d_styles_example:\\n\\nAn Example\\n----------\\n\\nFollowing is a more complete example on how to make the plot at the top of\\nthis page:\\n\\n.. code-block:: python\\n\\n   # import some modules\\n   import pylab\\n   from mplStyle import mgr as smgr\\n\\n   # create the plot\\n   fig, ax = pylab.subplots()\\n\\n   xdata = [ 1, 1.5,  2, 2.5,  3, 3.5,  4, 4.5,  4.75, 5 ]\\n   ydata = [ 1, 1.75, 2, 2.75, 3, 2.75, 2, 2.25, 2.75, 3 ]\\n   line = ax.plot( xdata, ydata )\\n\\n   rect = mpylab.Rectangle( (2.8, 1.0), 0.4, 1.2 )\\n   ax.add_patch( rect )\\n\\n   figTitle = fig.suptitle( \"Figure Title\" )\\n   axTitle = ax.set_title( \"Axes Title\" )\\n   xLabel = ax.set_xlabel( \"X-Axis Label\" )\\n   yLabel = ax.set_ylabel( \"Y-Axis Label\" )\\n\\n   figText = fig.text( 0.02, 0.02, \"FigureText\" )\\n   txt = ax.text( 4.2, 1.1, \"Text\" )\\n\\n   # Create the style\\n   style = smgr.create( \"My Style\" )\\n   style.bgColor = \\'white\\'\\n   style.fgColor = \\'black\\'\\n   # Figure\\n   style.figure.width = 10\\n   style.figure.height = 10\\n   # Axes\\n   style.axes.axisBelow = True\\n   style.axes.leftEdge.color = \\'magenta\\'\\n   style.axes.leftEdge.width = 5\\n   style.axes.leftEdge.style = \\'--\\'\\n   style.axes.bottomEdge.color = \\'magenta\\'\\n   style.axes.bottomEdge.width = 5\\n   style.axes.bottomEdge.style = \\'dashed\\'\\n   style.axes.topEdge.visible = False\\n   style.axes.rightEdge.visible = False\\n   style.axes.title.font.scale = 2.0\\n   style.axes.title.font.family = \\'sans-serif\\'\\n   # X-Axis\\n   style.axes.xAxis.autoscale = True\\n   style.axes.xAxis.dataMargin = 0.1\\n   style.axes.xAxis.label.font.scale = 1.2\\n   style.axes.xAxis.majorTicks.labels.font.scale = 0.75\\n   style.axes.xAxis.majorTicks.marks.visible = True\\n   style.axes.xAxis.majorTicks.grid.visible = True\\n   style.axes.xAxis.majorTicks.grid.color = \\'#B0B0B0\\'\\n   style.axes.xAxis.majorTicks.grid.width = 1.5\\n   style.axes.xAxis.majorTicks.grid.style = \\':\\'\\n   style.axes.xAxis.majorTicks.length = 15.0\\n   style.axes.xAxis.majorTicks.width = 1.5\\n   style.axes.xAxis.minorTicks.marks.visible = True\\n   style.axes.xAxis.minorTicks.grid.visible = True\\n   style.axes.xAxis.minorTicks.grid.color = \\'#B0B0B0\\'\\n   style.axes.xAxis.minorTicks.grid.width = 0.5\\n   style.axes.xAxis.minorTicks.grid.style = \\':\\'\\n   style.axes.xAxis.minorTicks.length = 5.0\\n   style.axes.xAxis.minorTicks.width = 0.5\\n   # Y-Axis\\n   style.axes.yAxis = style.axes.xAxis.copy()\\n   # Lines\\n   style.line.color = \"blue\"\\n   style.line.style = \\'dash-dot\\'\\n   style.line.width = 1.5\\n   style.line.marker.color = \\'red\\'\\n   style.line.marker.edgeColor = \\'green\\'\\n   style.line.marker.edgeWidth = 3\\n   style.line.marker.size = 20\\n   style.line.marker.style = \\'circle\\'\\n   style.line.marker.fill = \\'bottom\\'\\n   # Patches\\n   style.patch.color = \\'gold\\'\\n   style.patch.filled = True\\n   style.patch.edgeColor = \\'purple\\'\\n   style.patch.edgeWidth = 5\\n   # Text\\n   style.text.lineSpacing = 1.0\\n   style.text.font.size = 12\\n   style.text.font.family = \\'monospace\\'\\n\\n   # apply the style\\n   smgr.apply( fig, style )\\n'},\n",
       " {'repo': 'nasa/EADINLite',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': \"# EADINLite\\nEADIN_Lite Network Protocol \\n \\nAuthored by Eliot Aretskin-Hariton: eliot.d.aretskin-hariton@nasa.gov, [Intelligent Control and Autonomy Branch](http://www.grc.nasa.gov/WWW/cdtb/index.html),\\nNASA Glenn Research Center,\\n\\n\\n**Network Protocol Summary Stats:**\\n* Half-Duplex & Hard Real-Time, works with Wired or Wireless networks\\n* Command / Response protocol using 1 Master / Multiple Slave Architecture\\n* 8 Byte payload\\n* RTT Performance (See Table and Note below)  \\n      \\n|  Speed       |  TYPICAL        | WORSE CASE      |\\n|:------------:|:---------------:|:---------------:|\\n| 4000000 baud |     943 +/- 13  |   981 +/- 13    |\\n|  921600 baud |   1,197 +/- 15  |  1,280 +/-  7   |\\n|  115200 baud |   4,467 +/- 12  |  4,907 +/-  7   |\\n|    9600 baud |  45,798 +/- 12  | 50,750 +/- 20   |\\n|     units    | (micros 1-sigma)|(micros 1-sigma) |\\n\\nNote: Performance based on message Round Trip Time (RTT), which includes\\nformulation of the message by the master, receipt of message by slave\\nand recept of respons from slave by master. master -> slave -> master. \\nTime is expressed in microseconds.\\n\\n* Memory Requirements\\n\\n| Memeory Req. | Program Storage (Bytes) | Dynamic Memory (Bytes) |\\n|:------------:|:-----------------------:|:----------------------:|\\n| Default      | 9,496                   |    900                 |\\n| Minimum      | 8,604                   | 388                    |\\n\\n* Successfull Deployment\\n\\t* Arduino Yun\\n\\t* Arduino Mega 2560 (the $10 version)\\n* Expected Compatibility\\n\\t* Arduino Pro Micro\\n\\t* Any Arduino with Hardware Serial ports\\n\\n**Overview:**\\nThis code was created to support the Distributed Engine Control task\\nas part of the Fixed Wing Aeronautics project. The purpose of this research \\nwas to enable multiple microcontrollers to speak with eacho ther per the\\nprotocol specified in the preliminary release of EADIN BUS. EADIN BUS is a \\ncandidate for distributed control systems on aircraft engines and is being\\nworked on by the Distributed Engine Control Working Group (DECWG) \\nhttp://www.decwg.org/. The primary use of this code was to assist in the \\nmodelling of small local networks which contain 16 or fewer nodes. \\nUltimately, we expect this network to be implemented in an FPGA or ASIC \\nas opposed to it's current implementation on a microcontroller. \\n\\nThis communication protocol uses a master node which distributes \\ninformation between nodes through a call and response system. The RS-485 \\nnetwork is simplex and thus does not allow multiple nodes to talk at \\nthe same time. No time synchronization between nodes is required for \\nthis network. These factors enable the master to request information \\nfrom sensors and command actuators, one at a time. In the current \\nimplementation, no information is passed from individual nodes without \\nfirst going through the master node. \\n\\nWhile other communication protocols do exist like ModbusMaster and simple-modbus,\\nthe speed of these communication protocols on the RS-485 network was not \\nsufficient for our needs which required message send to reply receipt times\\nof 1 millisecond. Additionally, the other protocols did not implement the \\nsame message system as specified by the preliminary documents regarding\\nthe EADIN protocol.\\n\\n**Details:**\\nThe EADIN protocal as implemented by this code has the following structure:\\nTotal Size: 18 bytes\\n* Preamble: 3 bytes\\n\\t* 2 bytes start of message (0x00, 0x01)\\n\\t* 1 byte a sync  byte (0x55)\\n* Headder: 5 bytes \\n\\t* 1 byte request type (ENQ/ACK/NAK = 0x05/0x06/0x15)\\n\\t* 1 byte node_ID of desination\\n\\t* 1 byte node_ID of sender\\n\\t* 1 byte unique message number\\n\\t* 1 byte extra space to be used in future development\\n* Data: X bytes (8 bytes Default)\\n\\t* 8 bytes DATA_L (can be modified)\\n* Footer: 2 bytes \\n\\t* 2 bytes CRCFast (a 16 bit CRC, Default)\\n\\n\\n**Updates:**\\nVersion 2 is incompatible with previous versions as it is constructed with \\ndifferent function calls using an object oriented programming approach for\\neasier use. The code now contains built in timing functions which should\\nenable the user to simply call OBJ.read() OBJ.write() functions without worrying\\nabout inserting delays between the write and read operations. These delays\\nshould scale with network speed selected from 9600 - 4000000 baud. \\n\\n**References:**\\n[EADIN Lite Communication Network](http://www.techbriefs.com/component/content/article/ntb/tech-briefs/electronics-and-computers/23450)\\n\\nEliot Aretskin-Hariton, [Benchmarking Variants of a Hardware-in-the-Loop Simulation System](http://arc.aiaa.org/doi/abs/10.2514/6.2016-1425)\\n\\nEliot Aretskin-Hariton, [A Modular Framework for Modeling Hardware Elements in Distributed Engine Control Systems](http://arc.aiaa.org/doi/abs/10.2514/6.2014-3530)\\n\\nDennis Culley, [Developing an Integration Infrastructure for Distributed Engine Control Technologies](http://arc.aiaa.org/doi/abs/10.2514/6.2014-3532)\\n\\nRoss N. Williams, [A painless guide to CRC Error Detection Algorithms](http://www.ross.net/crc/download/crc_v3.txt)\"},\n",
       " {'repo': 'nasa/GFR',\n",
       "  'language': 'Fortran',\n",
       "  'readme_contents': '# GFR\\nGFR (Glenn Flux Reconstruction) is a high-order computational fluid dynamics\\n(CFD) Fortran code for large-eddy simulations. It is based on the simple and\\nefficient flux reconstruction method and accurate to arbitrary order through a\\nuser-supplied input parameter. It is currently capable of using unstructured\\ngrids containing quadrilateral and hexahedra elements.\\n'},\n",
       " {'repo': 'nasa/mct',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '\\n### _The desktop client is no longer under active development, as our development efforts are now focused on [Open MCT](https://nasa.github.io/openmct) for the web and mobile devices._\\n\\nOpen MCT Desktop\\n--\\n\\nThe [MCT](https://sites.google.com/site/openmct/) project was developed at the NASA Ames Research Center for use in spaceflight mission operations, but is equally applicable to any other data monitoring and control application.\\n\\nGetting Started\\n--\\n1. MCT is built using Maven (Java SE6), so start by downloading [maven 2.2.1 or greater](http://maven.apache.org/download.html)\\n2. Clone the git repository `git clone https://github.com/nasa/mct.git` into a local folder (referred to as `MCT_HOME`).\\n3. Run `mvn -N install` from the `MCT_HOME/superpom` directory.\\n4. Run `mvn clean install -Dmaven.test.skip=true -Ddistribution` from the `MCT_HOME/platform-assembly` directory.\\n   1. If Maven complains about missing dependencies org.eclipse:equinox-osgi:jar:3.5.1 or org.eclipse:equinox-osgi-services:jar:3.2.0, download the JARs for the two plugins from http://archive.eclipse.org/equinox/drops/R-3.5.1-200909170800/index.php.  Then follow the instructions Maven provides for installing the JARs.\\n5. The platform distribution archive can be found in the `MCT_HOME/platform-assembly/target` directory.\\n6. Extract the distribution archive, i.e. `mct-platform-1.8b4-dist.tar.gz` to the directory you wish to install MCT.\\n   The subdirectory `mct-platform-1.8b4` will be created from the archive (referred to as `MCT_DIST`).\\n7. Run `MCT.jar` from the extracted MCT directory. On most systems, this can be done with a double-click from a file browser; from the command line, `java -jar MCT.jar`\\n\\nWorking on MCT\\n--\\n[Work on MCT in Eclipse](https://github.com/nasa/mct/wiki/How-to-build-and-run-MCT-in-Eclipse)\\n\\n[Building a MySQL database](https://github.com/nasa/mct/wiki/Creating-a-MySQL-database-for-MCT)\\n\\n[Using a Derby database](https://github.com/nasa/mct/wiki/Using-Derby-in-MCT)\\n\\n[Contributing to MCT](https://github.com/nasa/mct/wiki/Contributing-to-MCT)\\n'},\n",
       " {'repo': 'nasa/openmct-heatmap',\n",
       "  'language': 'JavaScript',\n",
       "  'readme_contents': '# Open MCT Heatmap\\n\\nA plugin for [Open MCT](https://nasa.github.io/openmct)\\nadding heat map style visualizations of telemetry data.\\n\\n## Build\\n\\n```bash\\n$ git clone https://github.com/VWoeltjen/openmct-heatmap.git\\n$ cd openmct-heatmap\\n$ npm install\\n```\\n\\nA UMD module with associated source maps will be written to the\\n`dist` folder. When installed as a global, the plugin will be\\navailable as `HeatmapPlugin`.\\n\\n## Usage\\n\\nSee [`index.html`](index.html) for an example of use.\\n\\n## Developer Environment\\n\\nFollow build instructions, then trigger a build of `openmct`:\\n\\n```bash\\ncd node_modules/openmct\\nnpm install\\ncd ../..\\n```\\n\\nTo serve the application, use `webpack-dev-loader`:\\n\\n```bash\\nnpm install -g webpack webpack-dev-loader\\nwebpack-dev-loader\\n```\\n\\nThere is an example `index.html` included which provides\\na basic instance of Open MCT with this plugin installed for development\\npurposes.\\n'},\n",
       " {'repo': 'nasa/SC',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Stored Command\\n\\nNASA core Flight System Stored Command Application\\n\\n## Description\\n\\nThe Stored Command application (SC) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\nThe SC application allows a system to be autonomously commanded 24 hours a day using sequences of commands that are loaded to SC. Each command has a time tag associated with it, permitting the command to be released for distribution at predetermined times. SC supports both Absolute Time tagged command Sequences (ATSs) as well as multiple Relative Time tagged command Sequences (RTSs).\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/MM',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# Memory Manager\\n\\nNASA core Flight System Memory Manager Application\\n\\n## Description\\n\\nThe Memory Manager application (MM) is a core Flight System (cFS) application that is a plug in to the Core Flight Executive (cFE) component of the cFS.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\nThe MM application is used for the loading and dumping system memory. MM provides an operator interface to the memory manipulation functions contained in the PSP (Platform Support Package) and OSAL (Operating System Abstraction Layer) components of the cFS. MM provides the ability to load and dump memory via command parameters, as well as, from files. Supports symbolic addressing.\\n\\nMM requires use of the [cFS application library](https://github.com/nasa/cfs_lib).\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/cfs_lib',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '# cfs_lib\\n\\nNASA core Flight System Application Library\\n\\n## Description\\n\\nThe cFS Application Library (cfs_lib) is a core Flight System (cFS) library provides functions used by several cFS applictions.\\n\\nThe cFS is a platform and project independent reusable software framework and set of reusable applications developed by NASA Goddard Space Flight Center. This framework is used as the basis for the flight software for satellite data systems and instruments, but can be used on other embedded systems. More information on the cFS can be found at [http://cfs.gsfc.nasa.gov](http://cfs.gsfc.nasa.gov)\\n\\n## License\\n\\nThis software is licensed under the NASA Open Source Agreement. http://ti.arc.nasa.gov/opensource/nosa\\n'},\n",
       " {'repo': 'nasa/LH2Sim',\n",
       "  'language': 'Matlab',\n",
       "  'readme_contents': '# LH2Sim\\n\\nMatlab simulation of cryogenic tanks with temperature stratification.\\n\\n## Quickstart\\n\\nIn Matlab, run `testSim.m`. This script sets up the model parameters, runs the\\nsimulation, and plots the results.\\n\\n## Citation\\n\\nFor details on system being simulated and the physics models, see:\\n\\nM. Daigle, J. Boschee, M. Foygel, and V. Smelyanskiy, \"Temperature Stratification in a Cryogenic Fuel Tank,\"\" AIAA Journal of Thermophysics and Heat Transfer, vol. 27, no. 1, pp. 116-126, January 2013.\\n\\nTo cite this software, please use:\\n\\nM. Daigle, M. Foygel, V. Smelyanskiy, J. Boschee; LH2Sim [Computer software]. (2017). Retrieved from https://github.com/nasa/LH2Sim.\\n\\n## Contributions\\n\\nContributions are welcome. Please submit pull requests to the `develop` branch. By contributing to this project, you are promising that the work contributed is your own and you have the rights to contribute it.\\n\\n## Notices\\n\\nCopyright ©2016 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/OpenVSP3Plugin',\n",
       "  'language': 'Java',\n",
       "  'readme_contents': '= OpenVSP3Plugin\\nA plugin to run OpenVSP in a MDAO frameworks.\\n\\n== What is OpenVSP?\\n\\nOpen Vehicle Sketch Pad (OpenVSP) is a parametric aircraft geometry tool.\\nOpenVSP allows the user to create a 3D model of an aircraft defined by common engineering parameters.\\nThis model can be processed into formats suitable for engineering analysis.footnoteref:[openvsporg, from www.openvsp.org/learn.shtml]\\n\\nThe predecessors to OpenVSP have been developed by J.R. Gloudemans and others for NASA since the early 1990\\'s.\\nOn January 10 2012, OpenVSP was released as an open source project under the NASA Open Source Agreement (NOSA) version 1.3.footnoteref:[openvsporg]\\n\\n== What is the OpenVPS3Plugin?\\n\\nThe OpenVSP3Plugin is a JAVA software tool that allows for the interface between OpenVSP and an analysis framework such as *Phoenix Integration\\'s ModelCenter* or the NASA developed *OpenMDAO* software.\\nThe OpenVSP3Plugin is designed to work on any computer desktop platform (Mac, Linux, and Windows) that has a local installation of JAVA.\\n\\n.[big]#The OpenVSP3Plugin performs three main functions:#\\n1. Parse an OpenVSP v. 3 file\\n2. Auto-generate an OpenVSP script and a design (.des) file\\n3. Execute OpenVSP with the auto-generated script and apply user selected design variables in the design file. (ModelCenter only)\\n\\nThe user selects the design variables written in the design file through the interactive graphical user interface.\\n\\n== Build\\n\\nTo build the OpenVSP3Plugin, edit the build.bat file and run it.\\nEdit the build.bat file and set *JDK_HOME* to the path of your JAVA installation.\\nIf you have *ModelCenter* installed set *MC_INSTALL* to the path of your *ModelCenter* installation.\\n\\n\\n[source,options-\"nowrap\"]\\n----\\n:: JDK_HOME is required and must point to an installed JDK \\nset \"JDK_HOME=C:/Program Files/Java/jdk1.8.0_151\"\\n\\n:: MC_INSTALL is optional but required to use the plugin with ModelCenter\\n:: Comment this line if you don\\'t have ModelCenter\\n::set \"MC_INSTALL=C:/Program Files/Phoenix Integration/ModelCenter 12.0\"\\n----\\n\\n== Prebuilt releases (OpenVSP3Plugin.jar)\\n\\nPrebuilt releases will be built using the latest source code and JAVA 8 JDK and ModelCenter 12 interfaces\\n\\n== Installation\\n\\nThere is no custom installer for the OpenVSP3Plugin, it must be done by hand. +\\nThe OpenVSP3Plugin uses 2 environment variables:\\n\\n`*OpenVSP_EXE* (__Required__)`:: This needs to be set to the path of the OpenVSP executable (vsp.exe) that the plugin will use.\\n`*TMP* (__ModelCenter only__)`:: (probably already set on your machine) Is used to create temporary folders in which OpenVSP is executed, and the output files are parsed.\\n\\n== OpenMDAO\\n\\nTo use the OpenVSP3Plugin in an OpenMDAO environment simply execute the OpenVSP3Plugin.jar file.\\nThis can be accomplished by running the following command: +\\n`java -jar OpenVSP3Plugin.jar`\\n\\nIn this mode all temporary files are written to the current working directory.\\n\\n== ModelCenter\\n\\nSee ModelCenter help on installing plugins and \"registrationless plugins\".\\n\\nOpenVSP3Plugin logging can be viewed in the ModelCenter JAVA Console.\\nOpen the \"Preferences\" dialog, `Tools->Preferences...`, select the `Java Plug-Ins` tab,\\nand check the `Show Java Console` checkbox.\\n\\n== Documentation\\n\\nA presentation on how to use the OpenVSP3Plugin is located here:\\nlink:presentations/welstead-openvsp3plugin.pdf[OpenVSP Workshop 2017 OpenVSP3Plugin]\\n\\n'},\n",
       " {'repo': 'nasa/SAFE-DART',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# Software Architecture Framework for Extensibility (SAFE-DART)\\n\\n## What is SAFE-DART?\\nSAFE-DART is a simple framework for projects written in the C++ programming language and using the Qt framework. SAFE-DART simplifies the modularization of object-oriented software, allowing software implementations of the components of a software system to be provided at run-time by modules (shared libraries).\\n\\nSAFE-DART is split into two parts: the SAFE-DART library (libsafedart) and the SAFE-DART executable (safedart). The library is linked to any projects which use SAFE-DART, and provides the necessary functionality for creating and using modules via SAFE-DART. The executable provides a way to start applications which are built as SAFE-DART modules, and its use is optional.\\n\\n## Building SAFE-DART\\nIn order to make use of SAFE-DART, the libsafedart (library) project must first be built. This will create a shared library which may be linked into any applications which are to use SAFE-DART. This can be done in two ways on Linux:\\n\\n1. Via the GUI: Open libsafedart/libsafedart.pro in Qt Creator and build.\\n2. Via the terminal: `cd libsafedart && qmake libsafedart.pro && make`\\n\\nThis will create a libsafedart shared library in the bin directory located in the same directory as this README.\\n\\nThe safedart (executable) project may be built similarly:\\n\\n1. Via the GUI: Open safedart/safedart.pro in Qt Creator and build.\\n2. Via the terminal: `cd safedart && qmake safedart.pro && make`\\n\\nThe safedart executable will also be placed in the bin directory.\\n\\n## Using SAFE-DART\\nA sample project setup, which can be run via the SAFE-DART executable, can be found in the `examples/greet` directory of the repository.\\n\\n### Project Setup\\nA project can be linked to libsafedart by changing the following in its project file:\\n\\n1. Add the libsafedart directory to its INCLUDEPATH (`INCLUDEPATH += <path to SAFE-DART repository>/libsafedart`).\\n2. Add the library directory and name to LIBS (`LIBS += -L<path to SAFE-DART repository>/bin -lsafedart`).\\n\\nThe project may then include SAFE-DART headers and make use of its functionality.\\n\\n### Getting a Builder\\nDepending, the Builder may be acquired in one of two ways:\\n\\n1. Provided to user code via the SAFE-DART executable. This approach has two main advantages:\\n    1. The SAFE-DART configuration file can be changed via the command-line.\\n    2. The application code can easily be tested all the way up to its main entry point.\\n2. Created by the user code. This has the advantage of allowing the application more control of SAFE-DART\\'s behavior.\\n\\n### Defining an Interface\\nAn interface to a class is shared by any binary which makes use of the type. It should be written in a header which is shared by all modules that make use of the component it represents. In general, interfaces should contain only pure virtual methods to reduce the possibility of different implementations within different modules, and should also have a virtual destructor. An abstract class that extends `QObject` (a Qt class) may also be used. \\n\\nThe interface should also be declared as an interface via `Q_DECLARE_INTERFACE` after the class definition. An example is shown below.\\n\\n```\\n#include <QObject>\\nclass Greeter\\n{\\npublic:\\n    virtual ~Greeter() {}\\n    virtual void greet() = 0;\\n};\\nQ_DECLARE_INTERFACE(Greeter, \"Greeter\")\\n```\\n\\n### Defining an Implementation\\nAn implementation of an interface works mostly as normal, with a few additions:\\n\\n1. If the interface does not extend `QObject`, then the implementation must.\\n2. The implementation should extend `Reflectable<T>`, where `T` is the implementation type.\\n3. The interface must be specified via `Q_INTERFACES`.\\n4. The implementation must have one of the following explicit constructors, shown in order of preference:\\n    1. `Q_INVOKABLE T(Builder *)`\\n    2. `Q_INVOKABLE T()`\\n\\nAn example of an implementation of the `Greeter` interface above is shown below:\\n\\n```\\n#include <iostream>\\n#include <builder.h>\\n#include <greeter.h>\\n#include <reflectable.h>\\nclass EnglishGreeter : public QObject, public Greeter, public Reflectable<EnglishGreeter>\\n{\\n    Q_OBJECT\\n    Q_INTERFACES(Greeter)\\npublic:\\n    Q_INVOKABLE EnglishGreeter() {}\\n    void greet() override\\n    {\\n        std::cout << \"Hello world!\" << std::endl;\\n    }\\n};\\n```\\n\\n### Creating an Application\\nCreating an application that may be used via the SAFE-DART executable is done by defining an implementation of an interface provided by SAFE-DART. This interface is called `Application`, and it contains only a single method (with the same signature as the standard `main` function).\\n\\nThe application can use the `T(Builder *)` constructor to receive an instance of the `Builder` class. The `Builder` instance can then be used to get an implementation of an interface (in this case, `Greeter`).\\n\\nNote that the `Builder` returns a `QSharedPointer` to the implementation, and will return the same pointer as long as it continues to exist. The `Builder` itself holds a weak reference, so the object will be freed automatically once the application no longer holds a pointer to it.\\n\\nAn example implementation of `Application` that does this is as follows:\\n\\n```\\n#include <application.h>\\n#include <builder.h>\\nclass GreetApplication : public QObject, public Application, public Reflectable<GreetApplication>\\n{\\n    Q_OBJECT\\n    Q_INTERFACES(Application)\\npublic:\\n    Q_INVOKABLE GreetApplication(Builder *builder) : _builder(builder) {}\\n    int main(int argc, char **argv) override\\n    {\\n        QSharedPointer<Greeter> greeter = _builder->get<Greeter>();\\n        greeter->greet();\\n        return 0;\\n    }\\n\\nprivate:\\n    Builder *_builder;\\n};\\n```\\n\\n### Configuring SAFE-DART\\nSAFE-DART uses a configuration file for two things:\\n\\n1. Locations to load modules from. Only applicable when using the SAFE-DART executable.\\n    1. The `@module_dirs` key lists directories from which to load all modules. \\n    2. The `@module_files` key lists specific module files to load.\\n2. Implementations to use for interfaces. The key is the interface name, the value is the implementation name. Always used by the `Builder`.\\n\\nFor the application above, the configuration file should contain the following:\\n\\n```\\n[safedart]\\n@module_dirs=<path to directory containing modules>\\nGreeter=EnglishGreeter\\nModuleLoader=LibraryModuleLoader\\n```\\n\\nThe config file is set up by the SAFE-DART executable normally. By default, the SAFE-DART executable will use `safedart.ini` as the path and `safedart` as the section.The path to the config file, as well as the section within the file to use, can be changed using the `-f` and `-s` arguments respectively.\\n\\nPrograms not using the SAFE-DART executable may set up their own source of configuration data. SAFE-DART using the `Configuration` interface.\\n\\n### Running the Application\\nIn general, a SAFE-DART application can be run by invoking the SAFE-DART executable as follows:\\n\\n`<path to SAFE-DART executable> [-f <path to config file>] [-s <config section to use>] <name of implementation of Application to run>`.\\n\\nFor the example above, this could be as simple as:\\n\\n`safedart GreetApplication`\\n'},\n",
       " {'repo': 'nasa/RVLib',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '# RandomVariable\\n\\n## Description\\nThe RandomVariable Library is an C++ open source library for representing statistical uncertainty in a precise, readable, and usable manner. Value uncertainty can be represented with a parametric distribution such as Gaussian or a nonparametric distribution such as a weighted sample set. The library enables sampling from a parametric or fitting to a parametric distribution. Calculations may also be performed on multiple RandomVariable objects by sampling/overlaying techniques.\\n\\n* **RandomVariable**:\\n\\n  The RandomVariable class holds and represents the uncertainty data. It is the superclass of the two types of distributions and their respective subclasses. The Hierarchical structure allows for polymorphism and late binding on distribution objects.\\n\\n* **Translation**:\\n\\n  Translation is a namespace with functions handling Parametric and NonParametric types. By using an external namespace that can access the definitions of all Parametric and NonParametric distributions, we can decrease coupling between distribution objects and avoid circular dependencies. For instance, if a developer wanted to call a sampling function on a Gaussian distribution and receive and unweighted sample object, the Gaussian object would have to know the definition of an unweighted sample object whereas with Translation, the namespace can provide information about other classes and free distributions of that burden\\n\\n* **Statistics**:\\n\\n  Statistics is a small struct that allows the packaging of information that we can use to instantiate a distribution or detail a distribution in common terms (parameters for each distribution will have different meaning, but the Statistics struct will be \"universal\")\\n  \\n* **[UNDER CONSTRUCTION] RandomVariableContainer**:\\n\\n  The RandomVariableContainer object is a container for RandomVariable objects which allows the user to perform calculations on multiple RandomVariable objects. By passing a function pointer and the correct amount of RandomVariable object pointers.\\n\\n## Structure\\n![RV Hierarchy](images/Hierarchy.png)\\n  The abstract RandomVariable class is inherited by the Parametric and NonParametric classes (also abstract). Each distribution type is inherited by named distributions (Lognormal, Exponential, etc.) and data set representations (Weighted, Unweighted, etc.). The statistics struct is used by the RandomVariable object (and it\\'s descendants). The Translation namespace uses the RandomVariable and it\\'s derivatives.\\n\\n## Using\\n\\n### REQUIREMENTS\\n* Cmake (https://cmake.org)\\n* Compiler capable of building C++11\\n\\n### BUILD\\nTo start build properly enter RandomVariableProject directory then change into the build directory.\\n\\n```\\n$ cd RandomVariable Project\\n$ cd build\\n```\\n\\nexecute cmake and specify path to source as one directory higher\\n\\n`$ cmake ..`\\n\\nIf cmake is not able to locate your gcc/g++ compiler, you can use the command\\n\\n`$ cmake -D CMAKE_C_COMPILER=[location of gcc] -D CMAKE_CXX_COMPILER=[location of gcc] ..`\\n\\nNext run make from the build directory or make -C build from RandomVariableProject.\\n\\n### USE\\nRandom Variable Library is built into a library in the lib directory. This library can then be linked with other software products to be used.\\n\\n## Examples\\nThere are a number of examples in the Examples directory.\\n\\n## Semantics\\n* INCLUDES\\nIncludes are placed in each file dependent on them, unless an included file already contains the necessary information.\\nIf a library was included in two subclasses of a superclass, that include was not moved to the superclass to reduce the number of includes so an unnecessary include is avoided if the superclass were to be included on a different project.\\n* SYNTAX\\nAccessor function names follow the protocol:\\nAccess entails a calculation - solely the name of the attribute\\n  Examples: mean()\\nAccess does not entail calculation - ‘get’ + name of attribute\\n  Example: getMu()\\n* EXCEPTIONS:\\nstd::invalid_argument instances are thrown in functions with input range limits (due to user input) such as:\\npdf(), cdf(), caldIcdf(), setSigma(), etc.\\nThese exceptions are not caught in the function, but are left to be caught by users calling these functions directly. If another function is calling a function containing a possible throw statement, it will contain a try/catch block.\\n\\n## Applications\\nUncertainty representation can be used in an almost infinite number of fields from applied mathematics to telecommunications. Most notably it can be utilized when handling data from entities susceptible to inaccuracies such as machinery, telemetry, manual calculation. A primary application is the monitoring system for a fuel cell or structural component prognostics.\\n\\n## Contact\\nIf you have questions, please open an issue on github\\n\\n## Contributing\\nAll contributions are welcome! If you are having problems with the plugin, please open an issue on GitHub. If you would like to contribute directly, please feel free to open a pull request against the \"dev\" branch. Pull requests will be evaluated and integrated into the next official release.\\n\\n## Next Steps\\nSoftware developers expanding this library should improve the coupling between a select few distribution classes. Some shortcuts were taken where privacy and was not a concern and checking was unnecessary. If future contributors aim to make the software more secure and abstracted, they should decrease further direct access to the underlying data structure (overload index operator) and class private variables. Adding more distributions and data set representations should be trivial if they follow a similar format as the current objects. The process for setting up a calculation involving multiple RandomVariables could be make simpler.\\n\\n## Notices\\n\\nCopyright ©2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration. All Rights Reserved.\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF\\nANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED\\nTO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY\\nIMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR\\nFREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR\\nFREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE\\nSUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN\\nENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS,\\nRESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS\\nRESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY\\nDISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF\\nPRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity: RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE\\nUNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY\\nPRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY\\nLIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE,\\nINCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE\\nOF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED\\nSTATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR\\nRECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH\\nMATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/VirtualADAPT',\n",
       "  'language': 'Matlab',\n",
       "  'readme_contents': '# Motivation\\n\\nThe Advanced Diagnostic and Prognostic Testbed (ADAPT), developed at NASA Ames Research Center, is functionally representative of an electrical power system (EPS) on an exploration vehicle, and has been developed to:\\n* Serve as a technology-neutral basis for testing and evaluating software and hardware diagnostic systems,\\n* Allow accelerated testing of diagnostic algorithms by manually or algorithmically inserting faults,\\n* Provide a real-world physical system such that issues that might be disregarded in smaller-scale experiments and simulations are exposed,\\n* Act as a stepping stone between pure research and deployment in aerospace systems, thus creating a concrete path to maturing diagnostic technologies, and\\n* Develop analytical methods and software architectures in support of the above goals.\\n\\nThe ADAPT hardware includes components that can generate, store, distribute, and monitor electrical power. The EPS can deliver AC (Alternating Current) and DC (Direct Current) power to loads. A data acquisition and control system sends commands to and receives data from the EPS. The testbed operator stations are integrated into a software architecture that allows for nominal and faulty operations of the EPS, and includes a system for logging all relevant data to assess the performance of the health management applications.\\n\\n# The ADAPT Hardware\\n\\nThe major system components of ADAPT include power generation, storage, and distribution components. Two power generation sources are connected to three sets of batteries, which in turn supply two load banks. Each load bank has provision for 6 AC loads and 2 DC loads. To be more specific, ADAPT consists of the following three classes of components – power generation, power storage, and power distribution.\\n* **Power Generation:** The two sources of power generation include two battery chargers. The battery chargers are connected to appropriate wall outlets through relays. The two power generation sources can be interchangeably connected to the three batteries. Hardware relay logic prevents connecting one charge source to more than one battery at the same time, and from connecting one charging circuit to another charging circuit.\\n* **Power Storage:** Three sets of batteries are used to store energy for operation of the loads. Each “battery” consists of two 12-volt sealed lead acid batteries connected in series to produce a 24-volt output. Two battery sets are rated at 100 Amp-hrs and the third set is rated at 50 Amp-hrs. The batteries and the main circuit breakers are placed in a ventilated cabinet that is physically separated from the equipment racks; however, the switches for connecting the batteries to the upstream chargers or downstream loads are located in the equipment racks.\\n* **Power Distribution:** Electromechanical relays are used to route the power from the sources to the batteries; and from the batteries to the AC and DC loads. All relays are of the normally-open type. An inverter converts the 24-volt DC battery input to a 120-volt r.m.s. AC output. Circuit breakers are located at various points in the distribution network to prevent overcurrents from causing unintended damage to the system components.\\n\\n\\n# Virtual ADAPT\\n\\nVirtualADAPT is a high-fidelity, Matlab® Simulink®-based simulation testbed that emulates the ADAPT hardware for running offline health management experiments. This simulation testbed models all components of the ADAPT hardware within the power storage and power distribution subsystems. The physical components of the testbed, i.e., the batteries, relays, and the loads, are replaced by simulation modules that generate the same dynamic behaviors as the hardware test bed. \\n\\n\\n\\n## Installation\\n\\nFor the model to execute, the fault interface GUI functions must be on the Matlab path. To do this, run the script \\n[installVirtualADAPT.m](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/installVirtualADAPT.m).\\n\\nThe script [uninstallVirtualADAPT.m](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/uninstallVirtualADAPT.m) will remove the directories from the Matlab path.\\n\\n\\n## Quick Start Guide\\n\\nTo generate data, load [VirtualADAPT.mdl](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/VirtualADAPT.mdl) in Simulink and hit the start button. The simulation is configured to run indefinitely and write data to the MATLAB workspace as a matrix of floating-point values. The \\'Sensors\\' section of the file [Sensors And Actuators.txt](https://github.com/nasa/VirtualADAPT/blob/master/MATLAB/Sensors%20and%20Actuators.txt) contains the sensor names which correspond to the columns of this matrix.\\n\\nTo command a relay or circuit breaker, find the simulation input port for the desired actuator (eg. \\'EY144_CL\\') and \\ndouble-click the switch connected to it. Because the simulation is \\'solved\\' as quickly as possible rather than running \\nin real-time, it is easier to configure relays before starting the simulation.\\n\\nTo inject ADAPT faults, use the fault injection GUI which is opened (and closed) automatically when the Simulink model \\nis opened (and closed). Fault modes are specific to components but are of the following general modes when referring to\\nparameter values:\\n\\n* Nominal - Magnitude (M) unused. Nominal value (N) unaltered.\\n* Incipient - N + M\\\\*T, where T is the number of seconds since injection.\\n* Abrupt - (N + 1)\\\\*M\\n* Bias - N + M\\n* StuckAt - M\\n\\nFaults are added/removed to/from the Simulink model when added/removed using the GUI.\\n\\nA second method for injecting faults is to open the model in simulink, navigate to the desired block, and change the values \\nmanually. Alternately, the MATLAB functions get_param and set_param can be used to set these values from a script or the \\ncommand line.\\n\\n\\n## Extended Documentation\\n\\nThe document [VirtualADAPT.pdf](https://github.com/nasa/VirtualADAPT/blob/master/docs/VirtualADAPT.pdf) describes the Virtual ADAPT simulation model in more detail. Please refer to this document for instructions on running the simulation.\\n\\n## License\\n\\nThis software is released under the [NASA Open Source Agreement Version 1.3](https://github.com/nasa/VirtualADAPT/blob/master/License.pdf).\\n\\n## Notices\\n\\nCopyright © 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\n### Disclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/ominas_maps',\n",
       "  'language': None,\n",
       "  'readme_contents': '# ominas_maps\\nRepository for OMINAS map library\\n'},\n",
       " {'repo': 'nasa/SGNDI',\n",
       "  'language': None,\n",
       "  'readme_contents': '[![Build Status](https://travis-ci.org/thearn/SGNDI.svg?branch=master)](https://travis-ci.org/thearn/SGNDI)\\n[![Coverage Status](https://coveralls.io/repos/github/thearn/SGNDI/badge.svg?branch=master)](https://coveralls.io/github/thearn/SGNDI?branch=master)\\n\\nSeparable Grid N-Dimensional Interpolator (SGDNI)\\n-------------------------------------------------\\n\\n![alt text](example.png \"Example interpolation\")\\n\\nA multi-dimensional interpolating class with first-order gradients.\\nThis module provides a class `SeparableGridNDInterpolator` similair in\\ninterface to the interpolators provided by [`scipy.interpolate`](https://docs.scipy.org/doc/scipy/reference/interpolate.html).\\n\\nThis class provides interpolation on a regular grid in arbitrary dimensions, by applying\\na selected 1D interpolation class on each grid axis sequentially. These\\n1D interpolation classes are the ones provided by [`scipy.interpolate`](https://docs.scipy.org/doc/scipy/reference/interpolate.html), such\\nas [`CubicSpline`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.CubicSpline.html#scipy.interpolate.CubicSpline), [`UnivariateSpline`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html#scipy.interpolate.UnivariateSpline), or [`Akima1DInterpolator`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Akima1DInterpolator.html#scipy.interpolate.Akima1DInterpolator). By default, `CubicSpline` is used if no interpolator is specified.\\n\\nThis method can be considered a generalization of the class of multidimensional interpolators that operate on each dimension sequentially, such as [bilinear](https://en.wikipedia.org/wiki/Bilinear_interpolation), [bicubic](https://en.wikipedia.org/wiki/Bicubic_interpolation), [trilinear](https://en.wikipedia.org/wiki/Trilinear_interpolation), [tricubic](https://en.wikipedia.org/wiki/Tricubic_interpolation), etc. In other words, is provides n-linear, n-cubic, etc. interpolation capabilities within a single class.\\n\\nIf derivatives are provided by the chosen 1D interpolation method, then\\na gradient vector of the multidimensional interpolation may be computed\\nand and cached when the interpolation is performed. This can then be accessed by the `derivative` method. At the moment, only\\nfirst-order derivatives are supported.\\n\\nExamples\\n---------\\nThe shortest example might be fitting the XOR function:\\n\\n```python\\nimport numpy as np\\nfrom sgndi import SeparableGridNDInterpolator\\n\\na = np.array([0, 1])\\nb = np.array([0, 1])\\n\\nc = np.array([[0, 1], [1, 0]])\\n\\ncs = SeparableGridNDInterpolator([a, b], c)\\n\\nx = [0.8,0.1]\\n\\nvalue = cs(x)\\ngradient = cs.derivative(x)\\n\\nprint(value, gradient)\\n```\\n\\nwhich prints the interpolated value, and gradient vector\\n\\n```0.7400000000000001, [ 0.8 -0.6]```\\n\\n-------------------\\n\\nA more ambitious 4D paraboloid example, using `np.meshgrid` to create the structured (regular) grid:\\n\\n\\n```python\\nimport numpy as np\\nfrom sgndi import SeparableGridNDInterpolator\\n\\n# Let\\'s define a 4D function to test with:\\n\\ndef F(u,v,z,w):\\n\\treturn (u-5)**2 + (v-2)**2 + (z-5)**2 + (w-0.5)**2\\n\\n# Now create 1D arrays for each of the function parameters for sampling.\\n\\nU = np.linspace(0, 10, 10)\\nV = np.linspace(0, 4, 6)\\nZ = np.linspace(0, 10, 7)\\nW = np.linspace(0, 1, 8)\\npoints = [U, V, Z, W]\\n\\n# Create coordinate mesh\\n\\nu, v, z, w = np.meshgrid(*points, indexing=\\'ij\\')\\n\\n# Now create the 4D value array\\n\\nvalues = F(u, v, z, w)\\n\\n# Define a random point to interpolate at\\n\\nx = [5.26434, 2.121235, 2.7352, 0.5213345]\\n\\n# Create the interpolation class instance\\n\\ninterp = SeparableGridNDInterpolator(points, values)\\n\\n# Call the interpolation at the point above, which by default also\\n# computes the gradient of the interpolant at this point\\n\\ndef dF(u,v,z,w):\\n\\t# the actual gradient\\n\\treturn 2*(u-5), 2*(v-2), 2*(z-5), 2*(w-0.5)\\n\\nf = interp(x)\\ndfdx = interp.derivative(x)\\n\\nprint(\"actual value\", F(*x))\\nprint(\"computed value\", f)\\nprint(\"actual gradient:\", dF(*x))\\nprint(\"computed gradient:\", dfdx)\\n```\\n\\nThis produces the interpolated output:\\n\\n```\\nactual value 5.21434776171525\\ncomputed value 5.214347761715252\\nactual gradient: (0.5286799999999996, 0.24246999999999996, -4.5296, 0.04266900000000007)\\ncomputed gradient: [ 0.52868   0.24247  -4.5296    0.042669]\\n```\\n\\n---------------------------------\\n\\nA 2D example can show the performance visually. A very course sampling is used to product a 2D interpolation, making use of the 1D `UnivariateSpline` from scipy.interpolate with degree `k=5`. This is then used to produce a finer-scale approximation of the original function.\\n\\n```python\\nimport numpy as np\\nfrom sgndi import SeparableGridNDInterpolator\\nfrom scipy.interpolate import UnivariateSpline\\nimport matplotlib.pyplot as plt\\n\\ndef F(u,v):\\n    return u*np.cos(u*v) + v*np.sin(u*v)\\n\\ndef dF(u,v):\\n    return -u*v*np.sin(u*v) + v**2*np.cos(u*v) + np.cos(u*v), -u**2*np.sin(u*v) + u*v*np.cos(u*v) + np.sin(u*v)\\n\\n# Plot exact function, with high sampling, n = 200\\nU = np.linspace(0, 3, 200)\\nV = np.linspace(0, 3, 200)\\n\\npoints = [U, V]\\n\\nu, v = np.meshgrid(*points, indexing=\\'ij\\')\\n\\nvalues = F(u, v)\\nplt.subplot(131)\\nplt.imshow(values[::-1], extent=(U[0], U[-1], V[0], V[-1]))\\n\\n#--------------------------------\\n# Now gather & plot a very course sample for creating the interpolant, n = 10\\n\\nU = np.linspace(0, 3, 10)\\nV = np.linspace(0, 3, 10)\\n\\npoints = [U, V]\\n\\nu, v = np.meshgrid(*points, indexing=\\'ij\\')\\n\\nvalues = F(u, v)\\nplt.subplot(132)\\nplt.imshow(values[::-1], extent=(U[0], U[-1], V[0], V[-1]))\\n\\ninterp = SeparableGridNDInterpolator(points, values,\\n\\t\\t\\t\\tinterpolator = UnivariateSpline, interp_kwargs = {\\'k\\' : 5})\\n\\n#--------------------------------\\n# Test that the created interpolator can actually approximate a fine level\\n# of detail, n = 50\\n\\nU = np.linspace(0, 3, 50)\\nV = np.linspace(0, 3, 50)\\n\\npoints = [U, V]\\n\\nu, v = np.meshgrid(*points, indexing=\\'ij\\')\\n\\nvals = []\\nfor x in np.array([u.ravel(), v.ravel()]).T:\\n    f = interp(x)\\n    vals.append(f)\\n\\nA = np.array(vals).reshape(50, 50)\\n\\nplt.subplot(133)\\nplt.imshow(A[::-1], extent=(U[0], U[-1], V[0], V[-1]))\\n\\nplt.show()\\n```\\n\\nWhich gives the plot shown at the top of this readme.\\n\\nExample use in a numerical optimization\\n------------------------------\\n```python\\nimport numpy as np\\nfrom scipy.optimize import fmin_bfgs\\nfrom sgndi import SeparableGridNDInterpolator\\n\\ndef F(u, v, z, w):\\n    # min at u=5.234 v=2.128 z=5.531 w=0.574\\n    return (u - 5.234)**2 + (v - 2.128)**2 + (z - 5.531)**2 + (w - 0.574)**2\\n\\nU = np.linspace(0, 10, 10)\\nV = np.linspace(0, 10, 6)\\nZ = np.linspace(0, 10, 7)\\nW = np.linspace(0, 10, 8)\\n\\npoints = [U, V, Z, W]\\n\\nu, v, z, w = np.meshgrid(*points, indexing=\\'ij\\')\\n\\nvalues = F(u, v, z, w)\\n\\ninterp = SeparableGridNDInterpolator(points, values)\\n\\nx = np.zeros(4)\\n```\\n\\nWithout gradients:\\n```\\nprint(fmin_bfgs(interp, x))\\n```\\n\\n```\\nOptimization terminated successfully.\\n         Current function value: 0.000000\\n         Iterations: 3\\n         Function evaluations: 24\\n         Gradient evaluations: 4\\n[ 5.23399953  2.12799974  5.53100043  0.57400106]\\n```\\n\\nWith gradients:\\n```\\nprint(fmin_bfgs(interp, x, fprime=interp.derivative))\\n```\\n\\n```\\nOptimization terminated successfully.\\n         Current function value: 0.000000\\n         Iterations: 3\\n         Function evaluations: 4\\n         Gradient evaluations: 4\\n[ 5.234  2.128  5.531  0.574]\\n```\\n\\nLimitations\\n------------\\n- The interpolation can only be called one evaluation point at a time. I am working\\non vectorizing this to allow for a collection of points to be interpolated at once.\\n- Only first-order derivatives (gradients) are computed.\\n\\n\\n'},\n",
       " {'repo': 'nasa/knife',\n",
       "  'language': 'C',\n",
       "  'readme_contents': '\\nknife\\n\\nBoolean Subtraction Library for Polyhedra\\n\\nThe knife library calculates the boolean subtraction of arbitrary\\nwatertight triangular polyhedra. The result of this subtraction is\\nalso watertight triangular polyhedra. The triangular faces of the\\nresultant polyhedra are created with a Delaunay triangle mesher.\\nThese polyhedra are suitable for performing cut cell partial\\ndifferential equation solutions (i.e., computational fluid flow\\nsimulations). Tetrahedra as well as the median dual of a tetrahedral\\nmesh are standard inputs. The knife library is implemented with an\\nobject-oriented flavor in the C language.\\n\\nThe knife library can be called by FUN3D <http://fun3d.larc.nasa.gov/>\\nto perform dual tetrahedra cut cell flow and adjoint solutions.\\n\\nSee the INSTALL file for build instructions.\\n\\nMike Park\\n\\n\\n'},\n",
       " {'repo': 'nasa/DON-Federate-HLA2MPC',\n",
       "  'language': 'HTML',\n",
       "  'readme_contents': '# DON-Federate-HLA2MPC\\n\\nDistributed Observer Network 3 (DON3) combines NASA simulation technologies, NASA information technologies and commercial video game technology to provide a free immersive viewer for complex simulation information.  A key component is a standardized interface for simulation related information that is coupled with custom software integrated into the game environment.  The DON Federate is an HLA / IEEE 1516 federate that subscribes to simulation state information, formats that data to comply with the Model Process Control specification and then sends the information to DON clients for display.\\n'},\n",
       " {'repo': 'nasa/PyBlock',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': 'PyBlock README\\n--------------\\nPyBlock is a Python 2 or 3 module that enables the end user to estimate partial beam blockage using polarimetric radar data. The methodologies it uses depend on the self-consistency of polarimetric radar variables - reflectivity (Zh), differential reflectivity (Zdr), and specific differential phase (Kdp) - in pure rain. There are two methodologies currently available to the end user, both described in Lang et al. (2009): The KDP method, and the Fully Self-Consistent (FSC) method. Briefly, the KDP method will check the behavior of Zh and Zdr for a given range of Kdp both inside and outside of blocked azimuths, and use that to suggest corrections to these measurands. This is effectively a relative calibration of Z and Zdr. The FSC method uses a derived or specified self-consistency relationship to do an absolute calibration of Zh within the blocked regions. PyBlock implements these methodologies within an object-oriented Python framework. \\n\\nPyBlock Installation\\n-------------------\\nThe following dependencies need to be installed first:\\n\\n- A robust version of Python 2.7 or 3.4-3.6 (other versions untested) w/ most standard scientific packages (e.g., numpy, matplotlib, pandas, etc.) - Get one for free [here.](https://store.continuum.io/cshop/anaconda/)\\n- The `h5py` module (available via most package managers; e.g., conda)\\n- [The Python Atmospheric Radiation Measurement (ARM) Radar Toolkit (Py-ART)] (https://github.com/ARM-DOE/pyart)\\n- [CSU_RadarTools](https://github.com/CSU-Radarmet/CSU_RadarTools)\\n- [SkewT](https://pypi.python.org/pypi/SkewT) - Python 3 version can be found [here.](https://github.com/tjlang/SkewT)\\n- [DualPol] (https://github.com/nasa/DualPol)\\n\\nSpecific import calls in the PyBlock source code:\\n\\n```\\nfrom __future__ import division\\nfrom __future__ import print_function\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom warnings import warn\\nimport statsmodels.api as sm\\nimport os\\nimport h5py\\nimport pyart\\nimport dualpol\\nfrom csu_radartools import csu_misc\\nimport six\\n```\\n\\nTo install PyBlock, in the main directory for the package:\\n\\n```\\npython setup.py install\\n```\\n\\nUsing PyBlock\\n-------------\\nTo access everything:\\n```\\nimport pyblock\\n```\\n\\nTo see PyBlock in action, check out the IPython notebook provided in this distribution.\\n'},\n",
       " {'repo': 'nasa/MMM-Py',\n",
       "  'language': 'Jupyter Notebook',\n",
       "  'readme_contents': \"MMM-Py README\\n-------------\\n\\nThe National Oceanic and Atmospheric Administration (NOAA) regularly produces\\nnational 3D radar reflectivity mosaics via its Multi-Radar/Multi-Sensor (MRMS)\\nsystem. These mosaics are wonderful for storm and precipitation analysis and research,\\nbut they are distributed in odd formats that NOAA is ever changing. Sometimes you\\njust want to read a file and make a plot! \\n\\nThis is what MMM-Py is for. With it, you can read any version of the MRMS radar mosaics,\\npast or present, and you can analyze, plot, subsection, and output custom mosaics of your \\nown, which MMM-Py can ingest later. MMM-Py is free and open source. It is capable of \\nproducing publication-ready figures and analyses, but it also can do quicklook plots so \\nyou can check out the cool storm that just happened.\\n\\nFor more info about the latest version of MRMS, see [here.](https://docs.google.com/document/d/1LeVcn_taIXZgzZb5JgWqaVr0xVs7GmA6RpHcb8ZGiwk/edit)\\n\\n\\nMMM-Py Installation\\n-------------------\\n\\nMMM-Py works under Python 2.7 and 3.4-3.6 on most Mac/Linux setups. Windows installation is currently untested.\\n\\nPut `mmmpy.py` in your `PYTHONPATH`.\\n\\nYou'll need the following Python packages. Most are easily obtained or already installed\\nwith common Python frameworks such as [Anaconda](http://continuum.io/downloads):\\n`numpy`, `matplotlib`, `six`, `netCDF4`, `os`, `Basemap`, `struct`, `time`, `calendar`, `gzip`, `datetime`\\n\\nYou may also want to install `pygrib` from [here](https://pypi.python.org/pypi/pygrib). This is an optional dependency.\\n\\nGet MRMS-modified wgrib2 package and installation info from ftp://ftp.nssl.noaa.gov/projects/MRMS/GRIB2_DECODERS/MRMS_modified_wgrib2_v2.0.1-selectfiles.tgz\\n\\nInstall `wgrib2` and note the path to it. Modify the `BASE_PATH`, `TMPDIR`, `WGRIB2_PATH`, and `WGRIB2_NAME` \\nglobal variables in `mmmpy.py` as necessary. `TMPDIR` is where intermediate netCDFs created by `wgrib2`\\nwill go.\\n\\nWithout `wgrib2` or `pygrib`, MMM-Py can still read legacy MRMS binaries and netCDFs. The `pygrib` module will obviate the need to install `wgrib2`, as it enables direct ingest of the grib2 files without converting to netCDF.\\n\\n\\nUsing MMM-Py\\n------------\\n\\nTo access everything:\\n```\\nimport mmmpy\\n```\\nTo see MMM-Py in action, check out the IPython notebooks provided in this distribution.\\n\\nThis [conference presentation](https://ams.confex.com/ams/95Annual/webprogram/Paper262779.html) discusses MMM-Py (among other modules).\\n\\nMMM-Py was developed at the NASA Marshall Space Flight Center by Timothy Lang (timothy.j.lang@nasa.gov)\\n\\nSee LICENSE file for NASA open source license information.\\n\"},\n",
       " {'repo': 'nasa/KeplerPORTs',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '# KeplerPORTs\\nKeplerPORTs.py - Illustrate making use of numerous Kepler Planet Occurrence Rate Data Products for Data Release 25 and SOC 9.3 Kepler Pipeline version.  This code generates a detection contour according to the documentation\\n\\nBurke, C.J. & Catanzarite, J. 2017, \"Planet Detection Metrics: Per-Target Detection Contours for Data Release 25\", KSCI-19111-001\\n\\nAdditional recommended background reading\\n\\n- Earlier Data Release 24 version of detection contour\\n    * Burke et al. 2015, ApJ, 809, 8\\n- Transit injection and recovery tests for the Kepler pipeline\\n    * Christiansen et al. 2013, ApJS, 207, 35\\n    * Christiansen et al. 2015, ApJ, 810, 95   (One Year Kepler data)\\n    * Christiansen et al. 2016, ApJ, 828, 99   (Data Release 24)\\n    * Christiansen, J. L. 2017, Planet Detection Metrics: Pixel-Level Transit Injection Tests of Pipeline Detection Efficiency for Data Release 25 (KSCI-19110-001)\\n    * Burke & Catanzarite 2017, Planet Detection Metrics: Per-Target Flux-Level Transit Injection Tests of TPS for Data Release 25 (KSCI-19109-001)\\n- Kepler Target Noise and Data Quality metrics\\n    * Burke & Catanzarite 2016, Planet Detection Metrics: Window and One-Sigma Depth Functions for Data Release 25 (KSCI-19101-002)\\n    \\n**Assumptions** - python packages numpy, scipy, matplotlib, astropy, and h5py are available and files\\n\\n- detectEffData_alpha12_02272017.h5\\n- detectEffData_alpha12_SlopeLongShort_02272017.txt\\n- detectEffData_alpha_base_02272017.txt\\n- kplr003429335_dr25_onesigdepth.fits\\n- kplr003429335_dr25_window.fits\\nare available in the same directory as KeplerPORTs.py\\n\\n**Running**: python KeplerPORTs.py\\n\\n**Output**: Displays a series of figures and generates hardcopy\\n\\nNotices:\\n\\nCopyright © 2017 United States Government as represented by the Administrator of the National Aeronautics and Space Administration.  All Rights Reserved.\\n\\nNASA acknowledges the SETI Institute’s primary role in authoring and producing the KeplerPORTs (Kepler Planet Occurrence Rate Tools) under Cooperative Agreement Number NNX13AD01A.\\n\\n\\nDisclaimers\\n\\nNo Warranty: THE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY KIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO SPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT DOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE. THIS AGREEMENT DOES NOT, IN ANY MANNER, CONSTITUTE AN ENDORSEMENT BY GOVERNMENT AGENCY OR ANY PRIOR RECIPIENT OF ANY RESULTS, RESULTING DESIGNS, HARDWARE, SOFTWARE PRODUCTS OR ANY OTHER APPLICATIONS RESULTING FROM USE OF THE SUBJECT SOFTWARE.  FURTHER, GOVERNMENT AGENCY DISCLAIMS ALL WARRANTIES AND LIABILITIES REGARDING THIRD-PARTY SOFTWARE, IF PRESENT IN THE ORIGINAL SOFTWARE, AND DISTRIBUTES IT \"AS IS.\"\\n\\nWaiver and Indemnity:  RECIPIENT AGREES TO WAIVE ANY AND ALL CLAIMS AGAINST THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT.  IF RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE RESULTS IN ANY LIABILITIES, DEMANDS, DAMAGES, EXPENSES OR LOSSES ARISING FROM SUCH USE, INCLUDING ANY DAMAGES FROM PRODUCTS BASED ON, OR RESULTING FROM, RECIPIENT\\'S USE OF THE SUBJECT SOFTWARE, RECIPIENT SHALL INDEMNIFY AND HOLD HARMLESS THE UNITED STATES GOVERNMENT, ITS CONTRACTORS AND SUBCONTRACTORS, AS WELL AS ANY PRIOR RECIPIENT, TO THE EXTENT PERMITTED BY LAW.  RECIPIENT\\'S SOLE REMEDY FOR ANY SUCH MATTER SHALL BE THE IMMEDIATE, UNILATERAL TERMINATION OF THIS AGREEMENT.\\n'},\n",
       " {'repo': 'nasa/ESPRESSO',\n",
       "  'language': None,\n",
       "  'readme_contents': '# Project ESPRESSO\\n\\nThis repository will contain code products generated by the efforts of Project ESPRESSO, a NASA SSERVI node.\\n\\nhttps://www.espresso.institute\\n'},\n",
       " {'repo': 'nasa/visionworkbench',\n",
       "  'language': 'C++',\n",
       "  'readme_contents': '************************************************************************\\n1. INTRODUCTION\\n\\nThe NASA Vision Workbench is a modular, extensible, cross-platform\\ncomputer vision software framework written in C++.  It was designed to\\nsupport a variety of space exploration tasks, including automated\\nscience and engineering analysis, robot perception, and 2D/3D\\nenvironment reconstruction, though it can also serve as a\\ngeneral-purpose image processing and machine vision framework in other\\ncontexts as well.\\n\\nThis package is composed of several modules each of which provides a\\nseparate C++ library.  The core library provides the basic image and\\npixel data types as well as a range of fundamental image processing\\noperations.  The other modules provided in this release are:\\n\\n * Math: geometric, numeric, and other mathematical types and functions\\n * GPU: accelerated image processing using commodity graphics hardware\\n * HDR: creating, processing, and compressing high dynamic range images\\n * InterestPoint: Detecting, tracking, and matching interest points\\n * Mosaic: compositing, blending, and manipulating 2D image mosaics\\n * Camera: camera models and related types and functions\\n * Cartography: tools for manipulating geospatially-referenced images\\n\\nEach of these modules and their dependencies are discussed in greater\\ndetail in section 3, \"LIBRARY STRUCTURE\".\\n\\n************************************************************************\\n2. LICENSE (see COPYING for the full text)\\n\\nA. Copyright and License Summary\\n\\nCopyright (C) 2009 United States Government as represented by the\\nAdministrator of the National Aeronautics and Space Administration\\n(NASA).  All Rights Reserved.\\n\\nThis software is distributed under the NASA Open Source Agreement\\n(NOSA), version 1.3.  The NOSA has been approved by the Open Source\\nInitiative.  See the file \"COPYING\" at the top of the distribution\\ndirectory tree for the complete NOSA document.\\n\\nTHE SUBJECT SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY OF ANY\\nKIND, EITHER EXPRESSED, IMPLIED, OR STATUTORY, INCLUDING, BUT NOT\\nLIMITED TO, ANY WARRANTY THAT THE SUBJECT SOFTWARE WILL CONFORM TO\\nSPECIFICATIONS, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR\\nA PARTICULAR PURPOSE, OR FREEDOM FROM INFRINGEMENT, ANY WARRANTY THAT\\nTHE SUBJECT SOFTWARE WILL BE ERROR FREE, OR ANY WARRANTY THAT\\nDOCUMENTATION, IF PROVIDED, WILL CONFORM TO THE SUBJECT SOFTWARE.\\n\\nB. Third-Party Libraries\\n\\nThis distribution includes some bundled third-party software as a\\nconvenience to the user.  This software, located in the \"thirdparty/\"\\ndirectory, is not covered by the above-mentioned distribution\\nagreement or copyright.  See the included documentation for detailed\\ncopyright and license information for any third-party software.  In\\naddition, various pieces of the Vision Workbench depend on additional\\nthird-party libraries that the user is expected to have installed.\\nThe specific dependencies of each component of the Vision Workbench\\nare discussed section 3, \"LIBRARY STRUCTURE\", and information of where\\nto obtain non-bundled third-party libraries is provided in section 4,\\n\"INSTALLATION\".\\n\\n************************************************************************\\n3. LIBRARY STRUCTURE\\n\\nThe Vision Workbench software is located in the directory \"src/vw/\"\\nand consists of a core library and several optional libraries or\\n\"modules\".  Each module is contained in a subdirectory with the name\\nname as the module.\\n\\nA. The Main Vision Workbench Library\\n\\nAt the center of the Vision Workbench are three modules that are\\nin fact linked together to form a single fundamental library.\\n\\ni. The \"Core/\" module provides fundamental services that are not\\nspecific to image processing, such as C++ exception types and type\\ncomputation classes.\\n\\nii. The \"Image/\" module provides the pixel and image types and\\nfunctions that form the heart of the Vision Workbench, including\\nsupport for various color spaces, filtering operations, and other\\nimage processing primitives.\\n\\niii. The \"FileIO/\" module contains routines to support reading and\\nwriting images from and to disk in a variety of file formats.\\n\\nThe only required dependency of the Vision Workbench core is the Boost\\nC++ libraries, which provide a variety of low-level C++ library\\nservies.  (These are actually many libraries by different authors\\nreleased under a common license, but they are generally distributed as\\na package and we will treat them that way for simplicity.)  The file\\nI/O module has a number of optional dependencies, each providing\\nsupport for one or more image file formats.  Specifically these are:\\nlibpng for PNG files, libjpeg for JPEG/JFIF files, libtiff for TIFF\\nfiles, NetPBM for PBM/PGM/etc. files, and OpenEXR for EXR files.\\n\\nB. Math Module\\n\\nThis module provides a variety of mathematical data types and\\nalgorithms.  It is centered around fundamental vector, matrix, and\\nquaternion data types which support the usual range of mathematical\\noperations.  On top of that foundation there are higher-level types\\nand algorithms for geometric computation, linear algebra,\\noptimization, and statistics, and so forth.\\n\\nThe linear algebra numerical algorithms rely on the standard low-level\\nroutines provided by LAPACK (Linear Algebra Package) and BLAS (Basic\\nLinear Algebra Subprograms).  Many computers (e.g. those running OS X)\\ncome with optimized implementations of LAPACK and BLAS, in which case\\nyou can (and probably should) just use those.  To support other\\nplatforms we provide public-domain implementations automatically\\ntranslated from the original Fortran and taken from the online Netlib\\nrepository.\\n\\nC. GPU Module\\n\\nMost modern graphics hardware includes a general-purpose graphics\\nprocessing unit (GPU), and this module provides an interface to take\\nadvantage of that high-performance hardware for a variety of basic\\nimage-processing operations.  Under Linux it requires that you have\\ninstalled the OpenGL interface library GLEW.  It can optionally also\\ntake advantage of the NVIDIA CG library, which provides an alternative\\nlow-level framework which may be preferable on some graphics hardware.\\n\\nD. HDR Module\\n\\nWhile the core Vision Workbench library supports working with high\\ndynamic range (HDR) image data directly, most input and output devices\\n(e.g. digital cameras and displays) only support a limited dynamic range.\\nThis module provides tools for interfacing between the two worlds, by\\ngenerating HDR images from collections of ordinary images as well as\\nproviding several methods to compress HDR images for display on ordinary\\noutput devices.\\n\\nE. InterestPoint Module\\n\\nInterest points are points in an image that can be reliably detected and\\ntracked, such as corners or peaks.  This module provides tools for\\nlocating interest points in images in a variety of ways.  It can also\\ngenerate local descriptors for those points using several methods which\\ncan then be used to locate corresponding points in sets of images.\\n\\nF. Mosaic Module\\n\\nThe Mosaic module provides tools for working with large images\\nassembled from many smaller images, such as panoramas or aerial maps.\\nIt includes support for assembling source images into a mosaic,\\nblending the images in the mosaic to create a single large image, and\\ndicing the extremely large images that result into better formats for\\nvisualization.\\n\\nG. Camera Module\\n\\nThe Camera module provides a variety of data types and tools for\\nworking with camera models and camera data.  Most notably it includes\\nsupport for a variety of standard camera geometries, the ability to\\ntransform images between camera models, and the ability to interpret\\ncamera data stored in the EXIF headers of image files generated by\\ndigital cameras.\\n\\nH. Cartography Module\\n\\nThe Cartography module provides a variety of 2D and 3D mapping\\ncapabilities.  It allows you to georeference an image, specifying its\\nprojection, position within the projected space, and altitude\\nreference (or \"datum\"), and it supports reprojecting images into the\\ndesired projection.  It requires the PROJ.4 cartographic projection\\nlibrary, and also optionally depends on the GDAL library which\\nprovides support for a variety of GIS file formats.\\n\\nI. Testing Frameworks\\n\\nEach module includes a collection of tests located in that module\\'s\\n\"tests/\" subdirectory.  You can use these tests to confirm that the\\nlibrary is installed and working properly on your system.  To do this,\\nsimply run \"make check\" after building the library.  Please report any\\nerrors you encounter, using the contact information at the bottom of\\nthis file.  Note that while the tests do currently exercise a\\nsignificant portion of the library, they are not yet fully exhaustive.\\n\\n************************************************************************\\n4. INSTALLATION AND USE\\n\\nA. Obtaining the Software\\n\\nIf you are reading this text then presumably you have a copy of\\nthe software.  However, you can obtain the most recent version from\\n\\n  http://ti.arc.nasa.gov/visionworkbench\\n\\nBefore attempting to configure, build or install the Vision Workbench\\nyou should obtain and install any prerequisite libraries that you\\nneed.  The only absolute requirement is the Boost.  The others are\\neither required to build a specific module, or will enable a\\nparticular feature if available.  A complete list of dependencies\\nis shown in the table below, where each library is noted as being\\neither a required or optional dependency of one or modules.  All of\\nthese libraries are distributed under some variation on the themes\\nof the MIT and BSD licenses.  See each individual library\\'s\\ndocumentation for details.\\n\\n+---------+--------------------+------------------------------------+\\n| Library | Relevant Modules   | Source Website                     |\\n+---------+--------------------+------------------------------------+\\n| Boost   | Core, etc. (req.)  | http://www.boost.org/              |\\n| PROJ.4  | Cartography (req.) | http://www.remotesensing.org/proj/ |\\n| GDAL    | Cartography (opt.) | http://www.remotesensing.org/gdal/ |\\n| GLEW    | GPU (req.)         | http://glew.sourceforge.net/       |\\n| CG      | GPU (opt.)         | http://developer.nvidia.com/       |\\n| PNG     | FileIO (opt.)      | http://www.libpng.org/             |\\n| JPEG    | FileIO (opt.)      | http://www.ijg.org/                |\\n| TIFF    | FileIO (opt.)      | http://www.libtiff.org/            |\\n| OpenEXR | FileIO (opt.)      | http://www.openexr.com/            |\\n+---------+--------------------+------------------------------------+\\n\\nIn addition, the some Vision Workbench modules require other, lower\\nlevel modules to be built.  The internal Vision Workbench dependency\\ntable appears below.\\n\\n+------+--------+-------------+\\n| HDR  | Mosaic | Cartography |   Application-specific Toolkits\\n+------+--------+-------------+\\n+---------------+-------------+\\n|    FileIO     |    Camera   |   High-level Primatives\\n+---------------+-------------+\\n+---------------+-------------+\\n|    Image      |    Math     |   Low-level image processing/Linear Algrebra\\n+---------------+-------------+\\n+-----------------------------+\\n|             Core            |   Basic Programming Infrastructure\\n+-----------------------------+\\n\\n\\nB. Building and Installing\\n\\nOnce you have obtained and installed all of the prerequisite software\\nthe process of building the Vision Workbench itself is generally\\nstraightforward.  There are four steps:\\n\\ni. Configure the library.  This is usually as simple as running the\\n\"./configure\" script from within the root Vision Workbench package\\ndirectory.\\n\\nii. Build the library by running \"make\".\\n\\niii. Run the tests by running \"make check\".\\n\\niv. Install the library by running \"make install\".\\n\\nWhile this simple sequence will suffice for most users, the configure\\nscript has many options that you can use to adjust various properties,\\nsuch as compiler optimization flags or the search paths used to find\\nrequired libraries.  See the \"INSTALL\" file in this directory for more\\ndetailed information.\\n\\nC. Using the Library\\n\\nWhen you install the library it will place files into three\\nsubdirectories in the installation location.  The header files which\\nyou will need to develop software using the library are located in the\\n\"include/\" subdirectory.  The compiled libraries, which you will need\\nto link your software against, are located in the \"lib/\" subdirectory.\\nYou will need to configure your software development environment as\\nappropriate to locate these files.  Finally, a number of simple\\ncommand-line tools are provided in the \"bin/\" directory.  These are\\nintended primarily as demo applications, but many of them are in fact\\nuseful in their own right.  See the documentation for a complete list\\nof the tools.\\n\\n************************************************************************\\n5. DOCUMENTATION\\n\\nThe primary source of documentation is the Vision Workbook, which is\\nprovided in source form along with this distribution.  It includes a\\ngentle introduction to using the core image processing routines, as\\nwell as documentation for each of the high level Vision Workbench\\nmodules.  A copy of this document in PDF format should be available\\nfrom wherever you obtained this package.  The original source for this\\ndocument can be found in \"docs/workbook\" and it can be built by\\nrunning \"make workbook\".  This operation requires the latex typesetting\\npackage.\\n\\n************************************************************************\\n6. CONTACTS & CREDITS\\n\\nA. Mailing List\\n\\nAll bugs, feature requests, and general discussion should be sent to\\nthe Vision Workbench user mailing list:\\n\\n  vision-workbench@lists.nasa.gov\\n\\nTo subscribe to this list, send an empty email message with the subject\\n\\'subscribe\\' (without the quotes) to\\n\\n  vision-workbench-request@lists.nasa.gov\\n\\nTo contact the lead developers and project manager directly, send mail\\nto:\\n\\n  vision-workbench-owner@lists.nasa.gov\\n\\nPlease do NOT use this second list for technical inquiries, which\\nshould all be sent to the main vision-workbench list above.\\n\\nB. Credits\\n\\nThe Vision Workbench was developed within the Autonomous Systems and\\nRobotics area of the Inteligent Systems Division at NASA\\'s Ames\\nResearch Center.  It leverages the Intelligent Robotics Group\\'s (IRG)\\nextensive experience developing surface reconstruction and tools for\\nplanetary exploration---e.g. the Mars Pathfinder and Mars Exploration\\nRover missions---and rover autonomy.  It has also been developed in\\ncollaboration with the Adaptive Control and Evolvable Systems (ACES)\\ngroup, and draws on their experience developing computer vision\\ntechniques for autonomous vehicle control systems.\\n\\nSee the AUTHORS file for a complete list of developers.\\n'},\n",
       " {'repo': 'nasa/MAV',\n",
       "  'language': 'Limbo',\n",
       "  'readme_contents': '# MAV\\n\\nThis consists of modeling, analysis, and visualization of ATM concepts. \\n'},\n",
       " {'repo': 'nasa/NASA-Space-Weather-Media-Viewer',\n",
       "  'language': None,\n",
       "  'readme_contents': ''},\n",
       " {'repo': 'nasa/RtRetrievalFrameworkDoc',\n",
       "  'language': 'Python',\n",
       "  'readme_contents': '====================================\\nRT Retrieval Framework Documentation\\n====================================\\n\\nJet Propulsion Laboratory, California Institute of Technology. \\nCopyright 2016 California Institute of Technology. \\nU.S. Government sponsorship acknowledged.\\n\\nThis is the Sphinx source for the RT Retrieval Framework documentation.\\n\\nThe software is located at:\\nhttps://github.com/nasa/RtRetrievalFramework\\n\\nThis documentation rendered into HTML for is available at:\\nhttp://nasa.github.io/RtRetrievalFrameworkDoc/\\n'},\n",
       " {'repo': 'nasa/NASTRAN-95',\n",
       "  'language': 'FORTRAN',\n",
       "  'readme_contents': \"# NASTRAN-95\\n\\nNASTRAN has been released under the  \\n[NASA Open Source Agreement version 1.3](https://github.com/nasa/NASTRAN-95/raw/master/NASA%20Open%20Source%20Agreement-NASTRAN%2095.doc).\\n\\n\\nNASTRAN is the NASA Structural Analysis System, a finite element analysis program (FEA) completed in the early 1970's. It was the first of its kind and opened the door to computer-aided engineering. Subsections of a design can be modeled and then larger groupings of these elements can again be modeled. NASTRAN can handle elastic stability analysis, complex eigenvalues for vibration and dynamic stability analysis, dynamic response for transient and steady state loads, and random excitation, and static response to concentrated and distributed loads, thermal expansion, and enforced deformations.\\n\\nNOTE: There is no technical support available for this software.\\n\"}]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_github_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasa/cumulus</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Cumulus Framework\\n\\n[![npm version](https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nasa/openmct</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Open MCT [![license](https://img.shields.io/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nasa/astrobee_android</td>\n",
       "      <td>Java</td>\n",
       "      <td># Astrobee Robot Software - Android submodule\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nasa/astrobee</td>\n",
       "      <td>C++</td>\n",
       "      <td># Astrobee Robot Software\\n\\n### About\\n\\n&lt;p&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nasa/earthdata-search</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># [Earthdata Search](https://search.earthdata....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    repo    language  \\\n",
       "0           nasa/cumulus  JavaScript   \n",
       "1           nasa/openmct  JavaScript   \n",
       "2  nasa/astrobee_android        Java   \n",
       "3          nasa/astrobee         C++   \n",
       "4  nasa/earthdata-search  JavaScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # Cumulus Framework\\n\\n[![npm version](https:/...  \n",
       "1  # Open MCT [![license](https://img.shields.io/...  \n",
       "2  # Astrobee Robot Software - Android submodule\\...  \n",
       "3  # Astrobee Robot Software\\n\\n### About\\n\\n<p>\\...  \n",
       "4  # [Earthdata Search](https://search.earthdata....  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 3)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 291 entries, 0 to 290\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   repo             291 non-null    object\n",
      " 1   language         275 non-null    object\n",
      " 2   readme_contents  291 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nasa_repo_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nasa_repo_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nasa/cumulus</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Cumulus Framework\\n\\n[![npm version](https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>nasa/openmct</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Open MCT [![license](https://img.shields.io/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>nasa/astrobee_android</td>\n",
       "      <td>Java</td>\n",
       "      <td># Astrobee Robot Software - Android submodule\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>nasa/astrobee</td>\n",
       "      <td>C++</td>\n",
       "      <td># Astrobee Robot Software\\n\\n### About\\n\\n&lt;p&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>nasa/earthdata-search</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># [Earthdata Search](https://search.earthdata....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   repo    language  \\\n",
       "0           0           nasa/cumulus  JavaScript   \n",
       "1           1           nasa/openmct  JavaScript   \n",
       "2           2  nasa/astrobee_android        Java   \n",
       "3           3          nasa/astrobee         C++   \n",
       "4           4  nasa/earthdata-search  JavaScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # Cumulus Framework\\n\\n[![npm version](https:/...  \n",
       "1  # Open MCT [![license](https://img.shields.io/...  \n",
       "2  # Astrobee Robot Software - Android submodule\\...  \n",
       "3  # Astrobee Robot Software\\n\\n### About\\n\\n<p>\\...  \n",
       "4  # [Earthdata Search](https://search.earthdata....  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasa/cumulus</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Cumulus Framework\\n\\n[![npm version](https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nasa/openmct</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># Open MCT [![license](https://img.shields.io/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nasa/astrobee_android</td>\n",
       "      <td>Java</td>\n",
       "      <td># Astrobee Robot Software - Android submodule\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nasa/astrobee</td>\n",
       "      <td>C++</td>\n",
       "      <td># Astrobee Robot Software\\n\\n### About\\n\\n&lt;p&gt;\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nasa/earthdata-search</td>\n",
       "      <td>JavaScript</td>\n",
       "      <td># [Earthdata Search](https://search.earthdata....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    repo    language  \\\n",
       "0           nasa/cumulus  JavaScript   \n",
       "1           nasa/openmct  JavaScript   \n",
       "2  nasa/astrobee_android        Java   \n",
       "3          nasa/astrobee         C++   \n",
       "4  nasa/earthdata-search  JavaScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # Cumulus Framework\\n\\n[![npm version](https:/...  \n",
       "1  # Open MCT [![license](https://img.shields.io/...  \n",
       "2  # Astrobee Robot Software - Android submodule\\...  \n",
       "3  # Astrobee Robot Software\\n\\n### About\\n\\n<p>\\...  \n",
       "4  # [Earthdata Search](https://search.earthdata....  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
